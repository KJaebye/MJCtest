[32m[20221213 18:48:32 @logger.py:105][0m Log file set to ./tmp/pendulum/swingup/20221213_184832/log/pendulum_swingup-20221213_184832.log
[32m[20221213 18:48:32 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 18:48:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 18:48:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |           0.0019 |           0.0431 |          20.8414 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0005 |           0.0325 |          20.8096 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0077 |           0.0255 |          20.8089 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0040 |           0.0208 |          20.7918 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0081 |           0.0174 |          20.7865 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0035 |           0.0149 |          20.7869 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |           0.0005 |           0.0129 |          20.7820 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0035 |           0.0112 |          20.7770 |
[32m[20221213 18:48:33 @agent_ppo2.py:185][0m |          -0.0117 |           0.0098 |          20.7650 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |          -0.0029 |           0.0085 |          20.7766 |
[32m[20221213 18:48:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:48:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:34 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 0.00
[32m[20221213 18:48:34 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 0.00
[32m[20221213 18:48:34 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 18:48:34 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 18:48:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 18:48:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |          -0.0009 |           0.0074 |          20.9205 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |           0.0019 |           0.0064 |          20.9221 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |          -0.0010 |           0.0056 |          20.9110 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |          -0.0062 |           0.0048 |          20.9151 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |           0.0002 |           0.0042 |          20.9156 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |          -0.0030 |           0.0036 |          20.9163 |
[32m[20221213 18:48:34 @agent_ppo2.py:185][0m |          -0.0029 |           0.0031 |          20.9379 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |          -0.0021 |           0.0026 |          20.9280 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |           0.0028 |           0.0022 |          20.9321 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |          -0.0079 |           0.0019 |          20.9384 |
[32m[20221213 18:48:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:48:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:35 @agent_ppo2.py:143][0m Total time:       0.04 min
[32m[20221213 18:48:35 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 18:48:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 18:48:35 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:48:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |          -0.0013 |           0.0016 |          20.8617 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |          -0.0058 |           0.0013 |          20.8386 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |           0.0005 |           0.0011 |          20.8522 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |           0.0010 |           0.0009 |          20.8382 |
[32m[20221213 18:48:35 @agent_ppo2.py:185][0m |           0.0005 |           0.0007 |          20.8332 |
[32m[20221213 18:48:36 @agent_ppo2.py:185][0m |          -0.0018 |           0.0006 |          20.8324 |
[32m[20221213 18:48:36 @agent_ppo2.py:185][0m |          -0.0018 |           0.0005 |          20.8216 |
[32m[20221213 18:48:36 @agent_ppo2.py:185][0m |          -0.0012 |           0.0004 |          20.8140 |
[32m[20221213 18:48:36 @agent_ppo2.py:185][0m |           0.0024 |           0.0003 |          20.8084 |
[32m[20221213 18:48:36 @agent_ppo2.py:185][0m |           0.0003 |           0.0003 |          20.8034 |
[32m[20221213 18:48:36 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 18:48:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:36 @agent_ppo2.py:143][0m Total time:       0.06 min
[32m[20221213 18:48:36 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 18:48:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 18:48:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:36 @agent_ppo2.py:185][0m |          -0.0037 |           0.0002 |          20.8818 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |           0.0006 |           0.0002 |          20.8471 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |           0.0007 |           0.0001 |          20.8195 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |          -0.0067 |           0.0001 |          20.8151 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |           0.0001 |           0.0001 |          20.7967 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |          -0.0238 |           0.0001 |          20.7972 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.7570 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.7681 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.7479 |
[32m[20221213 18:48:37 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          20.7677 |
[32m[20221213 18:48:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:48:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:37 @agent_ppo2.py:143][0m Total time:       0.08 min
[32m[20221213 18:48:37 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 18:48:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 18:48:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          20.9012 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          20.8779 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.8526 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          20.8480 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          20.8323 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.8294 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.8176 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |          -0.0099 |           0.0000 |          20.8051 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          20.7961 |
[32m[20221213 18:48:38 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.7822 |
[32m[20221213 18:48:38 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:48:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:39 @agent_ppo2.py:143][0m Total time:       0.10 min
[32m[20221213 18:48:39 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 18:48:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 18:48:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          21.0467 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0167 |           0.0000 |          21.0096 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0000 |           0.0000 |          20.9701 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          20.9658 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.9571 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.9575 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.9497 |
[32m[20221213 18:48:39 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.9369 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.9204 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |           0.0000 |           0.0000 |          20.9175 |
[32m[20221213 18:48:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:48:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:40 @agent_ppo2.py:143][0m Total time:       0.12 min
[32m[20221213 18:48:40 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 18:48:40 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 18:48:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          21.1463 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.1349 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.0999 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.1158 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |          -0.0098 |           0.0000 |          21.1164 |
[32m[20221213 18:48:40 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0992 |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.0782 |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |          -0.0080 |           0.0000 |          21.0940 |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          21.0679 |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.0814 |
[32m[20221213 18:48:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:48:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:41 @agent_ppo2.py:143][0m Total time:       0.14 min
[32m[20221213 18:48:41 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 18:48:41 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 18:48:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:48:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          21.1729 |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1773 |
[32m[20221213 18:48:41 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1937 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.1805 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.2064 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.2074 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          21.2070 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.2082 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.2011 |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          21.2314 |
[32m[20221213 18:48:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:48:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:42 @agent_ppo2.py:143][0m Total time:       0.16 min
[32m[20221213 18:48:42 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 18:48:42 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 18:48:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:42 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          21.1478 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          21.1112 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.0869 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.0817 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          21.0779 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          21.0714 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.0547 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.0573 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.0436 |
[32m[20221213 18:48:43 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          21.0256 |
[32m[20221213 18:48:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:48:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:43 @agent_ppo2.py:143][0m Total time:       0.18 min
[32m[20221213 18:48:43 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 18:48:43 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 18:48:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.1761 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1617 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0122 |           0.0000 |          21.1412 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1476 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.1285 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1170 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          21.1081 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          21.1119 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.1013 |
[32m[20221213 18:48:44 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          21.1058 |
[32m[20221213 18:48:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:48:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:44 @agent_ppo2.py:143][0m Total time:       0.20 min
[32m[20221213 18:48:44 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 18:48:44 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 18:48:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:48:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          21.0399 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.9981 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0064 |           0.0000 |          20.9974 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          20.9915 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0125 |           0.0000 |          20.9732 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0081 |           0.0000 |          20.9752 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          20.9514 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |          -0.0131 |           0.0000 |          20.9526 |
[32m[20221213 18:48:45 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          20.9373 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          20.9356 |
[32m[20221213 18:48:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:48:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:46 @agent_ppo2.py:143][0m Total time:       0.22 min
[32m[20221213 18:48:46 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 18:48:46 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 18:48:46 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:48:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.2633 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          21.2766 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.2690 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.2795 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.2807 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.2753 |
[32m[20221213 18:48:46 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          21.3092 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.2953 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.2944 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.3007 |
[32m[20221213 18:48:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:48:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:47 @agent_ppo2.py:143][0m Total time:       0.24 min
[32m[20221213 18:48:47 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 18:48:47 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 18:48:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.0887 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.0811 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          21.0722 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          21.0607 |
[32m[20221213 18:48:47 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          21.0779 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0028 |           0.0000 |          21.0747 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.0803 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0668 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0849 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.0724 |
[32m[20221213 18:48:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:48:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:48 @agent_ppo2.py:143][0m Total time:       0.26 min
[32m[20221213 18:48:48 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 18:48:48 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 18:48:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0033 |           0.0000 |          21.0918 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |          -0.0100 |           0.0000 |          21.0971 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |          -0.0082 |           0.0000 |          21.1053 |
[32m[20221213 18:48:48 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.1118 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.1259 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.1379 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1445 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.1492 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.1566 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.1649 |
[32m[20221213 18:48:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:48:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:49 @agent_ppo2.py:143][0m Total time:       0.28 min
[32m[20221213 18:48:49 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 18:48:49 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 18:48:49 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:48:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |          -0.0014 |           0.0013 |          21.1104 |
[32m[20221213 18:48:49 @agent_ppo2.py:185][0m |          -0.0022 |           0.0013 |          21.1030 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0023 |           0.0013 |          21.0964 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0024 |           0.0013 |          21.0816 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0025 |           0.0013 |          21.0767 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0021 |           0.0013 |          21.0707 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0028 |           0.0013 |          21.0723 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0027 |           0.0013 |          21.0529 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0025 |           0.0013 |          21.0475 |
[32m[20221213 18:48:50 @agent_ppo2.py:185][0m |          -0.0025 |           0.0013 |          21.0336 |
[32m[20221213 18:48:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:48:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.80
[32m[20221213 18:48:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.00
[32m[20221213 18:48:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:50 @agent_ppo2.py:143][0m Total time:       0.30 min
[32m[20221213 18:48:50 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 18:48:50 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 18:48:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0089 |           0.0000 |          21.0928 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          21.0840 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.0481 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          21.0571 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.0362 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0102 |           0.0000 |          21.0131 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.9918 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          21.0018 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.9891 |
[32m[20221213 18:48:51 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.9849 |
[32m[20221213 18:48:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:48:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:51 @agent_ppo2.py:143][0m Total time:       0.32 min
[32m[20221213 18:48:51 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 18:48:51 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 18:48:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          21.1379 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0026 |           0.0000 |          21.1233 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          21.1244 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.1123 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.0973 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |          -0.0107 |           0.0000 |          21.0928 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          21.1120 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1058 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          21.1027 |
[32m[20221213 18:48:52 @agent_ppo2.py:185][0m |           0.0030 |           0.0000 |          21.0890 |
[32m[20221213 18:48:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:48:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:52 @agent_ppo2.py:143][0m Total time:       0.33 min
[32m[20221213 18:48:52 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 18:48:52 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 18:48:53 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:48:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          21.1068 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1063 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.0830 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.1130 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          21.0993 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          21.0913 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.0885 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.0998 |
[32m[20221213 18:48:53 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          21.1059 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1020 |
[32m[20221213 18:48:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:48:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:54 @agent_ppo2.py:143][0m Total time:       0.35 min
[32m[20221213 18:48:54 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 18:48:54 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 18:48:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |          -0.0093 |           0.0000 |          21.0064 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          20.9849 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          20.9859 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          20.9562 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.9550 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          20.9487 |
[32m[20221213 18:48:54 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          20.9732 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          20.9624 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          20.9365 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          20.9495 |
[32m[20221213 18:48:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:48:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:55 @agent_ppo2.py:143][0m Total time:       0.37 min
[32m[20221213 18:48:55 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 18:48:55 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 18:48:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          21.1453 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          21.1336 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          21.1133 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          21.0994 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.0679 |
[32m[20221213 18:48:55 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0528 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0365 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.0235 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |           0.0028 |           0.0000 |          21.0081 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.0017 |
[32m[20221213 18:48:56 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:48:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:56 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 18:48:56 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 18:48:56 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 18:48:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:48:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          21.1130 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |          -0.0099 |           0.0000 |          21.1016 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |          -0.0111 |           0.0000 |          21.1242 |
[32m[20221213 18:48:56 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          21.1217 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1541 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.1760 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.1806 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.2041 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |          -0.0125 |           0.0000 |          21.1817 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          21.2129 |
[32m[20221213 18:48:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:48:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:57 @agent_ppo2.py:143][0m Total time:       0.41 min
[32m[20221213 18:48:57 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 18:48:57 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 18:48:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.0585 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          21.0358 |
[32m[20221213 18:48:57 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          21.0264 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.9955 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.9769 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.9760 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.9505 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.9329 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          20.9305 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          20.9136 |
[32m[20221213 18:48:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:48:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:58 @agent_ppo2.py:143][0m Total time:       0.43 min
[32m[20221213 18:48:58 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 18:48:58 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 18:48:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          21.0881 |
[32m[20221213 18:48:58 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.1096 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.1104 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          21.1345 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0006 |           0.0000 |          21.1348 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          21.1344 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          21.1398 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1542 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.1544 |
[32m[20221213 18:48:59 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.1550 |
[32m[20221213 18:48:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:48:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:48:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:48:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:48:59 @agent_ppo2.py:143][0m Total time:       0.45 min
[32m[20221213 18:48:59 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 18:48:59 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 18:48:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:48:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0028 |           0.0000 |          21.1854 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.1622 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |          -0.0092 |           0.0000 |          21.1360 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1045 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.1061 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1132 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.0781 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.0725 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.0745 |
[32m[20221213 18:49:00 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          21.0530 |
[32m[20221213 18:49:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:49:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:00 @agent_ppo2.py:143][0m Total time:       0.47 min
[32m[20221213 18:49:00 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 18:49:00 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 18:49:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          21.1367 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.1403 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.1363 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0006 |           0.0000 |          21.1441 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          21.1446 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.1350 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.1409 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.1372 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1452 |
[32m[20221213 18:49:01 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          21.1433 |
[32m[20221213 18:49:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:49:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:02 @agent_ppo2.py:143][0m Total time:       0.48 min
[32m[20221213 18:49:02 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 18:49:02 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 18:49:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          21.1486 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.1574 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.1684 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.1679 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.1836 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1846 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |          -0.0101 |           0.0000 |          21.1942 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.2045 |
[32m[20221213 18:49:02 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.2314 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |          -0.0078 |           0.0000 |          21.2180 |
[32m[20221213 18:49:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:49:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:03 @agent_ppo2.py:143][0m Total time:       0.50 min
[32m[20221213 18:49:03 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 18:49:03 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 18:49:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1509 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1513 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1514 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          21.1566 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.1729 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |          -0.0205 |           0.0000 |          21.1869 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.1771 |
[32m[20221213 18:49:03 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.1935 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          21.2055 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          21.1994 |
[32m[20221213 18:49:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:49:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:04 @agent_ppo2.py:143][0m Total time:       0.52 min
[32m[20221213 18:49:04 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 18:49:04 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 18:49:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |           0.0026 |           0.0000 |          21.0742 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0569 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.0336 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          21.0266 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0231 |
[32m[20221213 18:49:04 @agent_ppo2.py:185][0m |          -0.0103 |           0.0000 |          21.0148 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.0140 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.0045 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.9942 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.9878 |
[32m[20221213 18:49:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:49:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:05 @agent_ppo2.py:143][0m Total time:       0.54 min
[32m[20221213 18:49:05 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 18:49:05 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 18:49:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          21.1597 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          21.1471 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          21.1292 |
[32m[20221213 18:49:05 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1262 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.1204 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0028 |           0.0000 |          21.1167 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |          -0.0067 |           0.0000 |          21.1131 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1002 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1097 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0940 |
[32m[20221213 18:49:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:49:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:06 @agent_ppo2.py:143][0m Total time:       0.56 min
[32m[20221213 18:49:06 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 18:49:06 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 18:49:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1840 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |          -0.0129 |           0.0000 |          21.1637 |
[32m[20221213 18:49:06 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.1314 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1563 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          21.1568 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.1695 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          21.1646 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.1615 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.1653 |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          21.1588 |
[32m[20221213 18:49:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:49:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:07 @agent_ppo2.py:143][0m Total time:       0.58 min
[32m[20221213 18:49:07 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 18:49:07 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 18:49:07 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:07 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.1033 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.0988 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1106 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          21.1200 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1021 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          21.0954 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.1061 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.0975 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1047 |
[32m[20221213 18:49:08 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.0940 |
[32m[20221213 18:49:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:49:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:08 @agent_ppo2.py:143][0m Total time:       0.60 min
[32m[20221213 18:49:08 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 18:49:08 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 18:49:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.0884 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |          -0.0110 |           0.0000 |          21.1153 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.0861 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          21.0945 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |          -0.0090 |           0.0000 |          21.0858 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1010 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.1059 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          21.1100 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1052 |
[32m[20221213 18:49:09 @agent_ppo2.py:185][0m |          -0.0099 |           0.0000 |          21.1140 |
[32m[20221213 18:49:09 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:09 @agent_ppo2.py:143][0m Total time:       0.62 min
[32m[20221213 18:49:09 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 18:49:09 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 18:49:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0030 |           0.0000 |          21.1293 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1180 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1115 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          21.1084 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.1009 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.1114 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          21.1114 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.0957 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          21.1105 |
[32m[20221213 18:49:10 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          21.0986 |
[32m[20221213 18:49:10 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:10 @agent_ppo2.py:143][0m Total time:       0.63 min
[32m[20221213 18:49:10 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 18:49:10 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 18:49:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |           0.0033 |           0.0000 |          21.1065 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1113 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          21.1021 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          21.0978 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.0834 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          21.0907 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          21.0849 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |          -0.0085 |           0.0000 |          21.1058 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          21.0861 |
[32m[20221213 18:49:11 @agent_ppo2.py:185][0m |          -0.0092 |           0.0000 |          21.0986 |
[32m[20221213 18:49:11 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:12 @agent_ppo2.py:143][0m Total time:       0.65 min
[32m[20221213 18:49:12 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 18:49:12 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 18:49:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          21.1855 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          21.1859 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          21.1756 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1642 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          21.1672 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          21.1671 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.1684 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1606 |
[32m[20221213 18:49:12 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.1603 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1730 |
[32m[20221213 18:49:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:13 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 18:49:13 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 18:49:13 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 18:49:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0028 |           0.0000 |          21.0434 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          21.0147 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.9924 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          20.9678 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          20.9542 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.9623 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.9405 |
[32m[20221213 18:49:13 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.9232 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          20.9316 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.9300 |
[32m[20221213 18:49:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:14 @agent_ppo2.py:143][0m Total time:       0.69 min
[32m[20221213 18:49:14 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 18:49:14 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 18:49:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |          -0.0083 |           0.0000 |          20.9851 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          20.9897 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          20.9787 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          20.9720 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0026 |           0.0000 |          20.9737 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.9747 |
[32m[20221213 18:49:14 @agent_ppo2.py:185][0m |           0.0026 |           0.0000 |          20.9601 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          20.9609 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          20.9440 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.9562 |
[32m[20221213 18:49:15 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:15 @agent_ppo2.py:143][0m Total time:       0.71 min
[32m[20221213 18:49:15 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 18:49:15 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 18:49:15 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          21.1507 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          21.1591 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          21.1738 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |           0.0000 |           0.0000 |          21.1907 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.1914 |
[32m[20221213 18:49:15 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          21.2130 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          21.2164 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |           0.0060 |           0.0000 |          21.2165 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          21.2304 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          21.2391 |
[32m[20221213 18:49:16 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:16 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221213 18:49:16 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 18:49:16 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 18:49:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 18:49:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.2120 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          21.1980 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.1757 |
[32m[20221213 18:49:16 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.1744 |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          21.1691 |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          21.1539 |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          21.1587 |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          21.1417 |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.1458 |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          21.1542 |
[32m[20221213 18:49:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:49:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:17 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 18:49:17 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 18:49:17 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 18:49:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:17 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          21.2246 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          21.2028 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.2073 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.1959 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.1794 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          21.1548 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.1560 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.1342 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          21.1312 |
[32m[20221213 18:49:18 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.1134 |
[32m[20221213 18:49:18 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:18 @agent_ppo2.py:143][0m Total time:       0.76 min
[32m[20221213 18:49:18 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 18:49:18 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 18:49:18 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          21.0597 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          21.0203 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          20.9875 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          20.9499 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.9180 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |          -0.0096 |           0.0000 |          20.8946 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.8675 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          20.8421 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |          -0.0076 |           0.0000 |          20.8262 |
[32m[20221213 18:49:19 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          20.8056 |
[32m[20221213 18:49:19 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:19 @agent_ppo2.py:143][0m Total time:       0.78 min
[32m[20221213 18:49:19 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 18:49:19 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 18:49:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          20.8933 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          20.8570 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.8269 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          20.8083 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          20.7915 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0017 |           0.0000 |          20.7516 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          20.7478 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          20.7293 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          20.7233 |
[32m[20221213 18:49:20 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.6943 |
[32m[20221213 18:49:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:20 @agent_ppo2.py:143][0m Total time:       0.80 min
[32m[20221213 18:49:20 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 18:49:20 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 18:49:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0906 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.0693 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.0618 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |          -0.0093 |           0.0000 |          21.0536 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.0535 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          21.0471 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          21.0405 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.0547 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          21.0376 |
[32m[20221213 18:49:21 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0461 |
[32m[20221213 18:49:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:49:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:22 @agent_ppo2.py:143][0m Total time:       0.82 min
[32m[20221213 18:49:22 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 18:49:22 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 18:49:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0006 |           1.1917 |          20.9475 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0022 |           1.1560 |          20.9033 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0020 |           1.1537 |          20.8628 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0157 |           1.2726 |          20.8139 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0004 |           1.1520 |          20.7636 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0023 |           1.1502 |          20.7445 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0030 |           1.1489 |          20.7077 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0021 |           1.1508 |          20.6803 |
[32m[20221213 18:49:22 @agent_ppo2.py:185][0m |          -0.0026 |           1.1497 |          20.6490 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |          -0.0030 |           1.1500 |          20.6338 |
[32m[20221213 18:49:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.40
[32m[20221213 18:49:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 37.00
[32m[20221213 18:49:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:23 @agent_ppo2.py:143][0m Total time:       0.84 min
[32m[20221213 18:49:23 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 18:49:23 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 18:49:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |           0.0023 |           0.0122 |          20.8393 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |           0.0007 |           0.0070 |          20.8746 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |           0.0018 |           0.0063 |          20.9117 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |          -0.0074 |           0.0063 |          20.9625 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |           0.0002 |           0.0062 |          20.9697 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |           0.0000 |           0.0061 |          20.9749 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |          -0.0024 |           0.0060 |          21.0061 |
[32m[20221213 18:49:23 @agent_ppo2.py:185][0m |           0.0012 |           0.0061 |          21.0245 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |           0.0002 |           0.0060 |          21.0432 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |           0.0012 |           0.0060 |          21.0282 |
[32m[20221213 18:49:24 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:49:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:24 @agent_ppo2.py:143][0m Total time:       0.86 min
[32m[20221213 18:49:24 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 18:49:24 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 18:49:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |          -0.0007 |           0.0032 |          20.9856 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |          -0.0027 |           0.0021 |          20.9625 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |           0.0011 |           0.0018 |          20.9353 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |          -0.0038 |           0.0017 |          20.9330 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |          -0.0040 |           0.0017 |          20.9284 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |          -0.0026 |           0.0017 |          20.9128 |
[32m[20221213 18:49:24 @agent_ppo2.py:185][0m |          -0.0082 |           0.0017 |          20.8988 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |          -0.0036 |           0.0017 |          20.8903 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |          -0.0106 |           0.0017 |          20.8896 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |          -0.0050 |           0.0017 |          20.8941 |
[32m[20221213 18:49:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:49:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:25 @agent_ppo2.py:143][0m Total time:       0.87 min
[32m[20221213 18:49:25 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 18:49:25 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 18:49:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |           0.0018 |           0.0005 |          20.9312 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |          -0.0051 |           0.0004 |          20.8868 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |          -0.0029 |           0.0004 |          20.8491 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |           0.0002 |           0.0004 |          20.8227 |
[32m[20221213 18:49:25 @agent_ppo2.py:185][0m |           0.0009 |           0.0003 |          20.8090 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |          -0.0095 |           0.0003 |          20.7910 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |          -0.0043 |           0.0003 |          20.7651 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |          -0.0076 |           0.0003 |          20.7652 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |           0.0026 |           0.0003 |          20.7664 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |          -0.0103 |           0.0003 |          20.7693 |
[32m[20221213 18:49:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:49:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:26 @agent_ppo2.py:143][0m Total time:       0.89 min
[32m[20221213 18:49:26 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 18:49:26 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 18:49:26 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |          -0.0012 |           0.0003 |          20.7507 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |           0.0050 |           0.0003 |          20.7888 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |           0.0039 |           0.0003 |          20.8229 |
[32m[20221213 18:49:26 @agent_ppo2.py:185][0m |          -0.0022 |           0.0003 |          20.8462 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |          -0.0053 |           0.0002 |          20.8400 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |           0.0014 |           0.0002 |          20.8466 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |           0.0056 |           0.0002 |          20.8483 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |          -0.0064 |           0.0002 |          20.8820 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |          -0.0036 |           0.0002 |          20.8797 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |           0.0002 |           0.0002 |          20.8931 |
[32m[20221213 18:49:27 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:49:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:27 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221213 18:49:27 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 18:49:27 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 18:49:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |           0.0018 |           0.0002 |          20.8340 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |          -0.0034 |           0.0002 |          20.8360 |
[32m[20221213 18:49:27 @agent_ppo2.py:185][0m |          -0.0070 |           0.0002 |          20.8361 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |          -0.0039 |           0.0002 |          20.8242 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |           0.0006 |           0.0002 |          20.8194 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |           0.0048 |           0.0002 |          20.8204 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |          -0.0037 |           0.0002 |          20.8210 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |          -0.0063 |           0.0002 |          20.8099 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |          -0.0030 |           0.0002 |          20.8061 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |           0.0007 |           0.0002 |          20.8113 |
[32m[20221213 18:49:28 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:28 @agent_ppo2.py:143][0m Total time:       0.93 min
[32m[20221213 18:49:28 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 18:49:28 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 18:49:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |          -0.0063 |           0.0002 |          20.7740 |
[32m[20221213 18:49:28 @agent_ppo2.py:185][0m |          -0.0053 |           0.0002 |          20.7852 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0064 |           0.0002 |          20.8063 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0057 |           0.0002 |          20.8271 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0046 |           0.0001 |          20.8549 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0001 |           0.0001 |          20.8694 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0086 |           0.0001 |          20.8669 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0061 |           0.0001 |          20.8830 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0149 |           0.0001 |          20.8954 |
[32m[20221213 18:49:29 @agent_ppo2.py:185][0m |          -0.0065 |           0.0001 |          20.9119 |
[32m[20221213 18:49:29 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:49:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:29 @agent_ppo2.py:143][0m Total time:       0.95 min
[32m[20221213 18:49:29 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 18:49:29 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 18:49:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:49:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |           0.0013 |           0.0001 |          20.8167 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0069 |           0.0001 |          20.7967 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |           0.0019 |           0.0001 |          20.7926 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0063 |           0.0001 |          20.7998 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0036 |           0.0001 |          20.8026 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0026 |           0.0001 |          20.8106 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0067 |           0.0001 |          20.8128 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0026 |           0.0001 |          20.8180 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |          -0.0066 |           0.0001 |          20.8302 |
[32m[20221213 18:49:30 @agent_ppo2.py:185][0m |           0.0030 |           0.0001 |          20.8254 |
[32m[20221213 18:49:30 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:49:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:30 @agent_ppo2.py:143][0m Total time:       0.97 min
[32m[20221213 18:49:30 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 18:49:30 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 18:49:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:49:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |           0.0008 |           0.0001 |          20.9534 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |          -0.0013 |           0.0001 |          20.9488 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |           0.0003 |           0.0001 |          20.9597 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |           0.0029 |           0.0001 |          20.9711 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |           0.0023 |           0.0001 |          20.9760 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |          -0.0029 |           0.0001 |          20.9635 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |          -0.0069 |           0.0001 |          20.9570 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |           0.0035 |           0.0001 |          20.9698 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |           0.0012 |           0.0001 |          20.9865 |
[32m[20221213 18:49:31 @agent_ppo2.py:185][0m |          -0.0044 |           0.0001 |          20.9749 |
[32m[20221213 18:49:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:49:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:32 @agent_ppo2.py:143][0m Total time:       0.99 min
[32m[20221213 18:49:32 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 18:49:32 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 18:49:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |          -0.0032 |           0.0001 |          21.0075 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |          -0.0023 |           0.0001 |          20.9984 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |          -0.0035 |           0.0001 |          20.9950 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |          -0.0023 |           0.0001 |          20.9920 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |          -0.0024 |           0.0001 |          20.9956 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |           0.0081 |           0.0001 |          21.0018 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |          -0.0027 |           0.0001 |          21.0040 |
[32m[20221213 18:49:32 @agent_ppo2.py:185][0m |           0.0173 |           0.0001 |          21.0061 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |           0.0039 |           0.0001 |          20.9798 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          21.0102 |
[32m[20221213 18:49:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:49:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:33 @agent_ppo2.py:143][0m Total time:       1.00 min
[32m[20221213 18:49:33 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 18:49:33 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 18:49:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |           0.0043 |           0.0000 |          20.9507 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.9207 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |           0.0035 |           0.0000 |          20.8837 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.8927 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.8700 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8635 |
[32m[20221213 18:49:33 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.8345 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8294 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.8192 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.8189 |
[32m[20221213 18:49:34 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:34 @agent_ppo2.py:143][0m Total time:       1.02 min
[32m[20221213 18:49:34 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 18:49:34 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 18:49:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          20.8557 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.8304 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8098 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.7976 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.7890 |
[32m[20221213 18:49:34 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7845 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.7823 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.7667 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |           0.0038 |           0.0000 |          20.7652 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.7478 |
[32m[20221213 18:49:35 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:35 @agent_ppo2.py:143][0m Total time:       1.04 min
[32m[20221213 18:49:35 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 18:49:35 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 18:49:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |           0.0131 |           0.0000 |          20.8167 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.8304 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.8429 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.8397 |
[32m[20221213 18:49:35 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8704 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.8842 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.8926 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8939 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.9051 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.9259 |
[32m[20221213 18:49:36 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:36 @agent_ppo2.py:143][0m Total time:       1.06 min
[32m[20221213 18:49:36 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 18:49:36 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 18:49:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.8685 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.8499 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8452 |
[32m[20221213 18:49:36 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.8683 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.8542 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8299 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.8182 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.8286 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.8256 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.8060 |
[32m[20221213 18:49:37 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:37 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221213 18:49:37 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 18:49:37 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 18:49:37 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.8312 |
[32m[20221213 18:49:37 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.8258 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.8247 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.8175 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.8206 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          20.8021 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8128 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8007 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.8019 |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.7998 |
[32m[20221213 18:49:38 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:38 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 18:49:38 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 18:49:38 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 18:49:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:38 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.8776 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.8778 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8707 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.8561 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |           0.0006 |           0.0000 |          20.8504 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.8315 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.8338 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8153 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8221 |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.8045 |
[32m[20221213 18:49:39 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:39 @agent_ppo2.py:143][0m Total time:       1.11 min
[32m[20221213 18:49:39 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 18:49:39 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 18:49:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:39 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8108 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.7994 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.8211 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8291 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8420 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.8383 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.8598 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8639 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.8572 |
[32m[20221213 18:49:40 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.8843 |
[32m[20221213 18:49:40 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:49:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:40 @agent_ppo2.py:143][0m Total time:       1.13 min
[32m[20221213 18:49:40 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 18:49:40 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 18:49:40 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.7615 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.7324 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.7441 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.7580 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.7782 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7835 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.7879 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.8005 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.7901 |
[32m[20221213 18:49:41 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.8128 |
[32m[20221213 18:49:41 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 17.00
[32m[20221213 18:49:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 17.00
[32m[20221213 18:49:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 17.00
[32m[20221213 18:49:41 @agent_ppo2.py:143][0m Total time:       1.15 min
[32m[20221213 18:49:41 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 18:49:41 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 18:49:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.8271 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8007 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7954 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.7825 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |           0.0198 |           0.0000 |          20.7599 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |           0.0071 |           0.0000 |          20.7503 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.7522 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.7474 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |           0.0044 |           0.0000 |          20.7518 |
[32m[20221213 18:49:42 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.7576 |
[32m[20221213 18:49:42 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:42 @agent_ppo2.py:143][0m Total time:       1.17 min
[32m[20221213 18:49:42 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 18:49:42 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 18:49:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.7851 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |           0.0103 |           0.0000 |          20.7695 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.7617 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.7472 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |           0.0085 |           0.0000 |          20.7185 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.6948 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.6874 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.6812 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.6782 |
[32m[20221213 18:49:43 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.6657 |
[32m[20221213 18:49:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:44 @agent_ppo2.py:143][0m Total time:       1.19 min
[32m[20221213 18:49:44 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 18:49:44 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 18:49:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.7414 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.7070 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.6853 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.6783 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.6602 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.6586 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.6310 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.6497 |
[32m[20221213 18:49:44 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.6307 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.6367 |
[32m[20221213 18:49:45 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:45 @agent_ppo2.py:143][0m Total time:       1.20 min
[32m[20221213 18:49:45 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 18:49:45 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 18:49:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.7453 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.7549 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |           0.0041 |           0.0000 |          20.7668 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8016 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8180 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8346 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.8418 |
[32m[20221213 18:49:45 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.8328 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8562 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.8646 |
[32m[20221213 18:49:46 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:46 @agent_ppo2.py:143][0m Total time:       1.22 min
[32m[20221213 18:49:46 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 18:49:46 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 18:49:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.8287 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |           0.0043 |           0.0000 |          20.8438 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.8512 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.8780 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          20.8979 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          20.8988 |
[32m[20221213 18:49:46 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.9083 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |           0.0176 |           0.0000 |          20.9343 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |           0.0036 |           0.0000 |          20.9018 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.9346 |
[32m[20221213 18:49:47 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:47 @agent_ppo2.py:143][0m Total time:       1.24 min
[32m[20221213 18:49:47 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 18:49:47 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 18:49:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.9307 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.9082 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |           0.0097 |           0.0000 |          20.9252 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.9175 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.8983 |
[32m[20221213 18:49:47 @agent_ppo2.py:185][0m |           0.0105 |           0.0000 |          20.9062 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.9111 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.8669 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8634 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.8629 |
[32m[20221213 18:49:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:48 @agent_ppo2.py:143][0m Total time:       1.26 min
[32m[20221213 18:49:48 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 18:49:48 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 18:49:48 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:49:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.8003 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.8091 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.8234 |
[32m[20221213 18:49:48 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.8479 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.8741 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          20.8941 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |           0.0104 |           0.0000 |          20.9188 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.9053 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.9139 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.9322 |
[32m[20221213 18:49:49 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:49:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:49 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 18:49:49 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 18:49:49 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 18:49:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.8396 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8094 |
[32m[20221213 18:49:49 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.7860 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.7812 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.7717 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.7796 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.7614 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.7575 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |           0.0046 |           0.0000 |          20.7496 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.7174 |
[32m[20221213 18:49:50 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:50 @agent_ppo2.py:143][0m Total time:       1.29 min
[32m[20221213 18:49:50 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 18:49:50 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 18:49:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |           0.0079 |           0.0000 |          20.8586 |
[32m[20221213 18:49:50 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.8049 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |           0.0065 |           0.0000 |          20.7870 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7636 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |           0.0156 |           0.0000 |          20.7821 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |           0.0081 |           0.0000 |          20.7612 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          20.7447 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          20.7247 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7297 |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7380 |
[32m[20221213 18:49:51 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:49:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:51 @agent_ppo2.py:143][0m Total time:       1.31 min
[32m[20221213 18:49:51 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 18:49:51 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 18:49:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:49:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:51 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7730 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.7857 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.8043 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.8295 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.8423 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.8680 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.8714 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.8785 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |           0.0006 |           0.0000 |          20.8806 |
[32m[20221213 18:49:52 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.8893 |
[32m[20221213 18:49:52 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:49:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:52 @agent_ppo2.py:143][0m Total time:       1.33 min
[32m[20221213 18:49:52 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 18:49:52 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 18:49:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.7985 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7983 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          20.8031 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.7997 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.8106 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.8183 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |           0.0030 |           0.0000 |          20.8193 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8501 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8616 |
[32m[20221213 18:49:53 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.8775 |
[32m[20221213 18:49:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:49:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:53 @agent_ppo2.py:143][0m Total time:       1.35 min
[32m[20221213 18:49:53 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 18:49:53 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 18:49:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |           0.0035 |           0.0000 |          20.8409 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8657 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |           0.0109 |           0.0000 |          20.8807 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.8945 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.9067 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.9288 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.9397 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.9466 |
[32m[20221213 18:49:54 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.9420 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.9579 |
[32m[20221213 18:49:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:49:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:55 @agent_ppo2.py:143][0m Total time:       1.37 min
[32m[20221213 18:49:55 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 18:49:55 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 18:49:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.9084 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.8922 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |           0.0082 |           0.0000 |          20.8790 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.8889 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.8871 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8673 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          20.8583 |
[32m[20221213 18:49:55 @agent_ppo2.py:185][0m |           0.0044 |           0.0000 |          20.8662 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8639 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.8533 |
[32m[20221213 18:49:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:49:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:56 @agent_ppo2.py:143][0m Total time:       1.39 min
[32m[20221213 18:49:56 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 18:49:56 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 18:49:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.9020 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.9050 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.9249 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |           0.0107 |           0.0000 |          20.9551 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.9570 |
[32m[20221213 18:49:56 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.9442 |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.9454 |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.9513 |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.9665 |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.9678 |
[32m[20221213 18:49:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:49:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:57 @agent_ppo2.py:143][0m Total time:       1.41 min
[32m[20221213 18:49:57 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 18:49:57 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 18:49:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |           0.0072 |           0.0000 |          20.9422 |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.9302 |
[32m[20221213 18:49:57 @agent_ppo2.py:185][0m |           0.0067 |           0.0000 |          20.8991 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.8926 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |           0.0107 |           0.0000 |          20.8909 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.9010 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8884 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.8829 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8826 |
[32m[20221213 18:49:58 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8696 |
[32m[20221213 18:49:58 @agent_ppo2.py:130][0m Policy update time: 1.37 s
[32m[20221213 18:49:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:49:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:49:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:49:59 @agent_ppo2.py:143][0m Total time:       1.43 min
[32m[20221213 18:49:59 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 18:49:59 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 18:49:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:49:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8704 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.9040 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.9351 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.9592 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          20.9752 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |           0.0139 |           0.0000 |          20.9977 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          21.0231 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          21.0446 |
[32m[20221213 18:49:59 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          21.0450 |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.0708 |
[32m[20221213 18:50:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:50:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:00 @agent_ppo2.py:143][0m Total time:       1.45 min
[32m[20221213 18:50:00 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 18:50:00 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 18:50:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:50:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.8786 |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8743 |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.8922 |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |           0.0089 |           0.0000 |          20.8968 |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.9024 |
[32m[20221213 18:50:00 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.9067 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          20.8999 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |           0.0048 |           0.0000 |          20.9024 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.9048 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          20.9040 |
[32m[20221213 18:50:01 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:50:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:01 @agent_ppo2.py:143][0m Total time:       1.47 min
[32m[20221213 18:50:01 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 18:50:01 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 18:50:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.9918 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.9680 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.9513 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.9398 |
[32m[20221213 18:50:01 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.9410 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.9257 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.9202 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          20.9138 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.8877 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.8979 |
[32m[20221213 18:50:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:50:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:02 @agent_ppo2.py:143][0m Total time:       1.49 min
[32m[20221213 18:50:02 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 18:50:02 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 18:50:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.9339 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.9281 |
[32m[20221213 18:50:02 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.9412 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          20.9503 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0064 |           0.0000 |          20.9783 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          20.9917 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.9977 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.9997 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          21.0036 |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          21.0339 |
[32m[20221213 18:50:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:50:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:03 @agent_ppo2.py:143][0m Total time:       1.51 min
[32m[20221213 18:50:03 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 18:50:03 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 18:50:03 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:03 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.9848 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.9477 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.9200 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.8932 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.8905 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.8563 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.8630 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8590 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8447 |
[32m[20221213 18:50:04 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.8290 |
[32m[20221213 18:50:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:04 @agent_ppo2.py:143][0m Total time:       1.53 min
[32m[20221213 18:50:04 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 18:50:04 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 18:50:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.9308 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.9075 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          20.9075 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.9094 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.9069 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |           0.0060 |           0.0000 |          20.8901 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |           0.0040 |           0.0000 |          20.8932 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          20.9012 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.8877 |
[32m[20221213 18:50:05 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8780 |
[32m[20221213 18:50:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:50:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:05 @agent_ppo2.py:143][0m Total time:       1.55 min
[32m[20221213 18:50:05 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 18:50:05 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 18:50:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.9793 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.9821 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.0070 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          21.0159 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          21.0207 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.0323 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          21.0606 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          21.0620 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          21.0688 |
[32m[20221213 18:50:06 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          21.0747 |
[32m[20221213 18:50:06 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:07 @agent_ppo2.py:143][0m Total time:       1.57 min
[32m[20221213 18:50:07 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 18:50:07 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 18:50:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |           0.0093 |           0.0000 |          21.0326 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          21.0483 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          21.0590 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0807 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0835 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          21.0975 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          21.1200 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1236 |
[32m[20221213 18:50:07 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.1378 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          21.1365 |
[32m[20221213 18:50:08 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:08 @agent_ppo2.py:143][0m Total time:       1.59 min
[32m[20221213 18:50:08 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 18:50:08 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 18:50:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.9654 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.9840 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          20.9708 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.9819 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.9894 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          21.0004 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          21.0039 |
[32m[20221213 18:50:08 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          21.0026 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          21.0140 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          21.0155 |
[32m[20221213 18:50:09 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:09 @agent_ppo2.py:143][0m Total time:       1.61 min
[32m[20221213 18:50:09 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 18:50:09 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 18:50:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |           0.0004 |           0.1608 |          21.0973 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0005 |           0.1577 |          21.0521 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0006 |           0.1561 |          21.0153 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0009 |           0.1571 |          20.9747 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0009 |           0.1569 |          20.9566 |
[32m[20221213 18:50:09 @agent_ppo2.py:185][0m |          -0.0012 |           0.1567 |          20.9166 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0007 |           0.1568 |          20.8907 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0014 |           0.1564 |          20.8570 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0016 |           0.1568 |          20.8198 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0017 |           0.1562 |          20.7958 |
[32m[20221213 18:50:10 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.40
[32m[20221213 18:50:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.00
[32m[20221213 18:50:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:10 @agent_ppo2.py:143][0m Total time:       1.62 min
[32m[20221213 18:50:10 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 18:50:10 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 18:50:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0083 |           0.0010 |          20.8639 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0013 |           0.0007 |          20.8747 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0001 |           0.0005 |          20.8944 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0046 |           0.0005 |          20.9072 |
[32m[20221213 18:50:10 @agent_ppo2.py:185][0m |          -0.0012 |           0.0004 |          20.9154 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |          -0.0016 |           0.0004 |          20.9062 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |           0.0039 |           0.0004 |          20.8953 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |          -0.0030 |           0.0004 |          20.9038 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |          -0.0041 |           0.0004 |          20.9096 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |           0.0030 |           0.0004 |          20.9319 |
[32m[20221213 18:50:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:11 @agent_ppo2.py:143][0m Total time:       1.64 min
[32m[20221213 18:50:11 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 18:50:11 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 18:50:11 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |          -0.0066 |           0.0002 |          20.8969 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |          -0.0088 |           0.0002 |          20.8404 |
[32m[20221213 18:50:11 @agent_ppo2.py:185][0m |          -0.0016 |           0.0002 |          20.7703 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |          -0.0055 |           0.0002 |          20.7419 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |           0.0029 |           0.0002 |          20.7170 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |           0.0017 |           0.0002 |          20.7094 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |          -0.0010 |           0.0002 |          20.6907 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |          -0.0070 |           0.0001 |          20.6654 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |           0.0003 |           0.0001 |          20.6516 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |          -0.0095 |           0.0001 |          20.6531 |
[32m[20221213 18:50:12 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:50:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:12 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221213 18:50:12 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 18:50:12 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 18:50:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |           0.0016 |           0.0001 |          20.8426 |
[32m[20221213 18:50:12 @agent_ppo2.py:185][0m |          -0.0023 |           0.0001 |          20.8260 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0113 |           0.0001 |          20.8470 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0087 |           0.0001 |          20.8347 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          20.8323 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0049 |           0.0001 |          20.8382 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0015 |           0.0001 |          20.8258 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0005 |           0.0001 |          20.8319 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0062 |           0.0001 |          20.8234 |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |           0.0022 |           0.0001 |          20.8246 |
[32m[20221213 18:50:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:13 @agent_ppo2.py:143][0m Total time:       1.68 min
[32m[20221213 18:50:13 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 18:50:13 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 18:50:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:13 @agent_ppo2.py:185][0m |          -0.0010 |           0.0001 |          20.8971 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0013 |           0.0001 |          20.9008 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |           0.0064 |           0.0001 |          20.8955 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0056 |           0.0001 |          20.9134 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0008 |           0.0001 |          20.9019 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0069 |           0.0001 |          20.8831 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0010 |           0.0001 |          20.9009 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0016 |           0.0001 |          20.9120 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0014 |           0.0001 |          20.9006 |
[32m[20221213 18:50:14 @agent_ppo2.py:185][0m |          -0.0004 |           0.0001 |          20.8841 |
[32m[20221213 18:50:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:14 @agent_ppo2.py:143][0m Total time:       1.70 min
[32m[20221213 18:50:14 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 18:50:14 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 18:50:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 18:50:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0022 |           0.0001 |          20.9519 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0023 |           0.0001 |          20.9565 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0019 |           0.0001 |          20.9791 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.9669 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |           0.0000 |           0.0000 |          20.9796 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.9892 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |           0.0100 |           0.0000 |          20.9979 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          21.0077 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          21.0036 |
[32m[20221213 18:50:15 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          21.0127 |
[32m[20221213 18:50:15 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:15 @agent_ppo2.py:143][0m Total time:       1.72 min
[32m[20221213 18:50:15 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 18:50:15 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 18:50:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |           0.0049 |           0.0000 |          20.8370 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |           0.0070 |           0.0000 |          20.8073 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.7872 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7974 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7862 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          20.7682 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.7749 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          20.7637 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |           0.0057 |           0.0000 |          20.7773 |
[32m[20221213 18:50:16 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.7574 |
[32m[20221213 18:50:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:50:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:17 @agent_ppo2.py:143][0m Total time:       1.74 min
[32m[20221213 18:50:17 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 18:50:17 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 18:50:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:50:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8741 |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.8450 |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.8189 |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.7855 |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.7547 |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.7306 |
[32m[20221213 18:50:17 @agent_ppo2.py:185][0m |           0.0082 |           0.0000 |          20.7364 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.7412 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.7314 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          20.7407 |
[32m[20221213 18:50:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:50:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:18 @agent_ppo2.py:143][0m Total time:       1.76 min
[32m[20221213 18:50:18 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 18:50:18 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 18:50:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8036 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.7589 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7495 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.7168 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.7165 |
[32m[20221213 18:50:18 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          20.7155 |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.7174 |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |           0.0042 |           0.0000 |          20.7117 |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.6775 |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.6987 |
[32m[20221213 18:50:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:19 @agent_ppo2.py:143][0m Total time:       1.78 min
[32m[20221213 18:50:19 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 18:50:19 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 18:50:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:50:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.7344 |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.7271 |
[32m[20221213 18:50:19 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.7357 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.7512 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.7501 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.7701 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.7815 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.7911 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.8043 |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.8054 |
[32m[20221213 18:50:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:50:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:20 @agent_ppo2.py:143][0m Total time:       1.80 min
[32m[20221213 18:50:20 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 18:50:20 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 18:50:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:50:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:20 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.7158 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |           0.0071 |           0.0000 |          20.6825 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.6670 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.6444 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.6455 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.6003 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.5784 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.5528 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.5497 |
[32m[20221213 18:50:21 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.5421 |
[32m[20221213 18:50:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:50:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:21 @agent_ppo2.py:143][0m Total time:       1.82 min
[32m[20221213 18:50:21 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 18:50:21 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 18:50:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:50:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.7119 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |           0.0074 |           0.0000 |          20.7534 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |           0.0222 |           0.0000 |          20.7751 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.8166 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8392 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.8494 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.8664 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8802 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.8994 |
[32m[20221213 18:50:22 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.8917 |
[32m[20221213 18:50:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:23 @agent_ppo2.py:143][0m Total time:       1.83 min
[32m[20221213 18:50:23 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 18:50:23 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 18:50:23 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8291 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          20.8545 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8824 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.9277 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          20.9367 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.9773 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |           0.0098 |           0.0000 |          21.0063 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          21.0360 |
[32m[20221213 18:50:23 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          21.0557 |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          21.0532 |
[32m[20221213 18:50:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:50:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 96.00
[32m[20221213 18:50:24 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 96.00
[32m[20221213 18:50:24 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 96.00
[32m[20221213 18:50:24 @agent_ppo2.py:143][0m Total time:       1.85 min
[32m[20221213 18:50:24 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 18:50:24 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 18:50:24 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:50:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8619 |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.8600 |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.8357 |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          20.8477 |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0070 |           0.0000 |          20.8583 |
[32m[20221213 18:50:24 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          20.8637 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          20.8578 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.8509 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0067 |           0.0000 |          20.8573 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0071 |           0.0000 |          20.8370 |
[32m[20221213 18:50:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:50:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:25 @agent_ppo2.py:143][0m Total time:       1.87 min
[32m[20221213 18:50:25 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 18:50:25 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 18:50:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.9393 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.9179 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8689 |
[32m[20221213 18:50:25 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.8513 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8266 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          20.8365 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8157 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8178 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.8108 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |           0.0165 |           0.0000 |          20.7884 |
[32m[20221213 18:50:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:50:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:26 @agent_ppo2.py:143][0m Total time:       1.89 min
[32m[20221213 18:50:26 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 18:50:26 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 18:50:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:50:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.8499 |
[32m[20221213 18:50:26 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8085 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          20.7950 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8174 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |           0.0091 |           0.0000 |          20.8168 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.8306 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.8277 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8376 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.8265 |
[32m[20221213 18:50:27 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          20.8392 |
[32m[20221213 18:50:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:50:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:27 @agent_ppo2.py:143][0m Total time:       1.92 min
[32m[20221213 18:50:27 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 18:50:27 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 18:50:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.8480 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8475 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.8420 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |           0.0072 |           0.0000 |          20.8311 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.8258 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.8281 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.8202 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |           0.0105 |           0.0000 |          20.8201 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |           0.0092 |           0.0000 |          20.8103 |
[32m[20221213 18:50:28 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8125 |
[32m[20221213 18:50:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:28 @agent_ppo2.py:143][0m Total time:       1.93 min
[32m[20221213 18:50:28 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 18:50:28 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 18:50:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.8083 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.7416 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.6899 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.6307 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.6208 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.6126 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.5747 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.5628 |
[32m[20221213 18:50:29 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.5333 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.5277 |
[32m[20221213 18:50:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:50:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:30 @agent_ppo2.py:143][0m Total time:       1.95 min
[32m[20221213 18:50:30 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 18:50:30 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 18:50:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.6838 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.7038 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.6993 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.7097 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          20.6993 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.7109 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |           0.0054 |           0.0000 |          20.7027 |
[32m[20221213 18:50:30 @agent_ppo2.py:185][0m |           0.0032 |           0.0000 |          20.7381 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.7498 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |           0.0039 |           0.0000 |          20.7364 |
[32m[20221213 18:50:31 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.00
[32m[20221213 18:50:31 @agent_ppo2.py:143][0m Total time:       1.97 min
[32m[20221213 18:50:31 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 18:50:31 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 18:50:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.6930 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |           0.0084 |           0.0000 |          20.6841 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.6996 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.7069 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.6893 |
[32m[20221213 18:50:31 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          20.7040 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |           0.0068 |           0.0000 |          20.6984 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.6576 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.6544 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.6724 |
[32m[20221213 18:50:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:50:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:32 @agent_ppo2.py:143][0m Total time:       1.99 min
[32m[20221213 18:50:32 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 18:50:32 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 18:50:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          20.7154 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.7126 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.7076 |
[32m[20221213 18:50:32 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          20.6956 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.6991 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.6976 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          20.6999 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.6709 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.6649 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |           0.0110 |           0.0000 |          20.6467 |
[32m[20221213 18:50:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:50:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:33 @agent_ppo2.py:143][0m Total time:       2.01 min
[32m[20221213 18:50:33 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 18:50:33 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 18:50:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          20.6265 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.6185 |
[32m[20221213 18:50:33 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.6010 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.5684 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |           0.0077 |           0.0000 |          20.5643 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |           0.0117 |           0.0000 |          20.5551 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.5454 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |           0.0094 |           0.0000 |          20.5181 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.5085 |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.4966 |
[32m[20221213 18:50:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:34 @agent_ppo2.py:143][0m Total time:       2.03 min
[32m[20221213 18:50:34 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 18:50:34 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 18:50:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:34 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          20.5121 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |           0.0082 |           0.0000 |          20.4791 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.4741 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.4816 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.4621 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.4473 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.4420 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.4946 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          20.4906 |
[32m[20221213 18:50:35 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.4553 |
[32m[20221213 18:50:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:50:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:35 @agent_ppo2.py:143][0m Total time:       2.05 min
[32m[20221213 18:50:35 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 18:50:35 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 18:50:35 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:50:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.6096 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.5820 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.5696 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.5778 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.5568 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |           0.0016 |           0.0000 |          20.5641 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.5627 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.5442 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.5583 |
[32m[20221213 18:50:36 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.5496 |
[32m[20221213 18:50:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:50:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:36 @agent_ppo2.py:143][0m Total time:       2.07 min
[32m[20221213 18:50:36 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 18:50:36 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 18:50:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.6347 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.6350 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |           0.0034 |           0.0000 |          20.6267 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.6166 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.6275 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.6087 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.6274 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.6376 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.6539 |
[32m[20221213 18:50:37 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.6523 |
[32m[20221213 18:50:37 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:38 @agent_ppo2.py:143][0m Total time:       2.08 min
[32m[20221213 18:50:38 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 18:50:38 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 18:50:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:50:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.6760 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.6982 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.7098 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.7427 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.7653 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          20.7977 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.8282 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |           0.0053 |           0.0000 |          20.8432 |
[32m[20221213 18:50:38 @agent_ppo2.py:185][0m |          -0.0068 |           0.0000 |          20.8676 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.8808 |
[32m[20221213 18:50:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:39 @agent_ppo2.py:143][0m Total time:       2.10 min
[32m[20221213 18:50:39 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 18:50:39 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 18:50:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          20.7253 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.7269 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.7331 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.7315 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |           0.0103 |           0.0000 |          20.7404 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.7576 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.7439 |
[32m[20221213 18:50:39 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.7647 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.7622 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.7708 |
[32m[20221213 18:50:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:50:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:40 @agent_ppo2.py:143][0m Total time:       2.12 min
[32m[20221213 18:50:40 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 18:50:40 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 18:50:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |           0.0026 |           0.0000 |          20.7103 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.7057 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.6991 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.6873 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |           0.0048 |           0.0000 |          20.6708 |
[32m[20221213 18:50:40 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.6375 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.6260 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |           0.0008 |           0.0000 |          20.6128 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.6074 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.6023 |
[32m[20221213 18:50:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:50:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:41 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 18:50:41 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 18:50:41 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 18:50:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:50:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.7433 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.7078 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.7127 |
[32m[20221213 18:50:41 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.7186 |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.7201 |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.7150 |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.7216 |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.6945 |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7135 |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.7163 |
[32m[20221213 18:50:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:50:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:42 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 18:50:42 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 18:50:42 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 18:50:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:42 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.5937 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.5438 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.5416 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.5692 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.5471 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |           0.0030 |           0.0000 |          20.5383 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |           0.0079 |           0.0000 |          20.5250 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.5205 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.5123 |
[32m[20221213 18:50:43 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.4918 |
[32m[20221213 18:50:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:50:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:43 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 18:50:43 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 18:50:43 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 18:50:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.6554 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.6417 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.6142 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.5813 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.5709 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.5511 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |           0.0097 |           0.0000 |          20.5349 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.5091 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.5049 |
[32m[20221213 18:50:44 @agent_ppo2.py:185][0m |           0.0038 |           0.0000 |          20.4838 |
[32m[20221213 18:50:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:50:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:44 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 18:50:44 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 18:50:44 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 18:50:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |           0.0004 |           2.6400 |          20.5219 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |           0.0025 |           2.4108 |          20.5012 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |          -0.0115 |           2.5558 |          20.4738 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |           0.0024 |           2.3925 |          20.4586 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |          -0.0125 |           2.6849 |          20.4412 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |           0.0007 |           2.4030 |          20.3993 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |          -0.0000 |           2.3917 |          20.3857 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |          -0.0003 |           2.3889 |          20.3491 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |           0.0008 |           2.3859 |          20.3082 |
[32m[20221213 18:50:45 @agent_ppo2.py:185][0m |          -0.0002 |           2.3919 |          20.2853 |
[32m[20221213 18:50:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:50:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 25.60
[32m[20221213 18:50:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 38.00
[32m[20221213 18:50:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:46 @agent_ppo2.py:143][0m Total time:       2.22 min
[32m[20221213 18:50:46 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 18:50:46 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 18:50:46 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |          -0.0009 |           0.0531 |          20.3473 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |           0.0008 |           0.0305 |          20.3654 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |           0.0004 |           0.0229 |          20.3666 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |          -0.0005 |           0.0194 |          20.3731 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |          -0.0016 |           0.0172 |          20.3688 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |          -0.0056 |           0.0161 |          20.3811 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |           0.0034 |           0.0153 |          20.3697 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |          -0.0074 |           0.0149 |          20.3810 |
[32m[20221213 18:50:46 @agent_ppo2.py:185][0m |          -0.0005 |           0.0145 |          20.3847 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0066 |           0.0142 |          20.3863 |
[32m[20221213 18:50:47 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.00
[32m[20221213 18:50:47 @agent_ppo2.py:143][0m Total time:       2.24 min
[32m[20221213 18:50:47 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 18:50:47 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 18:50:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0009 |           0.0077 |          20.3415 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0035 |           0.0046 |          20.3117 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |           0.0010 |           0.0037 |          20.2542 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0027 |           0.0034 |          20.2365 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0019 |           0.0033 |          20.2154 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0017 |           0.0033 |          20.1763 |
[32m[20221213 18:50:47 @agent_ppo2.py:185][0m |          -0.0016 |           0.0033 |          20.1599 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |           0.0015 |           0.0032 |          20.1538 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |           0.0010 |           0.0032 |          20.0947 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |          -0.0049 |           0.0032 |          20.0937 |
[32m[20221213 18:50:48 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.00
[32m[20221213 18:50:48 @agent_ppo2.py:143][0m Total time:       2.26 min
[32m[20221213 18:50:48 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 18:50:48 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 18:50:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |           0.0063 |           0.0011 |          20.1872 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |          -0.0037 |           0.0008 |          20.1487 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |          -0.0040 |           0.0007 |          20.0961 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |          -0.0035 |           0.0006 |          20.0479 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |           0.0008 |           0.0006 |          20.0350 |
[32m[20221213 18:50:48 @agent_ppo2.py:185][0m |           0.0011 |           0.0006 |          20.0018 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |          -0.0040 |           0.0006 |          20.0043 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |          -0.0091 |           0.0006 |          19.9904 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |           0.0011 |           0.0005 |          19.9659 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |          -0.0068 |           0.0005 |          19.9528 |
[32m[20221213 18:50:49 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:49 @agent_ppo2.py:143][0m Total time:       2.27 min
[32m[20221213 18:50:49 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 18:50:49 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 18:50:49 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |           0.0034 |           0.0004 |          19.9774 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |           0.0035 |           0.0004 |          19.9663 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |          -0.0067 |           0.0004 |          19.9655 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |          -0.0002 |           0.0004 |          19.9482 |
[32m[20221213 18:50:49 @agent_ppo2.py:185][0m |          -0.0044 |           0.0004 |          19.9001 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |           0.0022 |           0.0004 |          19.9147 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |          -0.0058 |           0.0004 |          19.9091 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |          -0.0047 |           0.0004 |          19.9022 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |           0.0064 |           0.0004 |          19.8938 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |          -0.0062 |           0.0004 |          19.8903 |
[32m[20221213 18:50:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:50:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:50 @agent_ppo2.py:143][0m Total time:       2.29 min
[32m[20221213 18:50:50 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 18:50:50 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 18:50:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |           0.0037 |           3.0220 |          20.0010 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |          -0.0082 |           2.9280 |          19.9675 |
[32m[20221213 18:50:50 @agent_ppo2.py:185][0m |          -0.0066 |           2.9962 |          19.9718 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0024 |           2.8097 |          19.9823 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0008 |           2.8010 |          19.9655 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0022 |           2.7995 |          19.9689 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0023 |           2.8059 |          19.9745 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0025 |           2.8083 |          19.9553 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0007 |           2.8096 |          19.9393 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0006 |           2.8087 |          19.9446 |
[32m[20221213 18:50:51 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.80
[32m[20221213 18:50:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.00
[32m[20221213 18:50:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:51 @agent_ppo2.py:143][0m Total time:       2.31 min
[32m[20221213 18:50:51 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 18:50:51 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 18:50:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |           0.0027 |           0.0991 |          20.0041 |
[32m[20221213 18:50:51 @agent_ppo2.py:185][0m |          -0.0003 |           0.0418 |          20.0042 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |          -0.0063 |           0.0318 |          20.0522 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |           0.0008 |           0.0280 |          20.0408 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |          -0.0027 |           0.0266 |          20.0699 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |           0.0015 |           0.0256 |          20.0377 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |          -0.0022 |           0.0251 |          20.0504 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |          -0.0060 |           0.0249 |          20.0711 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |          -0.0031 |           0.0249 |          20.0624 |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |           0.0026 |           0.0245 |          20.1032 |
[32m[20221213 18:50:52 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 18:50:52 @agent_ppo2.py:143][0m Total time:       2.33 min
[32m[20221213 18:50:52 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 18:50:52 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 18:50:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:52 @agent_ppo2.py:185][0m |           0.0012 |           0.0106 |          20.0971 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |           0.0014 |           0.0049 |          20.0203 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0020 |           0.0039 |          19.9753 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |           0.0072 |           0.0034 |          19.9066 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0066 |           0.0031 |          19.8633 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0017 |           0.0029 |          19.8334 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0114 |           0.0028 |          19.8158 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0048 |           0.0028 |          19.7977 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0040 |           0.0027 |          19.7559 |
[32m[20221213 18:50:53 @agent_ppo2.py:185][0m |          -0.0030 |           0.0027 |          19.7265 |
[32m[20221213 18:50:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:50:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:53 @agent_ppo2.py:143][0m Total time:       2.35 min
[32m[20221213 18:50:53 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 18:50:53 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 18:50:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |           0.0027 |           0.0024 |          19.7004 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |          -0.0021 |           0.0014 |          19.6558 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |           0.0045 |           0.0012 |          19.6562 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |           0.0007 |           0.0011 |          19.6289 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |          -0.0003 |           0.0010 |          19.6333 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |           0.0019 |           0.0010 |          19.6134 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |          -0.0030 |           0.0009 |          19.5815 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |          -0.0040 |           0.0009 |          19.5649 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |          -0.0010 |           0.0009 |          19.5449 |
[32m[20221213 18:50:54 @agent_ppo2.py:185][0m |          -0.0084 |           0.0009 |          19.5709 |
[32m[20221213 18:50:54 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.00
[32m[20221213 18:50:54 @agent_ppo2.py:143][0m Total time:       2.37 min
[32m[20221213 18:50:54 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 18:50:54 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 18:50:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |           0.0063 |           0.0011 |          19.6787 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |          -0.0011 |           0.0008 |          19.6481 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |           0.0003 |           0.0008 |          19.5885 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |           0.0018 |           0.0008 |          19.5401 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |           0.0039 |           0.0008 |          19.5144 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |           0.0025 |           0.0007 |          19.4658 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |          -0.0042 |           0.0007 |          19.4126 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |          -0.0013 |           0.0007 |          19.3889 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |           0.0118 |           0.0007 |          19.3838 |
[32m[20221213 18:50:55 @agent_ppo2.py:185][0m |          -0.0059 |           0.0007 |          19.3832 |
[32m[20221213 18:50:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:50:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:56 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 18:50:56 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 18:50:56 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 18:50:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |          -0.0025 |           0.0007 |          19.3293 |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |           0.0035 |           0.0006 |          19.3402 |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |           0.0011 |           0.0006 |          19.3647 |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |          -0.0076 |           0.0006 |          19.3158 |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |          -0.0061 |           0.0006 |          19.3250 |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |          -0.0012 |           0.0006 |          19.3170 |
[32m[20221213 18:50:56 @agent_ppo2.py:185][0m |          -0.0028 |           0.0006 |          19.3230 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0014 |           0.0006 |          19.3116 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0052 |           0.0006 |          19.3037 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0021 |           0.0006 |          19.3110 |
[32m[20221213 18:50:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:50:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:57 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 18:50:57 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 18:50:57 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 18:50:57 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:50:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |           0.0013 |           0.0006 |          19.3173 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0048 |           0.0006 |          19.2793 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0037 |           0.0006 |          19.2609 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0018 |           0.0006 |          19.2299 |
[32m[20221213 18:50:57 @agent_ppo2.py:185][0m |          -0.0024 |           0.0005 |          19.2208 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0053 |           0.0005 |          19.2259 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0047 |           0.0005 |          19.2004 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0063 |           0.0005 |          19.1835 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0058 |           0.0005 |          19.1868 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0019 |           0.0005 |          19.1515 |
[32m[20221213 18:50:58 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:50:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:58 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 18:50:58 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 18:50:58 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 18:50:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0002 |           0.0005 |          19.2651 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |           0.0017 |           0.0005 |          19.2784 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0014 |           0.0005 |          19.3174 |
[32m[20221213 18:50:58 @agent_ppo2.py:185][0m |          -0.0017 |           0.0005 |          19.3434 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |          -0.0042 |           0.0005 |          19.3368 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |          -0.0020 |           0.0005 |          19.3695 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |           0.0015 |           0.0005 |          19.3854 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |           0.0070 |           0.0005 |          19.3949 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |          -0.0057 |           0.0005 |          19.4388 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |          -0.0058 |           0.0004 |          19.4707 |
[32m[20221213 18:50:59 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:50:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:50:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:50:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:50:59 @agent_ppo2.py:143][0m Total time:       2.44 min
[32m[20221213 18:50:59 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 18:50:59 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 18:50:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:50:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |           0.0031 |           0.0004 |          19.4853 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |          -0.0039 |           0.0004 |          19.4877 |
[32m[20221213 18:50:59 @agent_ppo2.py:185][0m |          -0.0018 |           0.0004 |          19.4853 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |          -0.0028 |           0.0004 |          19.5086 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |          -0.0022 |           0.0004 |          19.4994 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |          -0.0027 |           0.0004 |          19.4881 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |           0.0046 |           0.0004 |          19.4882 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |           0.0019 |           0.0004 |          19.4709 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |          -0.0013 |           0.0004 |          19.4696 |
[32m[20221213 18:51:00 @agent_ppo2.py:185][0m |          -0.0030 |           0.0004 |          19.4589 |
[32m[20221213 18:51:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:51:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:00 @agent_ppo2.py:143][0m Total time:       2.46 min
[32m[20221213 18:51:00 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 18:51:00 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 18:51:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 18:51:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |          -0.0004 |           0.0004 |          19.4091 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |          -0.0025 |           0.0003 |          19.3913 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |          -0.0013 |           0.0003 |          19.4408 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |          -0.0010 |           0.0003 |          19.4277 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |           0.0082 |           0.0003 |          19.4543 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |           0.0072 |           0.0003 |          19.4915 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |          -0.0050 |           0.0003 |          19.5245 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |           0.0011 |           0.0003 |          19.5270 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |           0.0169 |           0.0003 |          19.5252 |
[32m[20221213 18:51:01 @agent_ppo2.py:185][0m |          -0.0023 |           0.0003 |          19.5547 |
[32m[20221213 18:51:01 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:01 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221213 18:51:01 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 18:51:01 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 18:51:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |           0.0079 |           0.0003 |          19.5764 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |          -0.0024 |           0.0003 |          19.5594 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |           0.0006 |           0.0002 |          19.5108 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |          -0.0018 |           0.0002 |          19.4972 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |          -0.0020 |           0.0002 |          19.4724 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |           0.0009 |           0.0002 |          19.4752 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |          -0.0024 |           0.0002 |          19.4712 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |          -0.0007 |           0.0002 |          19.4556 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |          -0.0012 |           0.0002 |          19.4447 |
[32m[20221213 18:51:02 @agent_ppo2.py:185][0m |           0.0054 |           0.0002 |          19.4096 |
[32m[20221213 18:51:02 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:03 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221213 18:51:03 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 18:51:03 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 18:51:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0010 |           0.0002 |          19.3967 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0021 |           0.0002 |          19.4099 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0024 |           0.0002 |          19.3983 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0029 |           0.0002 |          19.3969 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0029 |           0.0002 |          19.3634 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0019 |           0.0001 |          19.3443 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0035 |           0.0001 |          19.3629 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |           0.0018 |           0.0001 |          19.3588 |
[32m[20221213 18:51:03 @agent_ppo2.py:185][0m |          -0.0034 |           0.0001 |          19.3458 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0040 |           0.0001 |          19.3114 |
[32m[20221213 18:51:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:51:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:04 @agent_ppo2.py:143][0m Total time:       2.52 min
[32m[20221213 18:51:04 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 18:51:04 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 18:51:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0018 |           0.0001 |          19.4232 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0020 |           0.0001 |          19.4724 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0019 |           0.0001 |          19.5000 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0020 |           0.0001 |          19.5322 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0027 |           0.0001 |          19.5567 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0032 |           0.0001 |          19.5838 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |           0.0077 |           0.0001 |          19.5987 |
[32m[20221213 18:51:04 @agent_ppo2.py:185][0m |          -0.0025 |           0.0001 |          19.6373 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0016 |           0.0001 |          19.6572 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0034 |           0.0001 |          19.6612 |
[32m[20221213 18:51:05 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:05 @agent_ppo2.py:143][0m Total time:       2.54 min
[32m[20221213 18:51:05 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 18:51:05 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 18:51:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0017 |           0.0001 |          19.4523 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0033 |           0.0001 |          19.4290 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          19.4314 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          19.4140 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          19.4131 |
[32m[20221213 18:51:05 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          19.4385 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          19.4187 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          19.4371 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          19.4540 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          19.4550 |
[32m[20221213 18:51:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:51:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.00
[32m[20221213 18:51:06 @agent_ppo2.py:143][0m Total time:       2.56 min
[32m[20221213 18:51:06 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 18:51:06 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 18:51:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          19.5934 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          19.5355 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          19.4786 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |           0.0050 |           0.0000 |          19.4666 |
[32m[20221213 18:51:06 @agent_ppo2.py:185][0m |           0.0061 |           0.0000 |          19.4415 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |           0.0133 |           0.0000 |          19.3825 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          19.3371 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |           0.0060 |           0.0000 |          19.3332 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          19.3345 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          19.3556 |
[32m[20221213 18:51:07 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:51:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:07 @agent_ppo2.py:143][0m Total time:       2.58 min
[32m[20221213 18:51:07 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 18:51:07 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 18:51:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          19.3540 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          19.3745 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          19.4096 |
[32m[20221213 18:51:07 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          19.4223 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          19.4592 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          19.5137 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          19.5126 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |           0.0046 |           0.0000 |          19.5484 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.5866 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.6042 |
[32m[20221213 18:51:08 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:08 @agent_ppo2.py:143][0m Total time:       2.59 min
[32m[20221213 18:51:08 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 18:51:08 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 18:51:08 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |           0.0036 |           0.0000 |          19.5644 |
[32m[20221213 18:51:08 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          19.6151 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.6336 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          19.6537 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          19.6884 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          19.7136 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          19.7433 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          19.7748 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.7822 |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          19.7729 |
[32m[20221213 18:51:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:09 @agent_ppo2.py:143][0m Total time:       2.61 min
[32m[20221213 18:51:09 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 18:51:09 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 18:51:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:09 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          19.7980 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          19.8014 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |           0.0056 |           0.0000 |          19.7907 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          19.7877 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          19.7725 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          19.7465 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          19.7333 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |           0.0086 |           0.0000 |          19.7448 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |           0.0052 |           0.0000 |          19.7187 |
[32m[20221213 18:51:10 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          19.7126 |
[32m[20221213 18:51:10 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:10 @agent_ppo2.py:143][0m Total time:       2.63 min
[32m[20221213 18:51:10 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 18:51:10 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 18:51:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          19.7038 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          19.7128 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          19.7550 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          19.8103 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          19.8495 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          19.8734 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          19.9164 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |           0.0026 |           0.0000 |          19.9516 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0064 |           0.0000 |          19.9984 |
[32m[20221213 18:51:11 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          20.0522 |
[32m[20221213 18:51:11 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:11 @agent_ppo2.py:143][0m Total time:       2.65 min
[32m[20221213 18:51:11 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 18:51:11 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 18:51:11 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.0445 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.0641 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.0745 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.0798 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.1105 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0061 |           0.0000 |          20.1199 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          20.1446 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          20.1771 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.1797 |
[32m[20221213 18:51:12 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          20.1606 |
[32m[20221213 18:51:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:51:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:13 @agent_ppo2.py:143][0m Total time:       2.67 min
[32m[20221213 18:51:13 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 18:51:13 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 18:51:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.0732 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.0674 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.0986 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.1251 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          20.1757 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0064 |           0.0000 |          20.1884 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0072 |           0.0000 |          20.1997 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0072 |           0.0000 |          20.2223 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          20.2461 |
[32m[20221213 18:51:13 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          20.2592 |
[32m[20221213 18:51:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:14 @agent_ppo2.py:143][0m Total time:       2.69 min
[32m[20221213 18:51:14 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 18:51:14 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 18:51:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.2782 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.2504 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.2209 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.1828 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          20.1471 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.1280 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |           0.0139 |           0.0000 |          20.1226 |
[32m[20221213 18:51:14 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.1082 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.1030 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |           0.0109 |           0.0000 |          20.0559 |
[32m[20221213 18:51:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:15 @agent_ppo2.py:143][0m Total time:       2.70 min
[32m[20221213 18:51:15 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 18:51:15 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 18:51:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          20.1168 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.1883 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.2164 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.2527 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |           0.0034 |           0.0000 |          20.2956 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.3088 |
[32m[20221213 18:51:15 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.3450 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.3677 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.4188 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |           0.0113 |           0.0000 |          20.4449 |
[32m[20221213 18:51:16 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:51:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.00
[32m[20221213 18:51:16 @agent_ppo2.py:143][0m Total time:       2.72 min
[32m[20221213 18:51:16 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 18:51:16 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 18:51:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |          -0.0032 |           3.6505 |          20.3661 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |          -0.0009 |           2.8561 |          20.3087 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |           0.0011 |           2.6782 |          20.2964 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |           0.0020 |           2.6802 |          20.2805 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |           0.0021 |           2.6691 |          20.2924 |
[32m[20221213 18:51:16 @agent_ppo2.py:185][0m |          -0.0048 |           2.6688 |          20.2773 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |           0.0015 |           2.6660 |          20.2728 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |           0.0005 |           2.6676 |          20.2665 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |           0.0009 |           2.6578 |          20.2828 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |          -0.0057 |           2.7262 |          20.2842 |
[32m[20221213 18:51:17 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.40
[32m[20221213 18:51:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.00
[32m[20221213 18:51:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:17 @agent_ppo2.py:143][0m Total time:       2.74 min
[32m[20221213 18:51:17 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 18:51:17 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 18:51:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |           0.0013 |           0.1225 |          20.3754 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |          -0.0058 |           0.0591 |          20.3775 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |          -0.0007 |           0.0537 |          20.3839 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |          -0.0011 |           0.0525 |          20.3850 |
[32m[20221213 18:51:17 @agent_ppo2.py:185][0m |          -0.0026 |           0.0523 |          20.3635 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |          -0.0006 |           0.0523 |          20.3696 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |          -0.0030 |           0.0523 |          20.3503 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |          -0.0067 |           0.0521 |          20.3532 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |           0.0009 |           0.0522 |          20.3399 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |           0.0033 |           0.0525 |          20.3637 |
[32m[20221213 18:51:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:51:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:18 @agent_ppo2.py:143][0m Total time:       2.76 min
[32m[20221213 18:51:18 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 18:51:18 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 18:51:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |          -0.0004 |           0.0067 |          20.3903 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |           0.0020 |           0.0044 |          20.3549 |
[32m[20221213 18:51:18 @agent_ppo2.py:185][0m |          -0.0034 |           0.0036 |          20.3220 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |          -0.0093 |           0.0031 |          20.3270 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |          -0.0052 |           0.0028 |          20.2965 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |           0.0030 |           0.0025 |          20.2822 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |           0.0033 |           0.0023 |          20.2383 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |          -0.0064 |           0.0021 |          20.2238 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |          -0.0019 |           0.0020 |          20.2256 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |          -0.0031 |           0.0019 |          20.2030 |
[32m[20221213 18:51:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:51:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:19 @agent_ppo2.py:143][0m Total time:       2.78 min
[32m[20221213 18:51:19 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 18:51:19 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 18:51:19 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |           0.0031 |           4.0520 |          20.2616 |
[32m[20221213 18:51:19 @agent_ppo2.py:185][0m |           0.0031 |           3.2617 |          20.2603 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |           0.0022 |           3.2418 |          20.2793 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |           0.0056 |           3.1990 |          20.2574 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |           0.0040 |           3.1706 |          20.2456 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |           0.0029 |           3.1433 |          20.2313 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |          -0.0005 |           3.1044 |          20.2267 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |           0.0006 |           3.0840 |          20.2157 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |          -0.0049 |           3.1338 |          20.2372 |
[32m[20221213 18:51:20 @agent_ppo2.py:185][0m |          -0.0054 |           3.0316 |          20.2243 |
[32m[20221213 18:51:20 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.00
[32m[20221213 18:51:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.00
[32m[20221213 18:51:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:20 @agent_ppo2.py:143][0m Total time:       2.80 min
[32m[20221213 18:51:20 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 18:51:20 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 18:51:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |           0.0042 |           0.1462 |          20.1365 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0030 |           0.0661 |          20.1139 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0059 |           0.0469 |          20.0733 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |           0.0001 |           0.0388 |          20.0410 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0012 |           0.0358 |          19.9993 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0125 |           0.0347 |          19.9781 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0023 |           0.0343 |          19.9978 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0064 |           0.0342 |          19.9761 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |           0.0043 |           0.0341 |          19.9589 |
[32m[20221213 18:51:21 @agent_ppo2.py:185][0m |          -0.0023 |           0.0340 |          19.9653 |
[32m[20221213 18:51:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:21 @agent_ppo2.py:143][0m Total time:       2.81 min
[32m[20221213 18:51:21 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 18:51:21 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 18:51:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0011 |           0.0305 |          20.0293 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0053 |           0.0258 |          20.0274 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0051 |           0.0258 |          20.0304 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0032 |           0.0257 |          20.0515 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0052 |           0.0256 |          20.0592 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |           0.0009 |           0.0261 |          20.0725 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0043 |           0.0255 |          20.0408 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0047 |           0.0254 |          20.0492 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0043 |           0.0254 |          20.0421 |
[32m[20221213 18:51:22 @agent_ppo2.py:185][0m |          -0.0034 |           0.0253 |          20.0678 |
[32m[20221213 18:51:22 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:22 @agent_ppo2.py:143][0m Total time:       2.83 min
[32m[20221213 18:51:22 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 18:51:22 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 18:51:23 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0009 |           0.0257 |          20.1436 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0037 |           0.0216 |          20.1441 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0003 |           0.0213 |          20.1445 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0034 |           0.0210 |          20.1246 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0041 |           0.0208 |          20.1223 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0001 |           0.0206 |          20.1246 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0067 |           0.0204 |          20.0960 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |           0.0009 |           0.0204 |          20.0766 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0016 |           0.0202 |          20.0792 |
[32m[20221213 18:51:23 @agent_ppo2.py:185][0m |          -0.0030 |           0.0201 |          20.0928 |
[32m[20221213 18:51:23 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:24 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221213 18:51:24 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 18:51:24 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 18:51:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |           0.0004 |           0.0267 |          20.0771 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |           0.0083 |           0.0209 |          20.0542 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |          -0.0043 |           0.0182 |          20.0531 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |          -0.0008 |           0.0183 |          20.0315 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |           0.0016 |           0.0182 |          19.9537 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |          -0.0046 |           0.0182 |          19.9281 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |           0.0005 |           0.0181 |          19.9047 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |          -0.0037 |           0.0182 |          19.9064 |
[32m[20221213 18:51:24 @agent_ppo2.py:185][0m |          -0.0072 |           0.0181 |          19.8870 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0019 |           0.0182 |          19.8916 |
[32m[20221213 18:51:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:25 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221213 18:51:25 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 18:51:25 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 18:51:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0026 |           0.0212 |          19.9049 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0074 |           0.0179 |          19.9093 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0074 |           0.0175 |          19.9202 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0004 |           0.0173 |          19.9127 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0037 |           0.0172 |          19.9422 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |          -0.0014 |           0.0171 |          19.9711 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |           0.0033 |           0.0186 |          19.9858 |
[32m[20221213 18:51:25 @agent_ppo2.py:185][0m |           0.0064 |           0.0186 |          20.0223 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0029 |           0.0171 |          20.0178 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0017 |           0.0171 |          20.0225 |
[32m[20221213 18:51:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:26 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221213 18:51:26 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 18:51:26 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 18:51:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0022 |           0.0134 |          20.0296 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0034 |           0.0117 |          20.0227 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0051 |           0.0117 |          20.0291 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0034 |           0.0117 |          20.0419 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0009 |           0.0117 |          20.0326 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0035 |           0.0117 |          20.0697 |
[32m[20221213 18:51:26 @agent_ppo2.py:185][0m |          -0.0052 |           0.0117 |          20.0741 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0037 |           0.0116 |          20.0859 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0068 |           0.0116 |          20.1120 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0015 |           0.0117 |          20.1342 |
[32m[20221213 18:51:27 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:27 @agent_ppo2.py:143][0m Total time:       2.91 min
[32m[20221213 18:51:27 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 18:51:27 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 18:51:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |           0.0075 |           0.0115 |          20.1738 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0006 |           0.0097 |          20.2171 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0022 |           0.0096 |          20.2416 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0011 |           0.0095 |          20.2569 |
[32m[20221213 18:51:27 @agent_ppo2.py:185][0m |          -0.0000 |           0.0095 |          20.3136 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0030 |           0.0094 |          20.3320 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0035 |           0.0094 |          20.3545 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0043 |           0.0094 |          20.3513 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0001 |           0.0094 |          20.3619 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0040 |           0.0094 |          20.3887 |
[32m[20221213 18:51:28 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:28 @agent_ppo2.py:143][0m Total time:       2.93 min
[32m[20221213 18:51:28 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 18:51:28 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 18:51:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0016 |           0.0089 |          20.2239 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0035 |           0.0078 |          20.2124 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |          -0.0037 |           0.0078 |          20.2089 |
[32m[20221213 18:51:28 @agent_ppo2.py:185][0m |           0.0093 |           0.0086 |          20.1930 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0049 |           0.0077 |          20.2149 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0052 |           0.0077 |          20.2124 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0059 |           0.0077 |          20.2226 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0044 |           0.0077 |          20.1917 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0060 |           0.0077 |          20.2000 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0022 |           0.0077 |          20.1954 |
[32m[20221213 18:51:29 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:29 @agent_ppo2.py:143][0m Total time:       2.94 min
[32m[20221213 18:51:29 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 18:51:29 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 18:51:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |           0.0052 |           0.0068 |          20.3202 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |          -0.0027 |           0.0060 |          20.3488 |
[32m[20221213 18:51:29 @agent_ppo2.py:185][0m |           0.0039 |           0.0061 |          20.3698 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |          -0.0024 |           0.0059 |          20.3580 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |          -0.0014 |           0.0059 |          20.3444 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |          -0.0011 |           0.0059 |          20.3787 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |          -0.0029 |           0.0059 |          20.3939 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |          -0.0030 |           0.0059 |          20.3798 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |          -0.0027 |           0.0059 |          20.3827 |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |           0.0000 |           0.0059 |          20.3751 |
[32m[20221213 18:51:30 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:30 @agent_ppo2.py:143][0m Total time:       2.96 min
[32m[20221213 18:51:30 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 18:51:30 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 18:51:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:30 @agent_ppo2.py:185][0m |           0.0087 |           0.0060 |          20.3514 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0029 |           0.0051 |          20.4009 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0039 |           0.0051 |          20.4223 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |           0.0020 |           0.0051 |          20.4782 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0031 |           0.0051 |          20.5159 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0057 |           0.0051 |          20.5403 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0059 |           0.0051 |          20.5556 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0043 |           0.0051 |          20.5735 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0029 |           0.0051 |          20.6037 |
[32m[20221213 18:51:31 @agent_ppo2.py:185][0m |          -0.0049 |           0.0050 |          20.6162 |
[32m[20221213 18:51:31 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:31 @agent_ppo2.py:143][0m Total time:       2.98 min
[32m[20221213 18:51:31 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 18:51:31 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 18:51:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |          -0.0005 |           1.1780 |          20.6100 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |           0.0051 |           1.1040 |          20.6092 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |          -0.0048 |           1.1085 |          20.6075 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |           0.0031 |           1.0988 |          20.6352 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |          -0.0038 |           1.0978 |          20.6367 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |           0.0006 |           1.0954 |          20.6133 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |           0.0010 |           1.0971 |          20.6334 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |          -0.0038 |           1.1002 |          20.6531 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |          -0.0021 |           1.1006 |          20.6383 |
[32m[20221213 18:51:32 @agent_ppo2.py:185][0m |          -0.0058 |           1.1084 |          20.6377 |
[32m[20221213 18:51:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:51:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.60
[32m[20221213 18:51:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.00
[32m[20221213 18:51:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:32 @agent_ppo2.py:143][0m Total time:       3.00 min
[32m[20221213 18:51:32 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 18:51:32 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 18:51:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |           0.0031 |           0.0359 |          20.5222 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0027 |           0.0114 |          20.5291 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |           0.0109 |           0.0096 |          20.5251 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0005 |           0.0081 |          20.5360 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0019 |           0.0073 |          20.5370 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0014 |           0.0067 |          20.5181 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0026 |           0.0064 |          20.5245 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0024 |           0.0061 |          20.5389 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0053 |           0.0058 |          20.5354 |
[32m[20221213 18:51:33 @agent_ppo2.py:185][0m |          -0.0047 |           0.0056 |          20.5566 |
[32m[20221213 18:51:33 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:34 @agent_ppo2.py:143][0m Total time:       3.02 min
[32m[20221213 18:51:34 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 18:51:34 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 18:51:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |           0.0057 |           0.0218 |          20.5998 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |           0.0047 |           0.0075 |          20.5969 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |          -0.0094 |           0.0062 |          20.6125 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |           0.0006 |           0.0059 |          20.6374 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |          -0.0111 |           0.0057 |          20.6415 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |           0.0001 |           0.0057 |          20.6251 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |           0.0021 |           0.0056 |          20.6446 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |          -0.0020 |           0.0056 |          20.6465 |
[32m[20221213 18:51:34 @agent_ppo2.py:185][0m |          -0.0079 |           0.0056 |          20.6448 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |           0.0011 |           0.0056 |          20.6441 |
[32m[20221213 18:51:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:35 @agent_ppo2.py:143][0m Total time:       3.04 min
[32m[20221213 18:51:35 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 18:51:35 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 18:51:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |           0.0018 |           0.0064 |          20.5535 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |           0.0036 |           0.0038 |          20.5405 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |          -0.0085 |           0.0033 |          20.5488 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |          -0.0003 |           0.0031 |          20.5286 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |          -0.0031 |           0.0029 |          20.5499 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |          -0.0035 |           0.0028 |          20.5435 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |          -0.0048 |           0.0027 |          20.5391 |
[32m[20221213 18:51:35 @agent_ppo2.py:185][0m |          -0.0014 |           0.0026 |          20.5245 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |           0.0068 |           0.0026 |          20.5104 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |           0.0008 |           0.0026 |          20.5271 |
[32m[20221213 18:51:36 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:51:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:36 @agent_ppo2.py:143][0m Total time:       3.05 min
[32m[20221213 18:51:36 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 18:51:36 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 18:51:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0016 |           0.0035 |          20.6124 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0058 |           0.0023 |          20.6073 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0022 |           0.0022 |          20.6148 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0024 |           0.0022 |          20.6226 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0019 |           0.0022 |          20.6426 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0028 |           0.0021 |          20.6425 |
[32m[20221213 18:51:36 @agent_ppo2.py:185][0m |          -0.0008 |           0.0021 |          20.6535 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |          -0.0050 |           0.0021 |          20.6710 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |          -0.0025 |           0.0021 |          20.6762 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |          -0.0011 |           0.0020 |          20.7021 |
[32m[20221213 18:51:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:51:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 18:51:37 @agent_ppo2.py:143][0m Total time:       3.07 min
[32m[20221213 18:51:37 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 18:51:37 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 18:51:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |           0.0030 |           0.0026 |          20.5975 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |          -0.0047 |           0.0020 |          20.5954 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |           0.0021 |           0.0020 |          20.6148 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |          -0.0037 |           0.0020 |          20.6277 |
[32m[20221213 18:51:37 @agent_ppo2.py:185][0m |          -0.0003 |           0.0020 |          20.6367 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |          -0.0020 |           0.0019 |          20.6430 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |          -0.0002 |           0.0019 |          20.6593 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |           0.0069 |           0.0019 |          20.6711 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |           0.0165 |           0.0022 |          20.6625 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |          -0.0038 |           0.0019 |          20.6691 |
[32m[20221213 18:51:38 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:38 @agent_ppo2.py:143][0m Total time:       3.09 min
[32m[20221213 18:51:38 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 18:51:38 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 18:51:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |           0.0092 |           0.0018 |          20.6185 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |          -0.0050 |           0.0015 |          20.6017 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |           0.0007 |           0.0015 |          20.6026 |
[32m[20221213 18:51:38 @agent_ppo2.py:185][0m |          -0.0051 |           0.0015 |          20.5918 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |           0.0060 |           0.0015 |          20.5822 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |          -0.0054 |           0.0015 |          20.5889 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |           0.0099 |           0.0015 |          20.5810 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |          -0.0015 |           0.0014 |          20.5840 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |           0.0025 |           0.0014 |          20.5946 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |          -0.0073 |           0.0014 |          20.5919 |
[32m[20221213 18:51:39 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:39 @agent_ppo2.py:143][0m Total time:       3.11 min
[32m[20221213 18:51:39 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 18:51:39 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 18:51:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |           0.0024 |           0.0014 |          20.6923 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |          -0.0046 |           0.0012 |          20.7154 |
[32m[20221213 18:51:39 @agent_ppo2.py:185][0m |          -0.0046 |           0.0012 |          20.7404 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |           0.0007 |           0.0012 |          20.7891 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0054 |           0.0012 |          20.8352 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0066 |           0.0012 |          20.8665 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0029 |           0.0012 |          20.8958 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0048 |           0.0011 |          20.9278 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0043 |           0.0011 |          20.9442 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0025 |           0.0011 |          20.9595 |
[32m[20221213 18:51:40 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:40 @agent_ppo2.py:143][0m Total time:       3.13 min
[32m[20221213 18:51:40 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 18:51:40 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 18:51:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |           0.0205 |           0.0012 |          20.8184 |
[32m[20221213 18:51:40 @agent_ppo2.py:185][0m |          -0.0008 |           0.0010 |          20.8363 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |          -0.0035 |           0.0010 |          20.8362 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |           0.0020 |           0.0010 |          20.8411 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |          -0.0022 |           0.0009 |          20.8460 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |          -0.0045 |           0.0009 |          20.8462 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |           0.0031 |           0.0009 |          20.8530 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |          -0.0031 |           0.0009 |          20.8603 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |          -0.0040 |           0.0009 |          20.8638 |
[32m[20221213 18:51:41 @agent_ppo2.py:185][0m |          -0.0014 |           0.0009 |          20.8779 |
[32m[20221213 18:51:41 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:41 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221213 18:51:41 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 18:51:41 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 18:51:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 18:51:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0017 |           0.0008 |          20.8677 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0021 |           0.0007 |          20.8913 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0009 |           0.0007 |          20.9164 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0016 |           0.0007 |          20.9268 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0015 |           0.0007 |          20.9420 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0016 |           0.0007 |          20.9768 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0024 |           0.0007 |          20.9819 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0024 |           0.0007 |          20.9905 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |          -0.0028 |           0.0007 |          21.0041 |
[32m[20221213 18:51:42 @agent_ppo2.py:185][0m |           0.0055 |           0.0007 |          21.0264 |
[32m[20221213 18:51:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:51:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:42 @agent_ppo2.py:143][0m Total time:       3.17 min
[32m[20221213 18:51:42 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 18:51:42 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 18:51:43 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0010 |           0.0006 |          20.9887 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0028 |           0.0005 |          20.9949 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |           0.0044 |           0.0005 |          21.0094 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0008 |           0.0005 |          20.9849 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0039 |           0.0005 |          21.0276 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0007 |           0.0005 |          21.0428 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0034 |           0.0005 |          21.0608 |
[32m[20221213 18:51:43 @agent_ppo2.py:185][0m |          -0.0041 |           0.0005 |          21.0827 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0033 |           0.0005 |          21.0970 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0021 |           0.0005 |          21.1207 |
[32m[20221213 18:51:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:51:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:44 @agent_ppo2.py:143][0m Total time:       3.19 min
[32m[20221213 18:51:44 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 18:51:44 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 18:51:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:51:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0019 |           0.0004 |          21.0999 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0038 |           0.0004 |          21.0823 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0033 |           0.0004 |          21.0666 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0044 |           0.0004 |          21.0700 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |          -0.0040 |           0.0003 |          21.0647 |
[32m[20221213 18:51:44 @agent_ppo2.py:185][0m |           0.0016 |           0.0003 |          21.0628 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0046 |           0.0003 |          21.0731 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0047 |           0.0003 |          21.0857 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0032 |           0.0003 |          21.0791 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0048 |           0.0003 |          21.0860 |
[32m[20221213 18:51:45 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:45 @agent_ppo2.py:143][0m Total time:       3.21 min
[32m[20221213 18:51:45 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 18:51:45 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 18:51:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0010 |           0.0003 |          21.0016 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0023 |           0.0003 |          21.0076 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |           0.0004 |           0.0003 |          21.0366 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |          -0.0038 |           0.0002 |          21.0583 |
[32m[20221213 18:51:45 @agent_ppo2.py:185][0m |           0.0010 |           0.0002 |          21.0710 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0033 |           0.0002 |          21.0761 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |           0.0004 |           0.0002 |          21.1001 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0045 |           0.0002 |          21.1008 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0044 |           0.0002 |          21.1072 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0047 |           0.0002 |          21.1139 |
[32m[20221213 18:51:46 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:46 @agent_ppo2.py:143][0m Total time:       3.23 min
[32m[20221213 18:51:46 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 18:51:46 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 18:51:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0023 |           0.0002 |          20.9820 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0038 |           0.0002 |          20.9753 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |          -0.0034 |           0.0002 |          20.9438 |
[32m[20221213 18:51:46 @agent_ppo2.py:185][0m |           0.0007 |           0.0002 |          20.9458 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0028 |           0.0002 |          20.9413 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0042 |           0.0002 |          20.9391 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0045 |           0.0002 |          20.9387 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0045 |           0.0002 |          20.9380 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0037 |           0.0002 |          20.9301 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0044 |           0.0002 |          20.9426 |
[32m[20221213 18:51:47 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:47 @agent_ppo2.py:143][0m Total time:       3.24 min
[32m[20221213 18:51:47 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 18:51:47 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 18:51:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0018 |           0.0002 |          21.0825 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |          -0.0012 |           0.0001 |          21.0973 |
[32m[20221213 18:51:47 @agent_ppo2.py:185][0m |           0.0021 |           0.0001 |          21.1062 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0025 |           0.0001 |          21.1329 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0030 |           0.0001 |          21.1556 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0044 |           0.0001 |          21.1651 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0047 |           0.0001 |          21.2002 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0047 |           0.0001 |          21.2006 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          21.2150 |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          21.2205 |
[32m[20221213 18:51:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:48 @agent_ppo2.py:143][0m Total time:       3.26 min
[32m[20221213 18:51:48 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 18:51:48 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 18:51:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:48 @agent_ppo2.py:185][0m |          -0.0075 |           1.9827 |          21.1123 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |           0.0011 |           1.7234 |          21.1269 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |           0.0001 |           1.7049 |          21.1134 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |          -0.0002 |           1.7020 |          21.1023 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |           0.0005 |           1.7024 |          21.0755 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |           0.0001 |           1.6975 |          21.0727 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |          -0.0005 |           1.6961 |          21.0495 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |           0.0000 |           1.6925 |          21.0277 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |          -0.0004 |           1.6952 |          21.0323 |
[32m[20221213 18:51:49 @agent_ppo2.py:185][0m |          -0.0003 |           1.6937 |          21.0220 |
[32m[20221213 18:51:49 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 18.20
[32m[20221213 18:51:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 18:51:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:49 @agent_ppo2.py:143][0m Total time:       3.28 min
[32m[20221213 18:51:49 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 18:51:49 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 18:51:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |           0.0023 |           0.0538 |          21.1371 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |           0.0025 |           0.0291 |          21.1449 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |           0.0069 |           0.0267 |          21.1697 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |          -0.0056 |           0.0263 |          21.1913 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |          -0.0079 |           0.0265 |          21.1800 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |          -0.0027 |           0.0262 |          21.1899 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |          -0.0081 |           0.0264 |          21.1985 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |          -0.0061 |           0.0263 |          21.2230 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |          -0.0075 |           0.0265 |          21.2110 |
[32m[20221213 18:51:50 @agent_ppo2.py:185][0m |           0.0007 |           0.0262 |          21.2143 |
[32m[20221213 18:51:50 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:50 @agent_ppo2.py:143][0m Total time:       3.30 min
[32m[20221213 18:51:50 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 18:51:50 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 18:51:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0029 |           0.0086 |          21.1417 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0043 |           0.0056 |          21.1607 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0012 |           0.0045 |          21.1584 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0031 |           0.0041 |          21.1516 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0007 |           0.0039 |          21.1634 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0074 |           0.0038 |          21.1775 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |           0.0048 |           0.0037 |          21.1888 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0092 |           0.0037 |          21.1847 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |          -0.0001 |           0.0037 |          21.1944 |
[32m[20221213 18:51:51 @agent_ppo2.py:185][0m |           0.0007 |           0.0037 |          21.1998 |
[32m[20221213 18:51:51 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:51 @agent_ppo2.py:143][0m Total time:       3.32 min
[32m[20221213 18:51:51 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 18:51:51 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 18:51:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |           0.0053 |           0.0025 |          21.0713 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |           0.0034 |           0.0017 |          21.0449 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |          -0.0042 |           0.0016 |          21.0585 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |          -0.0025 |           0.0016 |          21.0619 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |          -0.0119 |           0.0016 |          21.0644 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |           0.0003 |           0.0016 |          21.0640 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |           0.0004 |           0.0016 |          21.0472 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |          -0.0041 |           0.0016 |          21.0437 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |          -0.0011 |           0.0016 |          21.0475 |
[32m[20221213 18:51:52 @agent_ppo2.py:185][0m |           0.0036 |           0.0016 |          21.0401 |
[32m[20221213 18:51:52 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.00
[32m[20221213 18:51:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 138.00
[32m[20221213 18:51:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 138.00
[32m[20221213 18:51:53 @agent_ppo2.py:143][0m Total time:       3.34 min
[32m[20221213 18:51:53 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 18:51:53 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 18:51:53 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:51:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |          -0.0014 |           0.0009 |          21.1348 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |           0.0076 |           0.0007 |          21.1349 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |           0.0021 |           0.0007 |          21.1253 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |          -0.0020 |           0.0007 |          21.1130 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |           0.0060 |           0.0007 |          21.1000 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |           0.0007 |           0.0007 |          21.0960 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |          -0.0006 |           0.0007 |          21.0947 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |          -0.0039 |           0.0007 |          21.0868 |
[32m[20221213 18:51:53 @agent_ppo2.py:185][0m |          -0.0075 |           0.0007 |          21.0746 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |          -0.0049 |           0.0007 |          21.0863 |
[32m[20221213 18:51:54 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:51:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:54 @agent_ppo2.py:143][0m Total time:       3.35 min
[32m[20221213 18:51:54 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 18:51:54 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 18:51:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |          -0.0014 |           0.0005 |          21.1324 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |          -0.0028 |           0.0005 |          21.1119 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |           0.0008 |           0.0005 |          21.1285 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |           0.0065 |           0.0005 |          21.1352 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |          -0.0029 |           0.0004 |          21.1528 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |           0.0020 |           0.0004 |          21.1529 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |          -0.0053 |           0.0004 |          21.1477 |
[32m[20221213 18:51:54 @agent_ppo2.py:185][0m |          -0.0066 |           0.0004 |          21.1590 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0023 |           0.0004 |          21.1637 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0013 |           0.0004 |          21.1608 |
[32m[20221213 18:51:55 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:55 @agent_ppo2.py:143][0m Total time:       3.37 min
[32m[20221213 18:51:55 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 18:51:55 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 18:51:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0017 |           0.0004 |          21.0511 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0044 |           0.0004 |          21.0482 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |           0.0012 |           0.0004 |          21.0492 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0053 |           0.0004 |          21.0611 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |           0.0024 |           0.0004 |          21.0560 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0051 |           0.0004 |          21.0507 |
[32m[20221213 18:51:55 @agent_ppo2.py:185][0m |          -0.0082 |           0.0004 |          21.0609 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |          -0.0041 |           0.0004 |          21.0585 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |           0.0009 |           0.0004 |          21.0586 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |          -0.0017 |           0.0004 |          21.0652 |
[32m[20221213 18:51:56 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:51:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:56 @agent_ppo2.py:143][0m Total time:       3.39 min
[32m[20221213 18:51:56 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 18:51:56 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 18:51:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:51:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |           0.0016 |           0.0004 |          21.0610 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |          -0.0032 |           0.0003 |          21.0693 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |          -0.0007 |           0.0003 |          21.0746 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |          -0.0049 |           0.0003 |          21.0876 |
[32m[20221213 18:51:56 @agent_ppo2.py:185][0m |          -0.0032 |           0.0003 |          21.1016 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0031 |           0.0003 |          21.1068 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0043 |           0.0003 |          21.1208 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0061 |           0.0003 |          21.1238 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0052 |           0.0003 |          21.1317 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0055 |           0.0003 |          21.1514 |
[32m[20221213 18:51:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:51:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:57 @agent_ppo2.py:143][0m Total time:       3.41 min
[32m[20221213 18:51:57 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 18:51:57 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 18:51:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0026 |           0.0003 |          21.0017 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0034 |           0.0003 |          21.0142 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0054 |           0.0003 |          21.0211 |
[32m[20221213 18:51:57 @agent_ppo2.py:185][0m |          -0.0058 |           0.0003 |          21.0211 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0067 |           0.0003 |          21.0336 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0036 |           0.0003 |          21.0278 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0058 |           0.0003 |          21.0410 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0075 |           0.0003 |          21.0389 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0044 |           0.0003 |          21.0348 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0056 |           0.0003 |          21.0379 |
[32m[20221213 18:51:58 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:51:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:58 @agent_ppo2.py:143][0m Total time:       3.43 min
[32m[20221213 18:51:58 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 18:51:58 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 18:51:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0006 |           0.0002 |          21.1049 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |          -0.0012 |           0.0002 |          21.1031 |
[32m[20221213 18:51:58 @agent_ppo2.py:185][0m |           0.0068 |           0.0002 |          21.0789 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0015 |           0.0002 |          21.1003 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0003 |           0.0002 |          21.0965 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0017 |           0.0002 |          21.0934 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0002 |           0.0002 |          21.0873 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0015 |           0.0002 |          21.0924 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0024 |           0.0002 |          21.1060 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0007 |           0.0002 |          21.1003 |
[32m[20221213 18:51:59 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:51:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:51:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:51:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:51:59 @agent_ppo2.py:143][0m Total time:       3.44 min
[32m[20221213 18:51:59 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 18:51:59 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 18:51:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:51:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0015 |           0.0002 |          21.0907 |
[32m[20221213 18:51:59 @agent_ppo2.py:185][0m |          -0.0027 |           0.0002 |          21.0700 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0050 |           0.0002 |          21.0712 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0047 |           0.0002 |          21.0624 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0047 |           0.0002 |          21.0392 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0005 |           0.0001 |          21.0537 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |           0.0086 |           0.0001 |          21.0399 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0042 |           0.0001 |          21.0282 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0009 |           0.0001 |          21.0341 |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |          -0.0045 |           0.0001 |          21.0301 |
[32m[20221213 18:52:00 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:00 @agent_ppo2.py:143][0m Total time:       3.46 min
[32m[20221213 18:52:00 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 18:52:00 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 18:52:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:00 @agent_ppo2.py:185][0m |           0.0014 |           0.0001 |          21.1171 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0024 |           0.0001 |          21.1075 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0018 |           0.0001 |          21.1084 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0003 |           0.0001 |          21.1056 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0034 |           0.0001 |          21.1090 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0042 |           0.0001 |          21.0958 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0045 |           0.0001 |          21.1007 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0040 |           0.0001 |          21.0927 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |          -0.0046 |           0.0001 |          21.0826 |
[32m[20221213 18:52:01 @agent_ppo2.py:185][0m |           0.0080 |           0.0001 |          21.1022 |
[32m[20221213 18:52:01 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:01 @agent_ppo2.py:143][0m Total time:       3.48 min
[32m[20221213 18:52:01 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 18:52:01 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 18:52:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:52:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |           0.0017 |           0.0001 |          21.1409 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |           0.0094 |           0.0001 |          21.1139 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0031 |           0.0001 |          21.1114 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          21.1037 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          21.1148 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          21.1060 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          21.1068 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          21.0980 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1096 |
[32m[20221213 18:52:02 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1090 |
[32m[20221213 18:52:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:52:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:02 @agent_ppo2.py:143][0m Total time:       3.50 min
[32m[20221213 18:52:02 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 18:52:02 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 18:52:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          21.0644 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          21.0932 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          21.0820 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          21.0993 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |           0.0071 |           0.0000 |          21.1000 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          21.1104 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          21.1539 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          21.1441 |
[32m[20221213 18:52:03 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          21.1464 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          21.1595 |
[32m[20221213 18:52:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:52:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:04 @agent_ppo2.py:143][0m Total time:       3.52 min
[32m[20221213 18:52:04 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 18:52:04 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 18:52:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.1240 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          21.1258 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          21.1251 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          21.1357 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |           0.0047 |           0.0000 |          21.1397 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |           0.0066 |           0.0000 |          21.1522 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |           0.0113 |           0.0000 |          21.1655 |
[32m[20221213 18:52:04 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1714 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          21.1783 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1880 |
[32m[20221213 18:52:05 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:05 @agent_ppo2.py:143][0m Total time:       3.54 min
[32m[20221213 18:52:05 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 18:52:05 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 18:52:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          21.1488 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.1536 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          21.1828 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          21.2104 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          21.2213 |
[32m[20221213 18:52:05 @agent_ppo2.py:185][0m |           0.0056 |           0.0000 |          21.2377 |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          21.2648 |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.2661 |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          21.2877 |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          21.3127 |
[32m[20221213 18:52:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:52:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:06 @agent_ppo2.py:143][0m Total time:       3.56 min
[32m[20221213 18:52:06 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 18:52:06 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 18:52:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:52:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          21.1098 |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          21.1049 |
[32m[20221213 18:52:06 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          21.1034 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |           0.0013 |           0.0000 |          21.1110 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          21.1098 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0076 |           0.0000 |          21.1216 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0074 |           0.0000 |          21.1305 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0078 |           0.0000 |          21.1326 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0079 |           0.0000 |          21.1491 |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0081 |           0.0000 |          21.1482 |
[32m[20221213 18:52:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:52:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:07 @agent_ppo2.py:143][0m Total time:       3.58 min
[32m[20221213 18:52:07 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 18:52:07 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 18:52:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 18:52:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:07 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          21.1534 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |           0.0002 |           0.0000 |          21.1565 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          21.1638 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          21.1656 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |           0.0047 |           0.0000 |          21.1625 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          21.1789 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          21.1644 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          21.1713 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |           0.0053 |           0.0000 |          21.1577 |
[32m[20221213 18:52:08 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          21.1448 |
[32m[20221213 18:52:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:08 @agent_ppo2.py:143][0m Total time:       3.60 min
[32m[20221213 18:52:08 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 18:52:08 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 18:52:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0712 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          21.0941 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          21.0988 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          21.1199 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          21.1164 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          21.1321 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          21.1391 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          21.1651 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          21.1602 |
[32m[20221213 18:52:09 @agent_ppo2.py:185][0m |           0.0158 |           0.0000 |          21.1779 |
[32m[20221213 18:52:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:09 @agent_ppo2.py:143][0m Total time:       3.62 min
[32m[20221213 18:52:09 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 18:52:09 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 18:52:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:52:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          21.1529 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          21.1537 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          21.1501 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          21.1493 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1479 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |           0.0043 |           0.0000 |          21.1539 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          21.1584 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.1704 |
[32m[20221213 18:52:10 @agent_ppo2.py:185][0m |           0.0068 |           0.0000 |          21.1560 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          21.1556 |
[32m[20221213 18:52:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:52:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:11 @agent_ppo2.py:143][0m Total time:       3.64 min
[32m[20221213 18:52:11 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 18:52:11 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 18:52:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          21.1067 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          21.0990 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1071 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.1011 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          21.1025 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          21.1008 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |           0.0006 |           0.0000 |          21.0911 |
[32m[20221213 18:52:11 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0869 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          21.0971 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          21.0853 |
[32m[20221213 18:52:12 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:12 @agent_ppo2.py:143][0m Total time:       3.66 min
[32m[20221213 18:52:12 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 18:52:12 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 18:52:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          21.1234 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |           0.0065 |           0.0000 |          21.1096 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |           0.0057 |           0.0000 |          21.0723 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.0926 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          21.0973 |
[32m[20221213 18:52:12 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          21.1072 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          21.1079 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.1087 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          21.1079 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          21.1109 |
[32m[20221213 18:52:13 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:13 @agent_ppo2.py:143][0m Total time:       3.67 min
[32m[20221213 18:52:13 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 18:52:13 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 18:52:13 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:52:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |           0.0078 |           0.0000 |          21.2404 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          21.2449 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          21.2750 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          21.2893 |
[32m[20221213 18:52:13 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          21.2876 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          21.3079 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          21.3278 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          21.3255 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |           0.0058 |           0.0000 |          21.3330 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          21.3488 |
[32m[20221213 18:52:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:14 @agent_ppo2.py:143][0m Total time:       3.69 min
[32m[20221213 18:52:14 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 18:52:14 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 18:52:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          21.0405 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |           0.0072 |           0.0000 |          21.0163 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          21.0262 |
[32m[20221213 18:52:14 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0405 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |           0.0030 |           0.0000 |          21.0334 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |           0.0109 |           0.0000 |          21.0416 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          21.0531 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          21.0569 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.0496 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          21.0469 |
[32m[20221213 18:52:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:15 @agent_ppo2.py:143][0m Total time:       3.71 min
[32m[20221213 18:52:15 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 18:52:15 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 18:52:15 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |           0.0034 |           0.0000 |          21.0572 |
[32m[20221213 18:52:15 @agent_ppo2.py:185][0m |           0.0067 |           0.0000 |          21.0613 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          21.0191 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          21.0494 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.0311 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          21.0319 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          21.0248 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          21.0369 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          21.0345 |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          21.0336 |
[32m[20221213 18:52:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:16 @agent_ppo2.py:143][0m Total time:       3.73 min
[32m[20221213 18:52:16 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 18:52:16 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 18:52:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:16 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          21.1095 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          21.1041 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |           0.0032 |           0.0000 |          21.0960 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          21.1010 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          21.1143 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          21.1163 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          21.1225 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          21.1224 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          21.1332 |
[32m[20221213 18:52:17 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          21.1505 |
[32m[20221213 18:52:17 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:17 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221213 18:52:17 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 18:52:17 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 18:52:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          21.0306 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          21.0080 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          21.0397 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          21.0318 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |           0.0039 |           0.0000 |          21.0316 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.0429 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0404 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |           0.0022 |           0.0000 |          21.0388 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          21.0391 |
[32m[20221213 18:52:18 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          21.0449 |
[32m[20221213 18:52:18 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:18 @agent_ppo2.py:143][0m Total time:       3.77 min
[32m[20221213 18:52:18 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 18:52:18 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 18:52:19 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |           0.0130 |           0.0000 |          21.0278 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          21.0182 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |           0.0043 |           0.0000 |          21.0419 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          21.0341 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          21.0500 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          21.0718 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          21.0700 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          21.0782 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          21.0743 |
[32m[20221213 18:52:19 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          21.0841 |
[32m[20221213 18:52:19 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:20 @agent_ppo2.py:143][0m Total time:       3.78 min
[32m[20221213 18:52:20 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 18:52:20 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 18:52:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.9782 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.9841 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.9782 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.9893 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.9859 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.9819 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.9934 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          20.9915 |
[32m[20221213 18:52:20 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.9922 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.9955 |
[32m[20221213 18:52:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:21 @agent_ppo2.py:143][0m Total time:       3.80 min
[32m[20221213 18:52:21 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 18:52:21 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 18:52:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          20.9777 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.9677 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |           0.0096 |           0.0000 |          20.9562 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |           0.0011 |           0.0000 |          20.9554 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.9435 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.9402 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.9362 |
[32m[20221213 18:52:21 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          20.9391 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.9394 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.9268 |
[32m[20221213 18:52:22 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:22 @agent_ppo2.py:143][0m Total time:       3.82 min
[32m[20221213 18:52:22 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 18:52:22 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 18:52:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.9512 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.9224 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.8980 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.9080 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.9099 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.9032 |
[32m[20221213 18:52:22 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.9154 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.9209 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.9219 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.9041 |
[32m[20221213 18:52:23 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:23 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221213 18:52:23 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 18:52:23 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 18:52:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.9371 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.9129 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.9004 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |           0.0050 |           0.0000 |          20.8950 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.8817 |
[32m[20221213 18:52:23 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8811 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.8754 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.8742 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |           0.0103 |           0.0000 |          20.8725 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0000 |           0.0000 |          20.8482 |
[32m[20221213 18:52:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:24 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221213 18:52:24 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 18:52:24 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 18:52:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          21.0694 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          21.0810 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          21.0719 |
[32m[20221213 18:52:24 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          21.0832 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          21.0823 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          21.0886 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          21.0819 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          21.0894 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          21.0856 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          21.0970 |
[32m[20221213 18:52:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:25 @agent_ppo2.py:143][0m Total time:       3.88 min
[32m[20221213 18:52:25 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 18:52:25 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 18:52:25 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:52:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |           0.0023 |           0.0000 |          20.9414 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.9359 |
[32m[20221213 18:52:25 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.9296 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          20.9235 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.9144 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8965 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.8881 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.8855 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |           0.0019 |           0.0000 |          20.8875 |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |           0.0064 |           0.0000 |          20.8831 |
[32m[20221213 18:52:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:26 @agent_ppo2.py:143][0m Total time:       3.90 min
[32m[20221213 18:52:26 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 18:52:26 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 18:52:26 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:26 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.9712 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.9705 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.9688 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.9762 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.9870 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.9880 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.9907 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.9980 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.9988 |
[32m[20221213 18:52:27 @agent_ppo2.py:185][0m |           0.0085 |           0.0000 |          21.0094 |
[32m[20221213 18:52:27 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:27 @agent_ppo2.py:143][0m Total time:       3.91 min
[32m[20221213 18:52:27 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 18:52:27 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 18:52:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.8845 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |           0.0021 |           0.0000 |          20.8783 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.8888 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.8922 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.9041 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.9070 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.9145 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.9024 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.9193 |
[32m[20221213 18:52:28 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.9177 |
[32m[20221213 18:52:28 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:28 @agent_ppo2.py:143][0m Total time:       3.93 min
[32m[20221213 18:52:28 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 18:52:28 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 18:52:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.7666 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |           0.0049 |           0.0000 |          20.7669 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7626 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.7624 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |           0.0071 |           0.0000 |          20.7592 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.7368 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.7488 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.7479 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.7604 |
[32m[20221213 18:52:29 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.7571 |
[32m[20221213 18:52:29 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:29 @agent_ppo2.py:143][0m Total time:       3.95 min
[32m[20221213 18:52:29 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 18:52:29 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 18:52:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:52:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.8815 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8912 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |           0.0027 |           0.0000 |          20.8899 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.9133 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.9180 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.9159 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.9427 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |           0.0143 |           0.0000 |          20.9384 |
[32m[20221213 18:52:30 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.9261 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.9604 |
[32m[20221213 18:52:31 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:31 @agent_ppo2.py:143][0m Total time:       3.97 min
[32m[20221213 18:52:31 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 18:52:31 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 18:52:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |           0.0035 |           0.0000 |          20.6811 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |           0.0096 |           0.0000 |          20.6925 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |           0.0030 |           0.0000 |          20.6984 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7092 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.7166 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.7169 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.7249 |
[32m[20221213 18:52:31 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7259 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.7307 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.7357 |
[32m[20221213 18:52:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:32 @agent_ppo2.py:143][0m Total time:       3.99 min
[32m[20221213 18:52:32 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 18:52:32 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 18:52:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |           0.0135 |           0.0000 |          20.6976 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.6845 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.6988 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.6874 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.7054 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.6907 |
[32m[20221213 18:52:32 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.6970 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |           0.0018 |           0.0000 |          20.7049 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.7009 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.7078 |
[32m[20221213 18:52:33 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 29.00
[32m[20221213 18:52:33 @agent_ppo2.py:143][0m Total time:       4.01 min
[32m[20221213 18:52:33 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 18:52:33 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 18:52:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.6372 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.6306 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.6200 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.6250 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.6330 |
[32m[20221213 18:52:33 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.6197 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.6144 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.6085 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.6058 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.6012 |
[32m[20221213 18:52:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:34 @agent_ppo2.py:143][0m Total time:       4.03 min
[32m[20221213 18:52:34 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 18:52:34 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 18:52:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.7382 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.7220 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.7109 |
[32m[20221213 18:52:34 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.6987 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.6907 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.6968 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.6842 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.6786 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.6721 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.6680 |
[32m[20221213 18:52:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:35 @agent_ppo2.py:143][0m Total time:       4.04 min
[32m[20221213 18:52:35 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 18:52:35 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 18:52:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |           0.0031 |           0.0000 |          20.6447 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.6351 |
[32m[20221213 18:52:35 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.6329 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          20.6350 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.6156 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |           0.0074 |           0.0000 |          20.6298 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          20.6300 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          20.6369 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.6340 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0061 |           0.0000 |          20.6199 |
[32m[20221213 18:52:36 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:36 @agent_ppo2.py:143][0m Total time:       4.06 min
[32m[20221213 18:52:36 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 18:52:36 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 18:52:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.7283 |
[32m[20221213 18:52:36 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.7121 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.6994 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.6931 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.6857 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.6790 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.6645 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.6573 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |           0.0069 |           0.0000 |          20.6599 |
[32m[20221213 18:52:37 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.6401 |
[32m[20221213 18:52:37 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:37 @agent_ppo2.py:143][0m Total time:       4.08 min
[32m[20221213 18:52:37 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 18:52:37 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 18:52:37 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0026 |           0.0473 |          20.9361 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0011 |           0.0460 |          20.9342 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0005 |           0.0459 |          20.9338 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0007 |           0.0460 |          20.9269 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0002 |           0.0458 |          20.9209 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0003 |           0.0458 |          20.9170 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0002 |           0.0458 |          20.9126 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0006 |           0.0457 |          20.9113 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0008 |           0.0457 |          20.9036 |
[32m[20221213 18:52:38 @agent_ppo2.py:185][0m |           0.0005 |           0.0458 |          20.8978 |
[32m[20221213 18:52:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.00
[32m[20221213 18:52:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.00
[32m[20221213 18:52:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:38 @agent_ppo2.py:143][0m Total time:       4.10 min
[32m[20221213 18:52:38 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 18:52:38 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 18:52:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0016 |           0.0007 |          20.7789 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0007 |           0.0005 |          20.7817 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0093 |           0.0005 |          20.7693 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0012 |           0.0005 |          20.7673 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0040 |           0.0005 |          20.7658 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0023 |           0.0005 |          20.7618 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0045 |           0.0005 |          20.7621 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0079 |           0.0005 |          20.7704 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0039 |           0.0005 |          20.7718 |
[32m[20221213 18:52:39 @agent_ppo2.py:185][0m |          -0.0024 |           0.0005 |          20.7670 |
[32m[20221213 18:52:39 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:39 @agent_ppo2.py:143][0m Total time:       4.12 min
[32m[20221213 18:52:39 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 18:52:39 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 18:52:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0050 |           0.0002 |          20.7597 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          20.7446 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0075 |           0.0001 |          20.7386 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0004 |           0.0001 |          20.7364 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |           0.0020 |           0.0001 |          20.7233 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0032 |           0.0001 |          20.7259 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0026 |           0.0001 |          20.7200 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |           0.0042 |           0.0001 |          20.7169 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0041 |           0.0001 |          20.7019 |
[32m[20221213 18:52:40 @agent_ppo2.py:185][0m |          -0.0045 |           0.0001 |          20.7007 |
[32m[20221213 18:52:40 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:41 @agent_ppo2.py:143][0m Total time:       4.14 min
[32m[20221213 18:52:41 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 18:52:41 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 18:52:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |           0.0010 |           0.0001 |          20.9624 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |          -0.0129 |           0.0001 |          20.9413 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |           0.0046 |           0.0001 |          20.9542 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |          -0.0001 |           0.0000 |          20.9682 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.9686 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |          -0.0093 |           0.0000 |          20.9632 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |           0.0053 |           0.0000 |          20.9685 |
[32m[20221213 18:52:41 @agent_ppo2.py:185][0m |          -0.0082 |           0.0000 |          20.9586 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.9861 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.9938 |
[32m[20221213 18:52:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:52:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:42 @agent_ppo2.py:143][0m Total time:       4.16 min
[32m[20221213 18:52:42 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 18:52:42 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 18:52:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.8276 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.8063 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.8028 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.8040 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.8103 |
[32m[20221213 18:52:42 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.8022 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          20.8057 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.8059 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.8230 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8228 |
[32m[20221213 18:52:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:52:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:43 @agent_ppo2.py:143][0m Total time:       4.17 min
[32m[20221213 18:52:43 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 18:52:43 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 18:52:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.8886 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.8914 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |           0.0046 |           0.0000 |          20.8965 |
[32m[20221213 18:52:43 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.9156 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.9077 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.9314 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.9185 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.9258 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.9263 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.9432 |
[32m[20221213 18:52:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:52:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:44 @agent_ppo2.py:143][0m Total time:       4.19 min
[32m[20221213 18:52:44 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 18:52:44 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 18:52:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:52:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.8475 |
[32m[20221213 18:52:44 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8448 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.8470 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.8465 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.8522 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.8598 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |           0.0078 |           0.0000 |          20.8548 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.8557 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.8609 |
[32m[20221213 18:52:45 @agent_ppo2.py:185][0m |          -0.0064 |           0.0000 |          20.8691 |
[32m[20221213 18:52:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:52:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:45 @agent_ppo2.py:143][0m Total time:       4.21 min
[32m[20221213 18:52:45 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 18:52:45 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 18:52:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.5818 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.5506 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          20.5572 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0073 |           0.0000 |          20.5737 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          20.5642 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |           0.0062 |           0.0000 |          20.5655 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          20.5742 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0079 |           0.0000 |          20.5735 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.5816 |
[32m[20221213 18:52:46 @agent_ppo2.py:185][0m |          -0.0076 |           0.0000 |          20.5890 |
[32m[20221213 18:52:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:52:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:46 @agent_ppo2.py:143][0m Total time:       4.23 min
[32m[20221213 18:52:46 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 18:52:46 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 18:52:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.5903 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.5952 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.5835 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.5886 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |           0.0035 |           0.0000 |          20.5956 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.6074 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.6040 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.6034 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.5990 |
[32m[20221213 18:52:47 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.6258 |
[32m[20221213 18:52:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:47 @agent_ppo2.py:143][0m Total time:       4.25 min
[32m[20221213 18:52:47 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 18:52:47 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 18:52:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.5622 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.5663 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.5639 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.5633 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.5665 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |           0.0081 |           0.0000 |          20.5692 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.5764 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.5620 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |           0.0078 |           0.0000 |          20.5721 |
[32m[20221213 18:52:48 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          20.5678 |
[32m[20221213 18:52:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:49 @agent_ppo2.py:143][0m Total time:       4.27 min
[32m[20221213 18:52:49 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 18:52:49 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 18:52:49 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.4613 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.4327 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |           0.0042 |           0.0000 |          20.4265 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.4044 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.4215 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.4171 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.4375 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.4243 |
[32m[20221213 18:52:49 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.4235 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0061 |           0.0000 |          20.4173 |
[32m[20221213 18:52:50 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:50 @agent_ppo2.py:143][0m Total time:       4.29 min
[32m[20221213 18:52:50 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 18:52:50 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 18:52:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.4829 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |           0.0012 |           0.0000 |          20.4830 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.4864 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.4878 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.4862 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.4848 |
[32m[20221213 18:52:50 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.4870 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.4875 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.4837 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.4888 |
[32m[20221213 18:52:51 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:51 @agent_ppo2.py:143][0m Total time:       4.31 min
[32m[20221213 18:52:51 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 18:52:51 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 18:52:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.3474 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.3440 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          20.3504 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.3418 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.3398 |
[32m[20221213 18:52:51 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.3426 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |           0.0044 |           0.0000 |          20.3373 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.3466 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.3424 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.3515 |
[32m[20221213 18:52:52 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:52 @agent_ppo2.py:143][0m Total time:       4.32 min
[32m[20221213 18:52:52 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 18:52:52 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 18:52:52 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:52:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.4489 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.4415 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.4305 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |           0.0113 |           0.0000 |          20.4162 |
[32m[20221213 18:52:52 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.4163 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.4102 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.4057 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.4124 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.4150 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.4220 |
[32m[20221213 18:52:53 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:53 @agent_ppo2.py:143][0m Total time:       4.34 min
[32m[20221213 18:52:53 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 18:52:53 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 18:52:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.5188 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.5072 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.5167 |
[32m[20221213 18:52:53 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.5074 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.5185 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.5153 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.5125 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.5219 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |           0.0061 |           0.0000 |          20.5193 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.5284 |
[32m[20221213 18:52:54 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:52:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:54 @agent_ppo2.py:143][0m Total time:       4.36 min
[32m[20221213 18:52:54 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 18:52:54 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 18:52:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.4817 |
[32m[20221213 18:52:54 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.4872 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.4882 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          20.4985 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.4964 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.4998 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.5054 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.5055 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.5064 |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.5057 |
[32m[20221213 18:52:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:52:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:55 @agent_ppo2.py:143][0m Total time:       4.38 min
[32m[20221213 18:52:55 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 18:52:55 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 18:52:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:55 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.3988 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.3852 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.3650 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.3579 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.3523 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.3425 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.3463 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.3419 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.3413 |
[32m[20221213 18:52:56 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.3454 |
[32m[20221213 18:52:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:56 @agent_ppo2.py:143][0m Total time:       4.40 min
[32m[20221213 18:52:56 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 18:52:56 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 18:52:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |           0.0043 |           0.0000 |          20.4496 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.4351 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          20.4447 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.4452 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.4708 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.4457 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |           0.0046 |           0.0000 |          20.4608 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.4603 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.4617 |
[32m[20221213 18:52:57 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.4577 |
[32m[20221213 18:52:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:52:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:52:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:52:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:57 @agent_ppo2.py:143][0m Total time:       4.42 min
[32m[20221213 18:52:57 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 18:52:57 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 18:52:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |          -0.0081 |           1.0642 |          20.2537 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0020 |           0.9752 |          20.2417 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0014 |           0.9741 |          20.2370 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0015 |           0.9307 |          20.2303 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |          -0.0047 |           0.8755 |          20.2177 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0005 |           0.8586 |          20.2069 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0004 |           0.8575 |          20.1976 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0002 |           0.8599 |          20.1892 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0004 |           0.8555 |          20.1767 |
[32m[20221213 18:52:58 @agent_ppo2.py:185][0m |           0.0001 |           0.8545 |          20.1718 |
[32m[20221213 18:52:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:52:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.20
[32m[20221213 18:52:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 19.00
[32m[20221213 18:52:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:52:59 @agent_ppo2.py:143][0m Total time:       4.44 min
[32m[20221213 18:52:59 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 18:52:59 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 18:52:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:52:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0000 |           0.0292 |          20.5709 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0027 |           0.0170 |          20.5659 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0049 |           0.0134 |          20.5670 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0006 |           0.0129 |          20.5610 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0012 |           0.0131 |          20.5749 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0038 |           0.0129 |          20.5637 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0072 |           0.0127 |          20.5737 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |          -0.0094 |           0.0130 |          20.5626 |
[32m[20221213 18:52:59 @agent_ppo2.py:185][0m |           0.0029 |           0.0126 |          20.5695 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0063 |           0.0126 |          20.5706 |
[32m[20221213 18:53:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:53:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:00 @agent_ppo2.py:143][0m Total time:       4.45 min
[32m[20221213 18:53:00 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 18:53:00 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 18:53:00 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0058 |           0.0021 |          20.4739 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0030 |           0.0021 |          20.4367 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0072 |           0.0020 |          20.4534 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0002 |           0.0019 |          20.4595 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0017 |           0.0019 |          20.4542 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |           0.0027 |           0.0019 |          20.4639 |
[32m[20221213 18:53:00 @agent_ppo2.py:185][0m |          -0.0063 |           0.0018 |          20.4681 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0026 |           0.0018 |          20.4760 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0073 |           0.0017 |          20.4749 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0022 |           0.0017 |          20.4785 |
[32m[20221213 18:53:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:53:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:01 @agent_ppo2.py:143][0m Total time:       4.47 min
[32m[20221213 18:53:01 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 18:53:01 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 18:53:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:53:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0001 |           0.0023 |          20.3585 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0008 |           0.0020 |          20.3491 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |           0.0033 |           0.0019 |          20.3385 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0007 |           0.0019 |          20.3448 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0016 |           0.0018 |          20.3352 |
[32m[20221213 18:53:01 @agent_ppo2.py:185][0m |          -0.0045 |           0.0018 |          20.3383 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |          -0.0045 |           0.0017 |          20.3383 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |          -0.0067 |           0.0017 |          20.3304 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |          -0.0054 |           0.0016 |          20.3251 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |          -0.0025 |           0.0016 |          20.3291 |
[32m[20221213 18:53:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:53:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:02 @agent_ppo2.py:143][0m Total time:       4.49 min
[32m[20221213 18:53:02 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 18:53:02 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 18:53:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |           0.0026 |           0.0012 |          20.3417 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |           0.0004 |           0.0011 |          20.3471 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |           0.0047 |           0.0011 |          20.3337 |
[32m[20221213 18:53:02 @agent_ppo2.py:185][0m |           0.0000 |           0.0010 |          20.3252 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |          -0.0061 |           0.0010 |          20.3135 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |          -0.0040 |           0.0010 |          20.2996 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |           0.0020 |           0.0009 |          20.2965 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |           0.0017 |           0.0009 |          20.2891 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |          -0.0044 |           0.0009 |          20.2780 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |          -0.0110 |           0.0008 |          20.2724 |
[32m[20221213 18:53:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:53:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:03 @agent_ppo2.py:143][0m Total time:       4.51 min
[32m[20221213 18:53:03 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 18:53:03 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 18:53:03 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |          -0.0084 |           0.0009 |          20.5896 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |           0.0041 |           0.0008 |          20.5865 |
[32m[20221213 18:53:03 @agent_ppo2.py:185][0m |          -0.0033 |           0.0008 |          20.5806 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |           0.0019 |           0.0008 |          20.5802 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |          -0.0069 |           0.0007 |          20.5714 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |           0.0027 |           0.0007 |          20.5738 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |          -0.0018 |           0.0007 |          20.5701 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |          -0.0056 |           0.0006 |          20.5631 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |          -0.0025 |           0.0006 |          20.5602 |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |           0.0011 |           0.0006 |          20.5587 |
[32m[20221213 18:53:04 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:04 @agent_ppo2.py:143][0m Total time:       4.53 min
[32m[20221213 18:53:04 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 18:53:04 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 18:53:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:04 @agent_ppo2.py:185][0m |           0.0004 |           0.0006 |          20.5800 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |           0.0014 |           0.0005 |          20.5661 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |          -0.0052 |           0.0005 |          20.5755 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |          -0.0020 |           0.0005 |          20.5700 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |           0.0064 |           0.0005 |          20.5549 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |          -0.0033 |           0.0005 |          20.5584 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |           0.0084 |           0.0004 |          20.5452 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |          -0.0012 |           0.0004 |          20.5499 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |          -0.0036 |           0.0004 |          20.5405 |
[32m[20221213 18:53:05 @agent_ppo2.py:185][0m |           0.0043 |           0.0004 |          20.5444 |
[32m[20221213 18:53:05 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:05 @agent_ppo2.py:143][0m Total time:       4.55 min
[32m[20221213 18:53:05 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 18:53:05 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 18:53:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0001 |           0.0004 |          20.6600 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0024 |           0.0003 |          20.6588 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |           0.0010 |           0.0003 |          20.6526 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0069 |           0.0003 |          20.6397 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0010 |           0.0003 |          20.6273 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |           0.0018 |           0.0003 |          20.6261 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0082 |           0.0003 |          20.6257 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0059 |           0.0003 |          20.6192 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0058 |           0.0002 |          20.6134 |
[32m[20221213 18:53:06 @agent_ppo2.py:185][0m |          -0.0001 |           0.0002 |          20.6133 |
[32m[20221213 18:53:06 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:06 @agent_ppo2.py:143][0m Total time:       4.57 min
[32m[20221213 18:53:06 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 18:53:06 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 18:53:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0044 |           0.0002 |          20.7277 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0071 |           0.0002 |          20.7102 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |           0.0012 |           0.0002 |          20.7098 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0003 |           0.0002 |          20.7035 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0035 |           0.0002 |          20.6993 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0030 |           0.0002 |          20.7023 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0017 |           0.0002 |          20.6984 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |          -0.0025 |           0.0002 |          20.6947 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |           0.0026 |           0.0001 |          20.6920 |
[32m[20221213 18:53:07 @agent_ppo2.py:185][0m |           0.0012 |           0.0001 |          20.6847 |
[32m[20221213 18:53:07 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:07 @agent_ppo2.py:143][0m Total time:       4.58 min
[32m[20221213 18:53:07 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 18:53:07 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 18:53:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0020 |           0.0001 |          20.8003 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0053 |           0.0001 |          20.7854 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0028 |           0.0001 |          20.7982 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |           0.0047 |           0.0001 |          20.8069 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0033 |           0.0001 |          20.7982 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0057 |           0.0001 |          20.8093 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0069 |           0.0001 |          20.8109 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0089 |           0.0001 |          20.8090 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0080 |           0.0001 |          20.8155 |
[32m[20221213 18:53:08 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          20.8145 |
[32m[20221213 18:53:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:53:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:09 @agent_ppo2.py:143][0m Total time:       4.60 min
[32m[20221213 18:53:09 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 18:53:09 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 18:53:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |          -0.0030 |           0.0001 |          20.6757 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |           0.0045 |           0.0001 |          20.6584 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |           0.0017 |           0.0001 |          20.6341 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |           0.0006 |           0.0001 |          20.6177 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.6113 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.6174 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.5977 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |          -0.0079 |           0.0000 |          20.6046 |
[32m[20221213 18:53:09 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          20.5880 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.5829 |
[32m[20221213 18:53:10 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:10 @agent_ppo2.py:143][0m Total time:       4.62 min
[32m[20221213 18:53:10 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 18:53:10 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 18:53:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0008 |           0.0000 |          20.7843 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.7779 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.7954 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.7984 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |           0.0040 |           0.0000 |          20.7933 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.7998 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.8022 |
[32m[20221213 18:53:10 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.7881 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.8085 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.8068 |
[32m[20221213 18:53:11 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:11 @agent_ppo2.py:143][0m Total time:       4.64 min
[32m[20221213 18:53:11 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 18:53:11 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 18:53:11 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |           0.0051 |           0.0000 |          20.7473 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.7405 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.7397 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.7422 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.7388 |
[32m[20221213 18:53:11 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7373 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.7374 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.7344 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          20.7363 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.7303 |
[32m[20221213 18:53:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:12 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 18:53:12 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 18:53:12 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 18:53:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |           0.0034 |           0.0000 |          20.7783 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.7460 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.7424 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |           0.0050 |           0.0000 |          20.7618 |
[32m[20221213 18:53:12 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.7554 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.7587 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.7467 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.7471 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          20.7546 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.7565 |
[32m[20221213 18:53:13 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:13 @agent_ppo2.py:143][0m Total time:       4.68 min
[32m[20221213 18:53:13 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 18:53:13 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 18:53:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.6890 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |           0.0141 |           0.0000 |          20.6712 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.6250 |
[32m[20221213 18:53:13 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.6488 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.6433 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.6565 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.6586 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          20.6576 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |           0.0038 |           0.0000 |          20.6507 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |           0.0034 |           0.0000 |          20.6459 |
[32m[20221213 18:53:14 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 14.00
[32m[20221213 18:53:14 @agent_ppo2.py:143][0m Total time:       4.69 min
[32m[20221213 18:53:14 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 18:53:14 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 18:53:14 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.7384 |
[32m[20221213 18:53:14 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.7384 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.7414 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.7424 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7489 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.7523 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7528 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7514 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.7586 |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.7585 |
[32m[20221213 18:53:15 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:53:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:15 @agent_ppo2.py:143][0m Total time:       4.71 min
[32m[20221213 18:53:15 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 18:53:15 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 18:53:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:15 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.7366 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.7353 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.7242 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.7350 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |           0.0091 |           0.0000 |          20.7421 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |           0.0097 |           0.0000 |          20.7372 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.7451 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.7547 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          20.7558 |
[32m[20221213 18:53:16 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.7483 |
[32m[20221213 18:53:16 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:16 @agent_ppo2.py:143][0m Total time:       4.73 min
[32m[20221213 18:53:16 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 18:53:16 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 18:53:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.6696 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |           0.0041 |           0.0000 |          20.6484 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.6302 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.6258 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.6233 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.6227 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.6204 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.6221 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.6136 |
[32m[20221213 18:53:17 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.6166 |
[32m[20221213 18:53:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:53:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:17 @agent_ppo2.py:143][0m Total time:       4.75 min
[32m[20221213 18:53:17 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 18:53:17 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 18:53:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.6851 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.6682 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |           0.0084 |           0.0000 |          20.6649 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.6487 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.6538 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.6457 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.6413 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.6421 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.6354 |
[32m[20221213 18:53:18 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.6268 |
[32m[20221213 18:53:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:53:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:19 @agent_ppo2.py:143][0m Total time:       4.77 min
[32m[20221213 18:53:19 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 18:53:19 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 18:53:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.6471 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |          20.6270 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |           0.0042 |           0.0000 |          20.6173 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.6132 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.6147 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          20.6083 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.5849 |
[32m[20221213 18:53:19 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.5902 |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          20.5924 |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.5827 |
[32m[20221213 18:53:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:53:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:20 @agent_ppo2.py:143][0m Total time:       4.79 min
[32m[20221213 18:53:20 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 18:53:20 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 18:53:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:53:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.8430 |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          20.8148 |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.8220 |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |           0.0081 |           0.0000 |          20.8512 |
[32m[20221213 18:53:20 @agent_ppo2.py:185][0m |          -0.0059 |           0.0000 |          20.8420 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0070 |           0.0000 |          20.8549 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0075 |           0.0000 |          20.8635 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0077 |           0.0000 |          20.8718 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0078 |           0.0000 |          20.8711 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0079 |           0.0000 |          20.8902 |
[32m[20221213 18:53:21 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 18:53:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:21 @agent_ppo2.py:143][0m Total time:       4.81 min
[32m[20221213 18:53:21 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 18:53:21 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 18:53:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.7582 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          20.7139 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.7039 |
[32m[20221213 18:53:21 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.7062 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.6869 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |           0.0090 |           0.0000 |          20.6653 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.6640 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |           0.0151 |           0.0000 |          20.6622 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          20.6208 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |           0.0058 |           0.0000 |          20.6317 |
[32m[20221213 18:53:22 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:53:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:22 @agent_ppo2.py:143][0m Total time:       4.83 min
[32m[20221213 18:53:22 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 18:53:22 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 18:53:22 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.8845 |
[32m[20221213 18:53:22 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.8881 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.8797 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |           0.0060 |           0.0000 |          20.8918 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          20.8852 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.8936 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0061 |           0.0000 |          20.9005 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          20.8950 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0063 |           0.0000 |          20.8956 |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          20.8968 |
[32m[20221213 18:53:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:23 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221213 18:53:23 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 18:53:23 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 18:53:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:23 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.8386 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          20.8427 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |           0.0084 |           0.0000 |          20.8372 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.8458 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.8550 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.8463 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.8474 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.8505 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.8536 |
[32m[20221213 18:53:24 @agent_ppo2.py:185][0m |           0.0037 |           0.0000 |          20.8451 |
[32m[20221213 18:53:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:53:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:24 @agent_ppo2.py:143][0m Total time:       4.86 min
[32m[20221213 18:53:24 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 18:53:24 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 18:53:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          20.6995 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |           0.0076 |           0.0000 |          20.7081 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.7023 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.7085 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.7026 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.7094 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0061 |           0.0000 |          20.6962 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |           0.0054 |           0.0000 |          20.7021 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          20.7186 |
[32m[20221213 18:53:25 @agent_ppo2.py:185][0m |          -0.0068 |           0.0000 |          20.7173 |
[32m[20221213 18:53:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:53:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:26 @agent_ppo2.py:143][0m Total time:       4.88 min
[32m[20221213 18:53:26 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 18:53:26 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 18:53:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 18:53:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |          -0.0016 |           0.0000 |          20.6700 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |           0.0007 |           0.0000 |          20.6504 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.6560 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.6510 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |          -0.0013 |           0.0000 |          20.6527 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |           0.0020 |           0.0000 |          20.6530 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.6451 |
[32m[20221213 18:53:26 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.6518 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.6546 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.6477 |
[32m[20221213 18:53:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:53:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:27 @agent_ppo2.py:143][0m Total time:       4.90 min
[32m[20221213 18:53:27 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 18:53:27 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 18:53:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:53:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.5621 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.5582 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.5477 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.5525 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.5498 |
[32m[20221213 18:53:27 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.5547 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          20.5513 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.5530 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.5515 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |           0.0035 |           0.0000 |          20.5556 |
[32m[20221213 18:53:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:53:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:28 @agent_ppo2.py:143][0m Total time:       4.92 min
[32m[20221213 18:53:28 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 18:53:28 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 18:53:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.5605 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.5597 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.5644 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.5738 |
[32m[20221213 18:53:28 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.5784 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.5753 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0050 |           0.0000 |          20.5769 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.5798 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          20.5849 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.5859 |
[32m[20221213 18:53:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:53:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:29 @agent_ppo2.py:143][0m Total time:       4.94 min
[32m[20221213 18:53:29 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 18:53:29 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 18:53:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.4578 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.4556 |
[32m[20221213 18:53:29 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          20.4515 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |          20.4535 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0073 |           0.0000 |          20.4493 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0075 |           0.0000 |          20.4532 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0077 |           0.0000 |          20.4519 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          20.4575 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0079 |           0.0000 |          20.4515 |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |          -0.0079 |           0.0000 |          20.4551 |
[32m[20221213 18:53:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:53:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:30 @agent_ppo2.py:143][0m Total time:       4.96 min
[32m[20221213 18:53:30 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 18:53:30 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 18:53:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:53:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:30 @agent_ppo2.py:185][0m |           0.0034 |           0.0000 |          20.4568 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |           0.0100 |           0.0000 |          20.4367 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |          20.4342 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.4379 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |           0.0003 |           0.0000 |          20.4393 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |           0.0094 |           0.0000 |          20.4393 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.4413 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.4421 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.4484 |
[32m[20221213 18:53:31 @agent_ppo2.py:185][0m |           0.0099 |           0.0000 |          20.4448 |
[32m[20221213 18:53:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:53:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:31 @agent_ppo2.py:143][0m Total time:       4.98 min
[32m[20221213 18:53:31 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 18:53:31 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 18:53:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0002 |           0.0000 |          20.3949 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |           0.0103 |           0.0000 |          20.4038 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.3981 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0015 |           0.0000 |          20.3986 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.3979 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          20.3974 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.3945 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.3896 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.3866 |
[32m[20221213 18:53:32 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.3897 |
[32m[20221213 18:53:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:53:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:33 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221213 18:53:33 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 18:53:33 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 18:53:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |          20.4152 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.3978 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          20.4100 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.4034 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          20.4091 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |           0.0061 |           0.0000 |          20.4042 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.4011 |
[32m[20221213 18:53:33 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          20.4093 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.4043 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.4046 |
[32m[20221213 18:53:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:53:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:34 @agent_ppo2.py:143][0m Total time:       5.02 min
[32m[20221213 18:53:34 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 18:53:34 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 18:53:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |           0.0032 |           0.0000 |          20.4495 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.4581 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          20.4533 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          20.4483 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.4531 |
[32m[20221213 18:53:34 @agent_ppo2.py:185][0m |          -0.0007 |           0.0000 |          20.4569 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.4614 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.4701 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.4618 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.4703 |
[32m[20221213 18:53:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:53:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:35 @agent_ppo2.py:143][0m Total time:       5.04 min
[32m[20221213 18:53:35 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 18:53:35 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 18:53:35 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:53:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          20.2413 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          20.2263 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |           0.0177 |           0.0000 |          20.2353 |
[32m[20221213 18:53:35 @agent_ppo2.py:185][0m |          -0.0025 |           0.0000 |          20.2179 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |           0.0029 |           0.0000 |          20.2243 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.2146 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.2187 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |          -0.0006 |           0.0000 |          20.2141 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          20.2145 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |           0.0042 |           0.0000 |          20.2154 |
[32m[20221213 18:53:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:53:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:36 @agent_ppo2.py:143][0m Total time:       5.06 min
[32m[20221213 18:53:36 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 18:53:36 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 18:53:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:53:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |          -0.0003 |           0.0000 |          20.3012 |
[32m[20221213 18:53:36 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          20.3047 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.3081 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          20.2992 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          20.3053 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0012 |           0.0000 |          20.2937 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          20.2957 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |           0.0035 |           0.0000 |          20.2952 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.2913 |
[32m[20221213 18:53:37 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          20.2875 |
[32m[20221213 18:53:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:53:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:37 @agent_ppo2.py:143][0m Total time:       5.08 min
[32m[20221213 18:53:37 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 18:53:37 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 18:53:37 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |           0.0087 |           0.0000 |          20.1280 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.1221 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          20.1135 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.1235 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          20.1219 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0005 |           0.0000 |          20.1276 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          20.1212 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          20.1270 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          20.1093 |
[32m[20221213 18:53:38 @agent_ppo2.py:185][0m |          -0.0024 |           0.0000 |          20.1273 |
[32m[20221213 18:53:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:53:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:39 @agent_ppo2.py:143][0m Total time:       5.10 min
[32m[20221213 18:53:39 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 18:53:39 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 18:53:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0010 |           0.0000 |          20.2797 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          20.2748 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          20.2609 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          20.2701 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |           0.0096 |           0.0000 |          20.2775 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.2647 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |           0.0248 |           0.0000 |          20.2731 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.2959 |
[32m[20221213 18:53:39 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          20.2922 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          20.2848 |
[32m[20221213 18:53:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:53:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:40 @agent_ppo2.py:143][0m Total time:       5.12 min
[32m[20221213 18:53:40 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 18:53:40 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 18:53:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |           0.0004 |           0.0000 |          20.1388 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |          -0.0030 |           0.0000 |          20.1203 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          20.1147 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.1161 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          20.1122 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          20.1120 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |           0.0081 |           0.0000 |          20.1146 |
[32m[20221213 18:53:40 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          20.0959 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |           0.0033 |           0.0000 |          20.1029 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0064 |           0.0000 |          20.1100 |
[32m[20221213 18:53:41 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:41 @agent_ppo2.py:143][0m Total time:       5.14 min
[32m[20221213 18:53:41 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 18:53:41 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 18:53:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          20.1250 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          20.1258 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |          20.1198 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          20.1262 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          20.1290 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          20.1285 |
[32m[20221213 18:53:41 @agent_ppo2.py:185][0m |          -0.0068 |           0.0000 |          20.1314 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |          -0.0057 |           0.0000 |          20.1335 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |          -0.0074 |           0.0000 |          20.1360 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |          -0.0077 |           0.0000 |          20.1322 |
[32m[20221213 18:53:42 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:42 @agent_ppo2.py:143][0m Total time:       5.16 min
[32m[20221213 18:53:42 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 18:53:42 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 18:53:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |           0.0017 |           1.7495 |          19.8565 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |           0.0012 |           1.6556 |          19.8582 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |           0.0007 |           1.6512 |          19.8554 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |           0.0014 |           1.6478 |          19.8531 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |           0.0006 |           1.6399 |          19.8484 |
[32m[20221213 18:53:42 @agent_ppo2.py:185][0m |           0.0007 |           1.6412 |          19.8454 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |           0.0005 |           1.6351 |          19.8462 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |           0.0010 |           1.6307 |          19.8399 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |           0.0004 |           1.6363 |          19.8374 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |           0.0005 |           1.6306 |          19.8408 |
[32m[20221213 18:53:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 20.40
[32m[20221213 18:53:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 18:53:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:43 @agent_ppo2.py:143][0m Total time:       5.17 min
[32m[20221213 18:53:43 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 18:53:43 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 18:53:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |          -0.0069 |           0.0346 |          19.8468 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |          -0.0007 |           0.0225 |          19.8266 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |          -0.0033 |           0.0167 |          19.8307 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |          -0.0018 |           0.0136 |          19.8249 |
[32m[20221213 18:53:43 @agent_ppo2.py:185][0m |          -0.0076 |           0.0116 |          19.8226 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |          -0.0053 |           0.0105 |          19.8188 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |          -0.0023 |           0.0096 |          19.8155 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |          -0.0030 |           0.0089 |          19.8201 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |          -0.0040 |           0.0085 |          19.8139 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |           0.0038 |           0.0082 |          19.8174 |
[32m[20221213 18:53:44 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:44 @agent_ppo2.py:143][0m Total time:       5.19 min
[32m[20221213 18:53:44 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 18:53:44 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 18:53:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |           0.0093 |           0.0084 |          19.8862 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |           0.0044 |           0.0048 |          19.8740 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |           0.0026 |           0.0042 |          19.8724 |
[32m[20221213 18:53:44 @agent_ppo2.py:185][0m |          -0.0019 |           0.0041 |          19.8781 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0004 |           0.0041 |          19.8702 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0102 |           0.0041 |          19.8792 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0048 |           0.0040 |          19.8744 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0077 |           0.0040 |          19.8805 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0067 |           0.0041 |          19.8727 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0057 |           0.0040 |          19.8771 |
[32m[20221213 18:53:45 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.00
[32m[20221213 18:53:45 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221213 18:53:45 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 18:53:45 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 18:53:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:53:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0005 |           0.0014 |          20.0545 |
[32m[20221213 18:53:45 @agent_ppo2.py:185][0m |          -0.0019 |           0.0009 |          20.0313 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |          -0.0055 |           0.0008 |          20.0367 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |           0.0030 |           0.0007 |          20.0286 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |          -0.0013 |           0.0007 |          20.0255 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |          -0.0026 |           0.0007 |          20.0318 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |          -0.0039 |           0.0007 |          20.0376 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |          -0.0055 |           0.0007 |          20.0382 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |           0.0009 |           0.0007 |          20.0324 |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |          -0.0023 |           0.0007 |          20.0404 |
[32m[20221213 18:53:46 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:46 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221213 18:53:46 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 18:53:46 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 18:53:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:46 @agent_ppo2.py:185][0m |           0.0035 |           0.0004 |          19.8162 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0027 |           0.0004 |          19.7996 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0001 |           0.0004 |          19.7943 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0079 |           0.0004 |          19.7849 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0127 |           0.0004 |          19.7749 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0027 |           0.0004 |          19.7705 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0026 |           0.0004 |          19.7669 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0055 |           0.0004 |          19.7610 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0047 |           0.0004 |          19.7527 |
[32m[20221213 18:53:47 @agent_ppo2.py:185][0m |          -0.0026 |           0.0004 |          19.7586 |
[32m[20221213 18:53:47 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:53:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:47 @agent_ppo2.py:143][0m Total time:       5.25 min
[32m[20221213 18:53:47 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 18:53:47 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 18:53:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |           0.0017 |           0.0004 |          20.0657 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0097 |           0.0004 |          20.0632 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |           0.0007 |           0.0004 |          20.0569 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0090 |           0.0004 |          20.0672 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0037 |           0.0004 |          20.0568 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0069 |           0.0004 |          20.0595 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0108 |           0.0004 |          20.0623 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0119 |           0.0004 |          20.0662 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |           0.0026 |           0.0003 |          20.0587 |
[32m[20221213 18:53:48 @agent_ppo2.py:185][0m |          -0.0043 |           0.0003 |          20.0605 |
[32m[20221213 18:53:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:48 @agent_ppo2.py:143][0m Total time:       5.27 min
[32m[20221213 18:53:48 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 18:53:48 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 18:53:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:53:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |           0.0006 |           0.0003 |          19.8624 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |           0.0034 |           0.0003 |          19.8551 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0053 |           0.0003 |          19.8460 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0047 |           0.0003 |          19.8445 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0051 |           0.0003 |          19.8337 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0018 |           0.0003 |          19.8342 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0029 |           0.0003 |          19.8360 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0031 |           0.0003 |          19.8336 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0023 |           0.0003 |          19.8341 |
[32m[20221213 18:53:49 @agent_ppo2.py:185][0m |          -0.0040 |           0.0003 |          19.8335 |
[32m[20221213 18:53:49 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:53:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:49 @agent_ppo2.py:143][0m Total time:       5.28 min
[32m[20221213 18:53:49 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 18:53:49 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 18:53:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0018 |           0.0003 |          19.7913 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0020 |           0.0003 |          19.7881 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0029 |           0.0003 |          19.7771 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0004 |           0.0003 |          19.7754 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |           0.0016 |           0.0003 |          19.7789 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0042 |           0.0003 |          19.7804 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |           0.0023 |           0.0003 |          19.7791 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0041 |           0.0003 |          19.7822 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0044 |           0.0003 |          19.7829 |
[32m[20221213 18:53:50 @agent_ppo2.py:185][0m |          -0.0045 |           0.0003 |          19.7829 |
[32m[20221213 18:53:50 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 30.00
[32m[20221213 18:53:51 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221213 18:53:51 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 18:53:51 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 18:53:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0024 |           1.6322 |          19.7449 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0015 |           1.5810 |          19.7385 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0020 |           1.5756 |          19.7300 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0013 |           1.5730 |          19.7192 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0012 |           1.5707 |          19.7160 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0013 |           1.5744 |          19.7128 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0011 |           1.5714 |          19.6992 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |           0.0012 |           1.5724 |          19.7087 |
[32m[20221213 18:53:51 @agent_ppo2.py:185][0m |          -0.0060 |           1.6437 |          19.6947 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |           0.0007 |           1.5718 |          19.7003 |
[32m[20221213 18:53:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:53:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 22.00
[32m[20221213 18:53:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 30.00
[32m[20221213 18:53:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:52 @agent_ppo2.py:143][0m Total time:       5.32 min
[32m[20221213 18:53:52 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 18:53:52 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 18:53:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |           0.0003 |           0.0468 |          19.6653 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |           0.0006 |           0.0251 |          19.6614 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |          -0.0002 |           0.0236 |          19.6396 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |           0.0030 |           0.0231 |          19.6374 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |          -0.0098 |           0.0231 |          19.6439 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |           0.0052 |           0.0231 |          19.6416 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |          -0.0028 |           0.0230 |          19.6394 |
[32m[20221213 18:53:52 @agent_ppo2.py:185][0m |           0.0016 |           0.0232 |          19.6370 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |           0.0049 |           0.0232 |          19.6403 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |          -0.0106 |           0.0232 |          19.6399 |
[32m[20221213 18:53:53 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:53 @agent_ppo2.py:143][0m Total time:       5.34 min
[32m[20221213 18:53:53 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 18:53:53 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 18:53:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |          -0.0017 |           0.0035 |          19.6476 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |           0.0003 |           0.0025 |          19.6425 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |          -0.0015 |           0.0020 |          19.6404 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |          -0.0020 |           0.0016 |          19.6316 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |           0.0038 |           0.0014 |          19.6331 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |           0.0007 |           0.0012 |          19.6250 |
[32m[20221213 18:53:53 @agent_ppo2.py:185][0m |           0.0022 |           0.0011 |          19.6266 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |          -0.0004 |           0.0010 |          19.6402 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |          -0.0030 |           0.0010 |          19.6240 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |           0.0017 |           0.0009 |          19.6292 |
[32m[20221213 18:53:54 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:54 @agent_ppo2.py:143][0m Total time:       5.36 min
[32m[20221213 18:53:54 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 18:53:54 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 18:53:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |          -0.0100 |           0.0006 |          19.7312 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |           0.0021 |           0.0005 |          19.7263 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |          -0.0042 |           0.0005 |          19.7240 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |           0.0017 |           0.0005 |          19.7273 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |          -0.0068 |           0.0005 |          19.7152 |
[32m[20221213 18:53:54 @agent_ppo2.py:185][0m |          -0.0050 |           0.0005 |          19.7158 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0046 |           0.0005 |          19.7200 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0041 |           0.0005 |          19.7149 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0005 |           0.0005 |          19.7191 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0017 |           0.0005 |          19.7156 |
[32m[20221213 18:53:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:55 @agent_ppo2.py:143][0m Total time:       5.37 min
[32m[20221213 18:53:55 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 18:53:55 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 18:53:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0054 |           0.0004 |          19.8253 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |           0.0016 |           0.0004 |          19.8018 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0040 |           0.0004 |          19.8047 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0074 |           0.0004 |          19.8071 |
[32m[20221213 18:53:55 @agent_ppo2.py:185][0m |          -0.0085 |           0.0004 |          19.8010 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0058 |           0.0004 |          19.8035 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0042 |           0.0004 |          19.8032 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0101 |           0.0004 |          19.8013 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0039 |           0.0004 |          19.7903 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0072 |           0.0004 |          19.7974 |
[32m[20221213 18:53:56 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:53:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:56 @agent_ppo2.py:143][0m Total time:       5.39 min
[32m[20221213 18:53:56 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 18:53:56 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 18:53:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:53:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0036 |           0.0004 |          19.6044 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |           0.0006 |           0.0004 |          19.5950 |
[32m[20221213 18:53:56 @agent_ppo2.py:185][0m |          -0.0009 |           0.0004 |          19.6018 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0009 |           0.0004 |          19.6043 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0067 |           0.0004 |          19.5937 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |           0.0003 |           0.0004 |          19.6056 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0033 |           0.0004 |          19.6016 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0010 |           0.0004 |          19.5936 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |           0.0023 |           0.0004 |          19.6022 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0008 |           0.0004 |          19.5986 |
[32m[20221213 18:53:57 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:57 @agent_ppo2.py:143][0m Total time:       5.41 min
[32m[20221213 18:53:57 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 18:53:57 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 18:53:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0007 |           0.0004 |          19.6999 |
[32m[20221213 18:53:57 @agent_ppo2.py:185][0m |          -0.0022 |           0.0004 |          19.6885 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |           0.0241 |           0.0003 |          19.6761 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0025 |           0.0003 |          19.6861 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0041 |           0.0003 |          19.6813 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0063 |           0.0003 |          19.6755 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0021 |           0.0003 |          19.6746 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0053 |           0.0003 |          19.6724 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |           0.0032 |           0.0003 |          19.6735 |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0091 |           0.0003 |          19.6740 |
[32m[20221213 18:53:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:53:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:58 @agent_ppo2.py:143][0m Total time:       5.43 min
[32m[20221213 18:53:58 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 18:53:58 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 18:53:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:53:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:53:58 @agent_ppo2.py:185][0m |          -0.0026 |           0.0003 |          19.5452 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0042 |           0.0003 |          19.5364 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0059 |           0.0003 |          19.5310 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0039 |           0.0003 |          19.5366 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |           0.0043 |           0.0003 |          19.5292 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0044 |           0.0003 |          19.5292 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0063 |           0.0003 |          19.5302 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0070 |           0.0003 |          19.5298 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0035 |           0.0003 |          19.5283 |
[32m[20221213 18:53:59 @agent_ppo2.py:185][0m |          -0.0082 |           0.0003 |          19.5277 |
[32m[20221213 18:53:59 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:53:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:53:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:53:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:53:59 @agent_ppo2.py:143][0m Total time:       5.45 min
[32m[20221213 18:53:59 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 18:53:59 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 18:53:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:53:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0030 |           0.0002 |          19.5090 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0054 |           0.0002 |          19.4717 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0034 |           0.0002 |          19.4575 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0055 |           0.0002 |          19.4701 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0041 |           0.0002 |          19.4586 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0067 |           0.0002 |          19.4577 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0071 |           0.0002 |          19.4613 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0047 |           0.0002 |          19.4571 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0062 |           0.0002 |          19.4556 |
[32m[20221213 18:54:00 @agent_ppo2.py:185][0m |          -0.0070 |           0.0002 |          19.4666 |
[32m[20221213 18:54:00 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 18:54:00 @agent_ppo2.py:143][0m Total time:       5.47 min
[32m[20221213 18:54:00 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 18:54:00 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 18:54:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0018 |           0.0002 |          19.5040 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0046 |           0.0002 |          19.4961 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0052 |           0.0002 |          19.4985 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0054 |           0.0002 |          19.5001 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0056 |           0.0002 |          19.4933 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0046 |           0.0002 |          19.4959 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0050 |           0.0001 |          19.4968 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0066 |           0.0001 |          19.4948 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0077 |           0.0001 |          19.4929 |
[32m[20221213 18:54:01 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          19.4940 |
[32m[20221213 18:54:01 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.00
[32m[20221213 18:54:01 @agent_ppo2.py:143][0m Total time:       5.48 min
[32m[20221213 18:54:01 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 18:54:01 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 18:54:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0020 |           0.0001 |          19.3339 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0033 |           0.0001 |          19.3266 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0042 |           0.0001 |          19.3074 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |           0.0042 |           0.0001 |          19.3091 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0051 |           0.0001 |          19.3045 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0058 |           0.0001 |          19.3058 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0053 |           0.0001 |          19.2951 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0004 |           0.0001 |          19.2915 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0058 |           0.0001 |          19.2886 |
[32m[20221213 18:54:02 @agent_ppo2.py:185][0m |          -0.0059 |           0.0001 |          19.2902 |
[32m[20221213 18:54:02 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:03 @agent_ppo2.py:143][0m Total time:       5.50 min
[32m[20221213 18:54:03 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 18:54:03 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 18:54:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |           0.0045 |           0.0001 |          19.6968 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0029 |           0.0001 |          19.6880 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0032 |           0.0001 |          19.6958 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0028 |           0.0001 |          19.6777 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          19.6926 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          19.6930 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |          19.6897 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0000 |           0.0000 |          19.6854 |
[32m[20221213 18:54:03 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |          19.6877 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |           0.0009 |           0.0000 |          19.6890 |
[32m[20221213 18:54:04 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:04 @agent_ppo2.py:143][0m Total time:       5.52 min
[32m[20221213 18:54:04 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 18:54:04 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 18:54:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          19.5343 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |           0.0040 |           0.0000 |          19.5271 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          19.5231 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.5032 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          19.5239 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          19.5180 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          19.5155 |
[32m[20221213 18:54:04 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          19.5140 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |           0.0036 |           0.0000 |          19.5063 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |          19.5071 |
[32m[20221213 18:54:05 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:05 @agent_ppo2.py:143][0m Total time:       5.54 min
[32m[20221213 18:54:05 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 18:54:05 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 18:54:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |          19.5272 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          19.5151 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          19.5036 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          19.5033 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0009 |           0.0000 |          19.4980 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          19.4893 |
[32m[20221213 18:54:05 @agent_ppo2.py:185][0m |           0.0001 |           0.0000 |          19.4906 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          19.4923 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |           0.0060 |           0.0000 |          19.4855 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |          -0.0019 |           0.0000 |          19.4851 |
[32m[20221213 18:54:06 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.00
[32m[20221213 18:54:06 @agent_ppo2.py:143][0m Total time:       5.56 min
[32m[20221213 18:54:06 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 18:54:06 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 18:54:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |          -0.0011 |           0.0000 |          19.4747 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          19.4657 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |           0.0050 |           0.0000 |          19.4655 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |          19.4624 |
[32m[20221213 18:54:06 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          19.4643 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0054 |           0.0000 |          19.4718 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          19.4600 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |           0.0033 |           0.0000 |          19.4685 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          19.4226 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0073 |           0.0000 |          19.4625 |
[32m[20221213 18:54:07 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:07 @agent_ppo2.py:143][0m Total time:       5.57 min
[32m[20221213 18:54:07 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 18:54:07 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 18:54:07 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:54:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          19.6459 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          19.6292 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0026 |           0.0000 |          19.6380 |
[32m[20221213 18:54:07 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          19.6331 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |           0.0049 |           0.0000 |          19.6256 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.6290 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |           0.0015 |           0.0000 |          19.6264 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          19.6294 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          19.6371 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          19.6346 |
[32m[20221213 18:54:08 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:08 @agent_ppo2.py:143][0m Total time:       5.59 min
[32m[20221213 18:54:08 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 18:54:08 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 18:54:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          19.5431 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          19.5354 |
[32m[20221213 18:54:08 @agent_ppo2.py:185][0m |          -0.0004 |           0.0000 |          19.5308 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0035 |           0.0000 |          19.5289 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |          19.5282 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          19.5223 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          19.5227 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          19.5283 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |          19.5169 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          19.5244 |
[32m[20221213 18:54:09 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 18:54:09 @agent_ppo2.py:143][0m Total time:       5.61 min
[32m[20221213 18:54:09 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 18:54:09 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 18:54:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          19.8347 |
[32m[20221213 18:54:09 @agent_ppo2.py:185][0m |          -0.0028 |           0.0000 |          19.8274 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |          -0.0036 |           0.0000 |          19.8196 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |          -0.0037 |           0.0000 |          19.8255 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |          -0.0020 |           0.0000 |          19.8204 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |           0.0014 |           0.0000 |          19.8193 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |           0.0056 |           0.0000 |          19.8110 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |          19.8209 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |          19.8193 |
[32m[20221213 18:54:10 @agent_ppo2.py:185][0m |          -0.0034 |           0.0000 |          19.8201 |
[32m[20221213 18:54:10 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:10 @agent_ppo2.py:143][0m Total time:       5.63 min
[32m[20221213 18:54:10 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 18:54:10 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 18:54:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:54:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          19.8703 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          19.8482 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          19.8451 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0031 |           0.0000 |          19.8437 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          19.8517 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          19.8573 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0058 |           0.0000 |          19.8479 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          19.8496 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          19.8499 |
[32m[20221213 18:54:11 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |          19.8467 |
[32m[20221213 18:54:11 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:54:11 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221213 18:54:11 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 18:54:11 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 18:54:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0023 |           0.0000 |          19.7730 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          19.7629 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          19.7512 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0053 |           0.0000 |          19.7495 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          19.7469 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          19.7434 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0060 |           0.0000 |          19.7408 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |           0.0005 |           0.0000 |          19.7393 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0066 |           0.0000 |          19.7303 |
[32m[20221213 18:54:12 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |          19.7512 |
[32m[20221213 18:54:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.00
[32m[20221213 18:54:12 @agent_ppo2.py:143][0m Total time:       5.67 min
[32m[20221213 18:54:12 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 18:54:12 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 18:54:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0018 |           0.0000 |          19.5875 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0033 |           0.0000 |          19.5721 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0043 |           0.0000 |          19.5714 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          19.5645 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |           0.0068 |           0.0000 |          19.5662 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |           0.0025 |           0.0000 |          19.5441 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |           0.0085 |           0.0000 |          19.5709 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          19.5663 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |          19.5732 |
[32m[20221213 18:54:13 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |          19.5677 |
[32m[20221213 18:54:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 18:54:13 @agent_ppo2.py:143][0m Total time:       5.68 min
[32m[20221213 18:54:13 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 18:54:13 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 18:54:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0027 |           0.0000 |          19.6394 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |           0.0010 |           0.0000 |          19.6065 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0041 |           0.0000 |          19.6055 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          19.6025 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |          19.5960 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |          19.5960 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0039 |           0.0000 |          19.5991 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |          19.6001 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |          19.6003 |
[32m[20221213 18:54:14 @agent_ppo2.py:185][0m |          -0.0048 |           0.0000 |          19.6055 |
[32m[20221213 18:54:14 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:54:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:54:15 @agent_ppo2.py:143][0m Total time:       5.70 min
[32m[20221213 18:54:15 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 18:54:15 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 18:54:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |           0.0011 |           1.8272 |          19.6120 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |           0.0001 |           1.8171 |          19.6022 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |           0.0000 |           1.8118 |          19.6027 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |           0.0002 |           1.7613 |          19.5879 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |           0.0003 |           1.7090 |          19.5880 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |          -0.0133 |           1.9167 |          19.5856 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |           0.0012 |           1.7119 |          19.5871 |
[32m[20221213 18:54:15 @agent_ppo2.py:185][0m |          -0.0008 |           1.6998 |          19.5831 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |          -0.0006 |           1.7011 |          19.5768 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |          -0.0008 |           1.7011 |          19.5695 |
[32m[20221213 18:54:16 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 20.20
[32m[20221213 18:54:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 43.00
[32m[20221213 18:54:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.00
[32m[20221213 18:54:16 @agent_ppo2.py:143][0m Total time:       5.72 min
[32m[20221213 18:54:16 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221213 18:54:16 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 18:54:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |          -0.0047 |           1.1742 |          19.7830 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |           0.0007 |           1.1048 |          19.7683 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |          -0.0018 |           1.1221 |          19.7647 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |          -0.0010 |           1.1050 |          19.7565 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |          -0.0095 |           1.1466 |          19.7667 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |           0.0010 |           1.1041 |          19.7590 |
[32m[20221213 18:54:16 @agent_ppo2.py:185][0m |           0.0004 |           1.1038 |          19.7615 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0004 |           1.1027 |          19.7606 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0002 |           1.1045 |          19.7577 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0003 |           1.1053 |          19.7501 |
[32m[20221213 18:54:17 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 14.80
[32m[20221213 18:54:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 74.00
[32m[20221213 18:54:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:54:17 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 18:54:17 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 18:54:17 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 18:54:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |           0.0001 |           0.0231 |          19.7009 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0057 |           0.0138 |          19.6916 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0038 |           0.0119 |          19.6835 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |           0.0014 |           0.0111 |          19.6764 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0102 |           0.0107 |          19.6724 |
[32m[20221213 18:54:17 @agent_ppo2.py:185][0m |          -0.0006 |           0.0104 |          19.6738 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0006 |           0.0102 |          19.6665 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0010 |           0.0101 |          19.6614 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0045 |           0.0100 |          19.6640 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0072 |           0.0100 |          19.6584 |
[32m[20221213 18:54:18 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:54:18 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221213 18:54:18 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221213 18:54:18 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 18:54:18 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:54:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |           0.0023 |           0.0106 |          19.9362 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0039 |           0.0053 |          19.9376 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0031 |           0.0045 |          19.9300 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |           0.0064 |           0.0041 |          19.9238 |
[32m[20221213 18:54:18 @agent_ppo2.py:185][0m |          -0.0006 |           0.0040 |          19.9171 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |           0.0011 |           0.0039 |          19.9170 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |          -0.0005 |           0.0038 |          19.9131 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |          -0.0048 |           0.0038 |          19.9113 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |          -0.0044 |           0.0038 |          19.9104 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |          -0.0105 |           0.0037 |          19.9059 |
[32m[20221213 18:54:19 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:54:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:54:19 @agent_ppo2.py:143][0m Total time:       5.78 min
[32m[20221213 18:54:19 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 18:54:19 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 18:54:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |           0.0062 |           0.0024 |          20.1557 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |           0.0002 |           0.0016 |          20.1312 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |           0.0010 |           0.0015 |          20.1242 |
[32m[20221213 18:54:19 @agent_ppo2.py:185][0m |          -0.0006 |           0.0015 |          20.1183 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |          -0.0076 |           0.0015 |          20.1224 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |           0.0027 |           0.0014 |          20.1236 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |          -0.0109 |           0.0013 |          20.1110 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |          -0.0084 |           0.0013 |          20.1128 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |          -0.0034 |           0.0012 |          20.1168 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |           0.0043 |           0.0012 |          20.1137 |
[32m[20221213 18:54:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 18:54:20 @agent_ppo2.py:143][0m Total time:       5.79 min
[32m[20221213 18:54:20 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221213 18:54:20 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 18:54:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |          -0.0225 |           1.4409 |          20.0477 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |           0.0009 |           1.2021 |          20.0317 |
[32m[20221213 18:54:20 @agent_ppo2.py:185][0m |          -0.0005 |           1.2020 |          20.0314 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |           0.0004 |           1.2014 |          20.0444 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |          -0.0004 |           1.2014 |          20.0315 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |          -0.0001 |           1.2001 |          20.0313 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |          -0.0002 |           1.2018 |          20.0341 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |          -0.0005 |           1.2006 |          20.0264 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |          -0.0002 |           1.2004 |          20.0256 |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |          -0.0008 |           1.2008 |          20.0242 |
[32m[20221213 18:54:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.80
[32m[20221213 18:54:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 34.00
[32m[20221213 18:54:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 34.00
[32m[20221213 18:54:21 @agent_ppo2.py:143][0m Total time:       5.81 min
[32m[20221213 18:54:21 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 18:54:21 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 18:54:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:54:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:21 @agent_ppo2.py:185][0m |           0.0010 |           0.4007 |          20.2496 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0012 |           0.3979 |          20.2514 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0001 |           0.3978 |          20.2443 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0005 |           0.3978 |          20.2305 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0015 |           0.3981 |          20.2306 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0009 |           0.3975 |          20.2357 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0002 |           0.3983 |          20.2391 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0020 |           0.3980 |          20.2319 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0045 |           0.3999 |          20.2274 |
[32m[20221213 18:54:22 @agent_ppo2.py:185][0m |          -0.0020 |           0.3985 |          20.2384 |
[32m[20221213 18:54:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.20
[32m[20221213 18:54:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 21.00
[32m[20221213 18:54:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:54:22 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221213 18:54:22 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221213 18:54:22 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 18:54:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |           0.0014 |           0.0047 |          20.1264 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0049 |           0.0027 |          20.1129 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |           0.0010 |           0.0026 |          20.1116 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0137 |           0.0025 |          20.1054 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |           0.0069 |           0.0024 |          20.1031 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0034 |           0.0024 |          20.1041 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0008 |           0.0024 |          20.1004 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0043 |           0.0023 |          20.0962 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0023 |           0.0023 |          20.0934 |
[32m[20221213 18:54:23 @agent_ppo2.py:185][0m |          -0.0038 |           0.0023 |          20.0819 |
[32m[20221213 18:54:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 18:54:23 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221213 18:54:23 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 18:54:23 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 18:54:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0002 |           0.0010 |          20.1626 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0020 |           0.0008 |          20.1323 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0018 |           0.0007 |          20.1159 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0026 |           0.0007 |          20.1206 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0069 |           0.0007 |          20.1144 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0043 |           0.0007 |          20.1134 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0042 |           0.0007 |          20.1219 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0060 |           0.0007 |          20.1059 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |          -0.0105 |           0.0007 |          20.1109 |
[32m[20221213 18:54:24 @agent_ppo2.py:185][0m |           0.0011 |           0.0007 |          20.1070 |
[32m[20221213 18:54:24 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.00
[32m[20221213 18:54:24 @agent_ppo2.py:143][0m Total time:       5.87 min
[32m[20221213 18:54:24 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221213 18:54:24 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 18:54:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |           0.0029 |           0.0006 |          20.1011 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |           0.0002 |           0.0005 |          20.0868 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |          -0.0029 |           0.0005 |          20.0786 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |          -0.0003 |           0.0005 |          20.0585 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |           0.0020 |           0.0005 |          20.0579 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |          -0.0028 |           0.0005 |          20.0557 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |           0.0109 |           0.0005 |          20.0482 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |          -0.0043 |           0.0005 |          20.0326 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |          -0.0106 |           0.0005 |          20.0445 |
[32m[20221213 18:54:25 @agent_ppo2.py:185][0m |          -0.0063 |           0.0005 |          20.0444 |
[32m[20221213 18:54:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 18:54:26 @agent_ppo2.py:143][0m Total time:       5.89 min
[32m[20221213 18:54:26 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 18:54:26 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 18:54:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0015 |           0.0005 |          20.1425 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0034 |           0.0004 |          20.1146 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0038 |           0.0004 |          20.0984 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0065 |           0.0004 |          20.0919 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0059 |           0.0004 |          20.0794 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0071 |           0.0004 |          20.0702 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0071 |           0.0004 |          20.0671 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0069 |           0.0004 |          20.0745 |
[32m[20221213 18:54:26 @agent_ppo2.py:185][0m |          -0.0063 |           0.0004 |          20.0651 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0072 |           0.0004 |          20.0610 |
[32m[20221213 18:54:27 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 41.00
[32m[20221213 18:54:27 @agent_ppo2.py:143][0m Total time:       5.90 min
[32m[20221213 18:54:27 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221213 18:54:27 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 18:54:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0002 |           0.0004 |          20.3692 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |           0.0021 |           0.0003 |          20.3611 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0009 |           0.0003 |          20.3552 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0044 |           0.0003 |          20.3617 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0049 |           0.0003 |          20.3664 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0043 |           0.0003 |          20.3654 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0021 |           0.0003 |          20.3610 |
[32m[20221213 18:54:27 @agent_ppo2.py:185][0m |          -0.0054 |           0.0003 |          20.3642 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0057 |           0.0003 |          20.3591 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0053 |           0.0003 |          20.3681 |
[32m[20221213 18:54:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:54:28 @agent_ppo2.py:143][0m Total time:       5.92 min
[32m[20221213 18:54:28 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 18:54:28 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 18:54:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |           0.0033 |           0.7148 |          20.1841 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0013 |           0.6412 |          20.1526 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0090 |           0.6434 |          20.1603 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0003 |           0.6427 |          20.1441 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0110 |           0.6521 |          20.1438 |
[32m[20221213 18:54:28 @agent_ppo2.py:185][0m |          -0.0016 |           0.6401 |          20.1403 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |           0.0028 |           0.6369 |          20.1379 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |          -0.0016 |           0.6337 |          20.1386 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |           0.0023 |           0.6338 |          20.1401 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |          -0.0073 |           0.6491 |          20.1377 |
[32m[20221213 18:54:29 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 28.80
[32m[20221213 18:54:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 73.00
[32m[20221213 18:54:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 18:54:29 @agent_ppo2.py:143][0m Total time:       5.94 min
[32m[20221213 18:54:29 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221213 18:54:29 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 18:54:29 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:54:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |           0.0007 |           0.4292 |          20.3687 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |          -0.0033 |           0.4158 |          20.3538 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |          -0.0003 |           0.4142 |          20.3463 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |          -0.0002 |           0.4144 |          20.3502 |
[32m[20221213 18:54:29 @agent_ppo2.py:185][0m |          -0.0023 |           0.4166 |          20.3474 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |          -0.0018 |           0.4139 |          20.3494 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |          -0.0024 |           0.4139 |          20.3551 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |          -0.0017 |           0.4135 |          20.3541 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |          -0.0009 |           0.4137 |          20.3558 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |           0.0005 |           0.4127 |          20.3481 |
[32m[20221213 18:54:30 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.40
[32m[20221213 18:54:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.00
[32m[20221213 18:54:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:30 @agent_ppo2.py:143][0m Total time:       5.96 min
[32m[20221213 18:54:30 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 18:54:30 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 18:54:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |           0.0014 |           0.0332 |          20.3124 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |           0.0033 |           0.0174 |          20.3078 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |           0.0033 |           0.0154 |          20.3055 |
[32m[20221213 18:54:30 @agent_ppo2.py:185][0m |           0.0034 |           0.0150 |          20.3017 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0005 |           0.0152 |          20.3039 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0021 |           0.0148 |          20.3027 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0080 |           0.0149 |          20.3003 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0033 |           0.0149 |          20.3033 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |           0.0056 |           0.0148 |          20.2932 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0048 |           0.0148 |          20.3018 |
[32m[20221213 18:54:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 18:54:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 18:54:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 18:54:31 @agent_ppo2.py:143][0m Total time:       5.98 min
[32m[20221213 18:54:31 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221213 18:54:31 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 18:54:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |           0.0013 |           1.6687 |          20.2781 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0036 |           1.3131 |          20.2583 |
[32m[20221213 18:54:31 @agent_ppo2.py:185][0m |          -0.0044 |           1.3526 |          20.2571 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |          -0.0048 |           1.3578 |          20.2426 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |          -0.0039 |           1.3001 |          20.2384 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |           0.0005 |           1.3053 |          20.2371 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |          -0.0021 |           1.3048 |          20.2318 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |          -0.0046 |           1.2995 |          20.2266 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |          -0.0013 |           1.3026 |          20.2290 |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |          -0.0057 |           1.3062 |          20.2209 |
[32m[20221213 18:54:32 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.40
[32m[20221213 18:54:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.00
[32m[20221213 18:54:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 18:54:32 @agent_ppo2.py:143][0m Total time:       6.00 min
[32m[20221213 18:54:32 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 18:54:32 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 18:54:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:54:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:32 @agent_ppo2.py:185][0m |           0.0056 |           0.8903 |          20.3226 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |           0.0018 |           0.8445 |          20.3059 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |           0.0033 |           0.8469 |          20.2950 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |          -0.0011 |           0.8450 |          20.2934 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |          -0.0032 |           0.8550 |          20.2893 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |           0.0001 |           0.8425 |          20.2779 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |          -0.0068 |           0.8413 |          20.2818 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |          -0.0030 |           0.8415 |          20.2725 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |           0.0030 |           0.8437 |          20.2689 |
[32m[20221213 18:54:33 @agent_ppo2.py:185][0m |          -0.0073 |           0.8831 |          20.2669 |
[32m[20221213 18:54:33 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 25.20
[32m[20221213 18:54:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 38.00
[32m[20221213 18:54:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.00
[32m[20221213 18:54:33 @agent_ppo2.py:143][0m Total time:       6.01 min
[32m[20221213 18:54:33 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221213 18:54:33 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 18:54:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0066 |           0.5400 |          20.3309 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |           0.0010 |           0.5012 |          20.3296 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |           0.0034 |           0.5012 |          20.3163 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0043 |           0.4996 |          20.3153 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0010 |           0.4999 |          20.3082 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0109 |           0.5140 |          20.3086 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0034 |           0.5044 |          20.3014 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0035 |           0.4986 |          20.2951 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0048 |           0.4995 |          20.2972 |
[32m[20221213 18:54:34 @agent_ppo2.py:185][0m |          -0.0066 |           0.5167 |          20.2883 |
[32m[20221213 18:54:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.60
[32m[20221213 18:54:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 15.00
[32m[20221213 18:54:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:54:34 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221213 18:54:34 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 18:54:34 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 18:54:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |           0.0027 |           2.5193 |          20.6096 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |           0.0003 |           2.3181 |          20.6008 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |          -0.0024 |           2.2272 |          20.5884 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |          -0.0032 |           2.1814 |          20.5713 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |           0.0001 |           2.1325 |          20.5626 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |          -0.0019 |           2.0701 |          20.5497 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |           0.0003 |           2.0084 |          20.5387 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |          -0.0013 |           1.9233 |          20.5387 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |          -0.0029 |           1.8471 |          20.5332 |
[32m[20221213 18:54:35 @agent_ppo2.py:185][0m |          -0.0035 |           1.7578 |          20.5251 |
[32m[20221213 18:54:35 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.80
[32m[20221213 18:54:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.00
[32m[20221213 18:54:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 35.00
[32m[20221213 18:54:35 @agent_ppo2.py:143][0m Total time:       6.05 min
[32m[20221213 18:54:35 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221213 18:54:35 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 18:54:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0067 |           0.9144 |          20.5485 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0010 |           0.8244 |          20.5381 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0034 |           0.8217 |          20.5328 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0018 |           0.8237 |          20.5364 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0024 |           0.8644 |          20.5408 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0007 |           0.8253 |          20.5372 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0015 |           0.8399 |          20.5352 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |           0.0034 |           0.8230 |          20.5347 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |          -0.0033 |           0.8224 |          20.5416 |
[32m[20221213 18:54:36 @agent_ppo2.py:185][0m |           0.0002 |           0.8393 |          20.5412 |
[32m[20221213 18:54:36 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.60
[32m[20221213 18:54:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:54:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:54:37 @agent_ppo2.py:143][0m Total time:       6.07 min
[32m[20221213 18:54:37 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 18:54:37 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 18:54:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |           0.0055 |           0.8822 |          20.6004 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |          -0.0035 |           0.8058 |          20.5618 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |          -0.0007 |           0.8090 |          20.5662 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |          -0.0087 |           0.8077 |          20.5514 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |           0.0023 |           0.8033 |          20.5420 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |          -0.0032 |           0.8035 |          20.5397 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |          -0.0077 |           0.8143 |          20.5371 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |          -0.0043 |           0.8093 |          20.5431 |
[32m[20221213 18:54:37 @agent_ppo2.py:185][0m |           0.0075 |           0.8043 |          20.5374 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |          -0.0114 |           0.8044 |          20.5368 |
[32m[20221213 18:54:38 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.80
[32m[20221213 18:54:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.00
[32m[20221213 18:54:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 41.00
[32m[20221213 18:54:38 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221213 18:54:38 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221213 18:54:38 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 18:54:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0032 |           4.1329 |          20.6889 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0004 |           4.0354 |          20.6751 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0029 |           3.9890 |          20.6549 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0002 |           3.9829 |          20.6329 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0004 |           3.9665 |          20.6279 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0017 |           3.9705 |          20.6363 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |           0.0010 |           3.9511 |          20.6366 |
[32m[20221213 18:54:38 @agent_ppo2.py:185][0m |          -0.0014 |           3.9490 |          20.6279 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0000 |           3.9598 |          20.6160 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0004 |           3.9568 |          20.6252 |
[32m[20221213 18:54:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.40
[32m[20221213 18:54:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 116.00
[32m[20221213 18:54:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:54:39 @agent_ppo2.py:143][0m Total time:       6.10 min
[32m[20221213 18:54:39 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 18:54:39 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 18:54:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0017 |           0.9207 |          20.6795 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |           0.0003 |           0.8676 |          20.6822 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0058 |           0.8666 |          20.6787 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0038 |           0.8685 |          20.6797 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0082 |           0.8710 |          20.6916 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0009 |           0.8628 |          20.6899 |
[32m[20221213 18:54:39 @agent_ppo2.py:185][0m |          -0.0013 |           0.8681 |          20.6925 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |           0.0025 |           0.8584 |          20.6832 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |          -0.0049 |           0.8636 |          20.6921 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |           0.0002 |           0.8571 |          20.6861 |
[32m[20221213 18:54:40 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.60
[32m[20221213 18:54:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 48.00
[32m[20221213 18:54:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 18:54:40 @agent_ppo2.py:143][0m Total time:       6.12 min
[32m[20221213 18:54:40 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221213 18:54:40 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 18:54:40 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:54:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |           0.0002 |           1.1846 |          20.7330 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |          -0.0017 |           1.1456 |          20.7062 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |          -0.0009 |           1.1608 |          20.7120 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |          -0.0010 |           1.1413 |          20.7119 |
[32m[20221213 18:54:40 @agent_ppo2.py:185][0m |          -0.0040 |           1.1375 |          20.7203 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0153 |           1.2349 |          20.7253 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0077 |           1.1561 |          20.7191 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0005 |           1.1210 |          20.7154 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0083 |           1.1211 |          20.7212 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0125 |           1.1480 |          20.7181 |
[32m[20221213 18:54:41 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 37.40
[32m[20221213 18:54:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.00
[32m[20221213 18:54:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 18:54:41 @agent_ppo2.py:143][0m Total time:       6.14 min
[32m[20221213 18:54:41 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 18:54:41 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 18:54:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0011 |           0.7629 |          20.5668 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0009 |           0.7524 |          20.5547 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0012 |           0.7487 |          20.5631 |
[32m[20221213 18:54:41 @agent_ppo2.py:185][0m |          -0.0044 |           0.7482 |          20.5598 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |           0.0089 |           0.7749 |          20.5492 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0003 |           0.7420 |          20.5596 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0023 |           0.7381 |          20.5531 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0022 |           0.7348 |          20.5599 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0050 |           0.7302 |          20.5587 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0078 |           0.7364 |          20.5470 |
[32m[20221213 18:54:42 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.40
[32m[20221213 18:54:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:54:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.00
[32m[20221213 18:54:42 @agent_ppo2.py:143][0m Total time:       6.16 min
[32m[20221213 18:54:42 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221213 18:54:42 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 18:54:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0018 |           0.7548 |          20.5957 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0039 |           0.7416 |          20.5718 |
[32m[20221213 18:54:42 @agent_ppo2.py:185][0m |          -0.0029 |           0.7406 |          20.5691 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |          -0.0021 |           0.7412 |          20.5681 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |          -0.0073 |           0.7331 |          20.5655 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |           0.0018 |           0.7331 |          20.5500 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |          -0.0018 |           0.7233 |          20.5644 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |          -0.0035 |           0.7227 |          20.5519 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |           0.0020 |           0.7321 |          20.5597 |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |          -0.0029 |           0.7144 |          20.5468 |
[32m[20221213 18:54:43 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.20
[32m[20221213 18:54:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 18:54:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 18:54:43 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221213 18:54:43 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 18:54:43 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 18:54:43 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:54:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:43 @agent_ppo2.py:185][0m |          -0.0016 |           0.9183 |          20.6175 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0079 |           0.9164 |          20.6132 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0011 |           0.8865 |          20.6070 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0006 |           0.8767 |          20.6061 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0024 |           0.8766 |          20.5991 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0038 |           0.8675 |          20.6039 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0031 |           0.8679 |          20.6003 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0033 |           0.8618 |          20.5956 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |          -0.0106 |           0.8713 |          20.5936 |
[32m[20221213 18:54:44 @agent_ppo2.py:185][0m |           0.0069 |           0.9589 |          20.5969 |
[32m[20221213 18:54:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:54:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.80
[32m[20221213 18:54:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 18:54:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:54:44 @agent_ppo2.py:143][0m Total time:       6.20 min
[32m[20221213 18:54:44 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221213 18:54:44 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 18:54:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0022 |           1.3735 |          20.6529 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |           0.0009 |           1.2222 |          20.6314 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0010 |           1.2233 |          20.6280 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0053 |           1.2167 |          20.6224 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0033 |           1.2553 |          20.6088 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0024 |           1.2175 |          20.5965 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0003 |           1.2151 |          20.5966 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0041 |           1.2173 |          20.5999 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0060 |           1.2107 |          20.5964 |
[32m[20221213 18:54:45 @agent_ppo2.py:185][0m |          -0.0096 |           1.2229 |          20.5783 |
[32m[20221213 18:54:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:54:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.20
[32m[20221213 18:54:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 18:54:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 18:54:45 @agent_ppo2.py:143][0m Total time:       6.22 min
[32m[20221213 18:54:45 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 18:54:45 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 18:54:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0062 |           1.2779 |          20.7140 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0029 |           1.2080 |          20.7117 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0139 |           1.2735 |          20.7007 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0022 |           1.2114 |          20.7026 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |           0.0020 |           1.2089 |          20.7001 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0038 |           1.2082 |          20.6801 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0006 |           1.2461 |          20.7011 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0027 |           1.2104 |          20.6963 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0045 |           1.2026 |          20.7030 |
[32m[20221213 18:54:46 @agent_ppo2.py:185][0m |          -0.0035 |           1.1992 |          20.6987 |
[32m[20221213 18:54:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:54:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.20
[32m[20221213 18:54:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.00
[32m[20221213 18:54:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:54:47 @agent_ppo2.py:143][0m Total time:       6.24 min
[32m[20221213 18:54:47 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221213 18:54:47 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 18:54:47 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:54:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |           0.0001 |           1.1510 |          20.8832 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0026 |           1.1130 |          20.8724 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0037 |           1.1373 |          20.8835 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0025 |           1.1118 |          20.8676 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0054 |           1.1121 |          20.8780 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0034 |           1.1128 |          20.8749 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0018 |           1.1082 |          20.8821 |
[32m[20221213 18:54:47 @agent_ppo2.py:185][0m |          -0.0014 |           1.1071 |          20.8853 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |          -0.0002 |           1.1149 |          20.8834 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |          -0.0036 |           1.1184 |          20.8789 |
[32m[20221213 18:54:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:54:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.60
[32m[20221213 18:54:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:54:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.00
[32m[20221213 18:54:48 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221213 18:54:48 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 18:54:48 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 18:54:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |           0.0010 |           1.2062 |          20.7441 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |          -0.0073 |           1.1842 |          20.7354 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |           0.0008 |           1.1843 |          20.7316 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |          -0.0025 |           1.1901 |          20.7291 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |           0.0019 |           1.1967 |          20.7238 |
[32m[20221213 18:54:48 @agent_ppo2.py:185][0m |          -0.0011 |           1.1843 |          20.7091 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |          -0.0109 |           1.2457 |          20.7148 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |           0.0080 |           1.2273 |          20.7083 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |           0.0033 |           1.1916 |          20.7100 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |          -0.0062 |           1.1909 |          20.7019 |
[32m[20221213 18:54:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:54:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.00
[32m[20221213 18:54:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 18:54:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:54:49 @agent_ppo2.py:143][0m Total time:       6.27 min
[32m[20221213 18:54:49 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221213 18:54:49 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 18:54:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |           0.0040 |           1.0576 |          20.7502 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |          -0.0056 |           0.9959 |          20.7207 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |          -0.0058 |           0.9818 |          20.7224 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |          -0.0060 |           0.9756 |          20.7373 |
[32m[20221213 18:54:49 @agent_ppo2.py:185][0m |          -0.0060 |           0.9677 |          20.7283 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |          -0.0088 |           0.9616 |          20.7324 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |           0.0060 |           1.0534 |          20.7358 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |          -0.0099 |           0.9607 |          20.7415 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |          -0.0064 |           0.9520 |          20.7345 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |          -0.0108 |           0.9589 |          20.7356 |
[32m[20221213 18:54:50 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.80
[32m[20221213 18:54:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 67.00
[32m[20221213 18:54:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 18:54:50 @agent_ppo2.py:143][0m Total time:       6.29 min
[32m[20221213 18:54:50 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 18:54:50 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 18:54:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |           0.0027 |           1.4562 |          20.6160 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |          -0.0027 |           1.4501 |          20.6205 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |           0.0097 |           1.4953 |          20.6124 |
[32m[20221213 18:54:50 @agent_ppo2.py:185][0m |          -0.0060 |           1.4311 |          20.6109 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |          -0.0012 |           1.4214 |          20.6059 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |           0.0005 |           1.4576 |          20.5934 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |          -0.0014 |           1.4260 |          20.6165 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |          -0.0021 |           1.4083 |          20.6135 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |           0.0043 |           1.4575 |          20.6091 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |          -0.0039 |           1.4019 |          20.6018 |
[32m[20221213 18:54:51 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.60
[32m[20221213 18:54:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 18:54:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:54:51 @agent_ppo2.py:143][0m Total time:       6.31 min
[32m[20221213 18:54:51 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221213 18:54:51 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 18:54:51 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:54:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |          -0.0050 |           1.3022 |          20.5045 |
[32m[20221213 18:54:51 @agent_ppo2.py:185][0m |          -0.0043 |           1.2749 |          20.4955 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0051 |           1.2695 |          20.4956 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0043 |           1.2704 |          20.4903 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0026 |           1.2723 |          20.4887 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |           0.0014 |           1.2698 |          20.4802 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0018 |           1.2586 |          20.4786 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0016 |           1.2578 |          20.4680 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0048 |           1.2546 |          20.4718 |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |          -0.0047 |           1.2526 |          20.4623 |
[32m[20221213 18:54:52 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.00
[32m[20221213 18:54:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 18:54:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:54:52 @agent_ppo2.py:143][0m Total time:       6.33 min
[32m[20221213 18:54:52 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 18:54:52 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 18:54:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:52 @agent_ppo2.py:185][0m |           0.0031 |           1.3578 |          20.5991 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |           0.0004 |           1.3351 |          20.5771 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0030 |           1.3297 |          20.5764 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0029 |           1.3277 |          20.5730 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0056 |           1.3273 |          20.5692 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0090 |           1.3236 |          20.5602 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0022 |           1.3334 |          20.5487 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0054 |           1.3295 |          20.5498 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0103 |           1.3238 |          20.5515 |
[32m[20221213 18:54:53 @agent_ppo2.py:185][0m |          -0.0093 |           1.3106 |          20.5443 |
[32m[20221213 18:54:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.80
[32m[20221213 18:54:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 18:54:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 18:54:53 @agent_ppo2.py:143][0m Total time:       6.35 min
[32m[20221213 18:54:53 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221213 18:54:53 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 18:54:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |           0.0018 |           1.3338 |          20.6016 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |          -0.0040 |           1.3140 |          20.5872 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |          -0.0058 |           1.3100 |          20.5657 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |           0.0083 |           1.3761 |          20.5639 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |          -0.0095 |           1.3229 |          20.5587 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |           0.0003 |           1.3520 |          20.5558 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |           0.0019 |           1.2976 |          20.5560 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |          -0.0018 |           1.3096 |          20.5540 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |          -0.0055 |           1.2977 |          20.5514 |
[32m[20221213 18:54:54 @agent_ppo2.py:185][0m |          -0.0088 |           1.2924 |          20.5503 |
[32m[20221213 18:54:54 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.40
[32m[20221213 18:54:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.00
[32m[20221213 18:54:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:54:54 @agent_ppo2.py:143][0m Total time:       6.37 min
[32m[20221213 18:54:54 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 18:54:54 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 18:54:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:54:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |           0.0026 |           1.1281 |          20.5559 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |           0.0036 |           1.1263 |          20.5384 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |          -0.0002 |           1.1319 |          20.5403 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |           0.0087 |           1.1703 |          20.5254 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |          -0.0038 |           1.1130 |          20.5185 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |          -0.0006 |           1.0978 |          20.5177 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |          -0.0055 |           1.0941 |          20.5257 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |          -0.0025 |           1.0895 |          20.5287 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |           0.0025 |           1.1145 |          20.5181 |
[32m[20221213 18:54:55 @agent_ppo2.py:185][0m |          -0.0016 |           1.0935 |          20.5302 |
[32m[20221213 18:54:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.20
[32m[20221213 18:54:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 18:54:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 18:54:55 @agent_ppo2.py:143][0m Total time:       6.38 min
[32m[20221213 18:54:55 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221213 18:54:55 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 18:54:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0036 |           1.1926 |          20.6243 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |           0.0003 |           1.1781 |          20.6064 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0062 |           1.1691 |          20.5908 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0067 |           1.1608 |          20.5886 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0060 |           1.1629 |          20.5851 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0028 |           1.1608 |          20.5771 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0062 |           1.1520 |          20.5753 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |           0.0032 |           1.2147 |          20.5696 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0059 |           1.1450 |          20.5750 |
[32m[20221213 18:54:56 @agent_ppo2.py:185][0m |          -0.0026 |           1.1372 |          20.5678 |
[32m[20221213 18:54:56 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:54:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 33.20
[32m[20221213 18:54:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 38.00
[32m[20221213 18:54:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:54:57 @agent_ppo2.py:143][0m Total time:       6.40 min
[32m[20221213 18:54:57 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 18:54:57 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 18:54:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0010 |           1.3152 |          20.6539 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0043 |           1.3006 |          20.6423 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0068 |           1.2804 |          20.6434 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0057 |           1.2808 |          20.6429 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0071 |           1.2689 |          20.6453 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0050 |           1.2710 |          20.6517 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0049 |           1.2885 |          20.6448 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0062 |           1.2670 |          20.6427 |
[32m[20221213 18:54:57 @agent_ppo2.py:185][0m |          -0.0023 |           1.2541 |          20.6392 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |          -0.0039 |           1.2494 |          20.6490 |
[32m[20221213 18:54:58 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:54:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 39.80
[32m[20221213 18:54:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.00
[32m[20221213 18:54:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:54:58 @agent_ppo2.py:143][0m Total time:       6.42 min
[32m[20221213 18:54:58 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221213 18:54:58 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 18:54:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |           0.0015 |           1.7403 |          20.6213 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |          -0.0068 |           1.7483 |          20.6052 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |           0.0008 |           1.7472 |          20.5870 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |          -0.0058 |           1.7264 |          20.5827 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |          -0.0063 |           1.7288 |          20.5894 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |          -0.0080 |           1.7111 |          20.5788 |
[32m[20221213 18:54:58 @agent_ppo2.py:185][0m |          -0.0009 |           1.7154 |          20.5764 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |           0.0096 |           1.8271 |          20.5681 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0046 |           1.7197 |          20.5630 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0092 |           1.7062 |          20.5565 |
[32m[20221213 18:54:59 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:54:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.80
[32m[20221213 18:54:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 18:54:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:54:59 @agent_ppo2.py:143][0m Total time:       6.44 min
[32m[20221213 18:54:59 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 18:54:59 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 18:54:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:54:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0020 |           1.3510 |          20.8481 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |           0.0021 |           1.3488 |          20.8346 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0032 |           1.3194 |          20.8317 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0041 |           1.3080 |          20.8205 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0032 |           1.2949 |          20.8254 |
[32m[20221213 18:54:59 @agent_ppo2.py:185][0m |          -0.0047 |           1.2863 |          20.8233 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |           0.0010 |           1.3090 |          20.8100 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0038 |           1.2706 |          20.8133 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0012 |           1.2879 |          20.8123 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0057 |           1.2493 |          20.8199 |
[32m[20221213 18:55:00 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 30.60
[32m[20221213 18:55:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 34.00
[32m[20221213 18:55:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:55:00 @agent_ppo2.py:143][0m Total time:       6.46 min
[32m[20221213 18:55:00 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221213 18:55:00 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 18:55:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0037 |           1.6202 |          20.7378 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |           0.0011 |           1.5834 |          20.7252 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0041 |           1.5552 |          20.7159 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0028 |           1.5403 |          20.7151 |
[32m[20221213 18:55:00 @agent_ppo2.py:185][0m |          -0.0026 |           1.5258 |          20.7081 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |          -0.0049 |           1.4999 |          20.7045 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |           0.0047 |           1.5105 |          20.7128 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |          -0.0031 |           1.4684 |          20.7135 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |          -0.0090 |           1.4633 |          20.6901 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |          -0.0151 |           1.4876 |          20.7047 |
[32m[20221213 18:55:01 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.80
[32m[20221213 18:55:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 18:55:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 34.00
[32m[20221213 18:55:01 @agent_ppo2.py:143][0m Total time:       6.48 min
[32m[20221213 18:55:01 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 18:55:01 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 18:55:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |          -0.0015 |           1.2722 |          20.7644 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |          -0.0044 |           1.2310 |          20.7661 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |           0.0025 |           1.2253 |          20.7601 |
[32m[20221213 18:55:01 @agent_ppo2.py:185][0m |           0.0021 |           1.2445 |          20.7611 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |          -0.0099 |           1.2282 |          20.7567 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |          -0.0059 |           1.1936 |          20.7594 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |           0.0010 |           1.2190 |          20.7518 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |          -0.0075 |           1.1849 |          20.7487 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |          -0.0041 |           1.1717 |          20.7558 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |           0.0001 |           1.1628 |          20.7579 |
[32m[20221213 18:55:02 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.60
[32m[20221213 18:55:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:55:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 18:55:02 @agent_ppo2.py:143][0m Total time:       6.49 min
[32m[20221213 18:55:02 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221213 18:55:02 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 18:55:02 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |           0.0004 |           1.0000 |          20.7360 |
[32m[20221213 18:55:02 @agent_ppo2.py:185][0m |          -0.0018 |           0.9950 |          20.7149 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0062 |           0.9812 |          20.7151 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0054 |           0.9729 |          20.7135 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |           0.0016 |           1.0017 |          20.7084 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0076 |           0.9629 |          20.7000 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0052 |           0.9556 |          20.7081 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0074 |           0.9498 |          20.7093 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0087 |           0.9514 |          20.7031 |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0072 |           0.9374 |          20.7034 |
[32m[20221213 18:55:03 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.40
[32m[20221213 18:55:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:55:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:55:03 @agent_ppo2.py:143][0m Total time:       6.51 min
[32m[20221213 18:55:03 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 18:55:03 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 18:55:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:03 @agent_ppo2.py:185][0m |          -0.0015 |           1.3541 |          20.7329 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0078 |           1.2499 |          20.7029 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0007 |           1.2286 |          20.6794 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0088 |           1.2014 |          20.6794 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0074 |           1.1986 |          20.6606 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0065 |           1.1828 |          20.6627 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0125 |           1.1865 |          20.6572 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0084 |           1.1671 |          20.6561 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0041 |           1.1672 |          20.6511 |
[32m[20221213 18:55:04 @agent_ppo2.py:185][0m |          -0.0068 |           1.1698 |          20.6510 |
[32m[20221213 18:55:04 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.00
[32m[20221213 18:55:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:55:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 31.00
[32m[20221213 18:55:04 @agent_ppo2.py:143][0m Total time:       6.53 min
[32m[20221213 18:55:04 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221213 18:55:04 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 18:55:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |           0.0089 |           2.8849 |          20.8334 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |           0.0047 |           2.6158 |          20.8346 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |          -0.0097 |           2.7007 |          20.8375 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |           0.0020 |           2.5391 |          20.8310 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |           0.0017 |           2.5265 |          20.8287 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |           0.0003 |           2.5265 |          20.8205 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |          -0.0017 |           2.4979 |          20.8257 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |          -0.0023 |           2.5084 |          20.8157 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |          -0.0015 |           2.4837 |          20.8231 |
[32m[20221213 18:55:05 @agent_ppo2.py:185][0m |           0.0031 |           2.5004 |          20.8190 |
[32m[20221213 18:55:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.60
[32m[20221213 18:55:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.00
[32m[20221213 18:55:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.00
[32m[20221213 18:55:05 @agent_ppo2.py:143][0m Total time:       6.55 min
[32m[20221213 18:55:05 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 18:55:05 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 18:55:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:55:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0029 |           1.5327 |          20.8917 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0042 |           1.4934 |          20.8866 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |           0.0002 |           1.5064 |          20.8894 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |           0.0030 |           1.4828 |          20.9051 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0034 |           1.4717 |          20.8999 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0035 |           1.4685 |          20.9115 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0062 |           1.4633 |          20.9067 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0080 |           1.4572 |          20.9058 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0038 |           1.5151 |          20.9105 |
[32m[20221213 18:55:06 @agent_ppo2.py:185][0m |          -0.0041 |           1.4466 |          20.9040 |
[32m[20221213 18:55:06 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.00
[32m[20221213 18:55:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 18:55:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 18:55:06 @agent_ppo2.py:143][0m Total time:       6.57 min
[32m[20221213 18:55:06 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221213 18:55:06 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 18:55:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0018 |           1.6513 |          20.7807 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |           0.0016 |           1.5431 |          20.7634 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0004 |           1.5190 |          20.7615 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0052 |           1.5064 |          20.7502 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0060 |           1.4974 |          20.7416 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0015 |           1.4925 |          20.7431 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0003 |           1.4918 |          20.7430 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0042 |           1.4740 |          20.7397 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |           0.0029 |           1.4815 |          20.7473 |
[32m[20221213 18:55:07 @agent_ppo2.py:185][0m |          -0.0033 |           1.4765 |          20.7409 |
[32m[20221213 18:55:07 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.40
[32m[20221213 18:55:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 18:55:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:55:08 @agent_ppo2.py:143][0m Total time:       6.59 min
[32m[20221213 18:55:08 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 18:55:08 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 18:55:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0003 |           2.3196 |          20.7736 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0094 |           2.3192 |          20.7523 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0029 |           2.2400 |          20.7505 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |           0.0016 |           2.2416 |          20.7363 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0049 |           2.2231 |          20.7375 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0038 |           2.2078 |          20.7299 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0045 |           2.2067 |          20.7255 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |          -0.0032 |           2.1893 |          20.7286 |
[32m[20221213 18:55:08 @agent_ppo2.py:185][0m |           0.0005 |           2.2189 |          20.7186 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0047 |           2.1789 |          20.7240 |
[32m[20221213 18:55:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.80
[32m[20221213 18:55:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 18:55:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.00
[32m[20221213 18:55:09 @agent_ppo2.py:143][0m Total time:       6.60 min
[32m[20221213 18:55:09 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221213 18:55:09 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 18:55:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0047 |           1.5289 |          20.8531 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0056 |           1.5013 |          20.8267 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0030 |           1.4902 |          20.8338 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0058 |           1.4744 |          20.8313 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0047 |           1.4667 |          20.8292 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0088 |           1.4657 |          20.8323 |
[32m[20221213 18:55:09 @agent_ppo2.py:185][0m |          -0.0058 |           1.4562 |          20.8302 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |           0.0024 |           1.4567 |          20.8206 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |          -0.0059 |           1.4490 |          20.8369 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |          -0.0072 |           1.4382 |          20.8283 |
[32m[20221213 18:55:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:55:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.20
[32m[20221213 18:55:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:55:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:55:10 @agent_ppo2.py:143][0m Total time:       6.62 min
[32m[20221213 18:55:10 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 18:55:10 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 18:55:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |          -0.0025 |           2.0672 |          20.7741 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |          -0.0011 |           2.0412 |          20.7525 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |           0.0002 |           2.0077 |          20.7317 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |          -0.0032 |           1.9907 |          20.7220 |
[32m[20221213 18:55:10 @agent_ppo2.py:185][0m |          -0.0086 |           1.9783 |          20.7121 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |           0.0036 |           2.0558 |          20.7050 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |          -0.0044 |           1.9668 |          20.6906 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |          -0.0066 |           1.9476 |          20.7004 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |          -0.0056 |           1.9250 |          20.6847 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |          -0.0029 |           1.9148 |          20.6947 |
[32m[20221213 18:55:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:55:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.80
[32m[20221213 18:55:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 18:55:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 18:55:11 @agent_ppo2.py:143][0m Total time:       6.64 min
[32m[20221213 18:55:11 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221213 18:55:11 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 18:55:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |           0.0004 |           1.9555 |          20.9908 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |          -0.0038 |           1.9403 |          20.9968 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |           0.0001 |           1.9215 |          20.9875 |
[32m[20221213 18:55:11 @agent_ppo2.py:185][0m |          -0.0027 |           1.8967 |          20.9904 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0058 |           1.8851 |          20.9720 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0035 |           1.8656 |          20.9870 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0005 |           1.8627 |          20.9765 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |           0.0005 |           1.8531 |          20.9827 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0037 |           1.8305 |          20.9808 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0056 |           1.8223 |          20.9681 |
[32m[20221213 18:55:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:55:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.60
[32m[20221213 18:55:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:55:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.00
[32m[20221213 18:55:12 @agent_ppo2.py:143][0m Total time:       6.66 min
[32m[20221213 18:55:12 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 18:55:12 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 18:55:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0010 |           1.5134 |          20.8126 |
[32m[20221213 18:55:12 @agent_ppo2.py:185][0m |          -0.0044 |           1.4916 |          20.8008 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0008 |           1.4805 |          20.8058 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0123 |           1.4714 |          20.7929 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0100 |           1.4656 |          20.7848 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0056 |           1.4468 |          20.7854 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0056 |           1.4412 |          20.7865 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |           0.0021 |           1.4393 |          20.7791 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0097 |           1.4312 |          20.7886 |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0071 |           1.4220 |          20.7866 |
[32m[20221213 18:55:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:55:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.40
[32m[20221213 18:55:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:55:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 18:55:13 @agent_ppo2.py:143][0m Total time:       6.68 min
[32m[20221213 18:55:13 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221213 18:55:13 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 18:55:13 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:13 @agent_ppo2.py:185][0m |          -0.0020 |           1.5397 |          20.9993 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0035 |           1.5174 |          20.9891 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0046 |           1.5287 |          20.9777 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0026 |           1.5075 |          20.9681 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0007 |           1.4981 |          20.9715 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0019 |           1.4913 |          20.9631 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0045 |           1.4868 |          20.9584 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |           0.0005 |           1.5150 |          20.9691 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |          -0.0033 |           1.4796 |          20.9624 |
[32m[20221213 18:55:14 @agent_ppo2.py:185][0m |           0.0048 |           1.5054 |          20.9622 |
[32m[20221213 18:55:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:55:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.80
[32m[20221213 18:55:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 18:55:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:55:14 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221213 18:55:14 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 18:55:14 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 18:55:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |           0.0023 |           1.2531 |          20.8535 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0007 |           1.2001 |          20.8510 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0026 |           1.1836 |          20.8555 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0062 |           1.1659 |          20.8579 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0010 |           1.1545 |          20.8732 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0038 |           1.1511 |          20.8664 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |           0.0042 |           1.1936 |          20.8572 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0048 |           1.1369 |          20.8626 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0033 |           1.1246 |          20.8668 |
[32m[20221213 18:55:15 @agent_ppo2.py:185][0m |          -0.0085 |           1.1256 |          20.8656 |
[32m[20221213 18:55:15 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.00
[32m[20221213 18:55:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 18:55:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 93.00
[32m[20221213 18:55:15 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221213 18:55:15 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221213 18:55:15 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 18:55:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0006 |           1.9767 |          20.9205 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |           0.0036 |           1.8229 |          20.9016 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |           0.0032 |           1.8574 |          20.9038 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0006 |           1.7754 |          20.9044 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0018 |           1.7525 |          20.9018 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |           0.0067 |           1.8343 |          20.9062 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0020 |           1.7429 |          20.8991 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0051 |           1.7304 |          20.9014 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0009 |           1.7172 |          20.9011 |
[32m[20221213 18:55:16 @agent_ppo2.py:185][0m |          -0.0031 |           1.7166 |          20.8909 |
[32m[20221213 18:55:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.40
[32m[20221213 18:55:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 72.00
[32m[20221213 18:55:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:55:17 @agent_ppo2.py:143][0m Total time:       6.73 min
[32m[20221213 18:55:17 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 18:55:17 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 18:55:17 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0010 |           2.0160 |          20.9788 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |           0.0054 |           1.9441 |          20.9837 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0035 |           1.9079 |          20.9947 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0005 |           1.8848 |          20.9982 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0006 |           1.8853 |          20.9988 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0091 |           1.9068 |          21.0050 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0028 |           1.8669 |          21.0080 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0068 |           1.8606 |          21.0040 |
[32m[20221213 18:55:17 @agent_ppo2.py:185][0m |          -0.0044 |           1.8329 |          21.0072 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |          -0.0062 |           1.8260 |          21.0152 |
[32m[20221213 18:55:18 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.80
[32m[20221213 18:55:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:55:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 37.00
[32m[20221213 18:55:18 @agent_ppo2.py:143][0m Total time:       6.75 min
[32m[20221213 18:55:18 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221213 18:55:18 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 18:55:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |           0.0046 |           2.2473 |          20.8546 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |          -0.0035 |           2.1509 |          20.8435 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |           0.0024 |           2.1316 |          20.8512 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |          -0.0030 |           2.1295 |          20.8406 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |           0.0035 |           2.1830 |          20.8433 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |          -0.0029 |           2.0922 |          20.8338 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |          -0.0053 |           2.0934 |          20.8323 |
[32m[20221213 18:55:18 @agent_ppo2.py:185][0m |          -0.0023 |           2.0784 |          20.8332 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |           0.0011 |           2.0665 |          20.8338 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0015 |           2.0705 |          20.8289 |
[32m[20221213 18:55:19 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 37.60
[32m[20221213 18:55:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 18:55:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 18:55:19 @agent_ppo2.py:143][0m Total time:       6.77 min
[32m[20221213 18:55:19 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 18:55:19 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 18:55:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0071 |           1.6794 |          20.8948 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0033 |           1.5984 |          20.8915 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0052 |           1.5712 |          20.8933 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0030 |           1.5732 |          20.8941 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0054 |           1.5466 |          20.9042 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0051 |           1.5379 |          20.9135 |
[32m[20221213 18:55:19 @agent_ppo2.py:185][0m |          -0.0065 |           1.5407 |          20.9137 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0039 |           1.5242 |          20.9185 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0046 |           1.5380 |          20.9225 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0100 |           1.5199 |          20.9178 |
[32m[20221213 18:55:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 72.00
[32m[20221213 18:55:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 78.00
[32m[20221213 18:55:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 18:55:20 @agent_ppo2.py:143][0m Total time:       6.79 min
[32m[20221213 18:55:20 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221213 18:55:20 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 18:55:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0004 |           2.1226 |          20.7915 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0056 |           2.0682 |          20.7631 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |           0.0012 |           2.0854 |          20.7635 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0006 |           2.0265 |          20.7436 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0040 |           2.0026 |          20.7477 |
[32m[20221213 18:55:20 @agent_ppo2.py:185][0m |          -0.0059 |           1.9875 |          20.7647 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |           0.0173 |           2.3128 |          20.7495 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |          -0.0069 |           2.0020 |          20.7608 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |          -0.0045 |           1.9504 |          20.7414 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |           0.0071 |           2.0534 |          20.7533 |
[32m[20221213 18:55:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.80
[32m[20221213 18:55:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 18:55:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.00
[32m[20221213 18:55:21 @agent_ppo2.py:143][0m Total time:       6.81 min
[32m[20221213 18:55:21 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 18:55:21 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 18:55:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |          -0.0198 |           2.5795 |          20.6901 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |          -0.0075 |           2.3920 |          20.6960 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |          -0.0035 |           2.3561 |          20.6832 |
[32m[20221213 18:55:21 @agent_ppo2.py:185][0m |          -0.0086 |           2.3298 |          20.6943 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0079 |           2.3168 |          20.6863 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0032 |           2.2948 |          20.6884 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0047 |           2.2821 |          20.6865 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0097 |           2.2723 |          20.6945 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0070 |           2.2462 |          20.6806 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0038 |           2.2342 |          20.6843 |
[32m[20221213 18:55:22 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.00
[32m[20221213 18:55:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 81.00
[32m[20221213 18:55:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 18:55:22 @agent_ppo2.py:143][0m Total time:       6.83 min
[32m[20221213 18:55:22 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221213 18:55:22 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 18:55:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0026 |           1.9785 |          20.6702 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0045 |           1.9179 |          20.6598 |
[32m[20221213 18:55:22 @agent_ppo2.py:185][0m |          -0.0090 |           1.8920 |          20.6650 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |           0.0010 |           1.8978 |          20.6736 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0030 |           1.8722 |          20.6835 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0072 |           1.8545 |          20.6825 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0025 |           1.8452 |          20.6865 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0067 |           1.8390 |          20.6814 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0034 |           1.8541 |          20.6892 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0085 |           1.8285 |          20.6948 |
[32m[20221213 18:55:23 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.20
[32m[20221213 18:55:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.00
[32m[20221213 18:55:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 18:55:23 @agent_ppo2.py:143][0m Total time:       6.84 min
[32m[20221213 18:55:23 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 18:55:23 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 18:55:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0097 |           5.0800 |          20.6797 |
[32m[20221213 18:55:23 @agent_ppo2.py:185][0m |          -0.0044 |           4.8744 |          20.6529 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0062 |           4.5635 |          20.6697 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0053 |           4.4318 |          20.6489 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0098 |           4.3053 |          20.6562 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0092 |           4.2394 |          20.6379 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0014 |           4.2576 |          20.6501 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0057 |           4.0685 |          20.6393 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0071 |           3.9799 |          20.6420 |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |          -0.0021 |           3.9606 |          20.6367 |
[32m[20221213 18:55:24 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.80
[32m[20221213 18:55:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.00
[32m[20221213 18:55:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 18:55:24 @agent_ppo2.py:143][0m Total time:       6.86 min
[32m[20221213 18:55:24 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221213 18:55:24 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 18:55:24 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:24 @agent_ppo2.py:185][0m |           0.0008 |           2.5507 |          20.6351 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |           0.0046 |           2.5618 |          20.6125 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0065 |           2.4513 |          20.6019 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0040 |           2.4469 |          20.6348 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0089 |           2.4416 |          20.6241 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0060 |           2.4341 |          20.6221 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0083 |           2.4154 |          20.6237 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0063 |           2.4272 |          20.6242 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0061 |           2.4124 |          20.6230 |
[32m[20221213 18:55:25 @agent_ppo2.py:185][0m |          -0.0028 |           2.4111 |          20.6162 |
[32m[20221213 18:55:25 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.40
[32m[20221213 18:55:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.00
[32m[20221213 18:55:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 18:55:25 @agent_ppo2.py:143][0m Total time:       6.88 min
[32m[20221213 18:55:25 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 18:55:25 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 18:55:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |           0.0034 |           3.8262 |          20.6255 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |          -0.0039 |           3.2109 |          20.6264 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |           0.0000 |           3.0233 |          20.6077 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |          -0.0033 |           2.9661 |          20.6041 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |          -0.0058 |           2.9124 |          20.5974 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |          -0.0029 |           2.9070 |          20.5930 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |           0.0031 |           2.9987 |          20.5841 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |          -0.0103 |           2.8579 |          20.5777 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |          -0.0033 |           2.8266 |          20.5722 |
[32m[20221213 18:55:26 @agent_ppo2.py:185][0m |           0.0076 |           2.9584 |          20.5778 |
[32m[20221213 18:55:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.20
[32m[20221213 18:55:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 79.00
[32m[20221213 18:55:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.00
[32m[20221213 18:55:26 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221213 18:55:26 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221213 18:55:26 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 18:55:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |           0.0069 |           5.1209 |          20.7306 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |          -0.0034 |           4.8979 |          20.7229 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |          -0.0022 |           4.8859 |          20.7111 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |           0.0036 |           4.8284 |          20.7030 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |          -0.0031 |           4.8073 |          20.7018 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |          -0.0036 |           4.7730 |          20.6958 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |           0.0014 |           4.7205 |          20.6907 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |           0.0012 |           4.7163 |          20.6843 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |          -0.0042 |           4.6783 |          20.6854 |
[32m[20221213 18:55:27 @agent_ppo2.py:185][0m |          -0.0001 |           4.6516 |          20.6831 |
[32m[20221213 18:55:27 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.60
[32m[20221213 18:55:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 18:55:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 18:55:27 @agent_ppo2.py:143][0m Total time:       6.92 min
[32m[20221213 18:55:27 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 18:55:27 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 18:55:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:55:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0037 |           3.3314 |          20.7103 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0027 |           3.2098 |          20.7057 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0081 |           3.1951 |          20.6983 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0023 |           3.1683 |          20.6801 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0067 |           3.1414 |          20.6768 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0005 |           3.1393 |          20.6795 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0029 |           3.1207 |          20.6794 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0090 |           3.1133 |          20.6755 |
[32m[20221213 18:55:28 @agent_ppo2.py:185][0m |          -0.0066 |           3.0960 |          20.6731 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0057 |           3.0787 |          20.6691 |
[32m[20221213 18:55:29 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 39.20
[32m[20221213 18:55:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 18:55:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 41.00
[32m[20221213 18:55:29 @agent_ppo2.py:143][0m Total time:       6.94 min
[32m[20221213 18:55:29 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221213 18:55:29 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 18:55:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0007 |           2.4668 |          20.8437 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0043 |           2.3736 |          20.8370 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0000 |           2.4043 |          20.8278 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0012 |           2.3406 |          20.8385 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |           0.0019 |           2.3613 |          20.8372 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |           0.0005 |           2.3397 |          20.8303 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0033 |           2.3003 |          20.8301 |
[32m[20221213 18:55:29 @agent_ppo2.py:185][0m |          -0.0044 |           2.2841 |          20.8299 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0104 |           2.2716 |          20.8389 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0099 |           2.2679 |          20.8330 |
[32m[20221213 18:55:30 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 23.60
[32m[20221213 18:55:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 18:55:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 18:55:30 @agent_ppo2.py:143][0m Total time:       6.95 min
[32m[20221213 18:55:30 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 18:55:30 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 18:55:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0027 |           3.4211 |          20.7050 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0005 |           3.3348 |          20.6986 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |           0.0008 |           3.3936 |          20.7099 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0093 |           3.2090 |          20.7170 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |           0.0021 |           3.2356 |          20.7062 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0057 |           3.1761 |          20.7143 |
[32m[20221213 18:55:30 @agent_ppo2.py:185][0m |          -0.0062 |           3.1412 |          20.7042 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |          -0.0071 |           3.1302 |          20.7149 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |          -0.0058 |           3.1413 |          20.7122 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |          -0.0064 |           3.1218 |          20.7201 |
[32m[20221213 18:55:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.00
[32m[20221213 18:55:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.00
[32m[20221213 18:55:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 18:55:31 @agent_ppo2.py:143][0m Total time:       6.97 min
[32m[20221213 18:55:31 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221213 18:55:31 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 18:55:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |           0.0149 |           4.6158 |          20.7301 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |           0.0025 |           4.2058 |          20.7264 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |           0.0006 |           4.1076 |          20.7214 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |          -0.0003 |           4.0903 |          20.7136 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |          -0.0038 |           4.0713 |          20.7014 |
[32m[20221213 18:55:31 @agent_ppo2.py:185][0m |           0.0037 |           4.2341 |          20.7025 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |           0.0085 |           4.2894 |          20.6942 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0072 |           4.0436 |          20.7010 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0040 |           3.9937 |          20.6916 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0016 |           4.0808 |          20.6899 |
[32m[20221213 18:55:32 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.20
[32m[20221213 18:55:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 18:55:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 18:55:32 @agent_ppo2.py:143][0m Total time:       6.99 min
[32m[20221213 18:55:32 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 18:55:32 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 18:55:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |           0.0043 |           3.5762 |          20.7640 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0037 |           3.4317 |          20.7545 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0083 |           3.4284 |          20.7453 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0021 |           3.3925 |          20.7445 |
[32m[20221213 18:55:32 @agent_ppo2.py:185][0m |          -0.0073 |           3.3909 |          20.7438 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |          -0.0030 |           3.3797 |          20.7287 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |          -0.0022 |           3.3739 |          20.7318 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |           0.0146 |           3.7048 |          20.7177 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |          -0.0036 |           3.3776 |          20.7215 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |          -0.0056 |           3.3638 |          20.7176 |
[32m[20221213 18:55:33 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.20
[32m[20221213 18:55:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:55:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 97.00
[32m[20221213 18:55:33 @agent_ppo2.py:143][0m Total time:       7.01 min
[32m[20221213 18:55:33 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221213 18:55:33 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 18:55:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |          -0.0010 |           3.7261 |          20.7334 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |          -0.0057 |           3.6107 |          20.7267 |
[32m[20221213 18:55:33 @agent_ppo2.py:185][0m |           0.0012 |           3.5427 |          20.7064 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |           0.0014 |           3.5469 |          20.6865 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0076 |           3.5084 |          20.6956 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0050 |           3.5014 |          20.6789 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0030 |           3.4813 |          20.6728 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0066 |           3.4634 |          20.6753 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0061 |           3.4474 |          20.6786 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0048 |           3.4243 |          20.6799 |
[32m[20221213 18:55:34 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.20
[32m[20221213 18:55:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.00
[32m[20221213 18:55:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.00
[32m[20221213 18:55:34 @agent_ppo2.py:143][0m Total time:       7.03 min
[32m[20221213 18:55:34 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 18:55:34 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 18:55:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0020 |           4.4781 |          20.8702 |
[32m[20221213 18:55:34 @agent_ppo2.py:185][0m |          -0.0018 |           4.3849 |          20.8409 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0036 |           4.3562 |          20.8344 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0032 |           4.3666 |          20.8176 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0013 |           4.3233 |          20.7961 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |           0.0012 |           4.2998 |          20.7917 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0082 |           4.3038 |          20.7772 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0057 |           4.3019 |          20.7728 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0055 |           4.3059 |          20.7648 |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |          -0.0030 |           4.2555 |          20.7525 |
[32m[20221213 18:55:35 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.40
[32m[20221213 18:55:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 18:55:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.00
[32m[20221213 18:55:35 @agent_ppo2.py:143][0m Total time:       7.05 min
[32m[20221213 18:55:35 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221213 18:55:35 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 18:55:35 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:35 @agent_ppo2.py:185][0m |           0.0012 |           5.1096 |          20.8530 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |           0.0004 |           4.8168 |          20.8340 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |           0.0062 |           4.8274 |          20.8468 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |          -0.0026 |           4.6775 |          20.8460 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |          -0.0109 |           4.7220 |          20.8484 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |           0.0060 |           5.0886 |          20.8420 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |          -0.0082 |           4.7083 |          20.8544 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |          -0.0095 |           4.6698 |          20.8530 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |          -0.0021 |           4.5539 |          20.8518 |
[32m[20221213 18:55:36 @agent_ppo2.py:185][0m |          -0.0003 |           4.5126 |          20.8492 |
[32m[20221213 18:55:36 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 94.80
[32m[20221213 18:55:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.00
[32m[20221213 18:55:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.00
[32m[20221213 18:55:36 @agent_ppo2.py:143][0m Total time:       7.06 min
[32m[20221213 18:55:36 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 18:55:36 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 18:55:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0006 |           9.1542 |          20.7718 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0022 |           8.4365 |          20.7426 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0082 |           8.1670 |          20.7299 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0033 |           7.9430 |          20.7174 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0086 |           7.7376 |          20.7179 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0037 |           7.6513 |          20.7177 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0151 |           7.6248 |          20.7129 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0075 |           7.3555 |          20.7126 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0094 |           7.2346 |          20.7026 |
[32m[20221213 18:55:37 @agent_ppo2.py:185][0m |          -0.0018 |           7.1669 |          20.7018 |
[32m[20221213 18:55:37 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.40
[32m[20221213 18:55:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.00
[32m[20221213 18:55:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.00
[32m[20221213 18:55:37 @agent_ppo2.py:143][0m Total time:       7.08 min
[32m[20221213 18:55:37 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221213 18:55:37 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 18:55:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0020 |           7.3504 |          20.9449 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0112 |           7.0543 |          20.9177 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0143 |           6.9828 |          20.9031 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0079 |           6.8976 |          20.8742 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0053 |           6.8900 |          20.8717 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0035 |           7.0314 |          20.8545 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0064 |           6.7088 |          20.8381 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0054 |           6.7056 |          20.8457 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0095 |           6.8532 |          20.8426 |
[32m[20221213 18:55:38 @agent_ppo2.py:185][0m |          -0.0141 |           6.6567 |          20.8207 |
[32m[20221213 18:55:38 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.80
[32m[20221213 18:55:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.00
[32m[20221213 18:55:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 105.00
[32m[20221213 18:55:38 @agent_ppo2.py:143][0m Total time:       7.10 min
[32m[20221213 18:55:38 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 18:55:38 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 18:55:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:55:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0016 |          10.1781 |          20.9434 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0034 |           9.4091 |          20.9188 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0029 |           9.1904 |          20.8927 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0106 |           9.1411 |          20.8800 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0134 |           8.9958 |          20.8767 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0088 |           8.8658 |          20.8646 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0047 |           8.7334 |          20.8759 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0103 |           8.8364 |          20.8723 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0098 |           8.5905 |          20.8618 |
[32m[20221213 18:55:39 @agent_ppo2.py:185][0m |          -0.0083 |           8.5021 |          20.8697 |
[32m[20221213 18:55:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.40
[32m[20221213 18:55:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.00
[32m[20221213 18:55:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.00
[32m[20221213 18:55:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 156.00
[32m[20221213 18:55:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 156.00
[32m[20221213 18:55:40 @agent_ppo2.py:143][0m Total time:       7.12 min
[32m[20221213 18:55:40 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221213 18:55:40 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 18:55:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0085 |          13.9601 |          21.0318 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0088 |          12.7387 |          21.0118 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0102 |          12.4284 |          21.0039 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0131 |          12.3324 |          21.0020 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0063 |          12.2008 |          20.9859 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0053 |          11.9509 |          20.9744 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0039 |          11.8066 |          20.9687 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0106 |          11.6321 |          20.9738 |
[32m[20221213 18:55:40 @agent_ppo2.py:185][0m |          -0.0110 |          11.5938 |          20.9672 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |          -0.0203 |          11.4999 |          20.9629 |
[32m[20221213 18:55:41 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.60
[32m[20221213 18:55:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.00
[32m[20221213 18:55:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.00
[32m[20221213 18:55:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 182.00
[32m[20221213 18:55:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 182.00
[32m[20221213 18:55:41 @agent_ppo2.py:143][0m Total time:       7.14 min
[32m[20221213 18:55:41 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 18:55:41 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 18:55:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |           0.0037 |          18.9673 |          21.0219 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |          -0.0049 |          17.2882 |          21.0041 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |           0.0033 |          16.6523 |          20.9857 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |          -0.0058 |          16.2461 |          20.9883 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |          -0.0080 |          15.7766 |          20.9781 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |          -0.0074 |          15.7137 |          20.9686 |
[32m[20221213 18:55:41 @agent_ppo2.py:185][0m |          -0.0030 |          15.2187 |          20.9791 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |           0.0014 |          14.9465 |          20.9866 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |          -0.0042 |          14.8497 |          20.9858 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |          -0.0056 |          14.6180 |          20.9811 |
[32m[20221213 18:55:42 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:55:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.40
[32m[20221213 18:55:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.00
[32m[20221213 18:55:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.00
[32m[20221213 18:55:42 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 353.00
[32m[20221213 18:55:42 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 353.00
[32m[20221213 18:55:42 @agent_ppo2.py:143][0m Total time:       7.16 min
[32m[20221213 18:55:42 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221213 18:55:42 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 18:55:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |           0.0071 |          29.8980 |          21.1056 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |          -0.0068 |          28.6833 |          21.0791 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |          -0.0011 |          28.0697 |          21.0682 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |          -0.0047 |          27.2934 |          21.0490 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |           0.0039 |          27.7162 |          21.0388 |
[32m[20221213 18:55:42 @agent_ppo2.py:185][0m |          -0.0007 |          26.7570 |          21.0280 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |          -0.0054 |          26.6067 |          21.0128 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |           0.0027 |          26.5369 |          21.0001 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |           0.0006 |          26.2134 |          20.9914 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |          -0.0036 |          25.9873 |          20.9793 |
[32m[20221213 18:55:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.80
[32m[20221213 18:55:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 189.00
[32m[20221213 18:55:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.00
[32m[20221213 18:55:43 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 935.00
[32m[20221213 18:55:43 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 935.00
[32m[20221213 18:55:43 @agent_ppo2.py:143][0m Total time:       7.17 min
[32m[20221213 18:55:43 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 18:55:43 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 18:55:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |          -0.0043 |          46.1433 |          21.0915 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |          -0.0045 |          39.1071 |          21.0227 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |          -0.0024 |          37.0405 |          21.0002 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |           0.0035 |          36.9460 |          20.9970 |
[32m[20221213 18:55:43 @agent_ppo2.py:185][0m |          -0.0060 |          34.4715 |          20.9850 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0067 |          33.5206 |          20.9736 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0017 |          33.3448 |          20.9730 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0059 |          32.6781 |          20.9538 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0017 |          32.3418 |          20.9477 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0004 |          31.5254 |          20.9490 |
[32m[20221213 18:55:44 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.20
[32m[20221213 18:55:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.00
[32m[20221213 18:55:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.00
[32m[20221213 18:55:44 @agent_ppo2.py:143][0m Total time:       7.19 min
[32m[20221213 18:55:44 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221213 18:55:44 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 18:55:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |           0.0013 |          85.1409 |          21.1194 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0083 |          69.6238 |          21.0635 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0098 |          66.7292 |          21.0256 |
[32m[20221213 18:55:44 @agent_ppo2.py:185][0m |          -0.0079 |          65.3022 |          20.9766 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0132 |          63.0249 |          20.9506 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0095 |          62.4880 |          20.9268 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |           0.0016 |          63.8792 |          20.9067 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0087 |          61.1547 |          20.9096 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0126 |          61.3896 |          20.8856 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0048 |          60.5006 |          20.8776 |
[32m[20221213 18:55:45 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.60
[32m[20221213 18:55:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 18:55:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 18:55:45 @agent_ppo2.py:143][0m Total time:       7.21 min
[32m[20221213 18:55:45 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 18:55:45 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 18:55:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0029 |          87.1042 |          21.1231 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |          -0.0024 |          69.5731 |          21.0875 |
[32m[20221213 18:55:45 @agent_ppo2.py:185][0m |           0.0017 |          67.1562 |          21.0687 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |          -0.0030 |          66.2929 |          21.0566 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |          -0.0044 |          66.6867 |          21.0407 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |           0.0002 |          65.0366 |          21.0479 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |          -0.0034 |          65.0380 |          21.0366 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |          -0.0064 |          64.6380 |          21.0131 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |          -0.0017 |          64.3205 |          21.0181 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |           0.0038 |          67.5024 |          21.0191 |
[32m[20221213 18:55:46 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.40
[32m[20221213 18:55:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.00
[32m[20221213 18:55:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.00
[32m[20221213 18:55:46 @agent_ppo2.py:143][0m Total time:       7.23 min
[32m[20221213 18:55:46 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221213 18:55:46 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 18:55:46 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |           0.0034 |          94.6723 |          21.0577 |
[32m[20221213 18:55:46 @agent_ppo2.py:185][0m |          -0.0010 |          81.3834 |          21.0414 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0022 |          78.2887 |          21.0087 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0034 |          76.9577 |          21.0009 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0016 |          76.2437 |          20.9690 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0032 |          75.8040 |          20.9666 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0046 |          75.6212 |          20.9803 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0018 |          75.1207 |          20.9770 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0086 |          75.3852 |          20.9549 |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |           0.0027 |          74.9675 |          20.9646 |
[32m[20221213 18:55:47 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.20
[32m[20221213 18:55:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.00
[32m[20221213 18:55:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 18:55:47 @agent_ppo2.py:143][0m Total time:       7.25 min
[32m[20221213 18:55:47 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 18:55:47 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 18:55:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:47 @agent_ppo2.py:185][0m |          -0.0021 |         120.8542 |          21.0545 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0022 |          90.1010 |          21.0506 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0031 |          83.0700 |          21.0457 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0023 |          81.7785 |          21.0206 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0024 |          79.7587 |          21.0378 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |           0.0016 |          79.8234 |          20.9996 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0026 |          78.7789 |          21.0030 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0028 |          78.5633 |          20.9960 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0032 |          78.2419 |          20.9841 |
[32m[20221213 18:55:48 @agent_ppo2.py:185][0m |          -0.0033 |          78.0083 |          20.9668 |
[32m[20221213 18:55:48 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:55:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 903.00
[32m[20221213 18:55:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.00
[32m[20221213 18:55:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:55:48 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 1000.00
[32m[20221213 18:55:48 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 1000.00
[32m[20221213 18:55:48 @agent_ppo2.py:143][0m Total time:       7.26 min
[32m[20221213 18:55:48 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221213 18:55:48 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 18:55:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0048 |         137.0176 |          21.0908 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0025 |         121.8602 |          21.0683 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0046 |         116.8505 |          21.0133 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0012 |         115.5863 |          21.0151 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |           0.0028 |         114.1068 |          20.9910 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0022 |         113.1083 |          20.9652 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0018 |         111.9173 |          20.9451 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0051 |         111.6826 |          20.9435 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0016 |         111.4461 |          20.9287 |
[32m[20221213 18:55:49 @agent_ppo2.py:185][0m |          -0.0005 |         110.6696 |          20.9044 |
[32m[20221213 18:55:49 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.20
[32m[20221213 18:55:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.00
[32m[20221213 18:55:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.00
[32m[20221213 18:55:49 @agent_ppo2.py:143][0m Total time:       7.28 min
[32m[20221213 18:55:49 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 18:55:49 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 18:55:50 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |           0.0012 |         130.7020 |          20.9641 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |           0.0047 |         121.0533 |          20.9048 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |          -0.0012 |         118.3603 |          20.8705 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |           0.0019 |         116.7941 |          20.8507 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |           0.0006 |         116.4105 |          20.8496 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |          -0.0043 |         115.6970 |          20.8294 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |           0.0018 |         115.7456 |          20.8153 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |          -0.0031 |         115.1369 |          20.7832 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |          -0.0026 |         114.7677 |          20.7702 |
[32m[20221213 18:55:50 @agent_ppo2.py:185][0m |          -0.0039 |         114.6230 |          20.7556 |
[32m[20221213 18:55:50 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.20
[32m[20221213 18:55:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 18:55:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 934.00
[32m[20221213 18:55:50 @agent_ppo2.py:143][0m Total time:       7.30 min
[32m[20221213 18:55:50 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221213 18:55:50 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 18:55:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |          -0.0021 |         153.7879 |          20.8680 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |          -0.0029 |         146.2078 |          20.8527 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |          -0.0018 |         143.6777 |          20.8010 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |           0.0021 |         142.7023 |          20.8163 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |           0.0004 |         140.5635 |          20.7802 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |          -0.0024 |         140.1328 |          20.7924 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |           0.0105 |         147.9821 |          20.7926 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |           0.0009 |         139.2370 |          20.7809 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |          -0.0050 |         137.7817 |          20.7707 |
[32m[20221213 18:55:51 @agent_ppo2.py:185][0m |          -0.0005 |         137.3454 |          20.7788 |
[32m[20221213 18:55:51 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.20
[32m[20221213 18:55:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 18:55:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.00
[32m[20221213 18:55:52 @agent_ppo2.py:143][0m Total time:       7.32 min
[32m[20221213 18:55:52 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 18:55:52 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 18:55:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |           0.0021 |         148.5261 |          20.8893 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |           0.0009 |         142.3211 |          20.8716 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |          -0.0023 |         140.3605 |          20.8592 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |           0.0010 |         139.5449 |          20.8269 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |           0.0024 |         138.8087 |          20.8331 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |          -0.0027 |         138.4246 |          20.8183 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |          -0.0017 |         137.9675 |          20.7953 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |          -0.0031 |         137.6645 |          20.8125 |
[32m[20221213 18:55:52 @agent_ppo2.py:185][0m |          -0.0038 |         137.1972 |          20.8191 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0053 |         137.2319 |          20.8090 |
[32m[20221213 18:55:53 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.20
[32m[20221213 18:55:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 18:55:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.00
[32m[20221213 18:55:53 @agent_ppo2.py:143][0m Total time:       7.34 min
[32m[20221213 18:55:53 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221213 18:55:53 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 18:55:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0020 |         152.0929 |          20.8329 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0025 |         143.3411 |          20.8120 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0024 |         141.6121 |          20.8289 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0015 |         140.7167 |          20.8254 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |           0.0023 |         140.3435 |          20.8316 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0067 |         138.9442 |          20.8112 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0026 |         137.6916 |          20.8206 |
[32m[20221213 18:55:53 @agent_ppo2.py:185][0m |          -0.0022 |         136.8267 |          20.8242 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |          -0.0043 |         136.5438 |          20.8261 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |          -0.0069 |         136.6116 |          20.8148 |
[32m[20221213 18:55:54 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.40
[32m[20221213 18:55:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 18:55:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 672.00
[32m[20221213 18:55:54 @agent_ppo2.py:143][0m Total time:       7.36 min
[32m[20221213 18:55:54 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 18:55:54 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 18:55:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |          -0.0003 |         157.3581 |          20.6974 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |          -0.0007 |         150.3179 |          20.6282 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |           0.0006 |         148.4221 |          20.5633 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |           0.0117 |         162.3098 |          20.5573 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |          -0.0040 |         146.9863 |          20.5162 |
[32m[20221213 18:55:54 @agent_ppo2.py:185][0m |           0.0004 |         146.5643 |          20.5211 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0040 |         145.8820 |          20.4909 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0039 |         145.3974 |          20.4791 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |           0.0088 |         153.0268 |          20.4607 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |           0.0063 |         153.6945 |          20.4070 |
[32m[20221213 18:55:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.60
[32m[20221213 18:55:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.00
[32m[20221213 18:55:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 18:55:55 @agent_ppo2.py:143][0m Total time:       7.37 min
[32m[20221213 18:55:55 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221213 18:55:55 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 18:55:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |           0.0031 |         182.6527 |          20.6530 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0025 |         168.3620 |          20.6576 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0001 |         163.9032 |          20.6708 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0018 |         163.2746 |          20.6585 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0030 |         161.7174 |          20.6256 |
[32m[20221213 18:55:55 @agent_ppo2.py:185][0m |          -0.0003 |         161.0898 |          20.6635 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |           0.0032 |         161.8480 |          20.6205 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |           0.0031 |         163.1493 |          20.6354 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |          -0.0007 |         159.3259 |          20.6129 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |          -0.0008 |         158.9320 |          20.6220 |
[32m[20221213 18:55:56 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:55:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 868.60
[32m[20221213 18:55:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 920.00
[32m[20221213 18:55:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 18:55:56 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221213 18:55:56 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 18:55:56 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 18:55:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |           0.0041 |         189.2746 |          20.6733 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |           0.0129 |         192.3816 |          20.6496 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |           0.0065 |         173.2158 |          20.6082 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |          -0.0022 |         171.8458 |          20.5916 |
[32m[20221213 18:55:56 @agent_ppo2.py:185][0m |          -0.0015 |         170.8211 |          20.5573 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |          -0.0006 |         170.0088 |          20.5431 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |           0.0050 |         175.1571 |          20.5152 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |          -0.0018 |         168.9265 |          20.5291 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |           0.0094 |         189.4712 |          20.5380 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |           0.0031 |         169.1705 |          20.4871 |
[32m[20221213 18:55:57 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.00
[32m[20221213 18:55:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 864.00
[32m[20221213 18:55:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.00
[32m[20221213 18:55:57 @agent_ppo2.py:143][0m Total time:       7.41 min
[32m[20221213 18:55:57 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221213 18:55:57 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 18:55:57 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:55:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |           0.0005 |         198.4647 |          20.5775 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |          -0.0007 |         188.6031 |          20.5474 |
[32m[20221213 18:55:57 @agent_ppo2.py:185][0m |          -0.0030 |         187.0701 |          20.5430 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0020 |         185.5132 |          20.5400 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |           0.0024 |         184.5321 |          20.5497 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0016 |         183.2836 |          20.5060 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0005 |         182.7251 |          20.5063 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0050 |         182.1815 |          20.4965 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0060 |         181.7195 |          20.5175 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0026 |         181.3638 |          20.4881 |
[32m[20221213 18:55:58 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:55:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.20
[32m[20221213 18:55:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.00
[32m[20221213 18:55:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 18:55:58 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221213 18:55:58 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 18:55:58 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 18:55:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |          -0.0001 |         181.0026 |          20.5621 |
[32m[20221213 18:55:58 @agent_ppo2.py:185][0m |           0.0027 |         176.8103 |          20.5701 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0009 |         174.6873 |          20.5629 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0002 |         174.0770 |          20.5833 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0012 |         173.3973 |          20.5922 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0017 |         172.9748 |          20.6053 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |           0.0044 |         179.2057 |          20.6289 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0018 |         171.5033 |          20.6406 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |           0.0003 |         170.7289 |          20.6154 |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0013 |         170.3844 |          20.6401 |
[32m[20221213 18:55:59 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:55:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.00
[32m[20221213 18:55:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 18:55:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 18:55:59 @agent_ppo2.py:143][0m Total time:       7.45 min
[32m[20221213 18:55:59 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221213 18:55:59 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 18:55:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:55:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:55:59 @agent_ppo2.py:185][0m |          -0.0061 |         180.2500 |          20.5466 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0060 |         159.5935 |          20.5129 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0087 |         156.1103 |          20.4854 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0051 |         154.0267 |          20.4538 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0051 |         152.8305 |          20.4023 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0080 |         150.5177 |          20.3855 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0008 |         148.8546 |          20.3394 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0067 |         147.6705 |          20.3078 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |           0.0015 |         153.1115 |          20.3126 |
[32m[20221213 18:56:00 @agent_ppo2.py:185][0m |          -0.0074 |         146.3856 |          20.3282 |
[32m[20221213 18:56:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:56:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.00
[32m[20221213 18:56:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 18:56:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:00 @agent_ppo2.py:143][0m Total time:       7.46 min
[32m[20221213 18:56:00 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 18:56:00 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 18:56:00 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:56:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0049 |         214.7102 |          20.4809 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0001 |         201.9145 |          20.4626 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |           0.0124 |         221.1685 |          20.4745 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0103 |         197.9966 |          20.4497 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0043 |         193.9195 |          20.4405 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |           0.0016 |         192.5525 |          20.4211 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0032 |         193.7215 |          20.3895 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0022 |         190.3552 |          20.3683 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0046 |         190.0275 |          20.3661 |
[32m[20221213 18:56:01 @agent_ppo2.py:185][0m |          -0.0043 |         189.5944 |          20.3626 |
[32m[20221213 18:56:01 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.40
[32m[20221213 18:56:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.00
[32m[20221213 18:56:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.00
[32m[20221213 18:56:01 @agent_ppo2.py:143][0m Total time:       7.48 min
[32m[20221213 18:56:01 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221213 18:56:01 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 18:56:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0001 |         206.3594 |          20.3364 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0058 |         191.6520 |          20.2528 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0027 |         190.4791 |          20.2390 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0031 |         189.4169 |          20.2113 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |           0.0000 |         189.5539 |          20.1435 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0062 |         187.8289 |          20.1181 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0060 |         188.5234 |          20.0834 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0052 |         187.6352 |          20.0760 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0031 |         187.3403 |          20.0664 |
[32m[20221213 18:56:02 @agent_ppo2.py:185][0m |          -0.0094 |         187.0947 |          20.0616 |
[32m[20221213 18:56:02 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.40
[32m[20221213 18:56:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 18:56:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.00
[32m[20221213 18:56:02 @agent_ppo2.py:143][0m Total time:       7.50 min
[32m[20221213 18:56:02 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 18:56:02 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 18:56:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0043 |         186.2557 |          19.9987 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0033 |         178.1698 |          19.9965 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0030 |         176.0922 |          19.9659 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0032 |         175.0959 |          19.9473 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0023 |         174.4441 |          19.9305 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0038 |         173.9557 |          19.9151 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0084 |         173.3599 |          19.8890 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0021 |         173.0554 |          19.8637 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0053 |         173.0460 |          19.8483 |
[32m[20221213 18:56:03 @agent_ppo2.py:185][0m |          -0.0027 |         172.4585 |          19.8507 |
[32m[20221213 18:56:03 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 597.60
[32m[20221213 18:56:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.00
[32m[20221213 18:56:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.00
[32m[20221213 18:56:04 @agent_ppo2.py:143][0m Total time:       7.52 min
[32m[20221213 18:56:04 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221213 18:56:04 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 18:56:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |           0.0003 |         212.4028 |          19.9792 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |           0.0016 |         212.5017 |          19.9882 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |           0.0001 |         207.8076 |          19.9679 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |          -0.0029 |         206.2686 |          20.0002 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |          -0.0035 |         206.1639 |          20.0082 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |          -0.0001 |         203.8903 |          19.9793 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |           0.0036 |         203.4524 |          19.9142 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |           0.0006 |         202.7377 |          19.9169 |
[32m[20221213 18:56:04 @agent_ppo2.py:185][0m |          -0.0033 |         203.6015 |          19.8925 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0002 |         203.8639 |          19.9125 |
[32m[20221213 18:56:05 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.80
[32m[20221213 18:56:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.00
[32m[20221213 18:56:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.00
[32m[20221213 18:56:05 @agent_ppo2.py:143][0m Total time:       7.54 min
[32m[20221213 18:56:05 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 18:56:05 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 18:56:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0018 |          99.5106 |          19.9707 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0053 |          85.8810 |          20.0399 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0062 |          82.4744 |          20.0673 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0071 |          82.8361 |          20.0947 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0042 |          79.9892 |          20.1535 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0017 |          80.1491 |          20.1656 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0090 |          78.2126 |          20.2239 |
[32m[20221213 18:56:05 @agent_ppo2.py:185][0m |          -0.0058 |          78.6804 |          20.2573 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |          -0.0066 |          77.1726 |          20.2506 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0044 |          78.8423 |          20.2819 |
[32m[20221213 18:56:06 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:56:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 170.00
[32m[20221213 18:56:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.00
[32m[20221213 18:56:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.00
[32m[20221213 18:56:06 @agent_ppo2.py:143][0m Total time:       7.55 min
[32m[20221213 18:56:06 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221213 18:56:06 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 18:56:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0023 |         202.9154 |          20.1780 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |          -0.0011 |         197.3781 |          20.2059 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0017 |         196.0591 |          20.2103 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0074 |         204.8540 |          20.2618 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0118 |         210.1899 |          20.2335 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0027 |         197.6900 |          20.2911 |
[32m[20221213 18:56:06 @agent_ppo2.py:185][0m |           0.0056 |         197.6277 |          20.2466 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0057 |         196.7400 |          20.2308 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |          -0.0000 |         195.4793 |          20.2389 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0030 |         198.3832 |          20.2529 |
[32m[20221213 18:56:07 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:56:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 930.20
[32m[20221213 18:56:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 18:56:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.00
[32m[20221213 18:56:07 @agent_ppo2.py:143][0m Total time:       7.57 min
[32m[20221213 18:56:07 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 18:56:07 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 18:56:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0017 |         216.3834 |          20.2982 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0005 |         211.4944 |          20.3262 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0007 |         210.2103 |          20.3476 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0086 |         222.3038 |          20.3579 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |          -0.0016 |         208.7448 |          20.3470 |
[32m[20221213 18:56:07 @agent_ppo2.py:185][0m |           0.0124 |         237.8113 |          20.3507 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |           0.0041 |         208.1084 |          20.2791 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |          -0.0037 |         207.8595 |          20.3622 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |          -0.0020 |         207.5388 |          20.3531 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |          -0.0036 |         207.2356 |          20.3732 |
[32m[20221213 18:56:08 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.00
[32m[20221213 18:56:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 18:56:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 690.00
[32m[20221213 18:56:08 @agent_ppo2.py:143][0m Total time:       7.59 min
[32m[20221213 18:56:08 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221213 18:56:08 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 18:56:08 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:56:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |           0.0027 |         225.4366 |          20.3918 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |           0.0001 |         219.7911 |          20.3462 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |           0.0005 |         218.5680 |          20.3478 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |          -0.0031 |         217.7574 |          20.3291 |
[32m[20221213 18:56:08 @agent_ppo2.py:185][0m |           0.0072 |         230.3143 |          20.3006 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0045 |         216.6824 |          20.3124 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0034 |         216.0855 |          20.3050 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0045 |         215.6923 |          20.3128 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0011 |         215.3635 |          20.3015 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0011 |         216.0922 |          20.2886 |
[32m[20221213 18:56:09 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.80
[32m[20221213 18:56:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 18:56:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 18:56:09 @agent_ppo2.py:143][0m Total time:       7.61 min
[32m[20221213 18:56:09 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 18:56:09 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 18:56:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |           0.0002 |         214.1494 |          20.2024 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |           0.0050 |         213.8621 |          20.1965 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0020 |         211.0363 |          20.1810 |
[32m[20221213 18:56:09 @agent_ppo2.py:185][0m |          -0.0017 |         210.5239 |          20.1615 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |          -0.0020 |         210.1204 |          20.1452 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |           0.0097 |         226.7205 |          20.1311 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |          -0.0036 |         209.5301 |          20.1185 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |           0.0012 |         210.0396 |          20.1157 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |           0.0044 |         211.7610 |          20.0868 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |          -0.0012 |         208.9283 |          20.0733 |
[32m[20221213 18:56:10 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.00
[32m[20221213 18:56:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.00
[32m[20221213 18:56:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 937.00
[32m[20221213 18:56:10 @agent_ppo2.py:143][0m Total time:       7.63 min
[32m[20221213 18:56:10 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221213 18:56:10 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 18:56:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |          -0.0028 |         231.2070 |          20.2150 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |          -0.0012 |         227.4573 |          20.2255 |
[32m[20221213 18:56:10 @agent_ppo2.py:185][0m |           0.0040 |         228.9894 |          20.2558 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0040 |         225.7456 |          20.2578 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0020 |         224.9479 |          20.3061 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |           0.0142 |         249.4345 |          20.3095 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0024 |         224.2232 |          20.2961 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0049 |         224.6235 |          20.3209 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0027 |         223.7109 |          20.3405 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |           0.0013 |         225.9765 |          20.3429 |
[32m[20221213 18:56:11 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.60
[32m[20221213 18:56:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 18:56:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 987.00
[32m[20221213 18:56:11 @agent_ppo2.py:143][0m Total time:       7.64 min
[32m[20221213 18:56:11 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 18:56:11 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 18:56:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:56:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0035 |         196.0646 |          20.2006 |
[32m[20221213 18:56:11 @agent_ppo2.py:185][0m |          -0.0046 |         181.3232 |          20.1894 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |          -0.0036 |         177.8740 |          20.1687 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |           0.0016 |         177.0703 |          20.1458 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |          -0.0054 |         176.6815 |          20.1387 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |          -0.0030 |         176.3890 |          20.1165 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |           0.0041 |         183.7722 |          20.1195 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |           0.0070 |         181.2044 |          20.1206 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |          -0.0045 |         176.2294 |          20.1092 |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |          -0.0041 |         175.9825 |          20.0305 |
[32m[20221213 18:56:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.00
[32m[20221213 18:56:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:56:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 18:56:12 @agent_ppo2.py:143][0m Total time:       7.66 min
[32m[20221213 18:56:12 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221213 18:56:12 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 18:56:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:12 @agent_ppo2.py:185][0m |          -0.0003 |         220.8445 |          20.0938 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0010 |         216.0848 |          20.0313 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0029 |         211.6024 |          20.0083 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0037 |         209.1751 |          19.9258 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0022 |         207.5014 |          19.9620 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0022 |         206.0861 |          19.9415 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0038 |         205.3066 |          19.8901 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0009 |         204.4864 |          19.8414 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0045 |         204.6772 |          19.8570 |
[32m[20221213 18:56:13 @agent_ppo2.py:185][0m |          -0.0016 |         204.4929 |          19.8085 |
[32m[20221213 18:56:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 933.40
[32m[20221213 18:56:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.00
[32m[20221213 18:56:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.00
[32m[20221213 18:56:13 @agent_ppo2.py:143][0m Total time:       7.68 min
[32m[20221213 18:56:13 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 18:56:13 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 18:56:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |           0.0006 |         257.6820 |          19.8897 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0051 |         243.6213 |          19.8422 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0012 |         240.7522 |          19.8407 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0069 |         239.2262 |          19.8548 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0036 |         238.2972 |          19.8439 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |           0.0026 |         240.7007 |          19.8492 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |           0.0029 |         236.5365 |          19.7997 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0024 |         235.8627 |          19.8056 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0061 |         235.4966 |          19.7903 |
[32m[20221213 18:56:14 @agent_ppo2.py:185][0m |          -0.0016 |         235.3223 |          19.7833 |
[32m[20221213 18:56:14 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.40
[32m[20221213 18:56:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 18:56:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.00
[32m[20221213 18:56:14 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221213 18:56:14 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221213 18:56:14 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 18:56:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |           0.0092 |         290.9781 |          19.8576 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |           0.0042 |         252.7439 |          19.8329 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |          -0.0000 |         250.5999 |          19.8707 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |          -0.0009 |         249.0941 |          19.8366 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |           0.0132 |         281.0904 |          19.8470 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |           0.0015 |         247.1895 |          19.8242 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |          -0.0005 |         245.8978 |          19.8110 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |          -0.0040 |         246.3180 |          19.7986 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |          -0.0050 |         245.2536 |          19.8254 |
[32m[20221213 18:56:15 @agent_ppo2.py:185][0m |          -0.0000 |         243.7685 |          19.8234 |
[32m[20221213 18:56:15 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.00
[32m[20221213 18:56:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 18:56:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.00
[32m[20221213 18:56:15 @agent_ppo2.py:143][0m Total time:       7.72 min
[32m[20221213 18:56:15 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 18:56:15 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 18:56:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |           0.0050 |         186.1307 |          19.7556 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0016 |         168.2417 |          19.6929 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |           0.0040 |         167.7149 |          19.6553 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |           0.0036 |         162.9738 |          19.6114 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0050 |         162.1261 |          19.5505 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0022 |         161.5164 |          19.5295 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0046 |         160.4241 |          19.4647 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0015 |         159.7764 |          19.4642 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0020 |         159.3709 |          19.4443 |
[32m[20221213 18:56:16 @agent_ppo2.py:185][0m |          -0.0060 |         159.5504 |          19.4287 |
[32m[20221213 18:56:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.80
[32m[20221213 18:56:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 18:56:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.00
[32m[20221213 18:56:17 @agent_ppo2.py:143][0m Total time:       7.74 min
[32m[20221213 18:56:17 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221213 18:56:17 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 18:56:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |           0.0002 |         228.9957 |          19.5108 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |          -0.0027 |         220.6100 |          19.4713 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |          -0.0037 |         218.0416 |          19.4846 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |           0.0112 |         235.1887 |          19.4160 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |          -0.0034 |         215.7849 |          19.3445 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |          -0.0012 |         215.5962 |          19.3060 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |          -0.0029 |         214.1102 |          19.2487 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |           0.0026 |         213.6157 |          19.2268 |
[32m[20221213 18:56:17 @agent_ppo2.py:185][0m |           0.0000 |         213.4205 |          19.2482 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0022 |         212.3998 |          19.2404 |
[32m[20221213 18:56:18 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.00
[32m[20221213 18:56:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.00
[32m[20221213 18:56:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 18:56:18 @agent_ppo2.py:143][0m Total time:       7.75 min
[32m[20221213 18:56:18 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 18:56:18 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 18:56:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |           0.0043 |         230.9985 |          19.2324 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0029 |         227.2511 |          19.2295 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0017 |         224.4105 |          19.2091 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |           0.0127 |         257.2327 |          19.1877 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0021 |         223.2205 |          19.1942 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0042 |         222.1100 |          19.2239 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0019 |         221.3584 |          19.2300 |
[32m[20221213 18:56:18 @agent_ppo2.py:185][0m |          -0.0027 |         220.6676 |          19.2192 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |          -0.0029 |         220.2156 |          19.2124 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |          -0.0013 |         221.0046 |          19.2298 |
[32m[20221213 18:56:19 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:56:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 677.80
[32m[20221213 18:56:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 18:56:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.00
[32m[20221213 18:56:19 @agent_ppo2.py:143][0m Total time:       7.77 min
[32m[20221213 18:56:19 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221213 18:56:19 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 18:56:19 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:56:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |           0.0003 |         188.3139 |          19.2501 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |           0.0075 |         186.5712 |          19.1636 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |          -0.0015 |         181.1343 |          19.0714 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |          -0.0003 |         183.0123 |          18.9588 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |          -0.0021 |         179.3323 |          18.9594 |
[32m[20221213 18:56:19 @agent_ppo2.py:185][0m |           0.0006 |         187.0000 |          18.9524 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0015 |         178.2077 |          18.9247 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0038 |         177.4748 |          18.8910 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0037 |         177.0510 |          18.8776 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |           0.0017 |         176.6961 |          18.8680 |
[32m[20221213 18:56:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.60
[32m[20221213 18:56:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 18:56:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:20 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221213 18:56:20 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 18:56:20 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 18:56:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0047 |         239.9507 |          18.8662 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0055 |         232.1267 |          18.8808 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0045 |         228.8186 |          18.8802 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0041 |         226.5772 |          18.8963 |
[32m[20221213 18:56:20 @agent_ppo2.py:185][0m |          -0.0052 |         224.5578 |          18.8420 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |          -0.0026 |         223.9101 |          18.8140 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |          -0.0073 |         221.9047 |          18.8335 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |          -0.0028 |         220.5896 |          18.8089 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |           0.0029 |         225.2272 |          18.8558 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |           0.0077 |         239.2805 |          18.8498 |
[32m[20221213 18:56:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.00
[32m[20221213 18:56:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 676.00
[32m[20221213 18:56:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.00
[32m[20221213 18:56:21 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221213 18:56:21 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221213 18:56:21 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 18:56:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |          -0.0008 |         237.0128 |          18.7846 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |           0.0006 |         233.8718 |          18.7063 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |           0.0150 |         265.4882 |          18.6106 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |          -0.0001 |         232.8443 |          18.5627 |
[32m[20221213 18:56:21 @agent_ppo2.py:185][0m |          -0.0001 |         234.9990 |          18.5082 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |           0.0050 |         243.6014 |          18.5045 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |          -0.0039 |         231.5984 |          18.4967 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |          -0.0029 |         231.1115 |          18.4087 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |           0.0001 |         230.7563 |          18.3215 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |          -0.0003 |         230.6491 |          18.2705 |
[32m[20221213 18:56:22 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.80
[32m[20221213 18:56:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 18:56:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 884.00
[32m[20221213 18:56:22 @agent_ppo2.py:143][0m Total time:       7.83 min
[32m[20221213 18:56:22 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 18:56:22 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 18:56:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:56:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |          -0.0027 |         231.4575 |          18.5244 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |          -0.0048 |         230.7571 |          18.5459 |
[32m[20221213 18:56:22 @agent_ppo2.py:185][0m |          -0.0040 |         229.9216 |          18.5844 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |          -0.0022 |         228.9635 |          18.6202 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |          -0.0005 |         228.4741 |          18.6399 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |          -0.0020 |         227.7619 |          18.6671 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |           0.0000 |         227.3630 |          18.6880 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |          -0.0030 |         227.2203 |          18.6571 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |           0.0036 |         230.6408 |          18.6730 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |          -0.0003 |         227.6589 |          18.7304 |
[32m[20221213 18:56:23 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.60
[32m[20221213 18:56:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 18:56:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.00
[32m[20221213 18:56:23 @agent_ppo2.py:143][0m Total time:       7.84 min
[32m[20221213 18:56:23 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221213 18:56:23 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 18:56:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |           0.0031 |         248.0519 |          18.5291 |
[32m[20221213 18:56:23 @agent_ppo2.py:185][0m |          -0.0036 |         244.3771 |          18.4758 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |           0.0019 |         243.3399 |          18.4087 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |           0.0027 |         242.1293 |          18.3445 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |          -0.0024 |         241.1428 |          18.2949 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |          -0.0061 |         242.1114 |          18.2903 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |           0.0111 |         270.2909 |          18.2090 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |          -0.0050 |         239.9341 |          18.2449 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |          -0.0014 |         240.0005 |          18.1988 |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |          -0.0023 |         242.3870 |          18.2322 |
[32m[20221213 18:56:24 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.40
[32m[20221213 18:56:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.00
[32m[20221213 18:56:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.00
[32m[20221213 18:56:24 @agent_ppo2.py:143][0m Total time:       7.86 min
[32m[20221213 18:56:24 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 18:56:24 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 18:56:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:24 @agent_ppo2.py:185][0m |          -0.0003 |         230.6946 |          18.1342 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0067 |         237.7132 |          18.0906 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |          -0.0008 |         230.1316 |          18.1629 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0059 |         235.9645 |          18.2147 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0008 |         229.8985 |          18.1021 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0005 |         229.8963 |          18.1916 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0215 |         279.2225 |          18.1950 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0155 |         260.4595 |          18.2133 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |          -0.0011 |         229.7825 |          18.2742 |
[32m[20221213 18:56:25 @agent_ppo2.py:185][0m |           0.0036 |         231.0076 |          18.2897 |
[32m[20221213 18:56:25 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 980.20
[32m[20221213 18:56:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 984.00
[32m[20221213 18:56:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.00
[32m[20221213 18:56:25 @agent_ppo2.py:143][0m Total time:       7.88 min
[32m[20221213 18:56:25 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221213 18:56:25 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 18:56:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0032 |         254.3988 |          18.3550 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0029 |         249.5866 |          18.3186 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0008 |         246.8793 |          18.2397 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0027 |         246.2251 |          18.2966 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0043 |         244.3226 |          18.2610 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0023 |         242.8918 |          18.2069 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0015 |         241.7970 |          18.2258 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0032 |         241.5088 |          18.2151 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |          -0.0047 |         240.9074 |          18.1917 |
[32m[20221213 18:56:26 @agent_ppo2.py:185][0m |           0.0029 |         245.3267 |          18.1912 |
[32m[20221213 18:56:26 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.20
[32m[20221213 18:56:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.00
[32m[20221213 18:56:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.00
[32m[20221213 18:56:26 @agent_ppo2.py:143][0m Total time:       7.90 min
[32m[20221213 18:56:26 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 18:56:26 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 18:56:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0011 |         255.4689 |          18.1738 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0030 |         238.3045 |          18.1092 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0098 |         245.0702 |          18.0851 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0008 |         235.7437 |          17.9763 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0038 |         235.1158 |          17.9705 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0149 |         259.0241 |          17.9065 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |          -0.0009 |         234.0366 |          17.9303 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0047 |         237.2865 |          17.9662 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0032 |         235.6288 |          17.9690 |
[32m[20221213 18:56:27 @agent_ppo2.py:185][0m |           0.0194 |         268.3524 |          17.9760 |
[32m[20221213 18:56:27 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.60
[32m[20221213 18:56:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.00
[32m[20221213 18:56:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:27 @agent_ppo2.py:143][0m Total time:       7.92 min
[32m[20221213 18:56:27 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221213 18:56:27 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 18:56:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |           0.0146 |         255.3273 |          18.0584 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |           0.0039 |         241.2384 |          18.0213 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |           0.0041 |         236.9885 |          17.9958 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |           0.0011 |         235.9553 |          18.0067 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |          -0.0012 |         234.8509 |          17.9837 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |          -0.0043 |         234.6417 |          17.9838 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |          -0.0026 |         234.3592 |          18.0233 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |           0.0047 |         236.3943 |          18.0066 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |          -0.0039 |         234.0349 |          18.0096 |
[32m[20221213 18:56:28 @agent_ppo2.py:185][0m |          -0.0051 |         234.0177 |          18.0170 |
[32m[20221213 18:56:28 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.80
[32m[20221213 18:56:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.00
[32m[20221213 18:56:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:56:29 @agent_ppo2.py:143][0m Total time:       7.93 min
[32m[20221213 18:56:29 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 18:56:29 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 18:56:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |           0.0008 |         161.2299 |          17.9617 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |           0.0030 |         139.3234 |          17.9873 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |           0.0015 |         139.4467 |          17.9688 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |          -0.0033 |         134.1978 |          17.9586 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |          -0.0093 |         133.5805 |          17.9559 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |          -0.0038 |         132.3942 |          17.9418 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |           0.0042 |         152.7002 |          17.9491 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |          -0.0071 |         131.2360 |          17.9862 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |           0.0008 |         130.6023 |          17.9619 |
[32m[20221213 18:56:29 @agent_ppo2.py:185][0m |          -0.0051 |         130.5070 |          17.9585 |
[32m[20221213 18:56:29 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.80
[32m[20221213 18:56:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 18:56:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 888.00
[32m[20221213 18:56:30 @agent_ppo2.py:143][0m Total time:       7.95 min
[32m[20221213 18:56:30 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221213 18:56:30 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 18:56:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:56:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |           0.0027 |         249.2637 |          17.9808 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |           0.0064 |         247.5299 |          17.9685 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |           0.0016 |         247.3198 |          18.0073 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |          -0.0006 |         242.2767 |          17.9843 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |          -0.0015 |         241.3299 |          17.9818 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |           0.0026 |         244.4360 |          17.9599 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |           0.0011 |         239.8383 |          17.9027 |
[32m[20221213 18:56:30 @agent_ppo2.py:185][0m |          -0.0012 |         239.1965 |          17.8837 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0011 |         238.8532 |          17.8790 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0002 |         238.2839 |          17.8781 |
[32m[20221213 18:56:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.40
[32m[20221213 18:56:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 18:56:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.00
[32m[20221213 18:56:31 @agent_ppo2.py:143][0m Total time:       7.97 min
[32m[20221213 18:56:31 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 18:56:31 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 18:56:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |           0.0008 |         235.8396 |          17.8285 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0020 |         234.5626 |          17.8690 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0004 |         234.2213 |          17.8884 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0025 |         233.8911 |          17.9202 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0013 |         233.6644 |          17.9513 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0015 |         233.4056 |          17.9720 |
[32m[20221213 18:56:31 @agent_ppo2.py:185][0m |          -0.0020 |         233.2822 |          17.9718 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0011 |         232.9341 |          17.9724 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0011 |         232.9416 |          18.0148 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0029 |         232.6569 |          18.0200 |
[32m[20221213 18:56:32 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.20
[32m[20221213 18:56:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 18:56:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:32 @agent_ppo2.py:143][0m Total time:       7.99 min
[32m[20221213 18:56:32 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221213 18:56:32 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 18:56:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0023 |         141.6418 |          18.0522 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0030 |         132.3772 |          18.0594 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0010 |         130.7366 |          18.0262 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0047 |         128.9521 |          18.0059 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0031 |         128.2123 |          18.0315 |
[32m[20221213 18:56:32 @agent_ppo2.py:185][0m |          -0.0035 |         127.5714 |          18.0231 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |          -0.0029 |         128.2255 |          18.0676 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |          -0.0050 |         127.0114 |          18.0445 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |          -0.0041 |         127.8725 |          18.0225 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |           0.0036 |         126.2165 |          18.0359 |
[32m[20221213 18:56:33 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.20
[32m[20221213 18:56:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.00
[32m[20221213 18:56:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.00
[32m[20221213 18:56:33 @agent_ppo2.py:143][0m Total time:       8.01 min
[32m[20221213 18:56:33 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 18:56:33 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 18:56:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:56:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |          -0.0015 |         165.8040 |          17.9213 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |           0.0066 |         169.7433 |          17.8602 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |           0.0033 |         162.3873 |          17.9039 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |          -0.0004 |         159.4696 |          17.8581 |
[32m[20221213 18:56:33 @agent_ppo2.py:185][0m |           0.0026 |         160.5806 |          17.8444 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |          -0.0029 |         158.0795 |          17.8236 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |          -0.0040 |         157.9019 |          17.8384 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |          -0.0022 |         157.2118 |          17.7962 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |          -0.0036 |         157.2150 |          17.7355 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |          -0.0026 |         156.8492 |          17.7643 |
[32m[20221213 18:56:34 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.00
[32m[20221213 18:56:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.00
[32m[20221213 18:56:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 689.00
[32m[20221213 18:56:34 @agent_ppo2.py:143][0m Total time:       8.03 min
[32m[20221213 18:56:34 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221213 18:56:34 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 18:56:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |           0.0031 |         269.7023 |          17.8553 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |           0.0040 |         243.8600 |          17.8663 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |          -0.0032 |         238.6690 |          17.8463 |
[32m[20221213 18:56:34 @agent_ppo2.py:185][0m |           0.0028 |         235.5879 |          17.8777 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |           0.0051 |         242.5191 |          17.8475 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |          -0.0023 |         232.5841 |          17.8854 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |          -0.0030 |         231.8456 |          17.8469 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |          -0.0015 |         230.7318 |          17.8678 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |          -0.0026 |         229.7306 |          17.8898 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |           0.0023 |         229.3756 |          17.8803 |
[32m[20221213 18:56:35 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.00
[32m[20221213 18:56:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 18:56:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:56:35 @agent_ppo2.py:143][0m Total time:       8.04 min
[32m[20221213 18:56:35 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 18:56:35 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 18:56:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |           0.0043 |          59.4118 |          17.8648 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |           0.0050 |          43.3706 |          17.8433 |
[32m[20221213 18:56:35 @agent_ppo2.py:185][0m |          -0.0032 |          42.6755 |          17.8493 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |          -0.0045 |          43.1763 |          17.8190 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |          -0.0040 |          42.1419 |          17.8255 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |           0.0017 |          41.9600 |          17.8254 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |           0.0032 |          43.0265 |          17.7340 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |          -0.0037 |          41.7293 |          17.7153 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |           0.0052 |          45.7413 |          17.7367 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |           0.0003 |          41.5969 |          17.7455 |
[32m[20221213 18:56:36 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.40
[32m[20221213 18:56:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.00
[32m[20221213 18:56:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.00
[32m[20221213 18:56:36 @agent_ppo2.py:143][0m Total time:       8.06 min
[32m[20221213 18:56:36 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221213 18:56:36 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 18:56:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |           0.0091 |         217.8039 |          17.7888 |
[32m[20221213 18:56:36 @agent_ppo2.py:185][0m |           0.0068 |         197.2245 |          17.7762 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |          -0.0019 |         195.0244 |          17.7844 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |          -0.0036 |         193.7394 |          17.7895 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |           0.0014 |         194.0043 |          17.8090 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |           0.0002 |         196.0259 |          17.8028 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |          -0.0017 |         192.9848 |          17.7763 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |          -0.0020 |         193.4831 |          17.8332 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |          -0.0056 |         192.5472 |          17.8247 |
[32m[20221213 18:56:37 @agent_ppo2.py:185][0m |          -0.0028 |         192.6964 |          17.8330 |
[32m[20221213 18:56:37 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:56:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.80
[32m[20221213 18:56:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.00
[32m[20221213 18:56:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 18:56:37 @agent_ppo2.py:143][0m Total time:       8.08 min
[32m[20221213 18:56:37 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 18:56:37 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 18:56:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |           0.0013 |         231.6416 |          17.6912 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |           0.0010 |         229.8458 |          17.6547 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0024 |         229.1982 |          17.5896 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0023 |         228.9192 |          17.5380 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0046 |         228.8814 |          17.5471 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0003 |         228.3669 |          17.5165 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0014 |         228.1231 |          17.4889 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0023 |         228.1338 |          17.4657 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |           0.0110 |         257.1558 |          17.4504 |
[32m[20221213 18:56:38 @agent_ppo2.py:185][0m |          -0.0025 |         228.0987 |          17.4636 |
[32m[20221213 18:56:38 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.20
[32m[20221213 18:56:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.00
[32m[20221213 18:56:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:38 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221213 18:56:38 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221213 18:56:38 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 18:56:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |           0.0058 |         128.8073 |          17.6795 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0012 |         122.0858 |          17.6708 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0002 |         121.2035 |          17.6873 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0010 |         120.6732 |          17.6762 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0022 |         119.7809 |          17.7243 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |           0.0036 |         119.7187 |          17.7053 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0039 |         119.3652 |          17.7024 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0047 |         119.1455 |          17.7848 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |          -0.0063 |         118.7767 |          17.7342 |
[32m[20221213 18:56:39 @agent_ppo2.py:185][0m |           0.0007 |         118.9098 |          17.7558 |
[32m[20221213 18:56:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.80
[32m[20221213 18:56:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.00
[32m[20221213 18:56:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.00
[32m[20221213 18:56:39 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221213 18:56:39 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 18:56:39 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 18:56:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |           0.0018 |          36.9616 |          17.6498 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |          -0.0030 |          30.8161 |          17.6041 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |          -0.0029 |          29.9683 |          17.6974 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |           0.0059 |          31.7931 |          17.6956 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |          -0.0039 |          29.3200 |          17.7469 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |           0.0041 |          29.8389 |          17.6979 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |          -0.0001 |          29.0056 |          17.7001 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |          -0.0013 |          28.8967 |          17.7178 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |          -0.0033 |          28.8141 |          17.7199 |
[32m[20221213 18:56:40 @agent_ppo2.py:185][0m |           0.0002 |          29.1815 |          17.7143 |
[32m[20221213 18:56:40 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.00
[32m[20221213 18:56:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:56:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:56:40 @agent_ppo2.py:143][0m Total time:       8.13 min
[32m[20221213 18:56:40 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221213 18:56:40 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 18:56:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:56:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |          -0.0001 |          76.4388 |          17.6695 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |          -0.0033 |          72.6587 |          17.5873 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |           0.0006 |          71.0495 |          17.5141 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |           0.0007 |          70.4124 |          17.4646 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |          -0.0032 |          70.0521 |          17.4311 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |           0.0001 |          69.8813 |          17.3913 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |          -0.0025 |          69.7344 |          17.3520 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |          -0.0020 |          69.6957 |          17.3428 |
[32m[20221213 18:56:41 @agent_ppo2.py:185][0m |           0.0025 |          69.7261 |          17.3491 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0003 |          69.2808 |          17.2932 |
[32m[20221213 18:56:42 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.00
[32m[20221213 18:56:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 871.00
[32m[20221213 18:56:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.00
[32m[20221213 18:56:42 @agent_ppo2.py:143][0m Total time:       8.15 min
[32m[20221213 18:56:42 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 18:56:42 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 18:56:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0026 |         226.9667 |          17.3273 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0039 |         198.7369 |          17.3370 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0057 |         196.7725 |          17.3351 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0053 |         195.4527 |          17.3565 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0050 |         194.1903 |          17.3244 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0080 |         193.3781 |          17.3159 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0031 |         193.3965 |          17.3152 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |          -0.0058 |         192.7983 |          17.3006 |
[32m[20221213 18:56:42 @agent_ppo2.py:185][0m |           0.0016 |         196.0614 |          17.2977 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0027 |         191.9888 |          17.2851 |
[32m[20221213 18:56:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.60
[32m[20221213 18:56:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.00
[32m[20221213 18:56:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.00
[32m[20221213 18:56:43 @agent_ppo2.py:143][0m Total time:       8.17 min
[32m[20221213 18:56:43 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221213 18:56:43 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 18:56:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0025 |         209.3073 |          17.2727 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0002 |         196.0434 |          17.2337 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0049 |         192.4508 |          17.2673 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0014 |         190.0523 |          17.2770 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0037 |         190.2149 |          17.3243 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0066 |         188.1064 |          17.2811 |
[32m[20221213 18:56:43 @agent_ppo2.py:185][0m |          -0.0023 |         188.5751 |          17.3233 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0051 |         187.6032 |          17.3120 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0090 |         187.4665 |          17.3300 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0090 |         187.2091 |          17.3775 |
[32m[20221213 18:56:44 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:56:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.80
[32m[20221213 18:56:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.00
[32m[20221213 18:56:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 18:56:44 @agent_ppo2.py:143][0m Total time:       8.19 min
[32m[20221213 18:56:44 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 18:56:44 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 18:56:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:56:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0021 |          48.7691 |          17.3433 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |           0.0023 |          35.9994 |          17.3729 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0020 |          35.2420 |          17.3818 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0023 |          34.8870 |          17.2826 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0027 |          34.6423 |          17.2764 |
[32m[20221213 18:56:44 @agent_ppo2.py:185][0m |          -0.0010 |          34.5265 |          17.2967 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |          -0.0064 |          34.4059 |          17.2512 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |          -0.0032 |          34.4454 |          17.2517 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |          -0.0019 |          38.7674 |          17.2359 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |           0.0015 |          34.2043 |          17.2006 |
[32m[20221213 18:56:45 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.40
[32m[20221213 18:56:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 18:56:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 872.00
[32m[20221213 18:56:45 @agent_ppo2.py:143][0m Total time:       8.21 min
[32m[20221213 18:56:45 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221213 18:56:45 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 18:56:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |           0.0114 |          35.0465 |          17.2197 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |          -0.0059 |          27.6558 |          17.2228 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |           0.0045 |          28.4977 |          17.2838 |
[32m[20221213 18:56:45 @agent_ppo2.py:185][0m |           0.0080 |          29.7109 |          17.3463 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0036 |          27.2931 |          17.3604 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0020 |          27.1896 |          17.3594 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0081 |          27.1947 |          17.4231 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0068 |          27.1193 |          17.4594 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0043 |          27.1102 |          17.4989 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0013 |          27.3680 |          17.5036 |
[32m[20221213 18:56:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:56:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.20
[32m[20221213 18:56:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:56:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.00
[32m[20221213 18:56:46 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221213 18:56:46 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 18:56:46 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 18:56:46 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:56:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |           0.0052 |         219.4471 |          17.3369 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |          -0.0018 |         216.1683 |          17.2400 |
[32m[20221213 18:56:46 @agent_ppo2.py:185][0m |           0.0021 |         214.6141 |          17.1617 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |           0.0051 |         224.0697 |          17.1066 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0028 |         214.5034 |          17.0950 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0020 |         213.5346 |          16.9775 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0074 |         213.7362 |          16.9164 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0066 |         213.2674 |          16.9105 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0033 |         213.3399 |          16.8648 |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0032 |         212.9651 |          16.8595 |
[32m[20221213 18:56:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:56:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.80
[32m[20221213 18:56:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 18:56:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.00
[32m[20221213 18:56:47 @agent_ppo2.py:143][0m Total time:       8.25 min
[32m[20221213 18:56:47 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221213 18:56:47 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 18:56:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:47 @agent_ppo2.py:185][0m |          -0.0003 |         223.5983 |          16.9190 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |          -0.0027 |         221.3917 |          16.8858 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |          -0.0039 |         220.0985 |          16.8883 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |           0.0005 |         219.6208 |          16.8714 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |           0.0038 |         231.5367 |          16.8140 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |           0.0007 |         222.3988 |          16.8465 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |          -0.0032 |         219.2746 |          16.8599 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |           0.0053 |         236.8269 |          16.8427 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |          -0.0045 |         219.6194 |          16.8413 |
[32m[20221213 18:56:48 @agent_ppo2.py:185][0m |          -0.0037 |         219.1365 |          16.8195 |
[32m[20221213 18:56:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:56:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 697.40
[32m[20221213 18:56:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.00
[32m[20221213 18:56:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 18:56:48 @agent_ppo2.py:143][0m Total time:       8.27 min
[32m[20221213 18:56:48 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 18:56:48 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 18:56:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |          -0.0012 |         222.1229 |          16.7770 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |           0.0046 |         221.9041 |          16.6887 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |          -0.0011 |         218.2566 |          16.7527 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |           0.0013 |         218.2853 |          16.6581 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |           0.0021 |         217.6827 |          16.6028 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |          -0.0027 |         217.1061 |          16.5270 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |          -0.0023 |         217.0084 |          16.5527 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |           0.0108 |         248.6115 |          16.5065 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |          -0.0002 |         216.4736 |          16.4876 |
[32m[20221213 18:56:49 @agent_ppo2.py:185][0m |           0.0009 |         216.4453 |          16.4018 |
[32m[20221213 18:56:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:56:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.40
[32m[20221213 18:56:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.00
[32m[20221213 18:56:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.00
[32m[20221213 18:56:49 @agent_ppo2.py:143][0m Total time:       8.28 min
[32m[20221213 18:56:49 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221213 18:56:49 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 18:56:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |          -0.0007 |         226.2441 |          16.4422 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |           0.0037 |         223.3435 |          16.3589 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |          -0.0018 |         223.1097 |          16.2547 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |           0.0054 |         222.5359 |          16.1585 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |          -0.0011 |         221.9785 |          16.0804 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |           0.0038 |         222.2489 |          15.9760 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |           0.0160 |         242.1805 |          15.9548 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |          -0.0030 |         221.9225 |          15.8849 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |          -0.0043 |         221.3432 |          15.8269 |
[32m[20221213 18:56:50 @agent_ppo2.py:185][0m |          -0.0031 |         221.6452 |          15.7426 |
[32m[20221213 18:56:50 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:56:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.00
[32m[20221213 18:56:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 931.00
[32m[20221213 18:56:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:51 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221213 18:56:51 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 18:56:51 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 18:56:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0108 |         245.0761 |          15.6135 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0027 |         229.8037 |          15.6252 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0009 |         229.6291 |          15.5512 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0001 |         229.4436 |          15.6440 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0128 |         248.6302 |          15.5690 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |          -0.0026 |         229.6084 |          15.6071 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0018 |         229.4102 |          15.5874 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |          -0.0021 |         229.0278 |          15.5732 |
[32m[20221213 18:56:51 @agent_ppo2.py:185][0m |           0.0019 |         229.5337 |          15.5818 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |          -0.0020 |         228.8574 |          15.5359 |
[32m[20221213 18:56:52 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 867.80
[32m[20221213 18:56:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.00
[32m[20221213 18:56:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 18:56:52 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221213 18:56:52 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221213 18:56:52 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 18:56:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:56:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |           0.0145 |         245.3033 |          15.5056 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |           0.0002 |         203.9723 |          15.5324 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |          -0.0005 |         202.7121 |          15.5445 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |          -0.0012 |         200.9985 |          15.5572 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |           0.0040 |         208.8386 |          15.5270 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |          -0.0015 |         199.7657 |          15.5195 |
[32m[20221213 18:56:52 @agent_ppo2.py:185][0m |           0.0001 |         199.2627 |          15.5683 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0045 |         198.8796 |          15.5558 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0044 |         198.6324 |          15.5766 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0016 |         198.4174 |          15.5835 |
[32m[20221213 18:56:53 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.80
[32m[20221213 18:56:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.00
[32m[20221213 18:56:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:56:53 @agent_ppo2.py:143][0m Total time:       8.34 min
[32m[20221213 18:56:53 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 18:56:53 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 18:56:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0004 |         244.4302 |          15.6847 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0059 |         233.5096 |          15.7502 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |           0.0005 |         232.1564 |          15.7870 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0024 |         229.2364 |          15.9131 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |           0.0057 |         236.5566 |          15.9997 |
[32m[20221213 18:56:53 @agent_ppo2.py:185][0m |          -0.0020 |         227.8485 |          16.0443 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0083 |         246.3546 |          16.0973 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |          -0.0027 |         226.8237 |          16.0924 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0031 |         236.7008 |          16.1559 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |          -0.0053 |         225.9954 |          16.1797 |
[32m[20221213 18:56:54 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.40
[32m[20221213 18:56:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.00
[32m[20221213 18:56:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.00
[32m[20221213 18:56:54 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221213 18:56:54 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221213 18:56:54 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 18:56:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0036 |         231.4431 |          16.3262 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0002 |         229.2126 |          16.3321 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |          -0.0012 |         229.0869 |          16.3614 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0056 |         230.2001 |          16.3756 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0130 |         262.3705 |          16.4380 |
[32m[20221213 18:56:54 @agent_ppo2.py:185][0m |           0.0039 |         228.4900 |          16.4351 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |          -0.0020 |         228.2588 |          16.4883 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |           0.0037 |         229.3384 |          16.5335 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |           0.0071 |         232.7289 |          16.5131 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |          -0.0017 |         228.0765 |          16.5500 |
[32m[20221213 18:56:55 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:56:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.00
[32m[20221213 18:56:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.00
[32m[20221213 18:56:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.00
[32m[20221213 18:56:55 @agent_ppo2.py:143][0m Total time:       8.37 min
[32m[20221213 18:56:55 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 18:56:55 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 18:56:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:56:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |          -0.0020 |         200.3696 |          16.6163 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |          -0.0009 |         196.2312 |          16.5409 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |          -0.0075 |         194.6735 |          16.5066 |
[32m[20221213 18:56:55 @agent_ppo2.py:185][0m |          -0.0022 |         193.9466 |          16.4384 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0017 |         193.8776 |          16.4676 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |           0.0051 |         200.3156 |          16.4377 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0036 |         193.4660 |          16.4179 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0002 |         193.7370 |          16.3803 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0015 |         193.2559 |          16.3862 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0022 |         192.9517 |          16.3881 |
[32m[20221213 18:56:56 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:56:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 647.40
[32m[20221213 18:56:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 18:56:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.00
[32m[20221213 18:56:56 @agent_ppo2.py:143][0m Total time:       8.39 min
[32m[20221213 18:56:56 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221213 18:56:56 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 18:56:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |           0.0081 |          37.2210 |          16.1807 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0004 |          30.1353 |          16.1813 |
[32m[20221213 18:56:56 @agent_ppo2.py:185][0m |          -0.0018 |          29.5296 |          16.1122 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0019 |          29.2169 |          16.1216 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0051 |          29.0512 |          16.0879 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |           0.0029 |          29.2232 |          16.1099 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0041 |          28.9104 |          16.1082 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0101 |          28.9230 |          16.1002 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0042 |          28.6907 |          16.0877 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0048 |          28.6239 |          16.0770 |
[32m[20221213 18:56:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.00
[32m[20221213 18:56:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:56:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.00
[32m[20221213 18:56:57 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221213 18:56:57 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 18:56:57 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 18:56:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |           0.0006 |         175.2900 |          16.1199 |
[32m[20221213 18:56:57 @agent_ppo2.py:185][0m |          -0.0033 |         149.7242 |          15.9091 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0033 |         148.7063 |          15.8554 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |           0.0006 |         147.2756 |          15.7815 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0043 |         146.7602 |          15.6896 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0037 |         146.3251 |          15.6025 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0071 |         145.8919 |          15.5471 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0064 |         145.9734 |          15.4836 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0078 |         145.3098 |          15.4402 |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0045 |         144.6592 |          15.4026 |
[32m[20221213 18:56:58 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:56:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.40
[32m[20221213 18:56:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.00
[32m[20221213 18:56:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 690.00
[32m[20221213 18:56:58 @agent_ppo2.py:143][0m Total time:       8.43 min
[32m[20221213 18:56:58 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221213 18:56:58 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 18:56:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:56:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:56:58 @agent_ppo2.py:185][0m |          -0.0006 |          33.3524 |          15.1482 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |           0.0037 |          27.2679 |          15.1852 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0052 |          26.6293 |          15.2077 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0084 |          26.1925 |          15.2232 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0087 |          25.9489 |          15.2447 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0053 |          25.8343 |          15.2119 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0045 |          25.6395 |          15.2380 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0046 |          25.7550 |          15.2413 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0029 |          25.4079 |          15.2474 |
[32m[20221213 18:56:59 @agent_ppo2.py:185][0m |          -0.0088 |          25.3772 |          15.2786 |
[32m[20221213 18:56:59 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:56:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.80
[32m[20221213 18:56:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 18:56:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:56:59 @agent_ppo2.py:143][0m Total time:       8.45 min
[32m[20221213 18:56:59 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 18:56:59 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 18:56:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |           0.0084 |         206.5788 |          15.1482 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0047 |         190.8747 |          15.1772 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0068 |         188.0929 |          15.1686 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0101 |         187.7611 |          15.2701 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0048 |         186.5120 |          15.2507 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |           0.0044 |         188.5717 |          15.2464 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0069 |         184.8110 |          15.2334 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0048 |         184.5920 |          15.3009 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0040 |         183.8938 |          15.2663 |
[32m[20221213 18:57:00 @agent_ppo2.py:185][0m |          -0.0024 |         183.4563 |          15.3326 |
[32m[20221213 18:57:00 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.80
[32m[20221213 18:57:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 18:57:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 41.00
[32m[20221213 18:57:00 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221213 18:57:00 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221213 18:57:00 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 18:57:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |           0.0093 |          33.2168 |          15.5373 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |           0.0044 |          27.1935 |          15.4258 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0003 |          26.5674 |          15.4122 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0058 |          26.2437 |          15.4182 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0070 |          26.0488 |          15.3814 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0036 |          25.9484 |          15.3932 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0056 |          25.8517 |          15.3304 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0027 |          25.7557 |          15.3019 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0076 |          25.6693 |          15.3015 |
[32m[20221213 18:57:01 @agent_ppo2.py:185][0m |          -0.0064 |          25.5630 |          15.2882 |
[32m[20221213 18:57:01 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.20
[32m[20221213 18:57:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:57:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 683.00
[32m[20221213 18:57:02 @agent_ppo2.py:143][0m Total time:       8.48 min
[32m[20221213 18:57:02 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 18:57:02 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 18:57:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |           0.0032 |          24.9548 |          15.3175 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |           0.0009 |          21.6516 |          15.2584 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |          -0.0037 |          21.1520 |          15.1524 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |          -0.0039 |          20.8891 |          15.1672 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |          -0.0049 |          20.7437 |          15.2057 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |          -0.0007 |          20.6013 |          15.2203 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |          -0.0077 |          20.5644 |          15.1670 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |          -0.0089 |          20.4717 |          15.1772 |
[32m[20221213 18:57:02 @agent_ppo2.py:185][0m |           0.0009 |          20.3821 |          15.2240 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |          -0.0041 |          20.3755 |          15.2131 |
[32m[20221213 18:57:03 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.80
[32m[20221213 18:57:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 40.00
[32m[20221213 18:57:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 18:57:03 @agent_ppo2.py:143][0m Total time:       8.50 min
[32m[20221213 18:57:03 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221213 18:57:03 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 18:57:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |           0.0071 |         225.3937 |          15.0127 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |           0.0042 |         211.0299 |          14.9074 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |           0.0116 |         217.3905 |          14.9120 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |           0.0029 |         206.1299 |          14.8860 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |          -0.0010 |         205.2241 |          14.7926 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |          -0.0006 |         204.4410 |          14.7322 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |           0.0023 |         204.1067 |          14.7461 |
[32m[20221213 18:57:03 @agent_ppo2.py:185][0m |          -0.0046 |         203.8320 |          14.6310 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |          -0.0010 |         203.2052 |          14.6607 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |          -0.0021 |         202.7227 |          14.6265 |
[32m[20221213 18:57:04 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.00
[32m[20221213 18:57:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.00
[32m[20221213 18:57:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:57:04 @agent_ppo2.py:143][0m Total time:       8.52 min
[32m[20221213 18:57:04 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 18:57:04 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 18:57:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |           0.0013 |         206.8676 |          14.6959 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |           0.0054 |         206.6889 |          14.6498 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |           0.0003 |         203.8658 |          14.6471 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |          -0.0013 |         203.6637 |          14.6566 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |           0.0065 |         204.3016 |          14.7131 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |           0.0007 |         203.6555 |          14.7188 |
[32m[20221213 18:57:04 @agent_ppo2.py:185][0m |           0.0012 |         204.1971 |          14.6956 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0113 |         212.3696 |          14.6857 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0013 |         203.2393 |          14.6892 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0023 |         203.1099 |          14.7002 |
[32m[20221213 18:57:05 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.80
[32m[20221213 18:57:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 18:57:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 923.00
[32m[20221213 18:57:05 @agent_ppo2.py:143][0m Total time:       8.54 min
[32m[20221213 18:57:05 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221213 18:57:05 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 18:57:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0183 |         247.6369 |          14.5719 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0094 |         219.0099 |          14.7639 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0069 |         225.3268 |          14.5012 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0003 |         216.2602 |          14.5457 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0071 |         230.3464 |          14.5729 |
[32m[20221213 18:57:05 @agent_ppo2.py:185][0m |           0.0019 |         214.9727 |          14.4825 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |           0.0015 |         214.7926 |          14.5280 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |           0.0005 |         214.3423 |          14.4585 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |          -0.0007 |         213.1296 |          14.4389 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |          -0.0004 |         212.7271 |          14.3874 |
[32m[20221213 18:57:06 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.40
[32m[20221213 18:57:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 18:57:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:57:06 @agent_ppo2.py:143][0m Total time:       8.56 min
[32m[20221213 18:57:06 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 18:57:06 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 18:57:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |          -0.0019 |         217.1040 |          14.5715 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |          -0.0002 |         214.8692 |          14.5693 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |           0.0146 |         244.5996 |          14.5757 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |           0.0014 |         215.1579 |          14.6877 |
[32m[20221213 18:57:06 @agent_ppo2.py:185][0m |          -0.0002 |         213.0895 |          14.6980 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |          -0.0031 |         212.7968 |          14.7635 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |           0.0005 |         213.6148 |          14.7123 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |          -0.0023 |         212.7719 |          14.7499 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |          -0.0013 |         212.3813 |          14.7276 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |           0.0179 |         253.2122 |          14.8148 |
[32m[20221213 18:57:07 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 859.20
[32m[20221213 18:57:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 908.00
[32m[20221213 18:57:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:57:07 @agent_ppo2.py:143][0m Total time:       8.58 min
[32m[20221213 18:57:07 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221213 18:57:07 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 18:57:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |           0.0004 |         214.6088 |          14.7759 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |          -0.0008 |         213.0303 |          14.7658 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |          -0.0000 |         213.1364 |          14.7478 |
[32m[20221213 18:57:07 @agent_ppo2.py:185][0m |           0.0018 |         213.8990 |          14.7675 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |          -0.0018 |         212.4899 |          14.7923 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |           0.0005 |         212.7271 |          14.8043 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |          -0.0021 |         212.4832 |          14.8632 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |          -0.0020 |         212.3245 |          14.8329 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |          -0.0026 |         212.1555 |          14.9175 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |          -0.0032 |         212.1399 |          14.9529 |
[32m[20221213 18:57:08 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:57:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:57:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.00
[32m[20221213 18:57:08 @agent_ppo2.py:143][0m Total time:       8.59 min
[32m[20221213 18:57:08 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 18:57:08 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 18:57:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |          -0.0009 |          75.3527 |          14.8986 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |           0.0026 |          49.6402 |          14.8649 |
[32m[20221213 18:57:08 @agent_ppo2.py:185][0m |           0.0014 |          48.4000 |          14.8445 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |           0.0031 |          48.1573 |          14.8308 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |           0.0004 |          48.0722 |          14.8206 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |          -0.0049 |          47.9955 |          14.8470 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |           0.0016 |          48.1099 |          14.8715 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |          -0.0082 |          47.9115 |          14.9107 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |           0.0002 |          47.8716 |          14.8948 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |           0.0020 |          48.4636 |          14.9745 |
[32m[20221213 18:57:09 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.80
[32m[20221213 18:57:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:57:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:57:09 @agent_ppo2.py:143][0m Total time:       8.61 min
[32m[20221213 18:57:09 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221213 18:57:09 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 18:57:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |           0.0017 |          48.9822 |          15.1548 |
[32m[20221213 18:57:09 @agent_ppo2.py:185][0m |          -0.0048 |          38.5425 |          15.2136 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0033 |          37.8535 |          15.2938 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0072 |          37.5162 |          15.3405 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0033 |          37.3536 |          15.3953 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0073 |          37.6874 |          15.4221 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0073 |          37.2144 |          15.4197 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0124 |          37.1125 |          15.4793 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0092 |          37.0531 |          15.5280 |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |          -0.0053 |          37.0619 |          15.5771 |
[32m[20221213 18:57:10 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.60
[32m[20221213 18:57:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:57:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 37.00
[32m[20221213 18:57:10 @agent_ppo2.py:143][0m Total time:       8.63 min
[32m[20221213 18:57:10 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 18:57:10 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 18:57:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:10 @agent_ppo2.py:185][0m |           0.0049 |         223.6631 |          15.4984 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |           0.0102 |         239.3080 |          15.4815 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |          -0.0023 |         214.2917 |          15.4731 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |           0.0079 |         215.3752 |          15.4116 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |          -0.0020 |         213.2819 |          15.4046 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |           0.0033 |         221.2151 |          15.4223 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |          -0.0024 |         212.5299 |          15.4544 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |           0.0057 |         225.2972 |          15.4442 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |           0.0008 |         211.9813 |          15.4227 |
[32m[20221213 18:57:11 @agent_ppo2.py:185][0m |           0.0028 |         212.2759 |          15.3843 |
[32m[20221213 18:57:11 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.00
[32m[20221213 18:57:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 18:57:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:57:11 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 18:57:11 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221213 18:57:11 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 18:57:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0004 |          43.5534 |          15.3099 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0036 |          36.4981 |          15.3749 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0055 |          36.0248 |          15.4431 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0030 |          35.7706 |          15.4761 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0066 |          35.6591 |          15.5588 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0066 |          35.4895 |          15.5966 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |           0.0001 |          35.3864 |          15.5997 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0001 |          37.1729 |          15.6950 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0038 |          35.2495 |          15.7262 |
[32m[20221213 18:57:12 @agent_ppo2.py:185][0m |          -0.0083 |          35.0776 |          15.7768 |
[32m[20221213 18:57:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.20
[32m[20221213 18:57:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 44.00
[32m[20221213 18:57:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 35.00
[32m[20221213 18:57:12 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221213 18:57:12 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 18:57:12 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 18:57:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0039 |          33.7263 |          15.9784 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |           0.0022 |          30.1283 |          16.0655 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0064 |          29.0605 |          16.1213 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0047 |          28.9339 |          16.0968 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0032 |          28.8155 |          16.1536 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0050 |          28.8337 |          16.1661 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0074 |          28.7009 |          16.2363 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0067 |          28.7761 |          16.2811 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0057 |          28.5290 |          16.3138 |
[32m[20221213 18:57:13 @agent_ppo2.py:185][0m |          -0.0062 |          28.5755 |          16.3409 |
[32m[20221213 18:57:13 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.20
[32m[20221213 18:57:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 18:57:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 18:57:13 @agent_ppo2.py:143][0m Total time:       8.68 min
[32m[20221213 18:57:13 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221213 18:57:13 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 18:57:14 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:57:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |           0.0020 |         200.8289 |          16.3319 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |           0.0081 |         202.6730 |          16.3692 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |          -0.0016 |         196.8272 |          16.3120 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |          -0.0017 |         196.6715 |          16.2672 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |          -0.0019 |         196.6740 |          16.2180 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |           0.0137 |         219.5677 |          16.2115 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |           0.0031 |         196.4648 |          16.1881 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |          -0.0020 |         196.3596 |          16.2199 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |          -0.0001 |         196.2372 |          16.2548 |
[32m[20221213 18:57:14 @agent_ppo2.py:185][0m |           0.0052 |         198.5433 |          16.2140 |
[32m[20221213 18:57:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 992.20
[32m[20221213 18:57:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 993.00
[32m[20221213 18:57:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 18:57:15 @agent_ppo2.py:143][0m Total time:       8.70 min
[32m[20221213 18:57:15 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 18:57:15 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 18:57:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |           0.0016 |         155.2476 |          16.2224 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |          -0.0076 |         148.0647 |          16.1888 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |          -0.0026 |         145.8675 |          16.1639 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |          -0.0042 |         145.2816 |          16.0993 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |           0.0005 |         144.8202 |          16.0794 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |          -0.0049 |         143.9781 |          16.0914 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |          -0.0053 |         143.3932 |          16.0889 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |           0.0017 |         143.2609 |          16.0412 |
[32m[20221213 18:57:15 @agent_ppo2.py:185][0m |          -0.0032 |         142.7999 |          16.0659 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |           0.0064 |         146.4792 |          16.0989 |
[32m[20221213 18:57:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.40
[32m[20221213 18:57:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 890.00
[32m[20221213 18:57:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.00
[32m[20221213 18:57:16 @agent_ppo2.py:143][0m Total time:       8.72 min
[32m[20221213 18:57:16 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221213 18:57:16 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 18:57:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0024 |          28.6450 |          16.0891 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0053 |          23.5525 |          16.1517 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0054 |          22.9114 |          16.2189 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0037 |          22.5368 |          16.2366 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0081 |          22.2885 |          16.2908 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0046 |          22.0293 |          16.3354 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0006 |          23.6482 |          16.3946 |
[32m[20221213 18:57:16 @agent_ppo2.py:185][0m |          -0.0074 |          21.7516 |          16.4175 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |          -0.0065 |          21.6014 |          16.4664 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |          -0.0067 |          21.3973 |          16.5070 |
[32m[20221213 18:57:17 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.00
[32m[20221213 18:57:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:57:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 18:57:17 @agent_ppo2.py:143][0m Total time:       8.74 min
[32m[20221213 18:57:17 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 18:57:17 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 18:57:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |           0.0026 |         203.4334 |          16.4398 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |           0.0022 |         200.2257 |          16.4288 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |          -0.0012 |         199.7731 |          16.3787 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |           0.0006 |         199.9357 |          16.3363 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |          -0.0010 |         199.4092 |          16.2143 |
[32m[20221213 18:57:17 @agent_ppo2.py:185][0m |          -0.0018 |         199.3830 |          16.2247 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |          -0.0004 |         199.3909 |          16.1743 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |           0.0079 |         210.8441 |          16.0876 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |           0.0005 |         199.0744 |          16.1109 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |           0.0019 |         203.3422 |          16.1159 |
[32m[20221213 18:57:18 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.60
[32m[20221213 18:57:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 18:57:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:57:18 @agent_ppo2.py:143][0m Total time:       8.76 min
[32m[20221213 18:57:18 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221213 18:57:18 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 18:57:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |          -0.0018 |         206.1440 |          15.9198 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |          -0.0042 |         203.1432 |          15.9044 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |           0.0077 |         213.9417 |          15.8700 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |           0.0001 |         202.1109 |          15.9276 |
[32m[20221213 18:57:18 @agent_ppo2.py:185][0m |          -0.0031 |         201.8724 |          15.8977 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |          -0.0007 |         201.7765 |          15.8682 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |          -0.0026 |         201.7121 |          15.7981 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |          -0.0019 |         201.5404 |          15.8423 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |          -0.0011 |         201.4548 |          15.7990 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |          -0.0007 |         201.5486 |          15.7206 |
[32m[20221213 18:57:19 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.80
[32m[20221213 18:57:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.00
[32m[20221213 18:57:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 889.00
[32m[20221213 18:57:19 @agent_ppo2.py:143][0m Total time:       8.77 min
[32m[20221213 18:57:19 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 18:57:19 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 18:57:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |          -0.0005 |         208.7717 |          15.6935 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |           0.0005 |         206.8015 |          15.6597 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |           0.0166 |         237.1181 |          15.5684 |
[32m[20221213 18:57:19 @agent_ppo2.py:185][0m |           0.0013 |         206.5139 |          15.5587 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |          -0.0009 |         206.2132 |          15.4850 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |          -0.0026 |         206.2919 |          15.4677 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |           0.0010 |         206.1234 |          15.3904 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |          -0.0032 |         206.1960 |          15.3827 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |          -0.0010 |         206.0629 |          15.3340 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |           0.0097 |         221.5199 |          15.3115 |
[32m[20221213 18:57:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.20
[32m[20221213 18:57:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.00
[32m[20221213 18:57:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:57:20 @agent_ppo2.py:143][0m Total time:       8.79 min
[32m[20221213 18:57:20 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221213 18:57:20 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 18:57:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |           0.0046 |          32.7279 |          15.4003 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |          -0.0008 |          24.4205 |          15.2724 |
[32m[20221213 18:57:20 @agent_ppo2.py:185][0m |          -0.0018 |          23.7849 |          15.2003 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |          -0.0051 |          23.5818 |          15.1501 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |          -0.0030 |          23.4868 |          15.1184 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |          -0.0070 |          23.4845 |          15.0800 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |          -0.0070 |          23.3485 |          15.0435 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |          -0.0065 |          23.2921 |          14.9959 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |           0.0000 |          24.4569 |          14.9403 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |          -0.0052 |          23.2607 |          14.9539 |
[32m[20221213 18:57:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.20
[32m[20221213 18:57:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 18:57:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:57:21 @agent_ppo2.py:143][0m Total time:       8.81 min
[32m[20221213 18:57:21 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 18:57:21 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 18:57:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |           0.0013 |          24.9515 |          14.8463 |
[32m[20221213 18:57:21 @agent_ppo2.py:185][0m |           0.0039 |          20.6116 |          14.8249 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0026 |          19.8239 |          14.8243 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0046 |          19.6287 |          14.8627 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |           0.0021 |          19.5643 |          14.8674 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0027 |          19.5058 |          14.9154 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0097 |          19.4170 |          14.9223 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0051 |          19.3584 |          14.9304 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0051 |          19.3120 |          14.9184 |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |          -0.0063 |          19.2947 |          14.9445 |
[32m[20221213 18:57:22 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.80
[32m[20221213 18:57:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 18:57:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 18:57:22 @agent_ppo2.py:143][0m Total time:       8.83 min
[32m[20221213 18:57:22 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221213 18:57:22 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 18:57:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:22 @agent_ppo2.py:185][0m |           0.0006 |          19.7727 |          14.8420 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0022 |          16.6304 |          14.7830 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0038 |          16.4237 |          14.7132 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0048 |          16.3121 |          14.6772 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0063 |          16.2813 |          14.6369 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |           0.0046 |          17.7511 |          14.5947 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |           0.0084 |          16.6988 |          14.5799 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0041 |          16.0211 |          14.5273 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0066 |          16.0147 |          14.5069 |
[32m[20221213 18:57:23 @agent_ppo2.py:185][0m |          -0.0071 |          15.9498 |          14.4744 |
[32m[20221213 18:57:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.40
[32m[20221213 18:57:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 18:57:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 18:57:23 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221213 18:57:23 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 18:57:23 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 18:57:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |          -0.0016 |         200.2564 |          14.2338 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |           0.0052 |         196.1931 |          14.2148 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |           0.0036 |         197.5898 |          14.2801 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |          -0.0020 |         194.2801 |          14.2342 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |          -0.0022 |         194.1603 |          14.2248 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |           0.0031 |         194.4131 |          14.1894 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |           0.0090 |         206.5155 |          14.1207 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |          -0.0054 |         194.0000 |          14.2073 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |          -0.0027 |         193.9830 |          14.2092 |
[32m[20221213 18:57:24 @agent_ppo2.py:185][0m |          -0.0034 |         193.8637 |          14.2215 |
[32m[20221213 18:57:24 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.00
[32m[20221213 18:57:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.00
[32m[20221213 18:57:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.00
[32m[20221213 18:57:24 @agent_ppo2.py:143][0m Total time:       8.87 min
[32m[20221213 18:57:24 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221213 18:57:24 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 18:57:24 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:57:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |           0.0048 |         215.9814 |          14.1649 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |          -0.0012 |         186.0628 |          14.1808 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |          -0.0034 |         182.3993 |          14.1240 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |          -0.0042 |         180.5205 |          14.1641 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |          -0.0002 |         181.0572 |          14.1651 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |           0.0026 |         178.9448 |          14.1542 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |          -0.0051 |         178.8103 |          14.0955 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |           0.0154 |         196.4726 |          14.1576 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |           0.0009 |         177.8638 |          14.0451 |
[32m[20221213 18:57:25 @agent_ppo2.py:185][0m |          -0.0040 |         177.5316 |          14.0173 |
[32m[20221213 18:57:25 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.20
[32m[20221213 18:57:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 18:57:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 18:57:25 @agent_ppo2.py:143][0m Total time:       8.88 min
[32m[20221213 18:57:25 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 18:57:25 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 18:57:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |           0.0001 |         205.8245 |          14.1906 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |           0.0043 |         206.4544 |          14.2532 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |          -0.0005 |         205.0303 |          14.2037 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |          -0.0017 |         204.9848 |          14.1912 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |          -0.0015 |         204.9595 |          14.1933 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |           0.0064 |         215.6534 |          14.1387 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |          -0.0013 |         204.9568 |          14.1684 |
[32m[20221213 18:57:26 @agent_ppo2.py:185][0m |          -0.0007 |         205.0205 |          14.2447 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0001 |         204.8446 |          14.2378 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |           0.0002 |         204.9123 |          14.2579 |
[32m[20221213 18:57:27 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:57:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:57:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:57:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 18:57:27 @agent_ppo2.py:143][0m Total time:       8.90 min
[32m[20221213 18:57:27 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221213 18:57:27 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 18:57:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |           0.0037 |          57.9145 |          14.0473 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0007 |          40.7707 |          14.0503 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0011 |          39.9804 |          14.0491 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0081 |          39.6884 |          14.0809 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0006 |          40.6180 |          14.0164 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0014 |          38.7794 |          13.9132 |
[32m[20221213 18:57:27 @agent_ppo2.py:185][0m |          -0.0034 |          38.5586 |          13.9977 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |           0.0001 |          38.4257 |          13.9397 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |          -0.0018 |          39.1136 |          13.9115 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |          -0.0087 |          38.2426 |          13.9110 |
[32m[20221213 18:57:28 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.80
[32m[20221213 18:57:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:57:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.00
[32m[20221213 18:57:28 @agent_ppo2.py:143][0m Total time:       8.92 min
[32m[20221213 18:57:28 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 18:57:28 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 18:57:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |          -0.0004 |          35.4887 |          14.1244 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |           0.0080 |          33.2552 |          14.0939 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |           0.0022 |          28.9606 |          14.1139 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |          -0.0048 |          28.7083 |          14.1358 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |           0.0020 |          31.4172 |          14.1913 |
[32m[20221213 18:57:28 @agent_ppo2.py:185][0m |          -0.0073 |          28.6017 |          14.1332 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0034 |          28.5526 |          14.1816 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0025 |          28.7148 |          14.1864 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0054 |          28.9904 |          14.2287 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0058 |          28.4087 |          14.2016 |
[32m[20221213 18:57:29 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.00
[32m[20221213 18:57:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 18:57:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 18:57:29 @agent_ppo2.py:143][0m Total time:       8.94 min
[32m[20221213 18:57:29 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221213 18:57:29 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 18:57:29 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:57:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0005 |          27.7749 |          14.2969 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0042 |          24.6387 |          14.3234 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0023 |          24.2406 |          14.4006 |
[32m[20221213 18:57:29 @agent_ppo2.py:185][0m |          -0.0058 |          24.0842 |          14.4571 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0073 |          23.9942 |          14.4631 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0033 |          23.8542 |          14.5032 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0058 |          23.8302 |          14.5370 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0038 |          23.6711 |          14.5878 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0056 |          23.6673 |          14.5898 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0072 |          23.6008 |          14.6174 |
[32m[20221213 18:57:30 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.40
[32m[20221213 18:57:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 18:57:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.00
[32m[20221213 18:57:30 @agent_ppo2.py:143][0m Total time:       8.96 min
[32m[20221213 18:57:30 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 18:57:30 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 18:57:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0011 |          23.1852 |          14.5430 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |           0.0074 |          21.4752 |          14.5943 |
[32m[20221213 18:57:30 @agent_ppo2.py:185][0m |          -0.0053 |          20.4186 |          14.5956 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |           0.0009 |          20.4469 |          14.6695 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |           0.0076 |          22.7540 |          14.6872 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |          -0.0057 |          20.3040 |          14.7864 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |          -0.0065 |          20.1164 |          14.8004 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |          -0.0036 |          20.0921 |          14.8427 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |          -0.0033 |          20.0915 |          14.8348 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |          -0.0009 |          20.2343 |          14.8516 |
[32m[20221213 18:57:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.80
[32m[20221213 18:57:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 18:57:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:57:31 @agent_ppo2.py:143][0m Total time:       8.98 min
[32m[20221213 18:57:31 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221213 18:57:31 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 18:57:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |           0.0031 |         203.2628 |          15.0278 |
[32m[20221213 18:57:31 @agent_ppo2.py:185][0m |           0.0018 |         202.0629 |          14.9963 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |          -0.0014 |         201.6206 |          14.9113 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |           0.0033 |         201.8252 |          14.9509 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |           0.0034 |         204.5153 |          14.8605 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |          -0.0006 |         201.4563 |          14.8841 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |          -0.0007 |         201.5915 |          14.8167 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |          -0.0036 |         201.2693 |          14.7699 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |          -0.0036 |         201.8180 |          14.8086 |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |          -0.0033 |         201.3099 |          14.7514 |
[32m[20221213 18:57:32 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:57:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:57:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:57:32 @agent_ppo2.py:143][0m Total time:       9.00 min
[32m[20221213 18:57:32 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 18:57:32 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 18:57:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:32 @agent_ppo2.py:185][0m |           0.0133 |          76.6219 |          14.7874 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |           0.0004 |          49.8967 |          14.8499 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0060 |          46.0804 |          14.8479 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0037 |          45.0734 |          14.7894 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0025 |          44.0102 |          14.7371 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0032 |          43.6031 |          14.6983 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |           0.0058 |          50.3769 |          14.7691 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0027 |          42.6807 |          14.7894 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0085 |          42.3767 |          14.7276 |
[32m[20221213 18:57:33 @agent_ppo2.py:185][0m |          -0.0012 |          41.9825 |          14.7604 |
[32m[20221213 18:57:33 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.00
[32m[20221213 18:57:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 18:57:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 18:57:33 @agent_ppo2.py:143][0m Total time:       9.01 min
[32m[20221213 18:57:33 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221213 18:57:33 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 18:57:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |           0.0006 |          23.4778 |          14.5900 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |           0.0021 |          19.3367 |          14.4760 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0038 |          18.8194 |          14.4206 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0015 |          18.8212 |          14.3635 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |           0.0010 |          18.6219 |          14.3362 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0005 |          19.3130 |          14.2650 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0057 |          18.6091 |          14.2597 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0045 |          18.5575 |          14.2384 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0036 |          18.4868 |          14.2251 |
[32m[20221213 18:57:34 @agent_ppo2.py:185][0m |          -0.0037 |          18.4818 |          14.1939 |
[32m[20221213 18:57:34 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.00
[32m[20221213 18:57:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.00
[32m[20221213 18:57:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 18:57:34 @agent_ppo2.py:143][0m Total time:       9.03 min
[32m[20221213 18:57:34 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 18:57:34 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 18:57:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |           0.0015 |         136.5403 |          14.1266 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |           0.0024 |         131.9511 |          14.0812 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |          -0.0011 |         131.3401 |          14.1259 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |          -0.0006 |         130.8303 |          14.0706 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |           0.0005 |         130.5139 |          14.0390 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |          -0.0014 |         129.9830 |          14.0484 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |           0.0039 |         130.4979 |          14.0070 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |           0.0014 |         129.4484 |          14.0135 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |          -0.0035 |         129.3472 |          14.0271 |
[32m[20221213 18:57:35 @agent_ppo2.py:185][0m |           0.0082 |         137.7412 |          14.0475 |
[32m[20221213 18:57:35 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.20
[32m[20221213 18:57:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 18:57:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:57:35 @agent_ppo2.py:143][0m Total time:       9.05 min
[32m[20221213 18:57:35 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221213 18:57:35 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 18:57:36 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:57:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |           0.0006 |         193.6610 |          14.0225 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |          -0.0033 |         190.4165 |          13.9144 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |           0.0048 |         197.4106 |          13.8713 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |          -0.0013 |         189.7067 |          13.8625 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |          -0.0005 |         189.3010 |          13.8232 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |          -0.0010 |         189.1245 |          13.7377 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |           0.0068 |         195.8240 |          13.6369 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |           0.0004 |         189.0608 |          13.6433 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |           0.0128 |         214.3049 |          13.5812 |
[32m[20221213 18:57:36 @agent_ppo2.py:185][0m |           0.0093 |         192.7973 |          13.5839 |
[32m[20221213 18:57:36 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.80
[32m[20221213 18:57:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.00
[32m[20221213 18:57:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:57:37 @agent_ppo2.py:143][0m Total time:       9.07 min
[32m[20221213 18:57:37 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 18:57:37 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 18:57:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |           0.0027 |          20.0855 |          13.3903 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |           0.0062 |          18.0777 |          13.3160 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |          -0.0037 |          17.6459 |          13.2116 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |          -0.0025 |          17.5280 |          13.1896 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |          -0.0060 |          17.5026 |          13.1712 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |          -0.0048 |          17.4820 |          13.1504 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |           0.0045 |          18.1980 |          13.1542 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |          -0.0056 |          17.3892 |          13.1400 |
[32m[20221213 18:57:37 @agent_ppo2.py:185][0m |          -0.0031 |          17.4997 |          13.1051 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0020 |          17.6641 |          13.0731 |
[32m[20221213 18:57:38 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.40
[32m[20221213 18:57:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:57:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 882.00
[32m[20221213 18:57:38 @agent_ppo2.py:143][0m Total time:       9.09 min
[32m[20221213 18:57:38 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221213 18:57:38 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 18:57:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0027 |         101.7084 |          12.7297 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0017 |          99.4737 |          12.6165 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0034 |          96.6523 |          12.5658 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0028 |          96.4201 |          12.5077 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0089 |          97.8358 |          12.4635 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0050 |          96.5056 |          12.4066 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |           0.0013 |          95.6655 |          12.3635 |
[32m[20221213 18:57:38 @agent_ppo2.py:185][0m |          -0.0068 |          95.4592 |          12.2945 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |          -0.0065 |          95.3008 |          12.2734 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |          -0.0008 |          95.9047 |          12.1740 |
[32m[20221213 18:57:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:57:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.60
[32m[20221213 18:57:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 892.00
[32m[20221213 18:57:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:57:39 @agent_ppo2.py:143][0m Total time:       9.11 min
[32m[20221213 18:57:39 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 18:57:39 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 18:57:39 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:57:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |           0.0027 |         209.0910 |          12.4547 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |           0.0012 |         201.5914 |          12.4758 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |           0.0044 |         199.9080 |          12.4398 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |           0.0005 |         200.3771 |          12.3910 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |           0.0013 |         199.1072 |          12.3596 |
[32m[20221213 18:57:39 @agent_ppo2.py:185][0m |          -0.0026 |         198.2498 |          12.3836 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |          -0.0011 |         197.9436 |          12.3475 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |          -0.0039 |         197.9669 |          12.3858 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |          -0.0016 |         198.0087 |          12.3942 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |           0.0002 |         197.7519 |          12.3997 |
[32m[20221213 18:57:40 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 685.00
[32m[20221213 18:57:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.00
[32m[20221213 18:57:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 18:57:40 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221213 18:57:40 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221213 18:57:40 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 18:57:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |           0.0010 |          36.3345 |          12.2602 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |           0.0031 |          27.6072 |          12.2125 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |           0.0019 |          28.0676 |          12.3489 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |          -0.0016 |          26.3840 |          12.3831 |
[32m[20221213 18:57:40 @agent_ppo2.py:185][0m |           0.0061 |          28.9644 |          12.4543 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |          -0.0078 |          26.1382 |          12.4149 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |           0.0077 |          26.1440 |          12.4623 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |          -0.0053 |          26.0280 |          12.5034 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |          -0.0034 |          26.0035 |          12.4920 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |           0.0025 |          26.0931 |          12.4640 |
[32m[20221213 18:57:41 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.20
[32m[20221213 18:57:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 18:57:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 18:57:41 @agent_ppo2.py:143][0m Total time:       9.14 min
[32m[20221213 18:57:41 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 18:57:41 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 18:57:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |          -0.0014 |          90.0915 |          12.6354 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |          -0.0021 |          74.8458 |          12.5606 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |           0.0022 |          72.7693 |          12.5498 |
[32m[20221213 18:57:41 @agent_ppo2.py:185][0m |           0.0002 |          71.5514 |          12.5084 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0011 |          71.0614 |          12.4604 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0077 |          70.1883 |          12.4571 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0098 |          69.7952 |          12.5575 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0044 |          69.6082 |          12.4515 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0081 |          69.1323 |          12.4918 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0041 |          69.1132 |          12.4994 |
[32m[20221213 18:57:42 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.60
[32m[20221213 18:57:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.00
[32m[20221213 18:57:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:57:42 @agent_ppo2.py:143][0m Total time:       9.16 min
[32m[20221213 18:57:42 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221213 18:57:42 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 18:57:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |           0.0014 |          33.0562 |          12.3242 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |          -0.0013 |          25.0841 |          12.2450 |
[32m[20221213 18:57:42 @agent_ppo2.py:185][0m |           0.0003 |          23.9528 |          12.3776 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0073 |          23.2060 |          12.3343 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |           0.0044 |          23.1407 |          12.3017 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0022 |          22.3887 |          12.2375 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0045 |          22.0492 |          12.1969 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0056 |          21.7557 |          12.2083 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0035 |          21.7467 |          12.2335 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0065 |          21.3898 |          12.2621 |
[32m[20221213 18:57:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.60
[32m[20221213 18:57:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.00
[32m[20221213 18:57:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.00
[32m[20221213 18:57:43 @agent_ppo2.py:143][0m Total time:       9.18 min
[32m[20221213 18:57:43 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 18:57:43 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 18:57:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0012 |          36.2940 |          12.2543 |
[32m[20221213 18:57:43 @agent_ppo2.py:185][0m |          -0.0072 |          29.5091 |          12.1102 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0058 |          28.9397 |          12.0906 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0052 |          28.6047 |          12.0295 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0034 |          28.4719 |          11.9419 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0037 |          28.3995 |          11.8628 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |           0.0014 |          29.4971 |          11.7673 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0012 |          28.2756 |          11.7650 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0022 |          28.7717 |          11.8370 |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |          -0.0079 |          28.1710 |          11.7528 |
[32m[20221213 18:57:44 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.20
[32m[20221213 18:57:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:57:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 18:57:44 @agent_ppo2.py:143][0m Total time:       9.20 min
[32m[20221213 18:57:44 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221213 18:57:44 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 18:57:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:44 @agent_ppo2.py:185][0m |           0.0049 |         160.7564 |          11.6696 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |           0.0037 |         152.6262 |          11.6771 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |          -0.0010 |         150.7767 |          11.7001 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |          -0.0002 |         149.7508 |          11.6605 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |           0.0000 |         148.7950 |          11.7055 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |           0.0001 |         148.4119 |          11.7156 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |           0.0022 |         149.4200 |          11.7503 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |           0.0010 |         148.9874 |          11.7975 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |           0.0010 |         148.0402 |          11.8132 |
[32m[20221213 18:57:45 @agent_ppo2.py:185][0m |          -0.0009 |         147.6823 |          11.8680 |
[32m[20221213 18:57:45 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.00
[32m[20221213 18:57:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.00
[32m[20221213 18:57:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 18:57:45 @agent_ppo2.py:143][0m Total time:       9.21 min
[32m[20221213 18:57:45 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 18:57:45 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 18:57:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0012 |          29.8885 |          11.7870 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0023 |          23.4960 |          11.7285 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0021 |          23.1199 |          11.5732 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0037 |          23.1055 |          11.5683 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0045 |          22.8361 |          11.5248 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0073 |          22.7823 |          11.5412 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0022 |          22.7731 |          11.4362 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0068 |          22.6509 |          11.3792 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0125 |          22.6565 |          11.3856 |
[32m[20221213 18:57:46 @agent_ppo2.py:185][0m |          -0.0027 |          22.8381 |          11.3489 |
[32m[20221213 18:57:46 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.00
[32m[20221213 18:57:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 18:57:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:57:46 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221213 18:57:46 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221213 18:57:46 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 18:57:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |          -0.0014 |          21.5993 |          11.5552 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |           0.0004 |          18.5560 |          11.5841 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |           0.0079 |          20.5514 |          11.6000 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |          -0.0029 |          18.1808 |          11.6789 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |          -0.0013 |          18.2204 |          11.6316 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |           0.0080 |          19.4475 |          11.6713 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |          -0.0020 |          18.0129 |          11.6357 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |          -0.0040 |          18.0133 |          11.5474 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |          -0.0029 |          17.9793 |          11.6091 |
[32m[20221213 18:57:47 @agent_ppo2.py:185][0m |           0.0053 |          19.2895 |          11.5652 |
[32m[20221213 18:57:47 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.20
[32m[20221213 18:57:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 18:57:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:57:48 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221213 18:57:48 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 18:57:48 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 18:57:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0068 |         180.5770 |          11.4191 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0028 |         178.8794 |          11.4589 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |          -0.0001 |         176.5293 |          11.4272 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |          -0.0008 |         176.1658 |          11.3736 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |          -0.0001 |         176.1921 |          11.3880 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0144 |         192.3562 |          11.3552 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0038 |         176.7350 |          11.5854 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0066 |         181.6615 |          11.4578 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0103 |         181.1398 |          11.5300 |
[32m[20221213 18:57:48 @agent_ppo2.py:185][0m |           0.0019 |         175.9613 |          11.5132 |
[32m[20221213 18:57:48 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:57:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:57:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:57:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 18:57:49 @agent_ppo2.py:143][0m Total time:       9.27 min
[32m[20221213 18:57:49 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221213 18:57:49 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 18:57:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0045 |          19.9050 |          11.5086 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0045 |          16.4161 |          11.4737 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0055 |          16.0938 |          11.4649 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0031 |          15.8945 |          11.4945 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0067 |          15.7148 |          11.4812 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0051 |          15.6256 |          11.5057 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |           0.0042 |          16.2944 |          11.5198 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0032 |          15.5415 |          11.4974 |
[32m[20221213 18:57:49 @agent_ppo2.py:185][0m |          -0.0017 |          15.9765 |          11.4868 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |           0.0048 |          16.6860 |          11.4309 |
[32m[20221213 18:57:50 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.40
[32m[20221213 18:57:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 18:57:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 888.00
[32m[20221213 18:57:50 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 1000.00
[32m[20221213 18:57:50 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221213 18:57:50 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 18:57:50 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 18:57:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |           0.0010 |          14.7708 |          11.1490 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |          -0.0001 |          13.5504 |          10.9464 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |          -0.0016 |          13.4929 |          10.7874 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |          -0.0024 |          13.4178 |          10.6770 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |          -0.0035 |          13.3936 |          10.5589 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |           0.0006 |          13.5516 |          10.4896 |
[32m[20221213 18:57:50 @agent_ppo2.py:185][0m |           0.0033 |          13.5685 |          10.4249 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |          -0.0043 |          13.3196 |          10.2618 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |          -0.0034 |          13.3325 |          10.1825 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |           0.0046 |          13.7274 |          10.0701 |
[32m[20221213 18:57:51 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:57:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.20
[32m[20221213 18:57:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 18:57:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.00
[32m[20221213 18:57:51 @agent_ppo2.py:143][0m Total time:       9.31 min
[32m[20221213 18:57:51 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221213 18:57:51 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 18:57:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |           0.0022 |         174.6528 |          10.0561 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |          -0.0011 |         171.7560 |           9.9528 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |           0.0006 |         172.2078 |          10.0642 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |          -0.0026 |         171.6278 |          10.0355 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |           0.0014 |         171.5818 |          10.1060 |
[32m[20221213 18:57:51 @agent_ppo2.py:185][0m |          -0.0002 |         171.2937 |          10.1769 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |          -0.0004 |         171.5169 |          10.2351 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |           0.0036 |         171.4580 |          10.2677 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |          -0.0005 |         171.1585 |          10.2218 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |          -0.0004 |         170.9923 |          10.2438 |
[32m[20221213 18:57:52 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 991.80
[32m[20221213 18:57:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 993.00
[32m[20221213 18:57:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.00
[32m[20221213 18:57:52 @agent_ppo2.py:143][0m Total time:       9.32 min
[32m[20221213 18:57:52 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 18:57:52 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 18:57:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |           0.0018 |         119.4986 |          10.4696 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |           0.0114 |         126.2523 |          10.4295 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |           0.0005 |         113.5982 |          10.3542 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |          -0.0019 |         112.9806 |          10.2709 |
[32m[20221213 18:57:52 @agent_ppo2.py:185][0m |          -0.0013 |         112.9179 |          10.2375 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0048 |         112.6830 |          10.2986 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0011 |         112.4682 |          10.2756 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0002 |         112.2130 |          10.2461 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0040 |         112.1708 |          10.2172 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0023 |         111.7920 |          10.2087 |
[32m[20221213 18:57:53 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.20
[32m[20221213 18:57:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 693.00
[32m[20221213 18:57:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 18:57:53 @agent_ppo2.py:143][0m Total time:       9.34 min
[32m[20221213 18:57:53 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221213 18:57:53 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 18:57:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |           0.0103 |         107.0853 |          10.1482 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0004 |          89.1787 |          10.1396 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0037 |          88.8652 |          10.1507 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0003 |          88.4278 |          10.1548 |
[32m[20221213 18:57:53 @agent_ppo2.py:185][0m |          -0.0038 |          88.1045 |          10.1688 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0011 |          87.8872 |          10.1713 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0058 |          87.4824 |          10.1467 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0015 |          87.3464 |          10.1411 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0018 |          87.5032 |          10.1894 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0045 |          87.0636 |          10.1212 |
[32m[20221213 18:57:54 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.60
[32m[20221213 18:57:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:57:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.00
[32m[20221213 18:57:54 @agent_ppo2.py:143][0m Total time:       9.36 min
[32m[20221213 18:57:54 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 18:57:54 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 18:57:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0046 |          12.3979 |           9.8030 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0078 |           9.3441 |           9.7434 |
[32m[20221213 18:57:54 @agent_ppo2.py:185][0m |          -0.0002 |           9.1058 |           9.7796 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0106 |           8.9771 |           9.8074 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0069 |           8.8993 |           9.8408 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0085 |           8.8530 |           9.7882 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0036 |           8.8697 |           9.8367 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0080 |           8.8149 |           9.8520 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0090 |           8.7360 |           9.9110 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0074 |           8.7099 |           9.9567 |
[32m[20221213 18:57:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.40
[32m[20221213 18:57:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 18:57:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 35.00
[32m[20221213 18:57:55 @agent_ppo2.py:143][0m Total time:       9.38 min
[32m[20221213 18:57:55 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221213 18:57:55 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 18:57:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0014 |           8.7930 |          10.1209 |
[32m[20221213 18:57:55 @agent_ppo2.py:185][0m |          -0.0020 |           8.1808 |          10.0125 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0030 |           8.1345 |           9.8275 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0038 |           8.0662 |           9.7917 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |           0.0037 |           8.2630 |           9.7092 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0009 |           7.9932 |           9.5371 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0031 |           7.9649 |           9.5145 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0036 |           7.9489 |           9.5802 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0036 |           7.9449 |           9.4983 |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |          -0.0060 |           7.9189 |           9.4011 |
[32m[20221213 18:57:56 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.60
[32m[20221213 18:57:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:57:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:57:56 @agent_ppo2.py:143][0m Total time:       9.40 min
[32m[20221213 18:57:56 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 18:57:56 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 18:57:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:56 @agent_ppo2.py:185][0m |           0.0046 |         195.8314 |           9.3063 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |          -0.0002 |         184.2217 |           9.1742 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |           0.0005 |         183.5196 |           9.1675 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |           0.0008 |         183.2626 |           9.2184 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |           0.0064 |         183.2273 |           9.2143 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |           0.0050 |         185.6512 |           9.1815 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |           0.0021 |         183.6902 |           9.2739 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |          -0.0025 |         182.6453 |           9.2582 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |          -0.0041 |         182.3686 |           9.2742 |
[32m[20221213 18:57:57 @agent_ppo2.py:185][0m |           0.0130 |         196.1410 |           9.3303 |
[32m[20221213 18:57:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.20
[32m[20221213 18:57:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 884.00
[32m[20221213 18:57:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 18:57:57 @agent_ppo2.py:143][0m Total time:       9.41 min
[32m[20221213 18:57:57 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221213 18:57:57 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 18:57:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:57:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |           0.0026 |         142.7077 |           9.3893 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |           0.0016 |         135.3876 |           9.2477 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |          -0.0057 |         133.2932 |           9.0558 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |           0.0077 |         143.0254 |           8.9599 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |           0.0007 |         131.3399 |           8.8671 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |          -0.0021 |         130.5544 |           8.8208 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |          -0.0038 |         130.3433 |           8.7842 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |          -0.0055 |         130.3652 |           8.6703 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |          -0.0053 |         129.8315 |           8.6369 |
[32m[20221213 18:57:58 @agent_ppo2.py:185][0m |          -0.0032 |         129.2073 |           8.5451 |
[32m[20221213 18:57:58 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:57:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.60
[32m[20221213 18:57:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.00
[32m[20221213 18:57:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 18:57:58 @agent_ppo2.py:143][0m Total time:       9.43 min
[32m[20221213 18:57:58 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 18:57:58 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 18:57:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:57:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |          -0.0009 |         201.4893 |           8.2338 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |           0.0122 |         216.9362 |           8.2770 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |          -0.0019 |         201.4576 |           8.2714 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |           0.0000 |         200.6059 |           8.2498 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |          -0.0025 |         200.4494 |           8.1400 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |          -0.0009 |         200.3880 |           8.1103 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |           0.0025 |         203.3124 |           8.0260 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |          -0.0006 |         200.2714 |           8.1136 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |           0.0025 |         200.3103 |           8.0966 |
[32m[20221213 18:57:59 @agent_ppo2.py:185][0m |           0.0001 |         200.8730 |           7.9961 |
[32m[20221213 18:57:59 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:57:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:57:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:57:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 18:57:59 @agent_ppo2.py:143][0m Total time:       9.45 min
[32m[20221213 18:57:59 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221213 18:57:59 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 18:58:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |           0.0010 |          16.7824 |           7.5679 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0030 |          13.4458 |           7.4570 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0026 |          13.1409 |           7.3671 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0013 |          13.0519 |           7.3574 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0018 |          13.0100 |           7.2305 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |           0.0036 |          14.1712 |           7.2047 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0003 |          12.9768 |           7.0688 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0056 |          12.9010 |           7.0943 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0079 |          12.9023 |           7.0286 |
[32m[20221213 18:58:00 @agent_ppo2.py:185][0m |          -0.0030 |          12.9426 |           6.9495 |
[32m[20221213 18:58:00 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.00
[32m[20221213 18:58:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:58:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 18:58:01 @agent_ppo2.py:143][0m Total time:       9.47 min
[32m[20221213 18:58:01 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 18:58:01 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 18:58:01 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:58:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0049 |         207.5466 |           6.9469 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |          -0.0005 |         204.7800 |           6.7991 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0031 |         205.1193 |           6.5369 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0045 |         205.7158 |           6.3480 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0092 |         224.8401 |           6.3765 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0114 |         216.5610 |           6.2189 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0046 |         203.9347 |           6.2675 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |           0.0029 |         203.9889 |           6.0308 |
[32m[20221213 18:58:01 @agent_ppo2.py:185][0m |          -0.0024 |         203.8198 |           5.9016 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |          -0.0001 |         203.5456 |           5.8981 |
[32m[20221213 18:58:02 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.80
[32m[20221213 18:58:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.00
[32m[20221213 18:58:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 18:58:02 @agent_ppo2.py:143][0m Total time:       9.49 min
[32m[20221213 18:58:02 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221213 18:58:02 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 18:58:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |          -0.0010 |         209.7218 |           5.5482 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |          -0.0011 |         206.0770 |           5.4136 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |           0.0012 |         205.1030 |           5.5060 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |           0.0113 |         217.1558 |           5.5457 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |          -0.0004 |         204.8597 |           5.6890 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |           0.0001 |         204.7292 |           5.5959 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |          -0.0001 |         204.7603 |           5.5818 |
[32m[20221213 18:58:02 @agent_ppo2.py:185][0m |           0.0032 |         208.7458 |           5.6496 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |           0.0000 |         204.8910 |           5.6549 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |           0.0027 |         205.0273 |           5.6498 |
[32m[20221213 18:58:03 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.00
[32m[20221213 18:58:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.00
[32m[20221213 18:58:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 663.00
[32m[20221213 18:58:03 @agent_ppo2.py:143][0m Total time:       9.51 min
[32m[20221213 18:58:03 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 18:58:03 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 18:58:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |           0.0028 |          15.9988 |           5.7706 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |          -0.0012 |           7.6892 |           6.0054 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |          -0.0007 |           6.7281 |           6.0821 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |          -0.0101 |           6.3008 |           6.0966 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |          -0.0056 |           5.9920 |           6.2838 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |          -0.0086 |           5.8014 |           6.3160 |
[32m[20221213 18:58:03 @agent_ppo2.py:185][0m |          -0.0081 |           5.6847 |           6.4241 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0084 |           5.5829 |           6.4807 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0031 |           5.4677 |           6.5116 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0110 |           5.3530 |           6.6163 |
[32m[20221213 18:58:04 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.40
[32m[20221213 18:58:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:58:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 18:58:04 @agent_ppo2.py:143][0m Total time:       9.52 min
[32m[20221213 18:58:04 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221213 18:58:04 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 18:58:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0032 |         148.9805 |           7.0283 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0053 |         143.2696 |           6.9025 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0040 |         142.5139 |           6.7766 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0022 |         142.1813 |           6.6394 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |           0.0038 |         142.2697 |           6.6118 |
[32m[20221213 18:58:04 @agent_ppo2.py:185][0m |          -0.0035 |         141.8574 |           6.4811 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |          -0.0008 |         141.6513 |           6.5163 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |           0.0085 |         162.7658 |           6.4188 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |           0.0037 |         147.4312 |           6.3420 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |          -0.0070 |         140.7002 |           6.3320 |
[32m[20221213 18:58:05 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.20
[32m[20221213 18:58:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:58:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.00
[32m[20221213 18:58:05 @agent_ppo2.py:143][0m Total time:       9.54 min
[32m[20221213 18:58:05 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 18:58:05 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 18:58:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |          -0.0032 |         112.3860 |           6.0891 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |          -0.0042 |         107.0692 |           6.2004 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |          -0.0035 |         105.3567 |           6.3767 |
[32m[20221213 18:58:05 @agent_ppo2.py:185][0m |          -0.0041 |         104.2435 |           6.4420 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0076 |         103.0767 |           6.4621 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0047 |         102.1712 |           6.5807 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0083 |         101.5488 |           6.6303 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0064 |         101.4830 |           6.7025 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0024 |         100.8847 |           6.7571 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0069 |         100.7937 |           6.8113 |
[32m[20221213 18:58:06 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.60
[32m[20221213 18:58:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 18:58:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.00
[32m[20221213 18:58:06 @agent_ppo2.py:143][0m Total time:       9.56 min
[32m[20221213 18:58:06 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221213 18:58:06 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 18:58:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |           0.0025 |         214.1542 |           6.9834 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |          -0.0014 |         212.7802 |           6.9150 |
[32m[20221213 18:58:06 @agent_ppo2.py:185][0m |           0.0053 |         216.5084 |           6.6600 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0010 |         212.5383 |           6.6861 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0011 |         212.5410 |           6.5608 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |          -0.0015 |         212.6220 |           6.5149 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0004 |         212.6563 |           6.4293 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0019 |         214.4365 |           6.3112 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0006 |         212.1669 |           6.3778 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0074 |         213.5573 |           6.5043 |
[32m[20221213 18:58:07 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:58:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 977.20
[32m[20221213 18:58:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 981.00
[32m[20221213 18:58:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 18:58:07 @agent_ppo2.py:143][0m Total time:       9.58 min
[32m[20221213 18:58:07 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 18:58:07 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 18:58:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |           0.0057 |         147.6482 |           6.0744 |
[32m[20221213 18:58:07 @agent_ppo2.py:185][0m |          -0.0001 |         145.2405 |           5.9059 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0030 |         144.9120 |           5.7298 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0022 |         144.3534 |           5.7227 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0018 |         144.1113 |           5.6337 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0015 |         143.6573 |           5.6575 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0022 |         143.4761 |           5.5987 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0056 |         143.5863 |           5.6410 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0022 |         143.2233 |           5.6395 |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |          -0.0028 |         142.8711 |           5.6050 |
[32m[20221213 18:58:08 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.00
[32m[20221213 18:58:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.00
[32m[20221213 18:58:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.00
[32m[20221213 18:58:08 @agent_ppo2.py:143][0m Total time:       9.60 min
[32m[20221213 18:58:08 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221213 18:58:08 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 18:58:08 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:58:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:08 @agent_ppo2.py:185][0m |           0.0004 |          17.1903 |           5.4325 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0051 |          13.5686 |           5.2968 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0024 |          13.2785 |           5.3423 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0060 |          13.1671 |           5.2939 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0054 |          13.1097 |           5.2855 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0070 |          13.0725 |           5.3103 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0019 |          13.0308 |           5.3737 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0064 |          13.0048 |           5.3498 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0066 |          12.9622 |           5.2839 |
[32m[20221213 18:58:09 @agent_ppo2.py:185][0m |          -0.0039 |          13.2698 |           5.4246 |
[32m[20221213 18:58:09 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.60
[32m[20221213 18:58:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:58:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.00
[32m[20221213 18:58:09 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 18:58:09 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 18:58:09 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 18:58:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |           0.0040 |          46.6228 |           5.2492 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |          -0.0011 |          29.6809 |           5.2727 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |           0.0038 |          28.7970 |           5.2076 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |           0.0026 |          28.0944 |           5.1322 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |           0.0040 |          27.5958 |           5.1302 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |           0.0069 |          29.7245 |           5.0026 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |          -0.0060 |          27.3527 |           5.0330 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |           0.0057 |          26.9896 |           4.9703 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |          -0.0057 |          26.7905 |           4.9270 |
[32m[20221213 18:58:10 @agent_ppo2.py:185][0m |          -0.0067 |          26.8794 |           4.9740 |
[32m[20221213 18:58:10 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.80
[32m[20221213 18:58:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 18:58:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:58:10 @agent_ppo2.py:143][0m Total time:       9.63 min
[32m[20221213 18:58:10 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221213 18:58:10 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 18:58:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |           0.0033 |         205.6027 |           4.7464 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |          -0.0004 |         202.2605 |           4.7181 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |          -0.0001 |         201.4499 |           4.5792 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |           0.0003 |         201.2093 |           4.3474 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |           0.0039 |         201.1196 |           4.3369 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |          -0.0013 |         201.1889 |           4.4191 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |           0.0052 |         201.8092 |           4.3810 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |           0.0004 |         201.0741 |           4.2491 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |          -0.0025 |         200.7652 |           4.2273 |
[32m[20221213 18:58:11 @agent_ppo2.py:185][0m |          -0.0028 |         200.8301 |           4.1410 |
[32m[20221213 18:58:11 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.60
[32m[20221213 18:58:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 18:58:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 988.00
[32m[20221213 18:58:11 @agent_ppo2.py:143][0m Total time:       9.65 min
[32m[20221213 18:58:11 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 18:58:11 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 18:58:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:58:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |          -0.0003 |         208.6674 |           3.6791 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |          -0.0007 |         206.6586 |           3.5839 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |           0.0153 |         219.3855 |           3.7062 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |           0.0009 |         205.9366 |           3.6708 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |           0.0033 |         211.2351 |           3.6352 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |          -0.0002 |         205.7114 |           3.7830 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |          -0.0006 |         205.5404 |           3.5484 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |           0.0042 |         211.3330 |           3.6589 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |          -0.0007 |         205.3784 |           3.5701 |
[32m[20221213 18:58:12 @agent_ppo2.py:185][0m |          -0.0017 |         205.3021 |           3.6065 |
[32m[20221213 18:58:12 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.80
[32m[20221213 18:58:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 18:58:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:58:13 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221213 18:58:13 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221213 18:58:13 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 18:58:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |           0.0027 |          20.5993 |           3.6911 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |          -0.0017 |          15.9308 |           3.6987 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |           0.0044 |          16.5610 |           3.7507 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |           0.0009 |          15.3923 |           3.8700 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |          -0.0030 |          15.1947 |           3.7684 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |           0.0015 |          15.1340 |           3.8148 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |           0.0136 |          15.0548 |           3.8519 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |          -0.0014 |          14.8931 |           3.8685 |
[32m[20221213 18:58:13 @agent_ppo2.py:185][0m |          -0.0031 |          14.9474 |           3.9860 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |          -0.0056 |          14.8275 |           3.9375 |
[32m[20221213 18:58:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.40
[32m[20221213 18:58:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 18:58:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 18:58:14 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 18:58:14 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 18:58:14 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 18:58:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |          -0.0009 |         223.6938 |           4.0237 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |           0.0033 |         216.7625 |           4.1227 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |          -0.0004 |         213.9386 |           4.0994 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |          -0.0014 |         213.1637 |           4.0268 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |           0.0184 |         252.3574 |           4.0127 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |           0.0103 |         233.4458 |           3.8576 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |           0.0014 |         212.2864 |           3.9461 |
[32m[20221213 18:58:14 @agent_ppo2.py:185][0m |           0.0020 |         217.1431 |           3.8269 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |           0.0009 |         211.6532 |           3.8254 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |          -0.0035 |         211.3037 |           3.8454 |
[32m[20221213 18:58:15 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.00
[32m[20221213 18:58:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 658.00
[32m[20221213 18:58:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 18:58:15 @agent_ppo2.py:143][0m Total time:       9.70 min
[32m[20221213 18:58:15 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221213 18:58:15 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 18:58:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |           0.0017 |         216.4099 |           3.5948 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |           0.0024 |         212.4084 |           3.4246 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |          -0.0014 |         209.7644 |           3.5702 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |           0.0010 |         208.1139 |           3.5811 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |          -0.0006 |         207.2378 |           3.5837 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |          -0.0017 |         206.5973 |           3.6118 |
[32m[20221213 18:58:15 @agent_ppo2.py:185][0m |          -0.0005 |         206.5538 |           3.7387 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |           0.0130 |         233.2970 |           3.5740 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |           0.0189 |         236.0911 |           3.9280 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0002 |         206.1467 |           3.7877 |
[32m[20221213 18:58:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.80
[32m[20221213 18:58:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 929.00
[32m[20221213 18:58:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:58:16 @agent_ppo2.py:143][0m Total time:       9.72 min
[32m[20221213 18:58:16 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 18:58:16 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 18:58:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0010 |         212.9037 |           3.5881 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0029 |         208.3682 |           3.5806 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0045 |         207.8752 |           3.6540 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0012 |         208.1411 |           3.6627 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0040 |         207.5343 |           3.6092 |
[32m[20221213 18:58:16 @agent_ppo2.py:185][0m |          -0.0015 |         207.3246 |           3.5931 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |          -0.0021 |         207.0492 |           3.6419 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |          -0.0046 |         206.8057 |           3.4906 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |          -0.0013 |         207.6184 |           3.3921 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |          -0.0015 |         206.8851 |           3.5622 |
[32m[20221213 18:58:17 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:58:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.40
[32m[20221213 18:58:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.00
[32m[20221213 18:58:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 18:58:17 @agent_ppo2.py:143][0m Total time:       9.74 min
[32m[20221213 18:58:17 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221213 18:58:17 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 18:58:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |           0.0121 |          41.2954 |           3.9806 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |           0.0028 |          28.1379 |           4.1111 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |           0.0012 |          26.6099 |           4.2375 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |          -0.0054 |          25.8248 |           4.4305 |
[32m[20221213 18:58:17 @agent_ppo2.py:185][0m |          -0.0048 |          25.2202 |           4.5588 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0056 |          24.8043 |           4.7467 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0094 |          24.6676 |           4.7619 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0044 |          24.4830 |           4.8616 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0055 |          24.1862 |           4.9328 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |           0.0002 |          24.3317 |           4.9599 |
[32m[20221213 18:58:18 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.00
[32m[20221213 18:58:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:58:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 18:58:18 @agent_ppo2.py:143][0m Total time:       9.76 min
[32m[20221213 18:58:18 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 18:58:18 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 18:58:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0104 |          81.3579 |           4.7876 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0194 |          72.8928 |           4.6032 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0211 |          72.5206 |           4.4476 |
[32m[20221213 18:58:18 @agent_ppo2.py:185][0m |          -0.0147 |          71.9506 |           4.2636 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0135 |          71.2001 |           4.1676 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0166 |          70.7382 |           4.1130 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0172 |          70.8499 |           3.9442 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0135 |          70.3521 |           3.9080 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0165 |          70.4551 |           3.7849 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0202 |          70.3500 |           3.6578 |
[32m[20221213 18:58:19 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.00
[32m[20221213 18:58:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.00
[32m[20221213 18:58:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:58:19 @agent_ppo2.py:143][0m Total time:       9.78 min
[32m[20221213 18:58:19 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221213 18:58:19 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 18:58:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:58:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0061 |         151.5743 |           3.4879 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0165 |         128.4675 |           3.1168 |
[32m[20221213 18:58:19 @agent_ppo2.py:185][0m |          -0.0100 |         125.3814 |           2.8223 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0126 |         125.6149 |           2.6053 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0159 |         121.9184 |           2.4099 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0211 |         121.8677 |           2.1870 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0162 |         120.5127 |           2.0314 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0170 |         121.2131 |           1.9646 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0201 |         119.5249 |           1.6949 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0251 |         119.4999 |           1.7502 |
[32m[20221213 18:58:20 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.60
[32m[20221213 18:58:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.00
[32m[20221213 18:58:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 18:58:20 @agent_ppo2.py:143][0m Total time:       9.80 min
[32m[20221213 18:58:20 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 18:58:20 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 18:58:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |          -0.0025 |          43.8173 |           0.2844 |
[32m[20221213 18:58:20 @agent_ppo2.py:185][0m |           0.0004 |          31.9415 |           0.3415 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |           0.0018 |          30.9397 |           0.2883 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |          -0.0025 |          29.7258 |           0.3168 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |          -0.0031 |          29.1822 |           0.4739 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |          -0.0079 |          28.8295 |           0.7148 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |          -0.0042 |          28.6669 |           0.7475 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |           0.0012 |          28.5339 |           0.6825 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |          -0.0042 |          28.4790 |           0.6743 |
[32m[20221213 18:58:21 @agent_ppo2.py:185][0m |          -0.0054 |          28.4590 |           0.8228 |
[32m[20221213 18:58:21 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.80
[32m[20221213 18:58:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:58:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:58:21 @agent_ppo2.py:143][0m Total time:       9.81 min
[32m[20221213 18:58:21 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221213 18:58:21 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 18:58:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |           0.0001 |          46.0899 |           1.2659 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |           0.0054 |          36.7117 |           1.0409 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0063 |          32.4590 |           0.9925 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0074 |          30.8635 |           0.9373 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0083 |          29.5729 |           0.7872 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0106 |          29.0654 |           0.7624 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0133 |          28.4359 |           0.5737 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0169 |          27.6689 |           0.4401 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0068 |          27.2357 |           0.3246 |
[32m[20221213 18:58:22 @agent_ppo2.py:185][0m |          -0.0068 |          27.1092 |           0.3039 |
[32m[20221213 18:58:22 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.80
[32m[20221213 18:58:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.00
[32m[20221213 18:58:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:58:22 @agent_ppo2.py:143][0m Total time:       9.83 min
[32m[20221213 18:58:22 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 18:58:22 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 18:58:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:58:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |           0.0018 |          16.5600 |           0.6678 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |          -0.0033 |          10.1778 |           0.7194 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |          -0.0037 |           8.9632 |           0.6756 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |          -0.0069 |           8.4176 |           0.7305 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |          -0.0040 |           7.9494 |           0.7593 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |          -0.0018 |           7.7185 |           0.7005 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |          -0.0038 |           7.5279 |           0.6950 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |           0.0035 |           7.7421 |           0.7085 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |           0.0077 |           7.2232 |           0.8658 |
[32m[20221213 18:58:23 @agent_ppo2.py:185][0m |           0.0053 |           7.1108 |           0.8899 |
[32m[20221213 18:58:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.60
[32m[20221213 18:58:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 18:58:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 18:58:23 @agent_ppo2.py:143][0m Total time:       9.85 min
[32m[20221213 18:58:23 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221213 18:58:23 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 18:58:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |           0.0090 |          16.8544 |           0.8379 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |           0.0046 |           9.9246 |           1.0276 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0034 |           8.6158 |           1.1683 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |           0.0028 |           8.0011 |           1.2502 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0057 |           7.6800 |           1.3031 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0099 |           7.3978 |           1.4067 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0055 |           7.1598 |           1.4157 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0024 |           7.0226 |           1.3776 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0037 |           6.8106 |           1.3927 |
[32m[20221213 18:58:24 @agent_ppo2.py:185][0m |          -0.0023 |           6.7134 |           1.3950 |
[32m[20221213 18:58:24 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.20
[32m[20221213 18:58:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 48.00
[32m[20221213 18:58:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 18:58:25 @agent_ppo2.py:143][0m Total time:       9.87 min
[32m[20221213 18:58:25 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 18:58:25 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 18:58:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |           0.0095 |         154.1772 |           1.2001 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0070 |         133.4433 |           1.1793 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0016 |         133.2175 |           1.0580 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0081 |         132.4777 |           0.9774 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0007 |         137.4271 |           0.8772 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0058 |         132.1833 |           0.7641 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0076 |         131.9860 |           0.6267 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0010 |         132.5112 |           0.5688 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |           0.0003 |         132.7360 |           0.6122 |
[32m[20221213 18:58:25 @agent_ppo2.py:185][0m |          -0.0081 |         131.5901 |           0.6548 |
[32m[20221213 18:58:25 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:58:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:58:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.00
[32m[20221213 18:58:26 @agent_ppo2.py:143][0m Total time:       9.89 min
[32m[20221213 18:58:26 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221213 18:58:26 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 18:58:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |          -0.0021 |         140.6333 |           0.4030 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |          -0.0007 |         137.0488 |           0.4768 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |          -0.0034 |         136.6633 |           0.2708 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |           0.0082 |         142.9587 |           0.2920 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |          -0.0034 |         136.0323 |           0.6358 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |           0.0072 |         143.4195 |           0.3610 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |           0.0022 |         136.4976 |           0.4276 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |          -0.0028 |         135.7381 |           0.4510 |
[32m[20221213 18:58:26 @agent_ppo2.py:185][0m |           0.0015 |         135.6527 |           0.4073 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |          -0.0006 |         135.5779 |           0.3835 |
[32m[20221213 18:58:27 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.00
[32m[20221213 18:58:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 18:58:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 18:58:27 @agent_ppo2.py:143][0m Total time:       9.90 min
[32m[20221213 18:58:27 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 18:58:27 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 18:58:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |          -0.0015 |         150.2408 |           0.0173 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |           0.0016 |         145.6163 |           0.0532 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |           0.0014 |         146.0221 |          -0.0409 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |           0.0031 |         147.9433 |          -0.1876 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |          -0.0059 |         145.0729 |          -0.6373 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |          -0.0021 |         144.8840 |          -0.6648 |
[32m[20221213 18:58:27 @agent_ppo2.py:185][0m |          -0.0037 |         144.8890 |          -0.7804 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |          -0.0015 |         144.6731 |          -0.9071 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |          -0.0028 |         145.0807 |          -0.9715 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |          -0.0023 |         144.6049 |          -1.1481 |
[32m[20221213 18:58:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.80
[32m[20221213 18:58:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 18:58:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:58:28 @agent_ppo2.py:143][0m Total time:       9.92 min
[32m[20221213 18:58:28 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221213 18:58:28 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 18:58:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |           0.0058 |          10.0536 |          -1.7468 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |           0.0112 |           7.0062 |          -1.7162 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |           0.0055 |           7.1047 |          -1.6260 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |          -0.0027 |           6.6120 |          -1.4913 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |          -0.0044 |           6.4076 |          -1.5188 |
[32m[20221213 18:58:28 @agent_ppo2.py:185][0m |          -0.0019 |           6.3566 |          -1.5541 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |          -0.0021 |           6.3316 |          -1.5677 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |          -0.0036 |           6.2408 |          -1.5303 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |          -0.0083 |           6.2137 |          -1.5135 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |          -0.0045 |           6.2003 |          -1.5657 |
[32m[20221213 18:58:29 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:58:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.40
[32m[20221213 18:58:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 18:58:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:58:29 @agent_ppo2.py:143][0m Total time:       9.94 min
[32m[20221213 18:58:29 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 18:58:29 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 18:58:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |           0.0060 |          98.5379 |          -1.1335 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |           0.0015 |          97.1988 |          -1.3768 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |           0.0060 |          94.9282 |          -1.5796 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |           0.0039 |          95.6715 |          -1.5508 |
[32m[20221213 18:58:29 @agent_ppo2.py:185][0m |          -0.0021 |          94.5341 |          -1.5792 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |           0.0047 |          96.6467 |          -1.6388 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |           0.0040 |          94.3829 |          -1.6534 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |           0.0022 |          94.2517 |          -1.5140 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |           0.0021 |          94.1831 |          -1.7099 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |          -0.0035 |          93.9543 |          -1.6072 |
[32m[20221213 18:58:30 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.20
[32m[20221213 18:58:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 874.00
[32m[20221213 18:58:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 18:58:30 @agent_ppo2.py:143][0m Total time:       9.96 min
[32m[20221213 18:58:30 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221213 18:58:30 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 18:58:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:58:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |          -0.0026 |           8.2735 |          -1.6051 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |          -0.0035 |           5.6088 |          -1.2856 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |          -0.0027 |           5.3071 |          -1.0457 |
[32m[20221213 18:58:30 @agent_ppo2.py:185][0m |          -0.0029 |           5.1576 |          -0.6650 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0073 |           5.0667 |          -0.3788 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |           0.0007 |           5.0027 |          -0.2094 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0023 |           4.9291 |          -0.0974 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0062 |           4.9651 |           0.0284 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0009 |           4.8708 |           0.1894 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0058 |           4.8377 |           0.2444 |
[32m[20221213 18:58:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.20
[32m[20221213 18:58:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 18:58:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 18:58:31 @agent_ppo2.py:143][0m Total time:       9.98 min
[32m[20221213 18:58:31 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 18:58:31 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 18:58:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |           0.0001 |           6.2876 |           0.4760 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0045 |           4.7010 |           0.7480 |
[32m[20221213 18:58:31 @agent_ppo2.py:185][0m |          -0.0053 |           4.5453 |           0.9577 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0090 |           4.4657 |           1.2985 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0114 |           4.3883 |           1.3767 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0108 |           4.3221 |           1.4825 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |           0.0083 |           4.5541 |           1.6391 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0066 |           4.2785 |           1.8105 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0030 |           4.2314 |           1.8074 |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0093 |           4.2355 |           1.9841 |
[32m[20221213 18:58:32 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.60
[32m[20221213 18:58:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:58:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 18:58:32 @agent_ppo2.py:143][0m Total time:      10.00 min
[32m[20221213 18:58:32 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221213 18:58:32 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 18:58:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:32 @agent_ppo2.py:185][0m |          -0.0023 |         141.0373 |           2.7583 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0009 |         135.8043 |           2.8223 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0020 |         135.2115 |           2.7906 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |           0.0034 |         139.3724 |           2.6424 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |           0.0044 |         136.4662 |           2.6686 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0044 |         135.0480 |           2.5806 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0034 |         135.1543 |           2.6116 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0029 |         134.9046 |           2.5775 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0088 |         135.0432 |           2.5438 |
[32m[20221213 18:58:33 @agent_ppo2.py:185][0m |          -0.0027 |         134.8219 |           2.5759 |
[32m[20221213 18:58:33 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.80
[32m[20221213 18:58:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.00
[32m[20221213 18:58:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:58:33 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221213 18:58:33 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 18:58:33 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 18:58:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:58:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |           0.0004 |           5.0547 |           2.0848 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |          -0.0036 |           4.1967 |           2.2550 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |           0.0008 |           4.0964 |           2.2150 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |          -0.0010 |           4.0841 |           2.2798 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |          -0.0030 |           4.0603 |           2.3687 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |          -0.0015 |           4.0260 |           2.3677 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |           0.0010 |           4.0216 |           2.2748 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |           0.0069 |           4.3468 |           2.3927 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |           0.0009 |           4.0747 |           2.4326 |
[32m[20221213 18:58:34 @agent_ppo2.py:185][0m |          -0.0056 |           4.0023 |           2.3184 |
[32m[20221213 18:58:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:58:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.00
[32m[20221213 18:58:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.00
[32m[20221213 18:58:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 18:58:34 @agent_ppo2.py:143][0m Total time:      10.03 min
[32m[20221213 18:58:34 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221213 18:58:34 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 18:58:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0026 |           7.3785 |           2.3455 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0023 |           5.0444 |           2.2330 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0070 |           4.8304 |           2.0536 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0025 |           4.7108 |           1.8496 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |           0.0067 |           4.5742 |           1.5855 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0098 |           4.4824 |           1.5518 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0045 |           4.4380 |           1.5176 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0043 |           4.3873 |           1.4350 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |          -0.0062 |           4.3510 |           1.4991 |
[32m[20221213 18:58:35 @agent_ppo2.py:185][0m |           0.0024 |           4.3221 |           1.4254 |
[32m[20221213 18:58:35 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.20
[32m[20221213 18:58:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:58:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.00
[32m[20221213 18:58:35 @agent_ppo2.py:143][0m Total time:      10.05 min
[32m[20221213 18:58:35 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 18:58:35 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 18:58:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |          -0.0020 |         136.3848 |           0.8127 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |           0.0001 |         130.1500 |           0.7069 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |          -0.0044 |         129.9517 |           0.6249 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |          -0.0050 |         129.6312 |           0.5505 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |           0.0017 |         129.0321 |           0.6325 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |           0.0025 |         134.7387 |           0.3011 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |          -0.0047 |         128.6605 |           0.3790 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |          -0.0092 |         128.8847 |           0.3710 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |          -0.0052 |         128.5777 |           0.3941 |
[32m[20221213 18:58:36 @agent_ppo2.py:185][0m |           0.0012 |         129.2065 |           0.3642 |
[32m[20221213 18:58:36 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.80
[32m[20221213 18:58:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 18:58:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 18:58:37 @agent_ppo2.py:143][0m Total time:      10.07 min
[32m[20221213 18:58:37 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221213 18:58:37 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 18:58:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0010 |         143.1368 |           0.4719 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0005 |         139.1008 |           0.6298 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0017 |         138.6265 |           0.7670 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0025 |         138.3808 |           1.0027 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0007 |         138.2687 |           1.0138 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |           0.0034 |         144.2093 |           1.2433 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |           0.0142 |         150.7271 |           1.3745 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0046 |         138.0976 |           1.6171 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0067 |         137.9237 |           1.5578 |
[32m[20221213 18:58:37 @agent_ppo2.py:185][0m |          -0.0049 |         137.6916 |           1.6887 |
[32m[20221213 18:58:37 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.20
[32m[20221213 18:58:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.00
[32m[20221213 18:58:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 894.00
[32m[20221213 18:58:38 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221213 18:58:38 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 18:58:38 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 18:58:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |           0.0036 |          38.6083 |           1.5910 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |          -0.0014 |          24.1648 |           1.3769 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |          -0.0007 |          23.2913 |           1.3274 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |          -0.0034 |          22.8734 |           1.2940 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |          -0.0011 |          23.2677 |           1.1627 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |          -0.0018 |          22.5480 |           1.0520 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |          -0.0052 |          22.3943 |           1.1559 |
[32m[20221213 18:58:38 @agent_ppo2.py:185][0m |           0.0051 |          24.1751 |           1.1182 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |           0.0008 |          22.1651 |           1.0915 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |           0.0034 |          22.6393 |           1.0675 |
[32m[20221213 18:58:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.80
[32m[20221213 18:58:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 18:58:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 992.00
[32m[20221213 18:58:39 @agent_ppo2.py:143][0m Total time:      10.10 min
[32m[20221213 18:58:39 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221213 18:58:39 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 18:58:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |          -0.0003 |         153.4334 |           0.9095 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |           0.0032 |         150.3356 |           0.7684 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |           0.0050 |         150.4386 |           0.9367 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |          -0.0013 |         149.7197 |           0.9294 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |           0.0035 |         151.5541 |           0.8368 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |          -0.0000 |         149.5472 |           1.0338 |
[32m[20221213 18:58:39 @agent_ppo2.py:185][0m |          -0.0003 |         149.4895 |           0.9654 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |           0.0135 |         163.3669 |           1.1388 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |           0.0051 |         149.4601 |           1.0761 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |           0.0015 |         149.4649 |           0.9608 |
[32m[20221213 18:58:40 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.60
[32m[20221213 18:58:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 18:58:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.00
[32m[20221213 18:58:40 @agent_ppo2.py:143][0m Total time:      10.12 min
[32m[20221213 18:58:40 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 18:58:40 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 18:58:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |           0.0035 |          54.7091 |           1.1811 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |          -0.0069 |          50.0210 |           1.0079 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |           0.0024 |          49.5684 |           0.9296 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |          -0.0026 |          49.1644 |           0.9054 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |           0.0007 |          48.9228 |           0.9977 |
[32m[20221213 18:58:40 @agent_ppo2.py:185][0m |          -0.0037 |          48.6537 |           0.8617 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |          -0.0011 |          48.5923 |           0.9088 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |           0.0002 |          48.3276 |           0.7870 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |           0.0033 |          48.3265 |           0.8160 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |          -0.0036 |          48.2319 |           0.8021 |
[32m[20221213 18:58:41 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:58:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.40
[32m[20221213 18:58:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.00
[32m[20221213 18:58:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:58:41 @agent_ppo2.py:143][0m Total time:      10.14 min
[32m[20221213 18:58:41 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221213 18:58:41 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 18:58:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:58:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |           0.0015 |          16.9150 |           0.6513 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |          -0.0013 |          14.4731 |           0.5394 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |           0.0142 |          15.0840 |           0.4681 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |          -0.0036 |          14.2057 |           0.4482 |
[32m[20221213 18:58:41 @agent_ppo2.py:185][0m |          -0.0039 |          14.0400 |           0.3678 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0039 |          13.9719 |           0.2762 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0047 |          13.9311 |           0.2744 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |           0.0016 |          14.0589 |           0.1503 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0006 |          13.8124 |           0.1054 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0087 |          13.7916 |          -0.0003 |
[32m[20221213 18:58:42 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.60
[32m[20221213 18:58:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 18:58:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 18:58:42 @agent_ppo2.py:143][0m Total time:      10.16 min
[32m[20221213 18:58:42 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 18:58:42 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 18:58:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |           0.0003 |         140.9535 |          -0.4085 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0011 |         138.1428 |          -0.4088 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0002 |         137.4223 |          -0.5261 |
[32m[20221213 18:58:42 @agent_ppo2.py:185][0m |          -0.0020 |         137.0395 |          -0.5972 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |          -0.0030 |         136.8458 |          -0.6146 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |          -0.0016 |         136.5038 |          -0.7218 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |           0.0015 |         136.4186 |          -0.6037 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |          -0.0033 |         136.0489 |          -0.6367 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |          -0.0008 |         135.7835 |          -0.4562 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |          -0.0021 |         135.5036 |          -0.4597 |
[32m[20221213 18:58:43 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 18:58:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.60
[32m[20221213 18:58:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 658.00
[32m[20221213 18:58:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.00
[32m[20221213 18:58:43 @agent_ppo2.py:143][0m Total time:      10.18 min
[32m[20221213 18:58:43 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221213 18:58:43 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 18:58:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |           0.0004 |          12.1460 |          -1.2854 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |           0.0032 |          10.6286 |          -1.4172 |
[32m[20221213 18:58:43 @agent_ppo2.py:185][0m |           0.0006 |          10.0868 |          -1.4721 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |           0.0046 |          10.0396 |          -1.3876 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |          -0.0055 |          10.0064 |          -1.4378 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |           0.0040 |          10.6699 |          -1.4236 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |           0.0058 |          10.3705 |          -1.4594 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |           0.0028 |           9.9249 |          -1.5270 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |          -0.0011 |           9.9231 |          -1.4595 |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |           0.0059 |          10.3640 |          -1.5662 |
[32m[20221213 18:58:44 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:58:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.00
[32m[20221213 18:58:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 18:58:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:58:44 @agent_ppo2.py:143][0m Total time:      10.20 min
[32m[20221213 18:58:44 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 18:58:44 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 18:58:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:58:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:44 @agent_ppo2.py:185][0m |           0.0082 |         154.0304 |          -1.1642 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |           0.0165 |         126.0181 |          -0.9368 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0035 |         123.2662 |          -1.5947 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0008 |         123.5993 |          -1.6230 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0046 |         122.4572 |          -1.5522 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0009 |         122.0042 |          -1.4968 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0029 |         121.5447 |          -1.6753 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0073 |         121.1463 |          -1.6042 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0019 |         121.1859 |          -1.5593 |
[32m[20221213 18:58:45 @agent_ppo2.py:185][0m |          -0.0044 |         120.8032 |          -1.5550 |
[32m[20221213 18:58:45 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 18:58:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.60
[32m[20221213 18:58:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 889.00
[32m[20221213 18:58:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:58:45 @agent_ppo2.py:143][0m Total time:      10.21 min
[32m[20221213 18:58:45 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221213 18:58:45 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 18:58:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |           0.0071 |          15.7490 |          -2.1066 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |          -0.0046 |           4.9275 |          -1.9826 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |          -0.0063 |           3.7155 |          -2.1172 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |          -0.0046 |           3.2895 |          -2.0441 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |           0.0017 |           3.0080 |          -1.8456 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |           0.0010 |           2.7959 |          -1.9647 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |          -0.0134 |           2.6615 |          -1.9673 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |          -0.0069 |           2.5695 |          -1.8718 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |          -0.0036 |           2.5052 |          -1.8754 |
[32m[20221213 18:58:46 @agent_ppo2.py:185][0m |           0.0004 |           2.4547 |          -1.8164 |
[32m[20221213 18:58:46 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:58:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.00
[32m[20221213 18:58:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:58:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:58:46 @agent_ppo2.py:143][0m Total time:      10.23 min
[32m[20221213 18:58:46 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 18:58:46 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 18:58:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |           0.0131 |          22.0218 |          -1.3483 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0078 |          15.9375 |          -1.3488 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0003 |          14.2797 |          -1.1814 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0026 |          13.7844 |          -1.3377 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0024 |          13.6717 |          -1.1965 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0041 |          12.9410 |          -1.2815 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0050 |          12.5844 |          -1.2906 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0005 |          12.6144 |          -1.3624 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0054 |          12.0764 |          -1.2296 |
[32m[20221213 18:58:47 @agent_ppo2.py:185][0m |          -0.0030 |          11.9198 |          -1.2461 |
[32m[20221213 18:58:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:58:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.80
[32m[20221213 18:58:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.00
[32m[20221213 18:58:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:58:47 @agent_ppo2.py:143][0m Total time:      10.25 min
[32m[20221213 18:58:47 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221213 18:58:47 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 18:58:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0074 |          19.7499 |          -1.6335 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0036 |          17.5694 |          -1.4805 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0052 |          17.0285 |          -1.3063 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0054 |          16.9210 |          -1.1413 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |           0.0010 |          16.8084 |          -1.1059 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0093 |          16.4696 |          -1.0282 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0072 |          16.3933 |          -0.9550 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0070 |          16.1564 |          -0.8048 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0055 |          16.0324 |          -0.8197 |
[32m[20221213 18:58:48 @agent_ppo2.py:185][0m |          -0.0058 |          15.8337 |          -0.6682 |
[32m[20221213 18:58:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.20
[32m[20221213 18:58:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.00
[32m[20221213 18:58:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.00
[32m[20221213 18:58:49 @agent_ppo2.py:143][0m Total time:      10.27 min
[32m[20221213 18:58:49 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 18:58:49 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 18:58:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |           0.0026 |           4.5765 |           0.1271 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |          -0.0029 |           2.8544 |           0.3333 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |           0.0017 |           2.8266 |           0.4038 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |          -0.0028 |           2.8148 |           0.4162 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |          -0.0014 |           2.8078 |           0.4305 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |          -0.0011 |           2.8099 |           0.3987 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |           0.0090 |           2.8443 |           0.4072 |
[32m[20221213 18:58:49 @agent_ppo2.py:185][0m |          -0.0002 |           2.7797 |           0.5521 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |          -0.0013 |           2.7706 |           0.5688 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |           0.0006 |           2.7739 |           0.5327 |
[32m[20221213 18:58:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:58:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.00
[32m[20221213 18:58:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:58:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 18:58:50 @agent_ppo2.py:143][0m Total time:      10.29 min
[32m[20221213 18:58:50 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221213 18:58:50 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 18:58:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |           0.0015 |          16.9635 |           0.4710 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |          -0.0005 |          10.0193 |           0.4191 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |           0.0017 |           9.4361 |           0.5966 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |           0.0058 |           9.1822 |           0.5902 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |          -0.0077 |           9.0560 |           0.5839 |
[32m[20221213 18:58:50 @agent_ppo2.py:185][0m |          -0.0007 |           8.8831 |           0.6179 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0023 |           8.7864 |           0.7048 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0038 |           8.7483 |           0.6880 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |           0.0014 |           8.7057 |           0.7018 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0050 |           8.6634 |           0.7658 |
[32m[20221213 18:58:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:58:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.60
[32m[20221213 18:58:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 18:58:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 18:58:51 @agent_ppo2.py:143][0m Total time:      10.31 min
[32m[20221213 18:58:51 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 18:58:51 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 18:58:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |           0.0000 |          78.1362 |           0.3558 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0021 |          72.5356 |           0.1592 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0007 |          71.7421 |          -0.1098 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0009 |          70.7814 |          -0.2604 |
[32m[20221213 18:58:51 @agent_ppo2.py:185][0m |          -0.0019 |          70.3067 |          -0.5037 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |          -0.0002 |          70.3039 |          -0.6536 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |          -0.0043 |          69.4994 |          -0.7679 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |          -0.0022 |          69.4696 |          -0.7629 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |          -0.0044 |          69.6024 |          -0.9410 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |           0.0016 |          68.9827 |          -0.9885 |
[32m[20221213 18:58:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.20
[32m[20221213 18:58:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 18:58:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:58:52 @agent_ppo2.py:143][0m Total time:      10.33 min
[32m[20221213 18:58:52 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221213 18:58:52 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 18:58:52 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:58:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |           0.0021 |           2.6222 |          -1.5899 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |           0.0076 |           2.3930 |          -1.3581 |
[32m[20221213 18:58:52 @agent_ppo2.py:185][0m |           0.0020 |           2.3388 |          -1.1046 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0029 |           2.2910 |          -0.9720 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0080 |           2.2652 |          -0.9116 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0053 |           2.2511 |          -0.6303 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0014 |           2.2375 |          -0.6585 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0046 |           2.2343 |          -0.4241 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0039 |           2.2194 |          -0.3961 |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0043 |           2.2155 |          -0.3287 |
[32m[20221213 18:58:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.40
[32m[20221213 18:58:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 18:58:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 18:58:53 @agent_ppo2.py:143][0m Total time:      10.35 min
[32m[20221213 18:58:53 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 18:58:53 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 18:58:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:53 @agent_ppo2.py:185][0m |          -0.0017 |         162.0632 |          -0.2127 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |           0.0087 |         169.1440 |          -0.0890 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |           0.0115 |         171.8079 |          -0.1250 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |           0.0119 |         180.3632 |           0.3244 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |           0.0020 |         160.3500 |           0.3268 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |          -0.0006 |         159.5720 |           0.2440 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |          -0.0013 |         159.2244 |           0.3183 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |          -0.0011 |         159.2584 |           0.2821 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |          -0.0028 |         159.7457 |           0.4711 |
[32m[20221213 18:58:54 @agent_ppo2.py:185][0m |          -0.0020 |         159.1416 |           0.3686 |
[32m[20221213 18:58:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:58:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:58:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.00
[32m[20221213 18:58:54 @agent_ppo2.py:143][0m Total time:      10.36 min
[32m[20221213 18:58:54 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221213 18:58:54 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 18:58:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |           0.0038 |          13.2002 |           1.2427 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |           0.0003 |          10.3127 |           1.2907 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |           0.0020 |           9.9751 |           1.3748 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |          -0.0043 |           9.7925 |           1.3867 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |          -0.0074 |           9.6621 |           1.5493 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |           0.0029 |           9.8706 |           1.5928 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |          -0.0048 |           9.5578 |           1.7315 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |          -0.0096 |           9.5589 |           1.8505 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |           0.0003 |           9.5518 |           1.8925 |
[32m[20221213 18:58:55 @agent_ppo2.py:185][0m |          -0.0086 |           9.4938 |           1.9709 |
[32m[20221213 18:58:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.60
[32m[20221213 18:58:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 18:58:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 18:58:55 @agent_ppo2.py:143][0m Total time:      10.38 min
[32m[20221213 18:58:55 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 18:58:55 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 18:58:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 18:58:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |          -0.0005 |           9.1091 |           1.9724 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |           0.0088 |           8.5280 |           1.8807 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |          -0.0036 |           8.2110 |           2.0916 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |           0.0014 |           8.1694 |           2.1421 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |           0.0029 |           8.2408 |           2.1570 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |          -0.0038 |           8.1409 |           2.1859 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |           0.0112 |           8.9512 |           2.1287 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |          -0.0030 |           8.1735 |           2.1726 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |          -0.0056 |           8.1721 |           2.2180 |
[32m[20221213 18:58:56 @agent_ppo2.py:185][0m |          -0.0052 |           8.0911 |           2.2321 |
[32m[20221213 18:58:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.20
[32m[20221213 18:58:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:58:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 18:58:57 @agent_ppo2.py:143][0m Total time:      10.40 min
[32m[20221213 18:58:57 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221213 18:58:57 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 18:58:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |           0.0002 |          76.6122 |           1.9919 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |           0.0036 |          73.2610 |           1.7571 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |          -0.0035 |          72.5789 |           1.5722 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |          -0.0052 |          72.4504 |           1.4542 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |          -0.0001 |          71.7573 |           1.2875 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |           0.0016 |          71.5221 |           1.2123 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |          -0.0011 |          71.0336 |           1.2028 |
[32m[20221213 18:58:57 @agent_ppo2.py:185][0m |          -0.0037 |          71.0042 |           1.1006 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0025 |          70.8404 |           1.1040 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0024 |          71.0370 |           1.0601 |
[32m[20221213 18:58:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:58:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.00
[32m[20221213 18:58:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.00
[32m[20221213 18:58:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 18:58:58 @agent_ppo2.py:143][0m Total time:      10.42 min
[32m[20221213 18:58:58 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 18:58:58 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 18:58:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |           0.0009 |         153.4920 |           0.8413 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0007 |         147.8277 |           0.8692 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0019 |         146.7872 |           0.9668 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |           0.0005 |         146.2142 |           0.9929 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0024 |         146.8353 |           0.9149 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0023 |         145.8365 |           0.8860 |
[32m[20221213 18:58:58 @agent_ppo2.py:185][0m |          -0.0011 |         145.4809 |           0.9874 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |           0.0031 |         145.6729 |           0.8127 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |          -0.0028 |         145.2340 |           0.7643 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |          -0.0056 |         145.3099 |           0.6414 |
[32m[20221213 18:58:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:58:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.20
[32m[20221213 18:58:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 18:58:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 18:58:59 @agent_ppo2.py:143][0m Total time:      10.44 min
[32m[20221213 18:58:59 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221213 18:58:59 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 18:58:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:58:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |           0.0013 |          52.2377 |           0.8604 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |          -0.0050 |          43.8372 |           0.6458 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |          -0.0051 |          42.1017 |           0.6024 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |          -0.0073 |          41.0054 |           0.4343 |
[32m[20221213 18:58:59 @agent_ppo2.py:185][0m |          -0.0074 |          40.4529 |           0.4369 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |          -0.0011 |          39.7137 |           0.1953 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |           0.0012 |          39.2290 |           0.0864 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |          -0.0065 |          38.7961 |           0.0659 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |          -0.0022 |          38.5540 |          -0.0912 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |          -0.0004 |          38.4814 |          -0.1524 |
[32m[20221213 18:59:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:59:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 156.00
[32m[20221213 18:59:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.00
[32m[20221213 18:59:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:59:00 @agent_ppo2.py:143][0m Total time:      10.46 min
[32m[20221213 18:59:00 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 18:59:00 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 18:59:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |           0.0000 |          98.1122 |          -0.7026 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |           0.0000 |          91.9044 |          -0.5277 |
[32m[20221213 18:59:00 @agent_ppo2.py:185][0m |           0.0007 |          90.2115 |          -0.5783 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |          -0.0007 |          89.6849 |          -0.5647 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |          -0.0055 |          89.0970 |          -0.6844 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |          -0.0022 |          88.4803 |          -0.5779 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |           0.0050 |          88.8940 |          -0.6017 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |           0.0038 |          92.9610 |          -0.7852 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |          -0.0028 |          87.5514 |          -0.6934 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |           0.0024 |          87.7414 |          -0.6309 |
[32m[20221213 18:59:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:59:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.80
[32m[20221213 18:59:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 18:59:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 18:59:01 @agent_ppo2.py:143][0m Total time:      10.48 min
[32m[20221213 18:59:01 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221213 18:59:01 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 18:59:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |           0.0052 |           7.4468 |          -0.3418 |
[32m[20221213 18:59:01 @agent_ppo2.py:185][0m |          -0.0026 |           4.2655 |          -0.2836 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0013 |           3.7816 |           0.0006 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0118 |           3.6170 |          -0.0076 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0030 |           3.6570 |           0.0045 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0013 |           3.5305 |           0.1264 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0059 |           3.4391 |           0.1272 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |           0.0007 |           3.4072 |           0.0923 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0055 |           3.3885 |           0.2194 |
[32m[20221213 18:59:02 @agent_ppo2.py:185][0m |          -0.0070 |           3.3399 |           0.1794 |
[32m[20221213 18:59:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.20
[32m[20221213 18:59:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 18:59:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:59:02 @agent_ppo2.py:143][0m Total time:      10.50 min
[32m[20221213 18:59:02 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 18:59:02 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 18:59:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |           0.0131 |          97.5085 |           0.1177 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |           0.0175 |          87.4540 |           0.1229 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |           0.0055 |          76.1059 |           0.1442 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |          -0.0001 |          75.4670 |          -0.0096 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |          -0.0008 |          75.4106 |           0.0370 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |          -0.0066 |          75.1875 |          -0.1106 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |          -0.0004 |          75.1077 |          -0.0254 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |           0.0005 |          74.9785 |          -0.0341 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |          -0.0017 |          74.9045 |          -0.1462 |
[32m[20221213 18:59:03 @agent_ppo2.py:185][0m |          -0.0048 |          75.1120 |          -0.1340 |
[32m[20221213 18:59:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.80
[32m[20221213 18:59:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 18:59:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 18:59:03 @agent_ppo2.py:143][0m Total time:      10.52 min
[32m[20221213 18:59:03 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221213 18:59:03 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 18:59:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0038 |         196.0955 |          -0.1355 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0074 |         191.0568 |          -0.3515 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0041 |         189.3064 |          -0.2055 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |          -0.0038 |         189.3374 |          -0.4405 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |          -0.0026 |         188.9789 |          -0.5463 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0021 |         189.3612 |          -0.6423 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0096 |         192.7317 |          -0.7893 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0094 |         203.9646 |          -0.8177 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |           0.0063 |         191.5590 |          -0.9429 |
[32m[20221213 18:59:04 @agent_ppo2.py:185][0m |          -0.0025 |         188.7307 |          -0.8886 |
[32m[20221213 18:59:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.80
[32m[20221213 18:59:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.00
[32m[20221213 18:59:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 18:59:05 @agent_ppo2.py:143][0m Total time:      10.54 min
[32m[20221213 18:59:05 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 18:59:05 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 18:59:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |           0.0034 |          12.1773 |          -0.8731 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |           0.0075 |           6.1723 |          -0.9024 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |          -0.0029 |           5.6488 |          -1.0287 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |           0.0082 |           5.3768 |          -1.0707 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |           0.0091 |           5.4837 |          -0.9942 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |           0.0038 |           5.2428 |          -1.0942 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |          -0.0017 |           4.9666 |          -1.2010 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |          -0.0047 |           4.8448 |          -1.1804 |
[32m[20221213 18:59:05 @agent_ppo2.py:185][0m |          -0.0087 |           4.7846 |          -1.2426 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |          -0.0090 |           4.7702 |          -1.0710 |
[32m[20221213 18:59:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.80
[32m[20221213 18:59:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 18:59:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 18:59:06 @agent_ppo2.py:143][0m Total time:      10.55 min
[32m[20221213 18:59:06 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221213 18:59:06 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 18:59:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |           0.0051 |          17.6003 |          -1.2919 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |           0.0042 |          10.0232 |          -1.1218 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |           0.0039 |           9.1281 |          -1.0659 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |           0.0033 |           8.9568 |          -1.0507 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |           0.0002 |           9.2359 |          -0.8732 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |          -0.0058 |           8.8619 |          -0.8643 |
[32m[20221213 18:59:06 @agent_ppo2.py:185][0m |          -0.0059 |           8.7778 |          -0.7610 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0013 |           8.7138 |          -0.7993 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0023 |           8.7345 |          -0.6402 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0120 |           8.7294 |          -0.6210 |
[32m[20221213 18:59:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:59:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.80
[32m[20221213 18:59:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 18:59:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 18:59:07 @agent_ppo2.py:143][0m Total time:      10.57 min
[32m[20221213 18:59:07 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 18:59:07 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 18:59:07 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0007 |          12.0014 |          -0.3657 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0001 |           8.8643 |          -0.4915 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |           0.0008 |           8.7143 |          -0.4640 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0019 |           8.2667 |          -0.4892 |
[32m[20221213 18:59:07 @agent_ppo2.py:185][0m |          -0.0090 |           8.0893 |          -0.4360 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0074 |           7.9655 |          -0.4537 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0054 |           7.9576 |          -0.4978 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0080 |           7.8996 |          -0.5111 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |           0.0042 |           8.0223 |          -0.5501 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0043 |           7.8526 |          -0.4145 |
[32m[20221213 18:59:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.80
[32m[20221213 18:59:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 18:59:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 18:59:08 @agent_ppo2.py:143][0m Total time:      10.59 min
[32m[20221213 18:59:08 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221213 18:59:08 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 18:59:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0030 |           8.7795 |          -0.9401 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |           0.0014 |           7.1005 |          -1.0018 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0077 |           6.9568 |          -1.1322 |
[32m[20221213 18:59:08 @agent_ppo2.py:185][0m |          -0.0028 |           6.8949 |          -1.3010 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |          -0.0078 |           6.8562 |          -1.2904 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |          -0.0033 |           6.8459 |          -1.3161 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |          -0.0045 |           6.8284 |          -1.3608 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |          -0.0068 |           6.8354 |          -1.3555 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |          -0.0066 |           6.8056 |          -1.4976 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |          -0.0024 |           6.7999 |          -1.4538 |
[32m[20221213 18:59:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.20
[32m[20221213 18:59:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 18:59:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 18:59:09 @agent_ppo2.py:143][0m Total time:      10.61 min
[32m[20221213 18:59:09 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 18:59:09 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 18:59:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |           0.0003 |           8.3724 |          -1.6137 |
[32m[20221213 18:59:09 @agent_ppo2.py:185][0m |           0.0060 |           6.9212 |          -1.6322 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0034 |           6.7156 |          -1.5013 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0005 |           6.6842 |          -1.3997 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0030 |           6.5333 |          -1.4533 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0081 |           6.5028 |          -1.3743 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0034 |           6.4413 |          -1.4035 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0049 |           6.4292 |          -1.3899 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0022 |           6.4687 |          -1.3855 |
[32m[20221213 18:59:10 @agent_ppo2.py:185][0m |          -0.0071 |           6.3937 |          -1.1999 |
[32m[20221213 18:59:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:59:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.80
[32m[20221213 18:59:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 18:59:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 18:59:10 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221213 18:59:10 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221213 18:59:10 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 18:59:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |           0.0090 |           5.8959 |          -1.5241 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |           0.0016 |           3.9384 |          -1.4420 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0009 |           3.7084 |          -1.5138 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0040 |           3.6358 |          -1.6594 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0041 |           3.6037 |          -1.8535 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0012 |           3.5609 |          -1.8630 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |           0.0015 |           3.5377 |          -1.9358 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0018 |           3.5215 |          -1.8798 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0038 |           3.5130 |          -2.0119 |
[32m[20221213 18:59:11 @agent_ppo2.py:185][0m |          -0.0008 |           3.5147 |          -1.8813 |
[32m[20221213 18:59:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.20
[32m[20221213 18:59:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:59:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:59:11 @agent_ppo2.py:143][0m Total time:      10.65 min
[32m[20221213 18:59:11 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 18:59:11 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 18:59:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0077 |         150.2159 |          -2.2960 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0038 |         145.5584 |          -2.3136 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0057 |         145.3208 |          -2.3823 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0078 |         145.1409 |          -2.3885 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0067 |         144.3863 |          -2.4340 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0066 |         145.1097 |          -2.5278 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0060 |         144.4472 |          -2.7183 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0061 |         144.7635 |          -2.7566 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0043 |         144.7282 |          -2.7292 |
[32m[20221213 18:59:12 @agent_ppo2.py:185][0m |          -0.0025 |         144.3287 |          -2.7295 |
[32m[20221213 18:59:12 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.60
[32m[20221213 18:59:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.00
[32m[20221213 18:59:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 18:59:13 @agent_ppo2.py:143][0m Total time:      10.67 min
[32m[20221213 18:59:13 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221213 18:59:13 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 18:59:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |           0.0013 |         131.3778 |          -2.7354 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |          -0.0021 |         126.5841 |          -2.4406 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |           0.0004 |         125.9512 |          -2.4081 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |          -0.0077 |         125.6173 |          -2.3106 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |           0.0011 |         125.6437 |          -2.2557 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |          -0.0056 |         124.7973 |          -2.1306 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |          -0.0046 |         125.5737 |          -2.0479 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |          -0.0073 |         124.7261 |          -1.9311 |
[32m[20221213 18:59:13 @agent_ppo2.py:185][0m |          -0.0034 |         124.4091 |          -1.9203 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |          -0.0059 |         124.2190 |          -2.0117 |
[32m[20221213 18:59:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.80
[32m[20221213 18:59:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 658.00
[32m[20221213 18:59:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.00
[32m[20221213 18:59:14 @agent_ppo2.py:143][0m Total time:      10.69 min
[32m[20221213 18:59:14 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 18:59:14 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 18:59:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |           0.0007 |         163.2919 |          -1.8820 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |           0.0087 |         179.3891 |          -2.0676 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |          -0.0024 |         158.2723 |          -1.9158 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |          -0.0009 |         158.0108 |          -2.0530 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |          -0.0048 |         158.2954 |          -2.2973 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |          -0.0028 |         157.6634 |          -2.3713 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |          -0.0047 |         157.8958 |          -2.3977 |
[32m[20221213 18:59:14 @agent_ppo2.py:185][0m |           0.0039 |         164.8414 |          -2.2977 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |          -0.0005 |         161.8319 |          -2.4479 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |          -0.0035 |         158.1937 |          -2.5572 |
[32m[20221213 18:59:15 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.80
[32m[20221213 18:59:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.00
[32m[20221213 18:59:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 18:59:15 @agent_ppo2.py:143][0m Total time:      10.71 min
[32m[20221213 18:59:15 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221213 18:59:15 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 18:59:15 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |           0.0061 |           7.8029 |          -3.1270 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |          -0.0021 |           5.1793 |          -3.2680 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |          -0.0045 |           4.9029 |          -3.4586 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |          -0.0014 |           4.7457 |          -3.4462 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |           0.0043 |           4.6395 |          -3.6833 |
[32m[20221213 18:59:15 @agent_ppo2.py:185][0m |           0.0006 |           4.6918 |          -3.6141 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |           0.0022 |           4.5973 |          -3.7796 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |          -0.0010 |           4.4964 |          -3.8522 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |          -0.0014 |           4.5898 |          -3.8801 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |          -0.0016 |           4.4208 |          -3.8872 |
[32m[20221213 18:59:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.80
[32m[20221213 18:59:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 18:59:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 665.00
[32m[20221213 18:59:16 @agent_ppo2.py:143][0m Total time:      10.72 min
[32m[20221213 18:59:16 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 18:59:16 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 18:59:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:59:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |           0.0101 |           7.4313 |          -3.7994 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |          -0.0061 |           5.0075 |          -3.6853 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |           0.0076 |           4.8701 |          -3.7109 |
[32m[20221213 18:59:16 @agent_ppo2.py:185][0m |           0.0008 |           4.8635 |          -3.5195 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |          -0.0029 |           4.8098 |          -3.6425 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |          -0.0030 |           4.7675 |          -3.6773 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |          -0.0026 |           4.8257 |          -3.6133 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |          -0.0058 |           4.7285 |          -3.5138 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |           0.0027 |           4.7358 |          -3.5814 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |           0.0031 |           4.8769 |          -3.5702 |
[32m[20221213 18:59:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:59:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.80
[32m[20221213 18:59:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 18:59:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.00
[32m[20221213 18:59:17 @agent_ppo2.py:143][0m Total time:      10.74 min
[32m[20221213 18:59:17 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221213 18:59:17 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 18:59:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |           0.0027 |           5.6117 |          -3.8718 |
[32m[20221213 18:59:17 @agent_ppo2.py:185][0m |           0.0078 |           4.5405 |          -3.7883 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |           0.0061 |           4.5255 |          -3.4801 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |          -0.0044 |           4.3885 |          -3.5234 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |          -0.0026 |           4.3674 |          -3.4189 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |           0.0002 |           4.3387 |          -3.4357 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |          -0.0022 |           4.3345 |          -3.3022 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |          -0.0078 |           4.3159 |          -3.1998 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |          -0.0030 |           4.3079 |          -3.1716 |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |          -0.0068 |           4.2904 |          -3.1197 |
[32m[20221213 18:59:18 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.40
[32m[20221213 18:59:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 18:59:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.00
[32m[20221213 18:59:18 @agent_ppo2.py:143][0m Total time:      10.76 min
[32m[20221213 18:59:18 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 18:59:18 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 18:59:18 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:18 @agent_ppo2.py:185][0m |           0.0042 |           6.6678 |          -3.0848 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0034 |           5.4425 |          -2.9954 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0067 |           5.2352 |          -2.8258 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0101 |           5.3074 |          -2.8118 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0049 |           5.1135 |          -2.5055 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0032 |           5.0404 |          -2.5183 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0110 |           4.9789 |          -2.4269 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |           0.0085 |           4.9616 |          -2.4078 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0042 |           4.9209 |          -2.5000 |
[32m[20221213 18:59:19 @agent_ppo2.py:185][0m |          -0.0080 |           4.8810 |          -2.3638 |
[32m[20221213 18:59:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.60
[32m[20221213 18:59:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 18:59:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 18:59:19 @agent_ppo2.py:143][0m Total time:      10.78 min
[32m[20221213 18:59:19 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221213 18:59:19 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 18:59:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0048 |           4.9062 |          -2.1080 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0062 |           3.7180 |          -2.1368 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0000 |           3.6330 |          -2.0709 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0004 |           3.6168 |          -2.1552 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |          -0.0000 |           3.5874 |          -2.2937 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0026 |           3.5764 |          -2.2390 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0048 |           3.5678 |          -2.2902 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0119 |           3.7240 |          -2.4175 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |           0.0017 |           3.5581 |          -2.4903 |
[32m[20221213 18:59:20 @agent_ppo2.py:185][0m |          -0.0109 |           3.5384 |          -2.7774 |
[32m[20221213 18:59:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.40
[32m[20221213 18:59:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 18:59:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 18:59:20 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221213 18:59:20 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 18:59:20 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 18:59:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |           0.0018 |           4.7393 |          -3.4297 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0063 |           4.1043 |          -3.2842 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0083 |           3.9883 |          -3.2386 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0021 |           3.9495 |          -3.1800 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0068 |           3.9216 |          -3.3941 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |           0.0088 |           3.9062 |          -3.3852 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |           0.0016 |           4.0215 |          -3.4387 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0077 |           3.8650 |          -3.2922 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0081 |           3.8751 |          -3.3652 |
[32m[20221213 18:59:21 @agent_ppo2.py:185][0m |          -0.0074 |           3.8561 |          -3.2974 |
[32m[20221213 18:59:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.40
[32m[20221213 18:59:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 18:59:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 649.00
[32m[20221213 18:59:22 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221213 18:59:22 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221213 18:59:22 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 18:59:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0005 |           3.6723 |          -3.1729 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |           0.0025 |           3.5122 |          -3.2422 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0033 |           3.4988 |          -3.3191 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0069 |           3.4938 |          -3.4698 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0036 |           3.4842 |          -3.6109 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0067 |           3.4836 |          -3.6089 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0018 |           3.4708 |          -3.5866 |
[32m[20221213 18:59:22 @agent_ppo2.py:185][0m |          -0.0071 |           3.4700 |          -3.6129 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |          -0.0004 |           3.4729 |          -3.6398 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |           0.0020 |           3.4669 |          -3.6696 |
[32m[20221213 18:59:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.40
[32m[20221213 18:59:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 18:59:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 18:59:23 @agent_ppo2.py:143][0m Total time:      10.84 min
[32m[20221213 18:59:23 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 18:59:23 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 18:59:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |          -0.0008 |          35.9018 |          -4.3546 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |          -0.0028 |          32.8413 |          -4.5177 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |          -0.0016 |          32.5300 |          -4.1796 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |           0.0010 |          32.2606 |          -4.2733 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |           0.0015 |          32.4580 |          -4.1500 |
[32m[20221213 18:59:23 @agent_ppo2.py:185][0m |           0.0031 |          32.2752 |          -3.9809 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |           0.0025 |          32.1330 |          -3.7650 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |          -0.0035 |          32.0105 |          -3.8964 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |          -0.0019 |          32.0728 |          -3.8378 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |          -0.0007 |          31.9209 |          -3.8541 |
[32m[20221213 18:59:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:59:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.00
[32m[20221213 18:59:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.00
[32m[20221213 18:59:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 18:59:24 @agent_ppo2.py:143][0m Total time:      10.86 min
[32m[20221213 18:59:24 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221213 18:59:24 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 18:59:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |           0.0046 |         160.9528 |          -3.4833 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |          -0.0037 |         152.5719 |          -3.5300 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |           0.0021 |         151.9004 |          -3.5613 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |          -0.0032 |         151.7631 |          -3.4523 |
[32m[20221213 18:59:24 @agent_ppo2.py:185][0m |          -0.0031 |         151.5893 |          -3.3980 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |          -0.0034 |         151.4892 |          -3.4656 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |           0.0002 |         151.1971 |          -3.3880 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |          -0.0000 |         151.7161 |          -3.4960 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |          -0.0006 |         151.3534 |          -3.6028 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |          -0.0014 |         151.1914 |          -3.3825 |
[32m[20221213 18:59:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:59:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.60
[32m[20221213 18:59:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 888.00
[32m[20221213 18:59:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.00
[32m[20221213 18:59:25 @agent_ppo2.py:143][0m Total time:      10.88 min
[32m[20221213 18:59:25 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 18:59:25 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 18:59:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |           0.0004 |         173.0092 |          -3.5704 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |          -0.0001 |         171.7285 |          -3.3946 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |          -0.0012 |         172.1498 |          -3.3030 |
[32m[20221213 18:59:25 @agent_ppo2.py:185][0m |           0.0043 |         174.9726 |          -3.2621 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |          -0.0010 |         171.8289 |          -3.1352 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |           0.0024 |         173.5275 |          -3.1922 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |          -0.0019 |         171.7156 |          -3.0712 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |          -0.0010 |         171.7349 |          -3.0149 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |           0.0007 |         172.7127 |          -3.1577 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |           0.0017 |         172.3353 |          -3.0909 |
[32m[20221213 18:59:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 18:59:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 18:59:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 18:59:26 @agent_ppo2.py:143][0m Total time:      10.89 min
[32m[20221213 18:59:26 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221213 18:59:26 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 18:59:26 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |           0.0020 |          49.1689 |          -3.3762 |
[32m[20221213 18:59:26 @agent_ppo2.py:185][0m |           0.0014 |          44.4437 |          -3.3878 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |          -0.0011 |          43.3908 |          -3.2357 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |           0.0034 |          44.3064 |          -3.2302 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |           0.0016 |          42.0002 |          -3.1732 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |          -0.0031 |          41.5884 |          -3.2531 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |          -0.0004 |          41.3053 |          -3.3094 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |           0.0010 |          41.1986 |          -3.2426 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |          -0.0034 |          40.9885 |          -3.2577 |
[32m[20221213 18:59:27 @agent_ppo2.py:185][0m |          -0.0010 |          40.9520 |          -3.0671 |
[32m[20221213 18:59:27 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 159.80
[32m[20221213 18:59:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 620.00
[32m[20221213 18:59:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.00
[32m[20221213 18:59:27 @agent_ppo2.py:143][0m Total time:      10.91 min
[32m[20221213 18:59:27 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 18:59:27 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 18:59:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |           0.0047 |           3.8151 |          -3.2452 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0063 |           2.8350 |          -3.0392 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0019 |           2.7078 |          -3.1377 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0070 |           2.6679 |          -3.0809 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0047 |           2.6387 |          -3.0555 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0065 |           2.6261 |          -3.0649 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0061 |           2.6145 |          -3.1668 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0021 |           2.6147 |          -3.2143 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |          -0.0061 |           2.5938 |          -3.2760 |
[32m[20221213 18:59:28 @agent_ppo2.py:185][0m |           0.0020 |           2.6773 |          -3.2800 |
[32m[20221213 18:59:28 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.20
[32m[20221213 18:59:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 67.00
[32m[20221213 18:59:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 18:59:28 @agent_ppo2.py:143][0m Total time:      10.93 min
[32m[20221213 18:59:28 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221213 18:59:28 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 18:59:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |           0.0015 |         185.8039 |          -3.4761 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |          -0.0010 |         181.2053 |          -3.2695 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |          -0.0010 |         181.0601 |          -3.0550 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |           0.0158 |         207.1785 |          -3.1064 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |           0.0011 |         180.6486 |          -2.7359 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |          -0.0017 |         180.5692 |          -2.8890 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |          -0.0013 |         180.7710 |          -2.7336 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |          -0.0031 |         180.6141 |          -2.6195 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |           0.0015 |         180.6626 |          -2.6190 |
[32m[20221213 18:59:29 @agent_ppo2.py:185][0m |          -0.0007 |         180.3542 |          -2.5962 |
[32m[20221213 18:59:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.60
[32m[20221213 18:59:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.00
[32m[20221213 18:59:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.00
[32m[20221213 18:59:30 @agent_ppo2.py:143][0m Total time:      10.95 min
[32m[20221213 18:59:30 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 18:59:30 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 18:59:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |           0.0024 |         189.5662 |          -2.5856 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |           0.0001 |         185.7990 |          -2.8612 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |           0.0134 |         202.7124 |          -2.8413 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |           0.0048 |         189.2009 |          -2.8277 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |           0.0008 |         185.2207 |          -3.2236 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |           0.0011 |         184.9032 |          -3.4344 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |          -0.0000 |         184.9649 |          -3.2496 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |          -0.0045 |         184.8754 |          -3.6008 |
[32m[20221213 18:59:30 @agent_ppo2.py:185][0m |          -0.0022 |         184.6634 |          -3.8279 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |          -0.0022 |         184.5740 |          -3.9898 |
[32m[20221213 18:59:31 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 876.20
[32m[20221213 18:59:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 882.00
[32m[20221213 18:59:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 18:59:31 @agent_ppo2.py:143][0m Total time:      10.97 min
[32m[20221213 18:59:31 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221213 18:59:31 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 18:59:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |           0.0032 |          15.2316 |          -3.9016 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |           0.0021 |           6.3988 |          -3.7033 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |           0.0002 |           6.2387 |          -3.5772 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |          -0.0078 |           6.1800 |          -3.3712 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |           0.0003 |           6.1350 |          -3.3103 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |          -0.0025 |           6.3652 |          -3.2323 |
[32m[20221213 18:59:31 @agent_ppo2.py:185][0m |           0.0035 |           6.2556 |          -3.0079 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |          -0.0050 |           6.0742 |          -3.1541 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |           0.0020 |           6.0406 |          -3.2213 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |          -0.0004 |           6.3678 |          -3.1916 |
[32m[20221213 18:59:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.80
[32m[20221213 18:59:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.00
[32m[20221213 18:59:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 18:59:32 @agent_ppo2.py:143][0m Total time:      10.99 min
[32m[20221213 18:59:32 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 18:59:32 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 18:59:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |           0.0009 |         117.1973 |          -4.0188 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |          -0.0067 |         101.8515 |          -3.9763 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |           0.0112 |         118.9935 |          -3.9590 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |           0.0002 |          98.2739 |          -3.8913 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |           0.0063 |          97.5693 |          -3.8284 |
[32m[20221213 18:59:32 @agent_ppo2.py:185][0m |          -0.0045 |          96.4851 |          -3.8436 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |          -0.0130 |          95.7036 |          -3.8373 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |          -0.0054 |          95.7837 |          -3.9212 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |          -0.0001 |          94.8448 |          -4.0420 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |           0.0013 |          94.7513 |          -4.0096 |
[32m[20221213 18:59:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.40
[32m[20221213 18:59:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 18:59:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.00
[32m[20221213 18:59:33 @agent_ppo2.py:143][0m Total time:      11.01 min
[32m[20221213 18:59:33 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221213 18:59:33 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 18:59:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |           0.0006 |         201.4137 |          -3.7099 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |           0.0067 |         207.8141 |          -3.6680 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |           0.0017 |         197.1209 |          -3.8126 |
[32m[20221213 18:59:33 @agent_ppo2.py:185][0m |          -0.0003 |         196.9010 |          -4.0206 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |          -0.0026 |         196.6923 |          -4.1136 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |          -0.0011 |         196.5368 |          -4.0215 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |          -0.0039 |         196.3517 |          -4.1431 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |          -0.0042 |         196.0716 |          -4.1046 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |           0.0060 |         204.1601 |          -4.3599 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |          -0.0007 |         195.9725 |          -4.2367 |
[32m[20221213 18:59:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.80
[32m[20221213 18:59:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 892.00
[32m[20221213 18:59:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 930.00
[32m[20221213 18:59:34 @agent_ppo2.py:143][0m Total time:      11.03 min
[32m[20221213 18:59:34 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 18:59:34 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 18:59:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |           0.0026 |          22.9680 |          -4.5420 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |           0.0027 |          12.7675 |          -4.2970 |
[32m[20221213 18:59:34 @agent_ppo2.py:185][0m |           0.0029 |          11.8339 |          -4.0046 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |           0.0000 |          11.3762 |          -3.9057 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |           0.0002 |          10.7570 |          -3.8423 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |          -0.0008 |          10.4779 |          -3.7515 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |           0.0010 |          10.6884 |          -3.7266 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |          -0.0011 |          10.1448 |          -3.6467 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |          -0.0039 |           9.9561 |          -3.4158 |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |          -0.0009 |           9.8200 |          -3.5793 |
[32m[20221213 18:59:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.40
[32m[20221213 18:59:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 18:59:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 18:59:35 @agent_ppo2.py:143][0m Total time:      11.05 min
[32m[20221213 18:59:35 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221213 18:59:35 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 18:59:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:35 @agent_ppo2.py:185][0m |          -0.0021 |         199.8325 |          -3.0127 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0037 |         196.1275 |          -2.9515 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0032 |         195.3634 |          -3.0576 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0030 |         195.2244 |          -3.1116 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0045 |         193.9948 |          -3.2201 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |           0.0047 |         199.9145 |          -3.1964 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0043 |         193.4818 |          -3.3248 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0018 |         193.2614 |          -3.3323 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0044 |         193.3974 |          -3.4031 |
[32m[20221213 18:59:36 @agent_ppo2.py:185][0m |          -0.0015 |         192.8995 |          -3.3055 |
[32m[20221213 18:59:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.80
[32m[20221213 18:59:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 18:59:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.00
[32m[20221213 18:59:36 @agent_ppo2.py:143][0m Total time:      11.06 min
[32m[20221213 18:59:36 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 18:59:36 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 18:59:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |           0.0031 |          21.7077 |          -3.7845 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0088 |           9.0250 |          -3.7406 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0004 |           8.2643 |          -3.6032 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0038 |           7.9547 |          -3.4407 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0039 |           7.7678 |          -3.4127 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0077 |           7.6856 |          -3.3270 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |           0.0034 |           7.5887 |          -3.3179 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0049 |           7.6298 |          -3.3496 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0063 |           7.5605 |          -3.1846 |
[32m[20221213 18:59:37 @agent_ppo2.py:185][0m |          -0.0058 |           7.5055 |          -3.0337 |
[32m[20221213 18:59:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.20
[32m[20221213 18:59:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 18:59:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.00
[32m[20221213 18:59:37 @agent_ppo2.py:143][0m Total time:      11.08 min
[32m[20221213 18:59:37 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221213 18:59:37 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 18:59:38 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0010 |         176.1168 |          -2.0398 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0039 |         172.6928 |          -2.0295 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0039 |         171.7283 |          -2.1199 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0040 |         171.4843 |          -2.1652 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0005 |         171.6684 |          -2.0727 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |           0.0154 |         198.0540 |          -1.9982 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |           0.0022 |         171.4302 |          -2.0027 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0011 |         170.8594 |          -2.0413 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0026 |         171.5449 |          -1.9455 |
[32m[20221213 18:59:38 @agent_ppo2.py:185][0m |          -0.0018 |         170.8768 |          -1.9567 |
[32m[20221213 18:59:38 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.20
[32m[20221213 18:59:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.00
[32m[20221213 18:59:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 18:59:39 @agent_ppo2.py:143][0m Total time:      11.10 min
[32m[20221213 18:59:39 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 18:59:39 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 18:59:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |           0.0040 |           9.1799 |          -2.0741 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |           0.0014 |           5.5991 |          -2.1456 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |           0.0004 |           5.4281 |          -2.1581 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |           0.0050 |           5.6305 |          -2.0903 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |           0.0097 |           5.7836 |          -1.9423 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |          -0.0015 |           5.3542 |          -1.8168 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |           0.0017 |           5.2353 |          -1.6639 |
[32m[20221213 18:59:39 @agent_ppo2.py:185][0m |          -0.0034 |           5.2188 |          -1.6430 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0011 |           5.1943 |          -1.5706 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0109 |           5.2064 |          -1.4875 |
[32m[20221213 18:59:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.00
[32m[20221213 18:59:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 18:59:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.00
[32m[20221213 18:59:40 @agent_ppo2.py:143][0m Total time:      11.12 min
[32m[20221213 18:59:40 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221213 18:59:40 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 18:59:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0009 |          55.1342 |          -2.0513 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0003 |          51.2540 |          -1.9353 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0054 |          50.6407 |          -1.8443 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0034 |          49.9533 |          -1.9856 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0038 |          49.7485 |          -1.7656 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |           0.0023 |          49.5246 |          -1.7493 |
[32m[20221213 18:59:40 @agent_ppo2.py:185][0m |          -0.0034 |          49.0010 |          -1.8441 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |          -0.0037 |          48.7269 |          -1.7183 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |          -0.0044 |          48.7283 |          -1.7558 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |          -0.0011 |          48.7090 |          -1.7880 |
[32m[20221213 18:59:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.60
[32m[20221213 18:59:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 647.00
[32m[20221213 18:59:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.00
[32m[20221213 18:59:41 @agent_ppo2.py:143][0m Total time:      11.14 min
[32m[20221213 18:59:41 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 18:59:41 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 18:59:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |          -0.0006 |          23.2267 |          -1.5162 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |           0.0051 |          17.8926 |          -1.2959 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |           0.0014 |          17.2032 |          -1.3086 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |          -0.0112 |          16.9992 |          -1.3547 |
[32m[20221213 18:59:41 @agent_ppo2.py:185][0m |          -0.0046 |          16.6890 |          -1.3310 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0037 |          16.2340 |          -1.2943 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0029 |          16.0505 |          -1.2125 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0082 |          15.8651 |          -1.1609 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0084 |          15.8785 |          -1.2905 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0057 |          15.6669 |          -1.3543 |
[32m[20221213 18:59:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.60
[32m[20221213 18:59:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 67.00
[32m[20221213 18:59:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 665.00
[32m[20221213 18:59:42 @agent_ppo2.py:143][0m Total time:      11.16 min
[32m[20221213 18:59:42 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221213 18:59:42 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 18:59:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |           0.0020 |         208.5065 |          -1.7957 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0018 |         203.9519 |          -1.7878 |
[32m[20221213 18:59:42 @agent_ppo2.py:185][0m |          -0.0021 |         203.2882 |          -1.8849 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |          -0.0041 |         203.0370 |          -1.7572 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |          -0.0033 |         203.2578 |          -1.9582 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |           0.0009 |         202.8758 |          -1.7583 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |          -0.0010 |         202.7676 |          -1.8735 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |           0.0066 |         217.4376 |          -1.9334 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |           0.0048 |         202.8923 |          -1.7174 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |           0.0135 |         235.3664 |          -1.8289 |
[32m[20221213 18:59:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.80
[32m[20221213 18:59:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.00
[32m[20221213 18:59:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.00
[32m[20221213 18:59:43 @agent_ppo2.py:143][0m Total time:      11.18 min
[32m[20221213 18:59:43 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 18:59:43 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 18:59:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |           0.0032 |         208.5710 |          -1.1362 |
[32m[20221213 18:59:43 @agent_ppo2.py:185][0m |          -0.0017 |         202.2403 |          -1.1987 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |          -0.0019 |         200.9495 |          -1.2339 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |           0.0081 |         210.7977 |          -1.4762 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |           0.0012 |         200.0744 |          -1.6501 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |           0.0116 |         226.7005 |          -1.9020 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |           0.0003 |         199.9076 |          -1.8252 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |           0.0003 |         199.8382 |          -2.0186 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |          -0.0008 |         199.5932 |          -2.0583 |
[32m[20221213 18:59:44 @agent_ppo2.py:185][0m |           0.0002 |         199.5964 |          -2.1527 |
[32m[20221213 18:59:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.00
[32m[20221213 18:59:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 932.00
[32m[20221213 18:59:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 933.00
[32m[20221213 18:59:44 @agent_ppo2.py:143][0m Total time:      11.20 min
[32m[20221213 18:59:44 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221213 18:59:44 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 18:59:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |           0.0030 |          26.2799 |          -2.8256 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |           0.0108 |          21.7621 |          -3.2124 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |          -0.0021 |          21.3461 |          -3.1879 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |          -0.0065 |          21.1387 |          -3.4307 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |           0.0012 |          22.5369 |          -3.4566 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |          -0.0019 |          21.0117 |          -3.4914 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |          -0.0015 |          20.9831 |          -3.4758 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |          -0.0023 |          20.8816 |          -3.5958 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |           0.0152 |          23.5265 |          -3.4545 |
[32m[20221213 18:59:45 @agent_ppo2.py:185][0m |           0.0021 |          20.9758 |          -3.4811 |
[32m[20221213 18:59:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.20
[32m[20221213 18:59:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:59:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 18:59:45 @agent_ppo2.py:143][0m Total time:      11.22 min
[32m[20221213 18:59:45 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 18:59:45 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 18:59:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |           0.0044 |          28.5772 |          -4.4393 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0058 |          23.8137 |          -4.1740 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0052 |          23.3265 |          -4.2765 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0066 |          22.9629 |          -4.0775 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0080 |          22.7853 |          -4.0851 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0028 |          22.5661 |          -3.8716 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0068 |          22.4357 |          -3.9105 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0056 |          22.2381 |          -4.0035 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0008 |          23.0607 |          -3.9662 |
[32m[20221213 18:59:46 @agent_ppo2.py:185][0m |          -0.0120 |          22.1064 |          -3.9465 |
[32m[20221213 18:59:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:59:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.80
[32m[20221213 18:59:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 18:59:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 18:59:47 @agent_ppo2.py:143][0m Total time:      11.24 min
[32m[20221213 18:59:47 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221213 18:59:47 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 18:59:47 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 18:59:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |           0.0060 |          16.8131 |          -3.2172 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0025 |          13.7879 |          -3.0662 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |           0.0115 |          14.7514 |          -2.9545 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0042 |          13.5489 |          -2.9393 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0072 |          13.4048 |          -2.8205 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0022 |          13.3606 |          -2.8205 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0010 |          13.3983 |          -2.7969 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0016 |          13.3127 |          -2.7843 |
[32m[20221213 18:59:47 @agent_ppo2.py:185][0m |          -0.0049 |          13.2525 |          -2.7130 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |          -0.0076 |          13.2188 |          -2.6099 |
[32m[20221213 18:59:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.00
[32m[20221213 18:59:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 44.00
[32m[20221213 18:59:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 18:59:48 @agent_ppo2.py:143][0m Total time:      11.25 min
[32m[20221213 18:59:48 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 18:59:48 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 18:59:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |           0.0028 |         196.2391 |          -2.5678 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |           0.0145 |         214.6098 |          -2.4832 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |          -0.0020 |         193.3573 |          -2.4390 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |          -0.0014 |         192.8660 |          -2.5749 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |           0.0001 |         192.7023 |          -2.4504 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |           0.0040 |         192.6797 |          -2.5426 |
[32m[20221213 18:59:48 @agent_ppo2.py:185][0m |          -0.0006 |         192.5804 |          -2.7093 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |          -0.0020 |         192.6700 |          -2.6110 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |          -0.0035 |         192.6139 |          -2.5386 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |           0.0006 |         192.4250 |          -2.6178 |
[32m[20221213 18:59:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221213 18:59:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 18:59:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 696.00
[32m[20221213 18:59:49 @agent_ppo2.py:143][0m Total time:      11.27 min
[32m[20221213 18:59:49 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221213 18:59:49 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 18:59:49 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |           0.0024 |         179.9261 |          -2.9834 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |           0.0042 |         176.5513 |          -2.9745 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |           0.0007 |         175.2843 |          -2.9454 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |           0.0016 |         174.2714 |          -2.7688 |
[32m[20221213 18:59:49 @agent_ppo2.py:185][0m |           0.0026 |         175.4088 |          -2.7044 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |           0.0059 |         184.3140 |          -2.6523 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |          -0.0028 |         173.2383 |          -2.5193 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |          -0.0039 |         173.4423 |          -2.4378 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |          -0.0077 |         173.2897 |          -2.6045 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |          -0.0016 |         172.8943 |          -2.6039 |
[32m[20221213 18:59:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:59:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.60
[32m[20221213 18:59:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 18:59:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 18:59:50 @agent_ppo2.py:143][0m Total time:      11.29 min
[32m[20221213 18:59:50 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 18:59:50 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 18:59:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |           0.0060 |          36.3155 |          -2.1514 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |           0.0078 |          11.3496 |          -2.1442 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |          -0.0001 |          10.4852 |          -1.9537 |
[32m[20221213 18:59:50 @agent_ppo2.py:185][0m |           0.0016 |          10.1164 |          -1.8173 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |           0.0088 |           9.8558 |          -1.6824 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |           0.0090 |           9.8933 |          -1.8200 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |          -0.0036 |           9.5708 |          -1.7492 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |           0.0003 |           9.5837 |          -1.7332 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |           0.0023 |           9.4494 |          -1.6276 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |          -0.0062 |           9.3480 |          -1.5158 |
[32m[20221213 18:59:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.80
[32m[20221213 18:59:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 18:59:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.00
[32m[20221213 18:59:51 @agent_ppo2.py:143][0m Total time:      11.31 min
[32m[20221213 18:59:51 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221213 18:59:51 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 18:59:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |           0.0021 |          12.8310 |          -1.3884 |
[32m[20221213 18:59:51 @agent_ppo2.py:185][0m |          -0.0006 |           7.8961 |          -1.5463 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0041 |           7.6614 |          -1.5937 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0121 |           7.6077 |          -1.7069 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0015 |           7.4975 |          -1.7668 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |           0.0031 |           7.4052 |          -1.8727 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0053 |           7.3559 |          -1.8397 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0022 |           7.3547 |          -1.8846 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0114 |           7.3383 |          -1.9729 |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |          -0.0035 |           7.3520 |          -2.1025 |
[32m[20221213 18:59:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.80
[32m[20221213 18:59:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 18:59:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 18:59:52 @agent_ppo2.py:143][0m Total time:      11.33 min
[32m[20221213 18:59:52 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 18:59:52 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 18:59:52 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 18:59:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:52 @agent_ppo2.py:185][0m |           0.0022 |         126.1853 |          -2.7641 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |           0.0000 |         121.3558 |          -2.6812 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0014 |         120.6437 |          -2.7753 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0006 |         120.1645 |          -2.8156 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0007 |         120.0843 |          -2.8818 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0048 |         119.8014 |          -2.9297 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0032 |         119.7550 |          -2.9698 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0019 |         119.6167 |          -2.9576 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |           0.0034 |         120.5777 |          -2.9528 |
[32m[20221213 18:59:53 @agent_ppo2.py:185][0m |          -0.0007 |         119.1752 |          -3.0295 |
[32m[20221213 18:59:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:59:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.60
[32m[20221213 18:59:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 18:59:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 18:59:53 @agent_ppo2.py:143][0m Total time:      11.35 min
[32m[20221213 18:59:53 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221213 18:59:53 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 18:59:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0006 |         200.2638 |          -3.6891 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0017 |         195.6724 |          -3.6252 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0025 |         194.7476 |          -3.6128 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0019 |         194.4866 |          -3.6414 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0007 |         194.6650 |          -3.5532 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0042 |         194.2427 |          -3.4803 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0010 |         194.2135 |          -3.4069 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0032 |         193.8394 |          -3.3264 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0009 |         193.9522 |          -3.4207 |
[32m[20221213 18:59:54 @agent_ppo2.py:185][0m |          -0.0048 |         193.8797 |          -3.5525 |
[32m[20221213 18:59:54 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:59:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.40
[32m[20221213 18:59:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 18:59:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 41.00
[32m[20221213 18:59:54 @agent_ppo2.py:143][0m Total time:      11.37 min
[32m[20221213 18:59:54 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 18:59:54 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 18:59:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |           0.0078 |         179.3428 |          -2.7507 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |          -0.0003 |         170.3380 |          -2.5872 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |           0.0008 |         169.2535 |          -2.3192 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |          -0.0038 |         169.0774 |          -2.2700 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |          -0.0044 |         168.4945 |          -2.0278 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |           0.0011 |         168.3520 |          -1.9905 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |          -0.0026 |         167.7027 |          -1.9619 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |          -0.0033 |         167.2885 |          -1.8879 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |           0.0093 |         183.2202 |          -1.8705 |
[32m[20221213 18:59:55 @agent_ppo2.py:185][0m |           0.0038 |         166.9692 |          -1.6640 |
[32m[20221213 18:59:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.60
[32m[20221213 18:59:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 18:59:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 18:59:56 @agent_ppo2.py:143][0m Total time:      11.39 min
[32m[20221213 18:59:56 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221213 18:59:56 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 18:59:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0004 |           8.9890 |          -1.8738 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0056 |           6.2394 |          -1.7769 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0004 |           5.9816 |          -1.8589 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0005 |           5.8646 |          -1.9770 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |          -0.0009 |           5.7048 |          -1.8910 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0024 |           5.7986 |          -1.9192 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0048 |           5.5534 |          -1.9855 |
[32m[20221213 18:59:56 @agent_ppo2.py:185][0m |           0.0042 |           5.4901 |          -1.9583 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |          -0.0078 |           5.4407 |          -1.8829 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |          -0.0005 |           5.4188 |          -1.7316 |
[32m[20221213 18:59:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:59:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.40
[32m[20221213 18:59:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 18:59:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 18:59:57 @agent_ppo2.py:143][0m Total time:      11.41 min
[32m[20221213 18:59:57 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 18:59:57 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 18:59:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |           0.0031 |          19.4122 |          -1.9123 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |          -0.0021 |           9.9803 |          -1.9388 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |           0.0011 |           9.4070 |          -1.8361 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |           0.0077 |           9.7929 |          -1.8131 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |          -0.0048 |           8.9422 |          -1.7479 |
[32m[20221213 18:59:57 @agent_ppo2.py:185][0m |          -0.0013 |           8.6198 |          -1.7310 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0115 |           8.5306 |          -1.6144 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0053 |           8.4507 |          -1.6353 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0117 |           8.3884 |          -1.5480 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0119 |           8.3410 |          -1.4733 |
[32m[20221213 18:59:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:59:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.20
[32m[20221213 18:59:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 18:59:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 18:59:58 @agent_ppo2.py:143][0m Total time:      11.42 min
[32m[20221213 18:59:58 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221213 18:59:58 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 18:59:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |           0.0021 |         199.0283 |          -1.7450 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |           0.0032 |         194.1511 |          -1.8761 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0014 |         191.4717 |          -2.2679 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0011 |         191.1413 |          -2.2961 |
[32m[20221213 18:59:58 @agent_ppo2.py:185][0m |          -0.0018 |         190.6035 |          -2.3696 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |           0.0006 |         190.2878 |          -2.2479 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |           0.0006 |         190.1265 |          -2.1059 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |          -0.0002 |         190.7918 |          -2.1962 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |          -0.0025 |         190.0547 |          -2.3348 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |           0.0141 |         199.0293 |          -2.4120 |
[32m[20221213 18:59:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:59:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.20
[32m[20221213 18:59:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 18:59:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 18:59:59 @agent_ppo2.py:143][0m Total time:      11.44 min
[32m[20221213 18:59:59 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 18:59:59 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 18:59:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 18:59:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |           0.0097 |          34.8715 |          -2.5590 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |          -0.0057 |          19.1762 |          -2.5399 |
[32m[20221213 18:59:59 @agent_ppo2.py:185][0m |           0.0045 |          18.7609 |          -2.4874 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |          -0.0041 |          18.3423 |          -2.2199 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |          -0.0065 |          18.2704 |          -2.2519 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |          -0.0016 |          18.1056 |          -2.2511 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |           0.0005 |          18.1055 |          -2.1551 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |          -0.0009 |          19.0926 |          -1.9557 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |           0.0033 |          18.1133 |          -1.8444 |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |           0.0000 |          17.9291 |          -1.8390 |
[32m[20221213 19:00:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:00:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.40
[32m[20221213 19:00:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 19:00:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.00
[32m[20221213 19:00:00 @agent_ppo2.py:143][0m Total time:      11.46 min
[32m[20221213 19:00:00 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221213 19:00:00 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 19:00:00 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:00 @agent_ppo2.py:185][0m |           0.0057 |         129.1217 |          -1.3242 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0028 |         123.7682 |          -1.4203 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0023 |         122.3860 |          -1.5151 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0057 |         121.9852 |          -1.6153 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0029 |         121.6713 |          -1.7178 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |           0.0092 |         125.4304 |          -1.6217 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0006 |         121.2485 |          -1.6159 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0011 |         120.7555 |          -1.5810 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |           0.0009 |         121.9749 |          -1.7523 |
[32m[20221213 19:00:01 @agent_ppo2.py:185][0m |          -0.0053 |         120.6492 |          -1.6549 |
[32m[20221213 19:00:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:00:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.80
[32m[20221213 19:00:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 19:00:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:00:01 @agent_ppo2.py:143][0m Total time:      11.48 min
[32m[20221213 19:00:01 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 19:00:01 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 19:00:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |           0.0076 |           7.0367 |          -1.5907 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |           0.0029 |           3.3053 |          -1.3534 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |           0.0017 |           2.9426 |          -1.1868 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |           0.0013 |           2.8146 |          -1.1050 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |           0.0042 |           2.7319 |          -0.9802 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |           0.0042 |           2.6927 |          -0.9138 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |          -0.0110 |           2.6538 |          -1.0356 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |          -0.0097 |           2.6359 |          -1.0761 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |          -0.0098 |           2.6103 |          -1.0020 |
[32m[20221213 19:00:02 @agent_ppo2.py:185][0m |          -0.0141 |           2.5894 |          -1.1239 |
[32m[20221213 19:00:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:00:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.00
[32m[20221213 19:00:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:00:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:00:02 @agent_ppo2.py:143][0m Total time:      11.50 min
[32m[20221213 19:00:02 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221213 19:00:02 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 19:00:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0011 |          48.8000 |          -1.3965 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |           0.0028 |          45.5304 |          -1.6115 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0022 |          44.4099 |          -1.6684 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0004 |          44.0697 |          -1.5475 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |           0.0019 |          43.6739 |          -1.6095 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0058 |          43.3082 |          -1.4570 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0074 |          43.0113 |          -1.4536 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0023 |          43.0183 |          -1.4295 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0029 |          42.7575 |          -1.5724 |
[32m[20221213 19:00:03 @agent_ppo2.py:185][0m |          -0.0011 |          42.6447 |          -1.5863 |
[32m[20221213 19:00:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:00:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.00
[32m[20221213 19:00:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.00
[32m[20221213 19:00:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 19:00:04 @agent_ppo2.py:143][0m Total time:      11.52 min
[32m[20221213 19:00:04 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 19:00:04 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 19:00:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0070 |           4.4819 |          -1.9752 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |           0.0057 |           2.5897 |          -1.8290 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0080 |           2.4160 |          -1.6244 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0114 |           2.3465 |          -1.6352 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0135 |           2.3109 |          -1.5374 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0127 |           2.2844 |          -1.4004 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0159 |           2.2734 |          -1.3500 |
[32m[20221213 19:00:04 @agent_ppo2.py:185][0m |          -0.0179 |           2.2519 |          -1.4300 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0181 |           2.2465 |          -1.4248 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0104 |           2.2324 |          -1.4493 |
[32m[20221213 19:00:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.40
[32m[20221213 19:00:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:00:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:00:05 @agent_ppo2.py:143][0m Total time:      11.54 min
[32m[20221213 19:00:05 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221213 19:00:05 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 19:00:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |           0.0041 |          16.0556 |          -1.0091 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0071 |          12.1757 |          -0.9371 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0043 |          11.1154 |          -0.8880 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0075 |          10.7889 |          -0.7835 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0046 |          10.6205 |          -0.5957 |
[32m[20221213 19:00:05 @agent_ppo2.py:185][0m |          -0.0019 |          10.5109 |          -0.5188 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |          -0.0037 |          10.3522 |          -0.4397 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |          -0.0036 |          10.3414 |          -0.4326 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |          -0.0060 |          10.1832 |          -0.3969 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |          -0.0042 |          10.2730 |          -0.3598 |
[32m[20221213 19:00:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:00:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.20
[32m[20221213 19:00:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 19:00:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.00
[32m[20221213 19:00:06 @agent_ppo2.py:143][0m Total time:      11.56 min
[32m[20221213 19:00:06 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 19:00:06 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 19:00:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |           0.0026 |         200.0744 |           0.1144 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |           0.0064 |         194.4447 |           0.3260 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |          -0.0018 |         193.6490 |           0.2588 |
[32m[20221213 19:00:06 @agent_ppo2.py:185][0m |          -0.0041 |         193.1892 |           0.3667 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |           0.0011 |         192.9935 |           0.2588 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |           0.0022 |         193.0172 |           0.4356 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |          -0.0010 |         192.7506 |           0.4288 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |          -0.0036 |         192.8048 |           0.3930 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |          -0.0032 |         192.5985 |           0.4700 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |           0.0020 |         193.3434 |           0.4546 |
[32m[20221213 19:00:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:00:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 19:00:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 19:00:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:00:07 @agent_ppo2.py:143][0m Total time:      11.58 min
[32m[20221213 19:00:07 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221213 19:00:07 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 19:00:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |           0.0044 |           8.4163 |           0.1743 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |          -0.0002 |           7.0638 |           0.2206 |
[32m[20221213 19:00:07 @agent_ppo2.py:185][0m |          -0.0039 |           6.9760 |          -0.0476 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0028 |           6.9426 |          -0.2140 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0066 |           6.9406 |          -0.1767 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0045 |           6.9284 |          -0.2695 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0052 |           6.9024 |          -0.1959 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0007 |           6.8911 |          -0.2604 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0094 |           6.9170 |          -0.0977 |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |           0.0036 |           6.9643 |          -0.1750 |
[32m[20221213 19:00:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:00:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.00
[32m[20221213 19:00:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 19:00:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:00:08 @agent_ppo2.py:143][0m Total time:      11.60 min
[32m[20221213 19:00:08 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 19:00:08 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 19:00:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:08 @agent_ppo2.py:185][0m |          -0.0002 |          59.6503 |          -0.8167 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |           0.0015 |          51.6016 |          -0.8053 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |           0.0072 |          49.1203 |          -1.0513 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |          -0.0034 |          47.4420 |          -1.0463 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |          -0.0044 |          47.1129 |          -1.1426 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |           0.0052 |          46.5221 |          -1.1994 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |           0.0016 |          45.6877 |          -1.1547 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |          -0.0020 |          45.7366 |          -1.1156 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |          -0.0025 |          45.3533 |          -1.2886 |
[32m[20221213 19:00:09 @agent_ppo2.py:185][0m |          -0.0009 |          45.3547 |          -1.2716 |
[32m[20221213 19:00:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.20
[32m[20221213 19:00:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.00
[32m[20221213 19:00:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.00
[32m[20221213 19:00:09 @agent_ppo2.py:143][0m Total time:      11.61 min
[32m[20221213 19:00:09 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221213 19:00:09 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 19:00:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |           0.0021 |          11.5497 |          -1.4533 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |           0.0029 |           9.1344 |          -1.3326 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |          -0.0033 |           8.8847 |          -1.2542 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |           0.0004 |           8.8602 |          -1.2767 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |           0.0047 |           8.9570 |          -1.2896 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |           0.0004 |           8.8428 |          -1.1341 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |          -0.0032 |           8.5863 |          -1.1734 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |          -0.0074 |           8.6406 |          -1.0346 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |          -0.0033 |           8.5241 |          -1.1723 |
[32m[20221213 19:00:10 @agent_ppo2.py:185][0m |          -0.0036 |           8.4962 |          -1.0268 |
[32m[20221213 19:00:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:00:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.20
[32m[20221213 19:00:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 72.00
[32m[20221213 19:00:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:00:10 @agent_ppo2.py:143][0m Total time:      11.63 min
[32m[20221213 19:00:10 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 19:00:10 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 19:00:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0018 |           6.2218 |          -0.4327 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0047 |           5.2902 |          -0.4748 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0012 |           5.2036 |          -0.4581 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0024 |           5.1313 |          -0.4786 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0060 |           5.0745 |          -0.6206 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0023 |           5.0571 |          -0.5894 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0067 |           5.0045 |          -0.6564 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0056 |           4.9586 |          -0.7113 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0045 |           4.9235 |          -0.7757 |
[32m[20221213 19:00:11 @agent_ppo2.py:185][0m |          -0.0044 |           4.9187 |          -0.8105 |
[32m[20221213 19:00:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:00:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.20
[32m[20221213 19:00:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.00
[32m[20221213 19:00:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 19:00:12 @agent_ppo2.py:143][0m Total time:      11.65 min
[32m[20221213 19:00:12 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221213 19:00:12 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 19:00:12 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |           0.0116 |          14.1583 |          -1.4842 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |           0.0059 |           9.7862 |          -1.5054 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |          -0.0037 |           9.3214 |          -1.4450 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |           0.0058 |           9.1740 |          -1.5260 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |          -0.0010 |           9.0582 |          -1.2992 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |          -0.0057 |           8.9432 |          -1.2737 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |          -0.0012 |           8.8100 |          -1.2082 |
[32m[20221213 19:00:12 @agent_ppo2.py:185][0m |          -0.0062 |           8.7592 |          -1.2568 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0002 |           8.7039 |          -1.3431 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |           0.0057 |           9.2684 |          -1.1978 |
[32m[20221213 19:00:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:00:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.60
[32m[20221213 19:00:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 19:00:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 19:00:13 @agent_ppo2.py:143][0m Total time:      11.67 min
[32m[20221213 19:00:13 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 19:00:13 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 19:00:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0008 |           8.7192 |          -0.5948 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0050 |           8.2450 |          -0.6751 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0025 |           8.0387 |          -0.6495 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0015 |           8.0204 |          -0.7536 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0038 |           7.9855 |          -0.8078 |
[32m[20221213 19:00:13 @agent_ppo2.py:185][0m |          -0.0050 |           8.0265 |          -0.8289 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |          -0.0039 |           8.0227 |          -0.8757 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |           0.0051 |           8.6990 |          -0.8170 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |          -0.0002 |           7.9813 |          -0.8623 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |          -0.0018 |           7.9050 |          -1.0039 |
[32m[20221213 19:00:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.20
[32m[20221213 19:00:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.00
[32m[20221213 19:00:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.00
[32m[20221213 19:00:14 @agent_ppo2.py:143][0m Total time:      11.69 min
[32m[20221213 19:00:14 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221213 19:00:14 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 19:00:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |          -0.0038 |          32.6191 |          -1.6018 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |           0.0003 |          17.6212 |          -1.6498 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |          -0.0089 |          16.5625 |          -1.7392 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |          -0.0032 |          16.1218 |          -1.7582 |
[32m[20221213 19:00:14 @agent_ppo2.py:185][0m |           0.0035 |          16.3597 |          -1.7986 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |           0.0024 |          15.5276 |          -1.7951 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |           0.0004 |          15.5499 |          -1.7692 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |           0.0018 |          15.5912 |          -1.7745 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |          -0.0122 |          15.0902 |          -1.9993 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |          -0.0051 |          15.0671 |          -1.9362 |
[32m[20221213 19:00:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:00:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 175.00
[32m[20221213 19:00:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 213.00
[32m[20221213 19:00:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.00
[32m[20221213 19:00:15 @agent_ppo2.py:143][0m Total time:      11.71 min
[32m[20221213 19:00:15 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 19:00:15 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 19:00:15 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |          -0.0052 |          43.6482 |          -2.7564 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |          -0.0011 |          35.7223 |          -2.9491 |
[32m[20221213 19:00:15 @agent_ppo2.py:185][0m |          -0.0005 |          33.8709 |          -3.0846 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0064 |          32.8212 |          -3.1968 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0034 |          32.8368 |          -3.3406 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0108 |          31.9204 |          -3.3506 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0060 |          31.0870 |          -3.4351 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0064 |          30.9583 |          -3.5131 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0032 |          30.1834 |          -3.5442 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |           0.0012 |          30.7970 |          -3.6040 |
[32m[20221213 19:00:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:00:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 157.80
[32m[20221213 19:00:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 354.00
[32m[20221213 19:00:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 19:00:16 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221213 19:00:16 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221213 19:00:16 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 19:00:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |           0.0101 |           9.3050 |          -4.1338 |
[32m[20221213 19:00:16 @agent_ppo2.py:185][0m |          -0.0030 |           8.2658 |          -4.1178 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |           0.0074 |           8.4855 |          -4.0861 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |          -0.0032 |           8.1595 |          -3.9750 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |          -0.0012 |           8.1188 |          -4.2094 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |          -0.0038 |           8.1173 |          -4.3046 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |          -0.0024 |           8.0935 |          -4.3287 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |           0.0012 |           8.2990 |          -4.3802 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |          -0.0067 |           8.1040 |          -4.3723 |
[32m[20221213 19:00:17 @agent_ppo2.py:185][0m |          -0.0044 |           8.0446 |          -4.5750 |
[32m[20221213 19:00:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.20
[32m[20221213 19:00:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.00
[32m[20221213 19:00:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 19:00:17 @agent_ppo2.py:143][0m Total time:      11.75 min
[32m[20221213 19:00:17 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 19:00:17 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 19:00:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |           0.0026 |         116.9168 |          -4.9171 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |           0.0049 |         110.6337 |          -5.1041 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0045 |         109.4502 |          -5.4327 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0028 |         105.9352 |          -5.5916 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0014 |         105.9735 |          -5.9284 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0049 |         105.9785 |          -6.0821 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0084 |         105.3449 |          -6.2763 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0091 |         105.5162 |          -6.5703 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |          -0.0041 |         104.2238 |          -6.6746 |
[32m[20221213 19:00:18 @agent_ppo2.py:185][0m |           0.0029 |         113.2790 |          -6.7576 |
[32m[20221213 19:00:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:00:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.00
[32m[20221213 19:00:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.00
[32m[20221213 19:00:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 591.00
[32m[20221213 19:00:18 @agent_ppo2.py:143][0m Total time:      11.77 min
[32m[20221213 19:00:18 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221213 19:00:18 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 19:00:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0036 |          86.2211 |          -7.6894 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |           0.0012 |          74.9333 |          -7.8294 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0070 |          73.1855 |          -7.8787 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0022 |          72.4372 |          -7.9220 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0051 |          71.3694 |          -7.9788 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0053 |          70.3141 |          -8.0590 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0060 |          70.2829 |          -8.0281 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0060 |          69.7431 |          -8.2280 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0124 |          69.4752 |          -8.1666 |
[32m[20221213 19:00:19 @agent_ppo2.py:185][0m |          -0.0031 |          69.6830 |          -7.8678 |
[32m[20221213 19:00:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.20
[32m[20221213 19:00:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.00
[32m[20221213 19:00:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.00
[32m[20221213 19:00:20 @agent_ppo2.py:143][0m Total time:      11.79 min
[32m[20221213 19:00:20 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 19:00:20 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 19:00:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |           0.0043 |          13.8219 |          -8.4588 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |           0.0026 |           6.9841 |          -8.5766 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |           0.0004 |           5.9549 |          -8.7333 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |          -0.0017 |           5.5256 |          -8.9414 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |          -0.0033 |           5.3034 |          -8.9383 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |          -0.0034 |           5.1404 |          -8.9669 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |          -0.0037 |           5.0458 |          -8.9083 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |          -0.0061 |           4.9365 |          -9.0665 |
[32m[20221213 19:00:20 @agent_ppo2.py:185][0m |          -0.0020 |           4.8485 |          -9.1659 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |          -0.0069 |           4.7993 |          -9.1658 |
[32m[20221213 19:00:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.20
[32m[20221213 19:00:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 19:00:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.00
[32m[20221213 19:00:21 @agent_ppo2.py:143][0m Total time:      11.80 min
[32m[20221213 19:00:21 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221213 19:00:21 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 19:00:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |          -0.0025 |           7.8878 |          -9.3199 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |           0.0007 |           6.7415 |          -9.0085 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |          -0.0044 |           6.5905 |          -8.8464 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |           0.0039 |           6.3454 |          -8.8506 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |           0.0023 |           6.2576 |          -8.7108 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |          -0.0040 |           6.2166 |          -8.8046 |
[32m[20221213 19:00:21 @agent_ppo2.py:185][0m |          -0.0030 |           6.2013 |          -8.7505 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |           0.0006 |           6.1694 |          -8.7490 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |           0.0035 |           6.1722 |          -8.7481 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |           0.0082 |           6.1706 |          -8.7972 |
[32m[20221213 19:00:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.80
[32m[20221213 19:00:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.00
[32m[20221213 19:00:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:00:22 @agent_ppo2.py:143][0m Total time:      11.82 min
[32m[20221213 19:00:22 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 19:00:22 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 19:00:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |           0.0117 |          27.6075 |          -8.8695 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |          -0.0028 |          22.7332 |          -8.7084 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |          -0.0021 |          21.6059 |          -8.5734 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |          -0.0028 |          21.3243 |          -8.3725 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |          -0.0035 |          20.6997 |          -8.3458 |
[32m[20221213 19:00:22 @agent_ppo2.py:185][0m |          -0.0026 |          20.4951 |          -8.2624 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |          -0.0074 |          20.5169 |          -8.1592 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |           0.0000 |          20.2823 |          -8.0203 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |          -0.0004 |          20.3974 |          -7.9691 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |          -0.0092 |          20.3451 |          -7.9116 |
[32m[20221213 19:00:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.80
[32m[20221213 19:00:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.00
[32m[20221213 19:00:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:00:23 @agent_ppo2.py:143][0m Total time:      11.84 min
[32m[20221213 19:00:23 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221213 19:00:23 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 19:00:23 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |           0.0046 |         222.3504 |          -7.5818 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |          -0.0019 |         193.3055 |          -7.5229 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |          -0.0058 |         191.5768 |          -7.8149 |
[32m[20221213 19:00:23 @agent_ppo2.py:185][0m |          -0.0036 |         191.1852 |          -7.9767 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |          -0.0050 |         191.1828 |          -8.0758 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |          -0.0058 |         190.7116 |          -8.0895 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |          -0.0072 |         190.0328 |          -8.1794 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |           0.0014 |         200.4089 |          -8.2864 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |          -0.0098 |         189.4751 |          -8.5098 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |          -0.0007 |         205.9256 |          -8.5260 |
[32m[20221213 19:00:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.60
[32m[20221213 19:00:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 19:00:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 19:00:24 @agent_ppo2.py:143][0m Total time:      11.86 min
[32m[20221213 19:00:24 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 19:00:24 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 19:00:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |           0.0116 |         198.5915 |          -8.8483 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |           0.0045 |         188.8228 |          -8.9768 |
[32m[20221213 19:00:24 @agent_ppo2.py:185][0m |           0.0106 |         204.0392 |          -9.0325 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |           0.0077 |         183.3306 |          -8.4725 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |           0.0010 |         183.4253 |          -8.9310 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |           0.0022 |         183.3247 |          -8.9430 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |           0.0022 |         182.8185 |          -8.8466 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |           0.0011 |         182.8312 |          -9.0145 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |           0.0104 |         197.9155 |          -9.1008 |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |          -0.0008 |         182.4648 |          -9.2804 |
[32m[20221213 19:00:25 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.80
[32m[20221213 19:00:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 19:00:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:00:25 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221213 19:00:25 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221213 19:00:25 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 19:00:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:25 @agent_ppo2.py:185][0m |          -0.0010 |         199.1390 |         -10.1794 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |          -0.0015 |         193.9259 |         -10.1555 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |          -0.0007 |         192.4235 |         -10.2808 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |           0.0077 |         200.3804 |         -10.4332 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |          -0.0004 |         191.0645 |         -10.2928 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |           0.0018 |         190.9450 |         -10.4872 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |          -0.0022 |         190.7378 |         -10.7723 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |          -0.0018 |         190.5719 |         -10.6888 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |           0.0003 |         190.3306 |         -10.6769 |
[32m[20221213 19:00:26 @agent_ppo2.py:185][0m |          -0.0025 |         190.3932 |         -10.6376 |
[32m[20221213 19:00:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.00
[32m[20221213 19:00:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:00:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:00:26 @agent_ppo2.py:143][0m Total time:      11.90 min
[32m[20221213 19:00:26 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 19:00:26 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 19:00:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:00:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |          -0.0004 |         198.4297 |         -10.7961 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |          -0.0004 |         197.2818 |         -11.1026 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |           0.0004 |         196.7728 |         -11.0738 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |          -0.0005 |         196.0518 |         -11.1396 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |          -0.0023 |         195.7998 |         -11.1330 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |           0.0020 |         195.4846 |         -11.2282 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |          -0.0028 |         195.2452 |         -11.0730 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |           0.0020 |         194.9672 |         -11.0291 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |           0.0033 |         197.2985 |         -11.4125 |
[32m[20221213 19:00:27 @agent_ppo2.py:185][0m |           0.0036 |         195.2660 |         -11.4463 |
[32m[20221213 19:00:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 988.00
[32m[20221213 19:00:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 988.00
[32m[20221213 19:00:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 889.00
[32m[20221213 19:00:27 @agent_ppo2.py:143][0m Total time:      11.92 min
[32m[20221213 19:00:27 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221213 19:00:27 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 19:00:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |          -0.0003 |         208.7684 |         -11.6381 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |           0.0050 |         207.2540 |         -11.6693 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |          -0.0003 |         204.5600 |         -11.8083 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |           0.0012 |         204.3819 |         -11.8809 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |           0.0006 |         203.7286 |         -11.9170 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |           0.0051 |         209.2485 |         -12.1147 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |           0.0125 |         220.0055 |         -11.9745 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |          -0.0001 |         203.1404 |         -12.3209 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |          -0.0040 |         203.0496 |         -12.3337 |
[32m[20221213 19:00:28 @agent_ppo2.py:185][0m |          -0.0025 |         202.8463 |         -12.1750 |
[32m[20221213 19:00:28 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.80
[32m[20221213 19:00:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 19:00:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 19:00:29 @agent_ppo2.py:143][0m Total time:      11.93 min
[32m[20221213 19:00:29 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 19:00:29 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 19:00:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0029 |         101.6974 |         -13.2184 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |           0.0026 |          78.1301 |         -13.5019 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0047 |          75.0018 |         -13.4983 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0086 |          73.6903 |         -13.5636 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0060 |          71.9272 |         -13.7535 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0034 |          70.6241 |         -13.7863 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0010 |          70.0105 |         -13.8428 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |          -0.0053 |          69.2001 |         -13.9876 |
[32m[20221213 19:00:29 @agent_ppo2.py:185][0m |           0.0026 |          69.1605 |         -13.8488 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |          -0.0025 |          68.4798 |         -14.1402 |
[32m[20221213 19:00:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:00:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.20
[32m[20221213 19:00:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 890.00
[32m[20221213 19:00:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 19:00:30 @agent_ppo2.py:143][0m Total time:      11.95 min
[32m[20221213 19:00:30 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221213 19:00:30 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 19:00:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |           0.0075 |         230.6087 |         -14.4089 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |           0.0034 |         220.9305 |         -14.3179 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |           0.0003 |         220.1656 |         -14.1835 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |           0.0006 |         219.9621 |         -13.8745 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |          -0.0012 |         219.2336 |         -13.8869 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |           0.0006 |         219.1227 |         -13.7959 |
[32m[20221213 19:00:30 @agent_ppo2.py:185][0m |           0.0010 |         220.4723 |         -13.5482 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |          -0.0005 |         218.8939 |         -13.3681 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |          -0.0039 |         218.7521 |         -13.3445 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |          -0.0005 |         218.6523 |         -13.3219 |
[32m[20221213 19:00:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:00:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.60
[32m[20221213 19:00:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 19:00:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:00:31 @agent_ppo2.py:143][0m Total time:      11.97 min
[32m[20221213 19:00:31 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 19:00:31 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 19:00:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |           0.0046 |         126.2222 |         -12.5705 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |           0.0070 |         120.4900 |         -12.9568 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |           0.0114 |         134.6104 |         -13.3865 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |          -0.0003 |         117.4198 |         -13.5141 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |          -0.0016 |         117.1283 |         -13.5936 |
[32m[20221213 19:00:31 @agent_ppo2.py:185][0m |          -0.0024 |         117.1434 |         -13.6708 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |          -0.0009 |         115.7399 |         -13.9878 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |           0.0003 |         115.0202 |         -14.1724 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |          -0.0047 |         115.1132 |         -14.2450 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |          -0.0002 |         113.9022 |         -14.1456 |
[32m[20221213 19:00:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.20
[32m[20221213 19:00:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 19:00:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:00:32 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 19:00:32 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221213 19:00:32 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 19:00:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |          -0.0033 |          12.8470 |         -15.0934 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |           0.0102 |           6.9822 |         -14.6890 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |           0.0059 |           6.2837 |         -14.4930 |
[32m[20221213 19:00:32 @agent_ppo2.py:185][0m |           0.0042 |           5.9714 |         -14.3068 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0109 |           5.8078 |         -14.0206 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0094 |           5.7191 |         -13.9137 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |          -0.0018 |           5.6438 |         -13.9160 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0015 |           5.6058 |         -13.7504 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0015 |           5.5579 |         -13.6995 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0037 |           5.7143 |         -13.4369 |
[32m[20221213 19:00:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:00:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.40
[32m[20221213 19:00:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:00:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 19:00:33 @agent_ppo2.py:143][0m Total time:      12.01 min
[32m[20221213 19:00:33 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 19:00:33 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 19:00:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0006 |         241.8167 |         -12.2523 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0013 |         234.2008 |         -12.2313 |
[32m[20221213 19:00:33 @agent_ppo2.py:185][0m |           0.0004 |         228.0271 |         -12.3083 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |           0.0012 |         225.8275 |         -12.2341 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |           0.0048 |         233.3570 |         -12.4188 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |           0.0023 |         223.4770 |         -12.3688 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |           0.0005 |         222.9463 |         -12.4311 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |           0.0103 |         230.1433 |         -12.2435 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |          -0.0005 |         221.5544 |         -12.3561 |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |           0.0004 |         221.0985 |         -12.4846 |
[32m[20221213 19:00:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:00:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.40
[32m[20221213 19:00:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 19:00:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.00
[32m[20221213 19:00:34 @agent_ppo2.py:143][0m Total time:      12.03 min
[32m[20221213 19:00:34 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221213 19:00:34 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 19:00:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:00:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:34 @agent_ppo2.py:185][0m |          -0.0039 |         228.9826 |         -13.0821 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |           0.0073 |         239.2270 |         -13.1228 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |          -0.0037 |         223.8191 |         -13.3230 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |          -0.0032 |         223.0644 |         -13.2626 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |          -0.0025 |         222.9656 |         -13.6111 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |           0.0132 |         251.7092 |         -13.6620 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |           0.0180 |         239.6591 |         -13.6716 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |          -0.0015 |         222.0874 |         -13.9016 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |          -0.0035 |         221.9442 |         -13.9975 |
[32m[20221213 19:00:35 @agent_ppo2.py:185][0m |          -0.0021 |         221.7354 |         -14.1629 |
[32m[20221213 19:00:35 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.80
[32m[20221213 19:00:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 19:00:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:00:35 @agent_ppo2.py:143][0m Total time:      12.05 min
[32m[20221213 19:00:35 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 19:00:35 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 19:00:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |           0.0079 |         124.5795 |         -14.5566 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |           0.0074 |          60.1110 |         -14.6208 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |           0.0009 |          56.2469 |         -14.4513 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |          -0.0039 |          54.6651 |         -14.3503 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |          -0.0012 |          53.5834 |         -14.2185 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |          -0.0042 |          53.2459 |         -14.0158 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |           0.0024 |          52.6263 |         -13.8294 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |          -0.0067 |          52.5467 |         -13.9951 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |          -0.0013 |          53.7275 |         -13.7690 |
[32m[20221213 19:00:36 @agent_ppo2.py:185][0m |          -0.0058 |          52.8112 |         -13.7616 |
[32m[20221213 19:00:36 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:00:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.00
[32m[20221213 19:00:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 81.00
[32m[20221213 19:00:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.00
[32m[20221213 19:00:36 @agent_ppo2.py:143][0m Total time:      12.07 min
[32m[20221213 19:00:36 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221213 19:00:36 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 19:00:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |           0.0075 |          36.6119 |         -13.1982 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0023 |          14.0915 |         -13.2578 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0004 |          12.9682 |         -13.3630 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0042 |          12.3940 |         -13.4743 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0046 |          12.0925 |         -13.5994 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0078 |          11.7766 |         -13.5974 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |           0.0030 |          12.4855 |         -13.8201 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0038 |          11.4309 |         -13.8141 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0028 |          11.2716 |         -13.7598 |
[32m[20221213 19:00:37 @agent_ppo2.py:185][0m |          -0.0038 |          11.1984 |         -13.6991 |
[32m[20221213 19:00:37 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.20
[32m[20221213 19:00:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 48.00
[32m[20221213 19:00:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.00
[32m[20221213 19:00:37 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221213 19:00:37 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 19:00:37 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 19:00:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:00:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |           0.0099 |         230.1336 |         -14.1741 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |          -0.0007 |         212.2998 |         -14.1873 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |           0.0016 |         212.6689 |         -14.3346 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |          -0.0014 |         210.5189 |         -14.3574 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |           0.0010 |         211.3589 |         -14.2563 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |           0.0026 |         215.9280 |         -14.5375 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |          -0.0021 |         209.5645 |         -14.4086 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |          -0.0060 |         209.4727 |         -14.6461 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |           0.0004 |         209.6764 |         -14.4505 |
[32m[20221213 19:00:38 @agent_ppo2.py:185][0m |          -0.0044 |         210.1633 |         -14.7305 |
[32m[20221213 19:00:38 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.00
[32m[20221213 19:00:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.00
[32m[20221213 19:00:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:00:39 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221213 19:00:39 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221213 19:00:39 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 19:00:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0024 |          60.6248 |         -15.6777 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |          -0.0035 |          46.8073 |         -15.6077 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0079 |          49.1085 |         -15.7292 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0049 |          47.4569 |         -15.7378 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0040 |          45.9868 |         -15.8977 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |          -0.0008 |          45.6782 |         -16.0485 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0023 |          45.7131 |         -15.7408 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0010 |          45.6212 |         -15.8028 |
[32m[20221213 19:00:39 @agent_ppo2.py:185][0m |           0.0013 |          45.4234 |         -15.8063 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0112 |          50.9829 |         -15.8920 |
[32m[20221213 19:00:40 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.40
[32m[20221213 19:00:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.00
[32m[20221213 19:00:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:00:40 @agent_ppo2.py:143][0m Total time:      12.12 min
[32m[20221213 19:00:40 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 19:00:40 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 19:00:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0029 |         226.3989 |         -15.8261 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0047 |         220.0380 |         -15.4366 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0014 |         219.7055 |         -15.6760 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0091 |         251.4949 |         -15.7912 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |          -0.0001 |         221.6053 |         -15.5743 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0009 |         219.4470 |         -15.4523 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0089 |         227.0683 |         -15.7900 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |           0.0116 |         230.8849 |         -15.5748 |
[32m[20221213 19:00:40 @agent_ppo2.py:185][0m |          -0.0039 |         219.0553 |         -15.6179 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |           0.0016 |         218.9003 |         -15.7887 |
[32m[20221213 19:00:41 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:00:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.60
[32m[20221213 19:00:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 890.00
[32m[20221213 19:00:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 19:00:41 @agent_ppo2.py:143][0m Total time:      12.14 min
[32m[20221213 19:00:41 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221213 19:00:41 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 19:00:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |           0.0096 |         212.4314 |         -15.2224 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |          -0.0012 |         196.4163 |         -15.1404 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |          -0.0002 |         195.9076 |         -15.2131 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |          -0.0013 |         195.1611 |         -15.6325 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |           0.0018 |         196.7201 |         -15.7315 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |          -0.0003 |         194.5890 |         -15.8821 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |           0.0001 |         194.2132 |         -15.8012 |
[32m[20221213 19:00:41 @agent_ppo2.py:185][0m |          -0.0041 |         193.8464 |         -15.8388 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |          -0.0005 |         194.4510 |         -16.0415 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |          -0.0056 |         193.5695 |         -16.0262 |
[32m[20221213 19:00:42 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.40
[32m[20221213 19:00:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 19:00:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:00:42 @agent_ppo2.py:143][0m Total time:      12.16 min
[32m[20221213 19:00:42 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 19:00:42 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 19:00:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |           0.0024 |          65.2668 |         -16.5045 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |          -0.0002 |          60.2260 |         -16.2462 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |          -0.0010 |          59.7641 |         -16.2480 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |           0.0023 |          59.0313 |         -15.9840 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |          -0.0033 |          58.7118 |         -15.7801 |
[32m[20221213 19:00:42 @agent_ppo2.py:185][0m |          -0.0002 |          58.5291 |         -15.6915 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |          -0.0015 |          58.2736 |         -15.4611 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |          -0.0040 |          58.1584 |         -15.4919 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |           0.0097 |          74.2871 |         -15.3527 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |          -0.0002 |          57.8962 |         -14.9840 |
[32m[20221213 19:00:43 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:00:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.20
[32m[20221213 19:00:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 19:00:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:00:43 @agent_ppo2.py:143][0m Total time:      12.17 min
[32m[20221213 19:00:43 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221213 19:00:43 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 19:00:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |           0.0187 |         252.6559 |         -14.7293 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |          -0.0019 |         225.8685 |         -14.6251 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |           0.0006 |         225.4438 |         -14.6274 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |          -0.0004 |         225.7418 |         -14.7286 |
[32m[20221213 19:00:43 @agent_ppo2.py:185][0m |          -0.0024 |         225.3219 |         -14.9781 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |           0.0002 |         225.3186 |         -15.0378 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |          -0.0029 |         225.3085 |         -15.0681 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |          -0.0007 |         225.3515 |         -15.1839 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |          -0.0018 |         225.2082 |         -15.5099 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |           0.0040 |         231.1112 |         -15.5251 |
[32m[20221213 19:00:44 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.20
[32m[20221213 19:00:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 887.00
[32m[20221213 19:00:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.00
[32m[20221213 19:00:44 @agent_ppo2.py:143][0m Total time:      12.19 min
[32m[20221213 19:00:44 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 19:00:44 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 19:00:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |          -0.0014 |         163.8426 |         -15.9227 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |           0.0051 |         152.8090 |         -15.8134 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |           0.0025 |         151.5099 |         -15.7363 |
[32m[20221213 19:00:44 @agent_ppo2.py:185][0m |           0.0041 |         150.9229 |         -15.8314 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |           0.0039 |         150.5690 |         -15.8612 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |           0.0113 |         159.3426 |         -16.0494 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |           0.0009 |         149.6233 |         -16.2169 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |          -0.0008 |         149.3238 |         -16.2450 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |          -0.0004 |         150.3846 |         -16.3046 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |           0.0009 |         148.8790 |         -16.2148 |
[32m[20221213 19:00:45 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.80
[32m[20221213 19:00:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.00
[32m[20221213 19:00:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.00
[32m[20221213 19:00:45 @agent_ppo2.py:143][0m Total time:      12.21 min
[32m[20221213 19:00:45 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221213 19:00:45 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 19:00:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:00:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |           0.0103 |          15.7840 |         -16.6498 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |           0.0009 |           8.1055 |         -16.4508 |
[32m[20221213 19:00:45 @agent_ppo2.py:185][0m |          -0.0006 |           7.5119 |         -16.5423 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |           0.0018 |           7.2066 |         -16.3620 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |          -0.0039 |           6.9785 |         -16.2987 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |          -0.0047 |           6.7947 |         -16.0771 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |          -0.0040 |           6.6813 |         -16.0903 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |           0.0099 |           6.5596 |         -16.2293 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |           0.0000 |           6.4926 |         -16.1839 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |          -0.0060 |           6.4269 |         -16.0712 |
[32m[20221213 19:00:46 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.00
[32m[20221213 19:00:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 19:00:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.00
[32m[20221213 19:00:46 @agent_ppo2.py:143][0m Total time:      12.23 min
[32m[20221213 19:00:46 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 19:00:46 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 19:00:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |           0.0039 |          11.3016 |         -16.4535 |
[32m[20221213 19:00:46 @agent_ppo2.py:185][0m |          -0.0028 |           7.3349 |         -16.1242 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |          -0.0056 |           6.9798 |         -16.0654 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |          -0.0015 |           6.9493 |         -16.0205 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |          -0.0047 |           6.8960 |         -15.8636 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |          -0.0055 |           6.5885 |         -16.0735 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |           0.0004 |           6.7337 |         -15.8010 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |           0.0011 |           6.9091 |         -15.6556 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |          -0.0065 |           6.6227 |         -15.6593 |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |          -0.0062 |           6.3863 |         -15.6303 |
[32m[20221213 19:00:47 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.80
[32m[20221213 19:00:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.00
[32m[20221213 19:00:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:00:47 @agent_ppo2.py:143][0m Total time:      12.25 min
[32m[20221213 19:00:47 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221213 19:00:47 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 19:00:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:47 @agent_ppo2.py:185][0m |           0.0055 |          32.7370 |         -15.1998 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0076 |          24.7218 |         -15.4077 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0012 |          24.1036 |         -15.4219 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0059 |          23.9545 |         -15.5740 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0109 |          24.0784 |         -15.5461 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |          -0.0029 |          23.6760 |         -15.7473 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0046 |          23.5018 |         -15.5997 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0025 |          24.6146 |         -15.7100 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |          -0.0043 |          23.8118 |         -15.7247 |
[32m[20221213 19:00:48 @agent_ppo2.py:185][0m |           0.0066 |          23.9787 |         -15.7400 |
[32m[20221213 19:00:48 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:00:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.20
[32m[20221213 19:00:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 73.00
[32m[20221213 19:00:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 19:00:48 @agent_ppo2.py:143][0m Total time:      12.26 min
[32m[20221213 19:00:48 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 19:00:48 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 19:00:48 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |           0.0043 |         107.1235 |         -16.3018 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |           0.0015 |         103.7427 |         -16.7212 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |           0.0011 |         103.0993 |         -16.8614 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |           0.0059 |         110.2693 |         -17.1147 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |          -0.0010 |         102.8645 |         -17.0883 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |          -0.0005 |         102.0143 |         -17.1599 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |          -0.0003 |         102.2271 |         -17.3816 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |          -0.0023 |         102.3653 |         -17.6924 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |          -0.0002 |         101.7987 |         -17.8469 |
[32m[20221213 19:00:49 @agent_ppo2.py:185][0m |           0.0028 |         102.1424 |         -17.7876 |
[32m[20221213 19:00:49 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.00
[32m[20221213 19:00:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.00
[32m[20221213 19:00:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:00:49 @agent_ppo2.py:143][0m Total time:      12.28 min
[32m[20221213 19:00:49 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221213 19:00:49 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 19:00:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |           0.0093 |         225.9416 |         -18.5923 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0002 |         217.0685 |         -18.8045 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0022 |         215.8737 |         -18.6701 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |           0.0001 |         217.2103 |         -18.6203 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0025 |         215.1151 |         -18.4526 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0010 |         214.8636 |         -18.5243 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0026 |         214.5364 |         -18.1069 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0013 |         214.4802 |         -18.0177 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0007 |         214.5797 |         -17.9602 |
[32m[20221213 19:00:50 @agent_ppo2.py:185][0m |          -0.0008 |         214.4721 |         -17.9932 |
[32m[20221213 19:00:50 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.60
[32m[20221213 19:00:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 19:00:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:00:50 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221213 19:00:50 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 19:00:50 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 19:00:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0001 |          13.6215 |         -17.3779 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |           0.0011 |           7.5731 |         -17.2184 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0042 |           7.0102 |         -17.1399 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0100 |           6.6717 |         -17.1445 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0038 |           6.5771 |         -17.1262 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0043 |           6.3676 |         -17.2001 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0068 |           6.2510 |         -17.0749 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0060 |           6.1931 |         -17.1827 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0075 |           6.1155 |         -17.1272 |
[32m[20221213 19:00:51 @agent_ppo2.py:185][0m |          -0.0101 |           6.1210 |         -17.2369 |
[32m[20221213 19:00:51 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.60
[32m[20221213 19:00:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:00:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 19:00:52 @agent_ppo2.py:143][0m Total time:      12.32 min
[32m[20221213 19:00:52 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221213 19:00:52 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 19:00:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |           0.0136 |         236.2479 |         -17.9640 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |           0.0007 |         209.7450 |         -17.9274 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |          -0.0027 |         207.7036 |         -18.0332 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |          -0.0026 |         206.5630 |         -18.4725 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |          -0.0020 |         205.9322 |         -18.4800 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |           0.0005 |         205.9265 |         -18.7575 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |           0.0095 |         220.2920 |         -18.9274 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |           0.0021 |         205.3313 |         -18.6966 |
[32m[20221213 19:00:52 @agent_ppo2.py:185][0m |          -0.0031 |         204.6756 |         -19.2405 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0015 |         204.6602 |         -19.0723 |
[32m[20221213 19:00:53 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.20
[32m[20221213 19:00:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.00
[32m[20221213 19:00:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 19:00:53 @agent_ppo2.py:143][0m Total time:      12.34 min
[32m[20221213 19:00:53 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 19:00:53 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 19:00:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0087 |         114.2041 |         -19.2889 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0058 |         101.9812 |         -19.4577 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0006 |         101.0233 |         -19.7638 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0006 |         100.4752 |         -19.6442 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |          -0.0025 |         100.1900 |         -19.6614 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0092 |         110.6426 |         -19.7889 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |          -0.0057 |          99.3179 |         -19.8550 |
[32m[20221213 19:00:53 @agent_ppo2.py:185][0m |           0.0116 |         118.6958 |         -19.8843 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |          -0.0025 |          99.9255 |         -19.6268 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |          -0.0022 |          98.9591 |         -19.7900 |
[32m[20221213 19:00:54 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.60
[32m[20221213 19:00:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 19:00:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.00
[32m[20221213 19:00:54 @agent_ppo2.py:143][0m Total time:      12.35 min
[32m[20221213 19:00:54 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221213 19:00:54 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 19:00:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |           0.0005 |         230.5115 |         -20.4842 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |           0.0009 |         227.1011 |         -20.6313 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |           0.0048 |         237.0223 |         -20.2325 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |           0.0187 |         245.6742 |         -20.2117 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |          -0.0007 |         223.4821 |         -20.0211 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |           0.0099 |         236.8371 |         -19.8629 |
[32m[20221213 19:00:54 @agent_ppo2.py:185][0m |           0.0000 |         222.2635 |         -20.0391 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |          -0.0036 |         221.2010 |         -19.6681 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |          -0.0017 |         221.3596 |         -19.6700 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |           0.0045 |         223.2065 |         -19.9852 |
[32m[20221213 19:00:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.60
[32m[20221213 19:00:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:00:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:00:55 @agent_ppo2.py:143][0m Total time:      12.37 min
[32m[20221213 19:00:55 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 19:00:55 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 19:00:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |           0.0102 |         247.5372 |         -19.6206 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |           0.0117 |         245.7614 |         -19.3602 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |           0.0059 |         230.7626 |         -18.9425 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |          -0.0013 |         223.2786 |         -18.9761 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |           0.0001 |         222.4464 |         -19.0737 |
[32m[20221213 19:00:55 @agent_ppo2.py:185][0m |          -0.0004 |         222.1154 |         -19.0726 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |          -0.0037 |         221.8490 |         -19.0510 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |          -0.0026 |         221.4085 |         -18.9100 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |           0.0012 |         221.4735 |         -19.0257 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |           0.0006 |         223.3234 |         -18.9853 |
[32m[20221213 19:00:56 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:00:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.00
[32m[20221213 19:00:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 19:00:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.00
[32m[20221213 19:00:56 @agent_ppo2.py:143][0m Total time:      12.39 min
[32m[20221213 19:00:56 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221213 19:00:56 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 19:00:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:00:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |           0.0014 |          23.6218 |         -18.0591 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |          -0.0010 |          14.8530 |         -17.9826 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |           0.0024 |          14.4548 |         -17.7845 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |          -0.0033 |          14.0422 |         -17.5628 |
[32m[20221213 19:00:56 @agent_ppo2.py:185][0m |           0.0034 |          14.5883 |         -17.7166 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |           0.0022 |          13.8173 |         -17.6887 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0087 |          13.4253 |         -17.3847 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |           0.0093 |          15.2445 |         -17.4026 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0003 |          13.6163 |         -17.2304 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0026 |          13.1047 |         -17.3320 |
[32m[20221213 19:00:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 73.80
[32m[20221213 19:00:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 75.00
[32m[20221213 19:00:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.00
[32m[20221213 19:00:57 @agent_ppo2.py:143][0m Total time:      12.41 min
[32m[20221213 19:00:57 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 19:00:57 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 19:00:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0051 |          17.6247 |         -17.6250 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0029 |          12.6295 |         -17.7005 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0050 |          12.2782 |         -17.4948 |
[32m[20221213 19:00:57 @agent_ppo2.py:185][0m |          -0.0031 |          12.0171 |         -17.5048 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0024 |          11.8827 |         -17.4184 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0117 |          11.7316 |         -17.3848 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0081 |          11.6183 |         -17.3264 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0039 |          11.5694 |         -17.2028 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0069 |          11.4876 |         -16.9630 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0071 |          11.4277 |         -17.0659 |
[32m[20221213 19:00:58 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.60
[32m[20221213 19:00:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:00:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 19:00:58 @agent_ppo2.py:143][0m Total time:      12.43 min
[32m[20221213 19:00:58 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221213 19:00:58 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 19:00:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:00:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0006 |          90.9172 |         -16.9209 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |          -0.0010 |          45.2722 |         -16.9442 |
[32m[20221213 19:00:58 @agent_ppo2.py:185][0m |           0.0002 |          41.9660 |         -16.7758 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |          -0.0017 |          42.7598 |         -16.5395 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |           0.0024 |          44.3004 |         -16.4642 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |           0.0004 |          39.2320 |         -16.4174 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |          -0.0023 |          38.6506 |         -16.5288 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |          -0.0033 |          38.4398 |         -16.2707 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |          -0.0075 |          38.2668 |         -16.2097 |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |          -0.0029 |          37.9428 |         -16.1952 |
[32m[20221213 19:00:59 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:00:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.60
[32m[20221213 19:00:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 195.00
[32m[20221213 19:00:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.00
[32m[20221213 19:00:59 @agent_ppo2.py:143][0m Total time:      12.45 min
[32m[20221213 19:00:59 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 19:00:59 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 19:00:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:00:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:00:59 @agent_ppo2.py:185][0m |          -0.0086 |          95.8682 |         -16.0024 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0067 |          87.2557 |         -16.0992 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |           0.0013 |          83.5231 |         -16.1128 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0081 |          83.2456 |         -16.2646 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0043 |          83.4639 |         -16.3361 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0074 |          83.0547 |         -16.4903 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0065 |          84.6480 |         -16.4787 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0061 |          81.5653 |         -16.4514 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0127 |          81.5545 |         -16.3842 |
[32m[20221213 19:01:00 @agent_ppo2.py:185][0m |          -0.0030 |          81.9983 |         -16.5268 |
[32m[20221213 19:01:00 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.20
[32m[20221213 19:01:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.00
[32m[20221213 19:01:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:01:00 @agent_ppo2.py:143][0m Total time:      12.46 min
[32m[20221213 19:01:00 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221213 19:01:00 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 19:01:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |           0.0002 |         232.0804 |         -15.5262 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |           0.0005 |         227.4716 |         -15.8951 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |          -0.0003 |         225.9110 |         -15.7934 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |           0.0018 |         225.6618 |         -15.9007 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |           0.0043 |         235.4047 |         -16.1288 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |           0.0001 |         229.0246 |         -16.1570 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |          -0.0021 |         225.3534 |         -16.0314 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |          -0.0040 |         225.1090 |         -16.3355 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |           0.0029 |         239.6124 |         -16.4046 |
[32m[20221213 19:01:01 @agent_ppo2.py:185][0m |          -0.0024 |         225.0085 |         -16.4797 |
[32m[20221213 19:01:01 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.60
[32m[20221213 19:01:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 887.00
[32m[20221213 19:01:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 19:01:01 @agent_ppo2.py:143][0m Total time:      12.48 min
[32m[20221213 19:01:01 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 19:01:01 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 19:01:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |           0.0065 |         227.4243 |         -17.4183 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |          -0.0004 |         225.8527 |         -17.6561 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |          -0.0005 |         225.6174 |         -17.4129 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |          -0.0006 |         225.3968 |         -17.5159 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |          -0.0036 |         225.4632 |         -17.4405 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |           0.0107 |         238.3486 |         -17.3241 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |          -0.0006 |         225.1159 |         -17.4782 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |           0.0094 |         231.7883 |         -17.5235 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |           0.0025 |         230.8218 |         -17.1787 |
[32m[20221213 19:01:02 @agent_ppo2.py:185][0m |           0.0006 |         224.9576 |         -17.2667 |
[32m[20221213 19:01:02 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.00
[32m[20221213 19:01:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 892.00
[32m[20221213 19:01:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.00
[32m[20221213 19:01:02 @agent_ppo2.py:143][0m Total time:      12.50 min
[32m[20221213 19:01:02 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221213 19:01:02 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 19:01:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |           0.0030 |         142.4678 |         -17.6602 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |          -0.0047 |         127.1027 |         -17.7099 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |          -0.0044 |         125.9366 |         -17.5914 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |           0.0009 |         126.2367 |         -17.5088 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |           0.0008 |         126.9625 |         -17.4329 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |           0.0031 |         129.0670 |         -17.4774 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |           0.0006 |         126.7290 |         -17.4407 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |          -0.0023 |         123.9831 |         -17.3162 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |          -0.0014 |         124.3619 |         -17.4269 |
[32m[20221213 19:01:03 @agent_ppo2.py:185][0m |          -0.0022 |         123.6354 |         -17.2598 |
[32m[20221213 19:01:03 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.20
[32m[20221213 19:01:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.00
[32m[20221213 19:01:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.00
[32m[20221213 19:01:04 @agent_ppo2.py:143][0m Total time:      12.52 min
[32m[20221213 19:01:04 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 19:01:04 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 19:01:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0240 |          46.0590 |         -16.0875 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0003 |          30.0064 |         -16.0346 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0076 |          30.0444 |         -16.2531 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0022 |          28.2075 |         -16.3951 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0064 |          27.9144 |         -16.4119 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |          -0.0027 |          27.7563 |         -16.5017 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0011 |          27.5897 |         -16.5503 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0031 |          27.6525 |         -16.6528 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |           0.0038 |          29.0214 |         -16.8078 |
[32m[20221213 19:01:04 @agent_ppo2.py:185][0m |          -0.0016 |          27.6332 |         -16.8279 |
[32m[20221213 19:01:04 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.20
[32m[20221213 19:01:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.00
[32m[20221213 19:01:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:01:05 @agent_ppo2.py:143][0m Total time:      12.54 min
[32m[20221213 19:01:05 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221213 19:01:05 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 19:01:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |          -0.0006 |         233.8901 |         -17.9993 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |           0.0053 |         239.2812 |         -18.3726 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |           0.0019 |         226.7286 |         -18.0016 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |          -0.0027 |         225.9306 |         -18.5401 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |           0.0002 |         227.0548 |         -18.4701 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |           0.0123 |         246.4543 |         -18.3340 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |          -0.0001 |         224.8949 |         -18.4376 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |          -0.0028 |         224.4868 |         -18.7685 |
[32m[20221213 19:01:05 @agent_ppo2.py:185][0m |          -0.0050 |         224.3133 |         -18.6564 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |          -0.0020 |         223.8937 |         -18.9555 |
[32m[20221213 19:01:06 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.40
[32m[20221213 19:01:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:01:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:01:06 @agent_ppo2.py:143][0m Total time:      12.55 min
[32m[20221213 19:01:06 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 19:01:06 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 19:01:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |          -0.0002 |         229.6974 |         -19.0818 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |          -0.0001 |         228.2997 |         -19.1482 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |           0.0020 |         228.0493 |         -19.1306 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |           0.0037 |         229.6207 |         -19.1432 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |           0.0025 |         227.8366 |         -19.0931 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |          -0.0008 |         227.6161 |         -19.5034 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |          -0.0003 |         227.4979 |         -19.4655 |
[32m[20221213 19:01:06 @agent_ppo2.py:185][0m |          -0.0019 |         227.4554 |         -19.6684 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |           0.0018 |         227.3918 |         -19.7403 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |          -0.0006 |         227.3792 |         -19.8225 |
[32m[20221213 19:01:07 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 933.60
[32m[20221213 19:01:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 935.00
[32m[20221213 19:01:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.00
[32m[20221213 19:01:07 @agent_ppo2.py:143][0m Total time:      12.57 min
[32m[20221213 19:01:07 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221213 19:01:07 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 19:01:07 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:01:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |           0.0046 |          40.4828 |         -20.5433 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |           0.0059 |          33.2629 |         -20.3655 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |          -0.0011 |          32.6215 |         -20.3635 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |          -0.0027 |          32.3579 |         -20.3707 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |          -0.0002 |          32.2227 |         -20.1436 |
[32m[20221213 19:01:07 @agent_ppo2.py:185][0m |          -0.0048 |          32.0840 |         -20.3414 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |          -0.0046 |          31.9862 |         -20.2144 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |           0.0047 |          32.2003 |         -19.9724 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |          -0.0037 |          31.8271 |         -19.9211 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |          -0.0029 |          32.4469 |         -19.8846 |
[32m[20221213 19:01:08 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.00
[32m[20221213 19:01:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 19:01:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 36.00
[32m[20221213 19:01:08 @agent_ppo2.py:143][0m Total time:      12.59 min
[32m[20221213 19:01:08 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 19:01:08 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 19:01:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |          -0.0011 |         160.3994 |         -19.8748 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |          -0.0022 |         151.9207 |         -19.7144 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |           0.0003 |         150.7353 |         -19.7451 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |          -0.0027 |         150.1912 |         -19.8889 |
[32m[20221213 19:01:08 @agent_ppo2.py:185][0m |           0.0012 |         149.8413 |         -19.8756 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |           0.0002 |         149.6558 |         -19.8357 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |          -0.0032 |         149.2691 |         -19.9238 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |          -0.0005 |         148.9179 |         -19.8430 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |           0.0032 |         151.4579 |         -20.0812 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |           0.0016 |         149.2636 |         -19.9110 |
[32m[20221213 19:01:09 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.80
[32m[20221213 19:01:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 19:01:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.00
[32m[20221213 19:01:09 @agent_ppo2.py:143][0m Total time:      12.61 min
[32m[20221213 19:01:09 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221213 19:01:09 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 19:01:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |          -0.0029 |         230.5935 |         -20.5263 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |           0.0004 |         228.2856 |         -20.8289 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |           0.0090 |         234.6214 |         -20.7914 |
[32m[20221213 19:01:09 @agent_ppo2.py:185][0m |          -0.0027 |         224.8359 |         -20.5605 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |           0.0002 |         224.3080 |         -20.4901 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |          -0.0047 |         224.1284 |         -20.3390 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |          -0.0024 |         223.8921 |         -20.5147 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |           0.0113 |         255.0021 |         -20.5835 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |           0.0022 |         228.2594 |         -20.4522 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |           0.0080 |         238.9377 |         -20.3227 |
[32m[20221213 19:01:10 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.60
[32m[20221213 19:01:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 19:01:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:01:10 @agent_ppo2.py:143][0m Total time:      12.63 min
[32m[20221213 19:01:10 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 19:01:10 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 19:01:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |          -0.0006 |         238.1106 |         -19.8022 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |          -0.0002 |         236.5112 |         -19.3008 |
[32m[20221213 19:01:10 @agent_ppo2.py:185][0m |           0.0003 |         236.2026 |         -19.6428 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |          -0.0010 |         235.9745 |         -19.6728 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |          -0.0036 |         235.9167 |         -19.6880 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |          -0.0034 |         235.8195 |         -19.7903 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |           0.0049 |         235.7952 |         -19.1324 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |           0.0000 |         235.9930 |         -19.8198 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |           0.0006 |         235.7101 |         -19.4328 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |           0.0002 |         235.6462 |         -19.5511 |
[32m[20221213 19:01:11 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.00
[32m[20221213 19:01:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 886.00
[32m[20221213 19:01:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 19:01:11 @agent_ppo2.py:143][0m Total time:      12.64 min
[32m[20221213 19:01:11 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221213 19:01:11 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 19:01:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |           0.0016 |         242.7446 |         -19.1048 |
[32m[20221213 19:01:11 @agent_ppo2.py:185][0m |          -0.0024 |         239.8916 |         -19.2788 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |           0.0008 |         239.1201 |         -19.5400 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |          -0.0023 |         239.0311 |         -19.4965 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |          -0.0021 |         238.3916 |         -19.5696 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |           0.0006 |         238.5648 |         -19.4561 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |           0.0111 |         273.0588 |         -19.8286 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |          -0.0006 |         237.7225 |         -19.5030 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |          -0.0006 |         237.5529 |         -19.3942 |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |           0.0059 |         239.2318 |         -19.4700 |
[32m[20221213 19:01:12 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 888.60
[32m[20221213 19:01:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 889.00
[32m[20221213 19:01:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:01:12 @agent_ppo2.py:143][0m Total time:      12.66 min
[32m[20221213 19:01:12 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 19:01:12 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 19:01:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:12 @agent_ppo2.py:185][0m |           0.0098 |          41.4742 |         -19.4190 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |          -0.0016 |          33.9761 |         -19.4109 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |          -0.0042 |          33.5975 |         -19.4300 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |           0.0025 |          33.6440 |         -19.4034 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |           0.0010 |          33.8826 |         -19.2013 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |           0.0150 |          37.7054 |         -19.1609 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |          -0.0026 |          35.4287 |         -19.2098 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |          -0.0021 |          33.3760 |         -19.1340 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |          -0.0002 |          33.4890 |         -19.1527 |
[32m[20221213 19:01:13 @agent_ppo2.py:185][0m |          -0.0005 |          33.7783 |         -19.1354 |
[32m[20221213 19:01:13 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:01:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.20
[32m[20221213 19:01:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:01:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:01:13 @agent_ppo2.py:143][0m Total time:      12.68 min
[32m[20221213 19:01:13 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221213 19:01:13 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 19:01:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0005 |         240.7778 |         -19.3001 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0037 |         242.2895 |         -19.7892 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |          -0.0005 |         236.9316 |         -19.7999 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0014 |         236.4926 |         -19.9777 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |          -0.0011 |         235.9580 |         -20.4469 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0040 |         244.9621 |         -20.4243 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0009 |         235.4021 |         -20.6926 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0079 |         254.6659 |         -21.1825 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |           0.0000 |         235.2521 |         -21.3440 |
[32m[20221213 19:01:14 @agent_ppo2.py:185][0m |          -0.0017 |         235.0162 |         -21.6505 |
[32m[20221213 19:01:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 631.20
[32m[20221213 19:01:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.00
[32m[20221213 19:01:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:01:14 @agent_ppo2.py:143][0m Total time:      12.70 min
[32m[20221213 19:01:14 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 19:01:14 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 19:01:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |           0.0047 |         238.2582 |         -22.1705 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |          -0.0015 |         235.3869 |         -22.6280 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |          -0.0017 |         235.0966 |         -22.2666 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |           0.0064 |         244.5278 |         -22.3691 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |          -0.0014 |         234.5642 |         -22.2179 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |          -0.0006 |         234.5263 |         -22.1790 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |          -0.0033 |         234.3064 |         -22.3952 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |           0.0066 |         248.1593 |         -22.4099 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |           0.0038 |         234.1531 |         -22.5518 |
[32m[20221213 19:01:15 @agent_ppo2.py:185][0m |          -0.0010 |         234.1980 |         -22.5406 |
[32m[20221213 19:01:15 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.00
[32m[20221213 19:01:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 19:01:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:01:15 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221213 19:01:15 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221213 19:01:15 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 19:01:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |           0.0057 |          41.0621 |         -23.7356 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |           0.0069 |          33.8108 |         -23.7076 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |          -0.0008 |          33.6056 |         -23.6011 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |          -0.0011 |          33.5243 |         -23.3821 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |          -0.0012 |          33.3243 |         -23.7344 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |           0.0001 |          33.4179 |         -23.7729 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |           0.0043 |          34.7754 |         -23.7404 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |          -0.0052 |          33.3237 |         -23.7361 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |          -0.0066 |          33.1948 |         -23.6467 |
[32m[20221213 19:01:16 @agent_ppo2.py:185][0m |          -0.0014 |          33.2009 |         -23.7611 |
[32m[20221213 19:01:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.80
[32m[20221213 19:01:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:01:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:01:17 @agent_ppo2.py:143][0m Total time:      12.73 min
[32m[20221213 19:01:17 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 19:01:17 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 19:01:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |           0.0063 |          34.5256 |         -23.4257 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |           0.0001 |          26.8729 |         -23.7566 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |           0.0020 |          26.9391 |         -24.0186 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |          -0.0063 |          26.2056 |         -24.2588 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |           0.0090 |          27.9984 |         -24.1558 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |          -0.0054 |          25.9131 |         -24.5260 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |           0.0039 |          26.1531 |         -24.8887 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |          -0.0011 |          25.7823 |         -24.6909 |
[32m[20221213 19:01:17 @agent_ppo2.py:185][0m |          -0.0064 |          25.4617 |         -24.8327 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |           0.0091 |          27.0086 |         -25.0276 |
[32m[20221213 19:01:18 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:01:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.00
[32m[20221213 19:01:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 19:01:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:01:18 @agent_ppo2.py:143][0m Total time:      12.75 min
[32m[20221213 19:01:18 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221213 19:01:18 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 19:01:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0089 |         169.5085 |         -26.5448 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0115 |         164.2438 |         -26.5862 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0075 |         165.3653 |         -26.5672 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0133 |         163.5345 |         -26.7706 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0156 |         163.1179 |         -27.0821 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0123 |         163.5901 |         -27.1201 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0162 |         162.5615 |         -27.2794 |
[32m[20221213 19:01:18 @agent_ppo2.py:185][0m |          -0.0092 |         163.0217 |         -27.3683 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |          -0.0126 |         162.3364 |         -27.4388 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |          -0.0159 |         162.2568 |         -27.5464 |
[32m[20221213 19:01:19 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.20
[32m[20221213 19:01:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.00
[32m[20221213 19:01:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 19:01:19 @agent_ppo2.py:143][0m Total time:      12.77 min
[32m[20221213 19:01:19 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 19:01:19 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 19:01:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |          -0.0006 |         197.3069 |         -27.9829 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |           0.0053 |         192.0351 |         -28.0061 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |          -0.0028 |         190.9322 |         -28.1550 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |           0.0035 |         190.9314 |         -27.6532 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |           0.0006 |         190.6388 |         -27.7911 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |           0.0062 |         191.3557 |         -27.3682 |
[32m[20221213 19:01:19 @agent_ppo2.py:185][0m |           0.0019 |         190.2422 |         -27.6434 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0000 |         190.1570 |         -27.7257 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |           0.0028 |         189.7624 |         -27.4432 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0004 |         190.1443 |         -27.6900 |
[32m[20221213 19:01:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.20
[32m[20221213 19:01:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.00
[32m[20221213 19:01:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:01:20 @agent_ppo2.py:143][0m Total time:      12.79 min
[32m[20221213 19:01:20 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221213 19:01:20 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 19:01:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |           0.0040 |          23.9964 |         -27.6761 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0060 |          16.9242 |         -27.3079 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0049 |          16.0767 |         -26.9470 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0039 |          15.5887 |         -26.6799 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0037 |          15.2857 |         -26.2316 |
[32m[20221213 19:01:20 @agent_ppo2.py:185][0m |          -0.0065 |          15.1542 |         -26.3329 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |          -0.0075 |          15.0510 |         -25.9141 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |          -0.0000 |          15.0193 |         -25.5399 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |          -0.0044 |          14.9809 |         -25.6116 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |          -0.0036 |          14.8774 |         -25.2461 |
[32m[20221213 19:01:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.80
[32m[20221213 19:01:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 19:01:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:01:21 @agent_ppo2.py:143][0m Total time:      12.81 min
[32m[20221213 19:01:21 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 19:01:21 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 19:01:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |           0.0058 |         215.6952 |         -23.6819 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |           0.0040 |         212.0356 |         -24.3021 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |           0.0010 |         210.2117 |         -24.0426 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |           0.0132 |         224.9292 |         -24.1774 |
[32m[20221213 19:01:21 @agent_ppo2.py:185][0m |           0.0009 |         209.2740 |         -24.4933 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |          -0.0011 |         209.1270 |         -24.3961 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |          -0.0011 |         209.1015 |         -24.1919 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |           0.0082 |         225.8131 |         -24.3495 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |          -0.0004 |         209.3520 |         -24.0029 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |           0.0121 |         216.7347 |         -24.1279 |
[32m[20221213 19:01:22 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:01:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.00
[32m[20221213 19:01:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:01:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 19:01:22 @agent_ppo2.py:143][0m Total time:      12.83 min
[32m[20221213 19:01:22 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221213 19:01:22 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 19:01:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |          -0.0017 |          18.3276 |         -24.7297 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |          -0.0020 |          13.6433 |         -24.2849 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |          -0.0033 |          13.3833 |         -24.4704 |
[32m[20221213 19:01:22 @agent_ppo2.py:185][0m |           0.0011 |          13.1819 |         -24.1058 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |          -0.0035 |          13.1216 |         -24.0675 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |          -0.0075 |          13.0215 |         -24.0785 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |          -0.0060 |          12.9253 |         -24.0058 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |           0.0050 |          13.0776 |         -24.1931 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |          -0.0017 |          12.8248 |         -24.0365 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |          -0.0062 |          12.8821 |         -23.9347 |
[32m[20221213 19:01:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.00
[32m[20221213 19:01:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 19:01:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:01:23 @agent_ppo2.py:143][0m Total time:      12.84 min
[32m[20221213 19:01:23 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 19:01:23 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 19:01:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |           0.0010 |         204.3202 |         -25.4417 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |           0.0027 |         203.6888 |         -25.5762 |
[32m[20221213 19:01:23 @agent_ppo2.py:185][0m |           0.0085 |         212.3422 |         -25.5925 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |           0.0014 |         203.9159 |         -25.9364 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |          -0.0013 |         203.6855 |         -26.1455 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |          -0.0026 |         203.5031 |         -25.8978 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |          -0.0011 |         203.7057 |         -26.3891 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |          -0.0033 |         203.6161 |         -26.3398 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |           0.0069 |         208.2932 |         -26.6334 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |           0.0024 |         204.4881 |         -27.2765 |
[32m[20221213 19:01:24 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:01:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:01:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:01:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:01:24 @agent_ppo2.py:143][0m Total time:      12.86 min
[32m[20221213 19:01:24 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221213 19:01:24 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 19:01:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |           0.0130 |         240.2728 |         -25.9074 |
[32m[20221213 19:01:24 @agent_ppo2.py:185][0m |           0.0006 |         210.7309 |         -25.8774 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |           0.0016 |         211.9331 |         -26.2959 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |           0.0196 |         233.5320 |         -26.7631 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |          -0.0038 |         209.1424 |         -26.6838 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |          -0.0058 |         209.0635 |         -26.9526 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |          -0.0019 |         208.9182 |         -26.4960 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |          -0.0025 |         208.9730 |         -26.9283 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |          -0.0022 |         208.8318 |         -26.9502 |
[32m[20221213 19:01:25 @agent_ppo2.py:185][0m |          -0.0031 |         208.8516 |         -27.4289 |
[32m[20221213 19:01:25 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.80
[32m[20221213 19:01:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.00
[32m[20221213 19:01:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.00
[32m[20221213 19:01:25 @agent_ppo2.py:143][0m Total time:      12.88 min
[32m[20221213 19:01:25 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 19:01:25 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 19:01:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |           0.0092 |          57.9601 |         -28.4071 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |           0.0022 |          23.4626 |         -27.9518 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0022 |          21.2649 |         -28.1368 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0026 |          20.2351 |         -28.1930 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0090 |          19.5522 |         -28.2148 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0021 |          19.0251 |         -28.4198 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0081 |          18.7927 |         -28.4130 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0036 |          18.5034 |         -28.4496 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |          -0.0037 |          18.1720 |         -28.4973 |
[32m[20221213 19:01:26 @agent_ppo2.py:185][0m |           0.0042 |          17.9289 |         -28.4763 |
[32m[20221213 19:01:26 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.40
[32m[20221213 19:01:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 19:01:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:01:26 @agent_ppo2.py:143][0m Total time:      12.90 min
[32m[20221213 19:01:26 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221213 19:01:26 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 19:01:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:01:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0011 |          23.4063 |         -30.2374 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |           0.0006 |          13.8808 |         -30.3876 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |           0.0007 |          12.8049 |         -30.3298 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0001 |          12.3119 |         -30.4392 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |           0.0085 |          13.5399 |         -30.4361 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0033 |          11.9472 |         -30.6018 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0034 |          11.5680 |         -30.6929 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0050 |          11.4913 |         -30.9400 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0007 |          11.2477 |         -30.7737 |
[32m[20221213 19:01:27 @agent_ppo2.py:185][0m |          -0.0081 |          11.1441 |         -30.6554 |
[32m[20221213 19:01:27 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:01:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.80
[32m[20221213 19:01:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:01:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:01:27 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221213 19:01:27 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 19:01:27 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 19:01:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |           0.0038 |          26.3660 |         -30.7371 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0007 |          18.4706 |         -30.4908 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0031 |          18.0696 |         -30.5999 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0046 |          17.9732 |         -30.2405 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |           0.0007 |          18.1501 |         -30.2440 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0020 |          17.7913 |         -30.0155 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |           0.0014 |          17.7943 |         -30.3931 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0078 |          17.8325 |         -30.2908 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0040 |          17.7018 |         -30.2404 |
[32m[20221213 19:01:28 @agent_ppo2.py:185][0m |          -0.0066 |          17.6437 |         -30.0544 |
[32m[20221213 19:01:28 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:01:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.00
[32m[20221213 19:01:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:01:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 19:01:29 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221213 19:01:29 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221213 19:01:29 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 19:01:29 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:01:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |           0.0060 |          73.7561 |         -29.9732 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |           0.0057 |          65.1955 |         -29.3847 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |           0.0038 |          63.6606 |         -28.7040 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |          -0.0027 |          63.3128 |         -29.3074 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |           0.0001 |          62.8750 |         -28.9039 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |          -0.0007 |          62.5717 |         -28.8895 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |          -0.0015 |          62.4318 |         -28.7458 |
[32m[20221213 19:01:29 @agent_ppo2.py:185][0m |           0.0000 |          62.3162 |         -28.5823 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |          -0.0045 |          62.2312 |         -28.5623 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |          -0.0065 |          62.1066 |         -28.5181 |
[32m[20221213 19:01:30 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:01:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 162.80
[32m[20221213 19:01:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 19:01:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 894.00
[32m[20221213 19:01:30 @agent_ppo2.py:143][0m Total time:      12.95 min
[32m[20221213 19:01:30 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 19:01:30 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 19:01:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |           0.0154 |          24.2157 |         -28.7991 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |           0.0008 |          17.8759 |         -28.6439 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |          -0.0001 |          17.5957 |         -28.8610 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |           0.0016 |          17.5734 |         -28.7798 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |          -0.0019 |          17.0260 |         -28.4995 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |          -0.0021 |          16.7945 |         -28.5436 |
[32m[20221213 19:01:30 @agent_ppo2.py:185][0m |           0.0013 |          16.7695 |         -28.8261 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |          -0.0039 |          16.6754 |         -28.5335 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |          -0.0000 |          16.6530 |         -28.6938 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |          -0.0061 |          16.7101 |         -28.6107 |
[32m[20221213 19:01:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 69.40
[32m[20221213 19:01:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.00
[32m[20221213 19:01:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.00
[32m[20221213 19:01:31 @agent_ppo2.py:143][0m Total time:      12.97 min
[32m[20221213 19:01:31 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221213 19:01:31 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 19:01:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |           0.0016 |         135.0665 |         -28.7989 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |           0.0003 |         131.2648 |         -28.5351 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |           0.0016 |         130.3854 |         -28.5552 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |          -0.0035 |         129.8345 |         -28.3325 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |          -0.0073 |         129.6171 |         -28.1218 |
[32m[20221213 19:01:31 @agent_ppo2.py:185][0m |          -0.0000 |         129.9848 |         -28.1598 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |          -0.0024 |         129.0691 |         -27.9488 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |          -0.0023 |         129.0198 |         -27.8365 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |          -0.0027 |         129.0592 |         -27.6699 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |          -0.0058 |         128.5610 |         -27.6744 |
[32m[20221213 19:01:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:01:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.20
[32m[20221213 19:01:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.00
[32m[20221213 19:01:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:01:32 @agent_ppo2.py:143][0m Total time:      12.99 min
[32m[20221213 19:01:32 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 19:01:32 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 19:01:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |           0.0005 |          62.1560 |         -26.6238 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |          -0.0044 |          57.6355 |         -26.6624 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |          -0.0004 |          57.1926 |         -26.7085 |
[32m[20221213 19:01:32 @agent_ppo2.py:185][0m |           0.0015 |          56.5494 |         -26.8849 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |           0.0013 |          56.4797 |         -26.7791 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |           0.0064 |          57.5959 |         -26.7265 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |           0.0060 |          55.9577 |         -26.6705 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |          -0.0033 |          55.8915 |         -26.6701 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |           0.0048 |          58.5291 |         -26.7610 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |          -0.0047 |          55.7086 |         -26.8176 |
[32m[20221213 19:01:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:01:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.00
[32m[20221213 19:01:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.00
[32m[20221213 19:01:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.00
[32m[20221213 19:01:33 @agent_ppo2.py:143][0m Total time:      13.01 min
[32m[20221213 19:01:33 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221213 19:01:33 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 19:01:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |          -0.0029 |         208.0241 |         -28.2907 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |          -0.0006 |         204.0457 |         -28.2943 |
[32m[20221213 19:01:33 @agent_ppo2.py:185][0m |          -0.0010 |         203.4273 |         -28.2276 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0028 |         202.9098 |         -28.4847 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0017 |         202.6706 |         -28.4258 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |          -0.0008 |         202.9408 |         -28.2599 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0025 |         202.5353 |         -28.2207 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0115 |         202.5197 |         -28.0099 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0001 |         202.2702 |         -28.2143 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0016 |         202.2039 |         -28.4260 |
[32m[20221213 19:01:34 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.80
[32m[20221213 19:01:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 19:01:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:01:34 @agent_ppo2.py:143][0m Total time:      13.03 min
[32m[20221213 19:01:34 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 19:01:34 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 19:01:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |           0.0004 |         140.2413 |         -28.7364 |
[32m[20221213 19:01:34 @agent_ppo2.py:185][0m |          -0.0038 |         136.3026 |         -28.7017 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |           0.0026 |         135.0642 |         -28.3522 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |          -0.0002 |         134.3824 |         -28.2358 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |          -0.0066 |         134.2138 |         -27.9687 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |          -0.0031 |         133.8231 |         -27.6746 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |           0.0130 |         160.2392 |         -27.2710 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |           0.0097 |         146.7502 |         -27.3039 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |           0.0019 |         133.2370 |         -26.9085 |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |           0.0017 |         133.2394 |         -26.9115 |
[32m[20221213 19:01:35 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.20
[32m[20221213 19:01:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.00
[32m[20221213 19:01:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.00
[32m[20221213 19:01:35 @agent_ppo2.py:143][0m Total time:      13.05 min
[32m[20221213 19:01:35 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221213 19:01:35 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 19:01:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:35 @agent_ppo2.py:185][0m |           0.0072 |         214.0529 |         -27.1172 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |          -0.0025 |         208.0289 |         -27.1482 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |           0.0026 |         208.6644 |         -27.1707 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |           0.0077 |         207.3877 |         -26.5709 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |           0.0010 |         209.2053 |         -27.0976 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |           0.0031 |         206.9872 |         -26.3838 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |          -0.0006 |         207.1207 |         -26.7930 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |           0.0021 |         207.1000 |         -26.7094 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |          -0.0014 |         206.9410 |         -26.6796 |
[32m[20221213 19:01:36 @agent_ppo2.py:185][0m |          -0.0019 |         206.7876 |         -26.6225 |
[32m[20221213 19:01:36 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.20
[32m[20221213 19:01:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.00
[32m[20221213 19:01:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 19:01:36 @agent_ppo2.py:143][0m Total time:      13.06 min
[32m[20221213 19:01:36 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 19:01:36 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 19:01:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |           0.0054 |          16.9796 |         -25.0990 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |           0.0059 |           8.1153 |         -24.7049 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |           0.0045 |           7.2593 |         -24.3642 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |          -0.0028 |           6.8502 |         -24.3628 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |           0.0011 |           6.5976 |         -24.4137 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |          -0.0092 |           6.1900 |         -24.0046 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |          -0.0031 |           5.9992 |         -24.1960 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |          -0.0086 |           5.8426 |         -23.8294 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |           0.0029 |           5.7482 |         -23.6652 |
[32m[20221213 19:01:37 @agent_ppo2.py:185][0m |          -0.0078 |           5.8114 |         -23.7008 |
[32m[20221213 19:01:37 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:01:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.00
[32m[20221213 19:01:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:01:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:01:37 @agent_ppo2.py:143][0m Total time:      13.08 min
[32m[20221213 19:01:37 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221213 19:01:37 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 19:01:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |          -0.0001 |         195.3683 |         -24.3711 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |           0.0046 |         191.6061 |         -24.5584 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |           0.0026 |         190.5598 |         -25.0260 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |          -0.0019 |         190.1121 |         -25.3253 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |          -0.0009 |         189.6491 |         -25.3825 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |           0.0014 |         189.4235 |         -25.7597 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |           0.0018 |         191.6404 |         -26.1027 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |           0.0038 |         190.5037 |         -25.8768 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |           0.0025 |         188.9811 |         -25.8401 |
[32m[20221213 19:01:38 @agent_ppo2.py:185][0m |          -0.0010 |         189.0044 |         -26.1774 |
[32m[20221213 19:01:38 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.00
[32m[20221213 19:01:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:01:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 74.00
[32m[20221213 19:01:39 @agent_ppo2.py:143][0m Total time:      13.10 min
[32m[20221213 19:01:39 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 19:01:39 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 19:01:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |           0.0040 |         207.7283 |         -25.6175 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |           0.0070 |         211.0523 |         -25.3145 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |           0.0023 |         203.1963 |         -24.6661 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |          -0.0023 |         202.7913 |         -25.0871 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |          -0.0009 |         202.4890 |         -24.7858 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |          -0.0011 |         202.5368 |         -24.8066 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |          -0.0021 |         202.2550 |         -24.2679 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |          -0.0009 |         202.2047 |         -24.2879 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |           0.0110 |         229.3113 |         -24.2082 |
[32m[20221213 19:01:39 @agent_ppo2.py:185][0m |           0.0108 |         215.1389 |         -23.9881 |
[32m[20221213 19:01:39 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.60
[32m[20221213 19:01:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.00
[32m[20221213 19:01:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.00
[32m[20221213 19:01:40 @agent_ppo2.py:143][0m Total time:      13.12 min
[32m[20221213 19:01:40 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221213 19:01:40 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 19:01:40 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:01:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |           0.0089 |         179.0654 |         -23.2594 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |           0.0007 |         172.5034 |         -23.1674 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0006 |         171.6028 |         -23.3914 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0011 |         170.9294 |         -23.5177 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0005 |         170.7055 |         -23.6506 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0017 |         170.6276 |         -23.6709 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0035 |         170.4910 |         -23.6056 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0000 |         170.3143 |         -23.6108 |
[32m[20221213 19:01:40 @agent_ppo2.py:185][0m |          -0.0052 |         170.0971 |         -23.8043 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |          -0.0041 |         169.9584 |         -23.7208 |
[32m[20221213 19:01:41 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.60
[32m[20221213 19:01:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.00
[32m[20221213 19:01:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.00
[32m[20221213 19:01:41 @agent_ppo2.py:143][0m Total time:      13.14 min
[32m[20221213 19:01:41 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 19:01:41 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 19:01:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |           0.0031 |          41.9315 |         -24.5488 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |           0.0111 |          27.3462 |         -24.3635 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |           0.0022 |          26.1040 |         -24.7106 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |          -0.0039 |          25.9047 |         -24.7971 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |          -0.0018 |          25.5521 |         -24.9813 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |           0.0006 |          25.3260 |         -24.7699 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |          -0.0024 |          26.4878 |         -24.9261 |
[32m[20221213 19:01:41 @agent_ppo2.py:185][0m |          -0.0039 |          25.1559 |         -25.3447 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |          -0.0031 |          26.7346 |         -25.3740 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |          -0.0024 |          25.0968 |         -25.0561 |
[32m[20221213 19:01:42 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.40
[32m[20221213 19:01:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:01:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 19:01:42 @agent_ppo2.py:143][0m Total time:      13.16 min
[32m[20221213 19:01:42 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221213 19:01:42 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 19:01:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |           0.0017 |         204.4765 |         -25.5855 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |           0.0064 |         202.5907 |         -25.0802 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |           0.0051 |         206.2567 |         -25.7425 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |           0.0011 |         202.0943 |         -25.5471 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |           0.0007 |         202.1445 |         -25.9219 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |          -0.0001 |         202.0751 |         -26.0892 |
[32m[20221213 19:01:42 @agent_ppo2.py:185][0m |          -0.0010 |         201.7869 |         -25.9622 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |           0.0097 |         225.1789 |         -25.8551 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |          -0.0024 |         202.3030 |         -25.8157 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |          -0.0036 |         201.9915 |         -25.8245 |
[32m[20221213 19:01:43 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.40
[32m[20221213 19:01:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:01:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 893.00
[32m[20221213 19:01:43 @agent_ppo2.py:143][0m Total time:      13.17 min
[32m[20221213 19:01:43 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 19:01:43 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 19:01:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |           0.0018 |          98.3537 |         -26.2824 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |          -0.0016 |          93.1121 |         -26.0384 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |          -0.0003 |          92.4492 |         -26.1700 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |          -0.0071 |          91.4969 |         -26.3765 |
[32m[20221213 19:01:43 @agent_ppo2.py:185][0m |          -0.0022 |          91.1904 |         -26.4933 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |          -0.0035 |          90.5884 |         -26.3694 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |          -0.0067 |          90.4934 |         -26.3382 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |          -0.0052 |          90.0578 |         -26.2753 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |          -0.0002 |          90.2891 |         -26.0615 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |          -0.0048 |          89.7957 |         -26.3983 |
[32m[20221213 19:01:44 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:01:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.00
[32m[20221213 19:01:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.00
[32m[20221213 19:01:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:01:44 @agent_ppo2.py:143][0m Total time:      13.19 min
[32m[20221213 19:01:44 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221213 19:01:44 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 19:01:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |           0.0028 |          33.3635 |         -26.5059 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |           0.0004 |          26.1500 |         -26.2603 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |          -0.0041 |          24.7505 |         -26.3163 |
[32m[20221213 19:01:44 @agent_ppo2.py:185][0m |           0.0000 |          24.6453 |         -26.2343 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |          -0.0012 |          24.3476 |         -26.1775 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |           0.0137 |          29.3009 |         -25.8730 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |          -0.0020 |          24.7207 |         -25.7144 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |           0.0041 |          24.0962 |         -25.6805 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |          -0.0065 |          23.9603 |         -25.7002 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |          -0.0094 |          24.0182 |         -25.7915 |
[32m[20221213 19:01:45 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.60
[32m[20221213 19:01:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 19:01:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:01:45 @agent_ppo2.py:143][0m Total time:      13.21 min
[32m[20221213 19:01:45 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 19:01:45 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 19:01:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |          -0.0029 |         229.1126 |         -25.4504 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |           0.0001 |         226.7136 |         -25.2759 |
[32m[20221213 19:01:45 @agent_ppo2.py:185][0m |           0.0016 |         224.5424 |         -25.8086 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |          -0.0008 |         222.8361 |         -25.8036 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |           0.0063 |         234.1790 |         -25.5231 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |           0.0123 |         222.7771 |         -24.6197 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |          -0.0037 |         221.6904 |         -26.4372 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |          -0.0033 |         221.9981 |         -26.5472 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |           0.0067 |         231.8701 |         -26.5780 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |          -0.0010 |         221.9004 |         -26.7383 |
[32m[20221213 19:01:46 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:01:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.60
[32m[20221213 19:01:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 893.00
[32m[20221213 19:01:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 19:01:46 @agent_ppo2.py:143][0m Total time:      13.23 min
[32m[20221213 19:01:46 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221213 19:01:46 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 19:01:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |           0.0012 |         227.3122 |         -26.9646 |
[32m[20221213 19:01:46 @agent_ppo2.py:185][0m |           0.0097 |         243.5337 |         -26.6985 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |           0.0008 |         224.6590 |         -26.9712 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0018 |         224.4164 |         -27.2048 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0018 |         224.1050 |         -27.2858 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0010 |         223.9163 |         -27.2205 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0002 |         224.0561 |         -27.2122 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0014 |         223.6294 |         -27.0855 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0024 |         223.5661 |         -27.1211 |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0006 |         223.6261 |         -27.3317 |
[32m[20221213 19:01:47 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:01:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 849.40
[32m[20221213 19:01:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.00
[32m[20221213 19:01:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.00
[32m[20221213 19:01:47 @agent_ppo2.py:143][0m Total time:      13.25 min
[32m[20221213 19:01:47 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 19:01:47 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 19:01:47 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:47 @agent_ppo2.py:185][0m |          -0.0014 |          33.9815 |         -27.6785 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |           0.0093 |          26.9923 |         -27.4350 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0005 |          25.7599 |         -27.4095 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0051 |          25.7098 |         -27.5751 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |           0.0005 |          25.6249 |         -27.5033 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0031 |          25.8045 |         -27.5185 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0026 |          25.5754 |         -27.6313 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0040 |          25.6307 |         -27.4093 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0041 |          25.6143 |         -27.5844 |
[32m[20221213 19:01:48 @agent_ppo2.py:185][0m |          -0.0076 |          25.6348 |         -27.5599 |
[32m[20221213 19:01:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.80
[32m[20221213 19:01:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:01:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 19:01:48 @agent_ppo2.py:143][0m Total time:      13.26 min
[32m[20221213 19:01:48 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221213 19:01:48 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 19:01:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |           0.0149 |           4.8242 |         -28.0487 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |           0.0062 |           2.6238 |         -28.0845 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |           0.0048 |           2.5047 |         -28.1600 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |          -0.0021 |           2.4783 |         -28.2482 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |          -0.0000 |           2.4364 |         -28.5313 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |          -0.0008 |           2.4137 |         -28.3169 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |           0.0032 |           2.3964 |         -28.4722 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |          -0.0028 |           2.3745 |         -28.4480 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |           0.0008 |           2.3653 |         -28.4646 |
[32m[20221213 19:01:49 @agent_ppo2.py:185][0m |           0.0034 |           2.3495 |         -28.3000 |
[32m[20221213 19:01:49 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.40
[32m[20221213 19:01:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 77.00
[32m[20221213 19:01:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 19:01:49 @agent_ppo2.py:143][0m Total time:      13.28 min
[32m[20221213 19:01:49 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 19:01:49 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 19:01:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0011 |           2.9503 |         -28.0437 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |          -0.0042 |           2.1051 |         -27.8687 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0021 |           2.0691 |         -28.4046 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0060 |           2.0564 |         -28.4731 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0022 |           2.0495 |         -28.4726 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0028 |           2.0450 |         -28.6271 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |          -0.0039 |           2.0390 |         -28.6579 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0079 |           2.0323 |         -28.6573 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |          -0.0013 |           2.0295 |         -28.8217 |
[32m[20221213 19:01:50 @agent_ppo2.py:185][0m |           0.0007 |           2.0275 |         -28.8451 |
[32m[20221213 19:01:50 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:01:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.40
[32m[20221213 19:01:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 72.00
[32m[20221213 19:01:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 19:01:50 @agent_ppo2.py:143][0m Total time:      13.30 min
[32m[20221213 19:01:50 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221213 19:01:50 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 19:01:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0026 |         216.9659 |         -29.3169 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0013 |         216.5946 |         -29.6559 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |          -0.0007 |         215.8178 |         -29.9670 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0060 |         218.2778 |         -29.8492 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |          -0.0009 |         216.3750 |         -30.1027 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0009 |         216.3134 |         -30.2207 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0104 |         228.2256 |         -30.1196 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0133 |         217.3538 |         -28.7938 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |           0.0036 |         216.1192 |         -29.9832 |
[32m[20221213 19:01:51 @agent_ppo2.py:185][0m |          -0.0017 |         215.9881 |         -30.3311 |
[32m[20221213 19:01:51 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:01:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:01:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:01:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.00
[32m[20221213 19:01:52 @agent_ppo2.py:143][0m Total time:      13.32 min
[32m[20221213 19:01:52 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 19:01:52 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 19:01:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |           0.0025 |           6.3654 |         -31.0697 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |          -0.0024 |           4.7422 |         -30.8890 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |           0.0054 |           4.7989 |         -30.7926 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |           0.0002 |           4.6794 |         -30.9620 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |           0.0010 |           4.6349 |         -30.8421 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |          -0.0002 |           4.6227 |         -30.7732 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |          -0.0012 |           4.6140 |         -30.6742 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |          -0.0014 |           4.6097 |         -31.0470 |
[32m[20221213 19:01:52 @agent_ppo2.py:185][0m |          -0.0046 |           4.6272 |         -30.9106 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0010 |           4.6303 |         -30.7598 |
[32m[20221213 19:01:53 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:01:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.80
[32m[20221213 19:01:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 73.00
[32m[20221213 19:01:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.00
[32m[20221213 19:01:53 @agent_ppo2.py:143][0m Total time:      13.34 min
[32m[20221213 19:01:53 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221213 19:01:53 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 19:01:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0099 |         233.1328 |         -30.4701 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0012 |         212.8122 |         -29.9993 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0007 |         212.8458 |         -30.2300 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0009 |         212.6586 |         -30.3175 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0017 |         212.5413 |         -30.5729 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |          -0.0020 |         212.3201 |         -31.0693 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0048 |         212.4107 |         -30.6446 |
[32m[20221213 19:01:53 @agent_ppo2.py:185][0m |           0.0005 |         212.5279 |         -31.1663 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |           0.0062 |         214.9664 |         -30.6884 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |           0.0049 |         212.0185 |         -31.1718 |
[32m[20221213 19:01:54 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:01:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 700.40
[32m[20221213 19:01:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 19:01:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:01:54 @agent_ppo2.py:143][0m Total time:      13.35 min
[32m[20221213 19:01:54 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 19:01:54 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 19:01:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:01:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |           0.0027 |         219.2928 |         -32.8085 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |           0.0001 |         217.6516 |         -33.0028 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |          -0.0011 |         217.6414 |         -33.3808 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |          -0.0002 |         217.6056 |         -33.9142 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |           0.0009 |         217.5659 |         -33.8595 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |          -0.0022 |         217.3809 |         -33.9878 |
[32m[20221213 19:01:54 @agent_ppo2.py:185][0m |          -0.0000 |         217.4568 |         -33.9872 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |          -0.0017 |         217.3403 |         -34.4107 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |           0.0011 |         217.3617 |         -34.7555 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |          -0.0007 |         217.4476 |         -34.7880 |
[32m[20221213 19:01:55 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:01:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:01:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:01:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:01:55 @agent_ppo2.py:143][0m Total time:      13.37 min
[32m[20221213 19:01:55 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221213 19:01:55 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 19:01:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |           0.0061 |           4.6108 |         -34.1018 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |          -0.0002 |           3.6139 |         -34.4225 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |           0.0074 |           3.5253 |         -34.4241 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |           0.0068 |           3.4249 |         -34.3549 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |          -0.0020 |           3.3940 |         -34.3180 |
[32m[20221213 19:01:55 @agent_ppo2.py:185][0m |           0.0060 |           3.3812 |         -34.0003 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |          -0.0015 |           3.3588 |         -34.4473 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |           0.0015 |           3.3211 |         -34.1723 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |           0.0070 |           3.3320 |         -34.2890 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |          -0.0020 |           3.2879 |         -34.2627 |
[32m[20221213 19:01:56 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:01:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.20
[32m[20221213 19:01:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 78.00
[32m[20221213 19:01:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.00
[32m[20221213 19:01:56 @agent_ppo2.py:143][0m Total time:      13.39 min
[32m[20221213 19:01:56 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 19:01:56 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 19:01:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |           0.0066 |         222.7661 |         -34.8555 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |          -0.0017 |         216.2098 |         -34.9140 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |           0.0092 |         228.4025 |         -35.1626 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |          -0.0005 |         216.5316 |         -35.2020 |
[32m[20221213 19:01:56 @agent_ppo2.py:185][0m |          -0.0011 |         216.3126 |         -35.4032 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |          -0.0017 |         216.2715 |         -35.5399 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |           0.0014 |         216.4633 |         -35.7576 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |           0.0007 |         216.2717 |         -35.5269 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |          -0.0011 |         216.1269 |         -35.3782 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |           0.0009 |         216.3280 |         -35.9099 |
[32m[20221213 19:01:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:01:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 888.20
[32m[20221213 19:01:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 889.00
[32m[20221213 19:01:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:01:57 @agent_ppo2.py:143][0m Total time:      13.41 min
[32m[20221213 19:01:57 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221213 19:01:57 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 19:01:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |          -0.0038 |         224.8192 |         -36.5396 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |          -0.0004 |         217.7913 |         -36.8184 |
[32m[20221213 19:01:57 @agent_ppo2.py:185][0m |          -0.0016 |         217.1552 |         -37.1173 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |           0.0006 |         216.5218 |         -36.9480 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |          -0.0017 |         216.4550 |         -37.3414 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |          -0.0083 |         216.5740 |         -37.2379 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |          -0.0013 |         217.2872 |         -37.4003 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |          -0.0078 |         215.9968 |         -37.5395 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |           0.0044 |         236.5534 |         -37.3451 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |           0.0008 |         216.0768 |         -37.1768 |
[32m[20221213 19:01:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:01:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 664.80
[32m[20221213 19:01:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 19:01:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.00
[32m[20221213 19:01:58 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221213 19:01:58 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 19:01:58 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 19:01:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |           0.0032 |          45.0975 |         -40.3893 |
[32m[20221213 19:01:58 @agent_ppo2.py:185][0m |           0.0038 |          36.0254 |         -40.4355 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |           0.0035 |          35.5549 |         -40.6987 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |           0.0026 |          35.4131 |         -41.0843 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |          -0.0024 |          35.1934 |         -41.0096 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |          -0.0089 |          35.4217 |         -41.5202 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |           0.0068 |          35.8172 |         -41.4698 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |          -0.0061 |          35.0053 |         -41.5243 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |           0.0008 |          34.8284 |         -41.7627 |
[32m[20221213 19:01:59 @agent_ppo2.py:185][0m |          -0.0002 |          34.7656 |         -41.8054 |
[32m[20221213 19:01:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:01:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 68.20
[32m[20221213 19:01:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.00
[32m[20221213 19:01:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 19:01:59 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221213 19:01:59 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221213 19:01:59 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 19:01:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:01:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |           0.0094 |          20.8284 |         -41.5691 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |           0.0030 |           8.0577 |         -41.7420 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |          -0.0003 |           7.4398 |         -41.7472 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |           0.0025 |           7.2213 |         -42.1888 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |           0.0043 |           6.9458 |         -42.0092 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |          -0.0009 |           6.8649 |         -42.0245 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |          -0.0033 |           6.8560 |         -41.8065 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |           0.0012 |           6.6377 |         -41.7696 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |          -0.0019 |           6.6118 |         -41.8808 |
[32m[20221213 19:02:00 @agent_ppo2.py:185][0m |          -0.0037 |           6.5413 |         -41.6740 |
[32m[20221213 19:02:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:02:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.00
[32m[20221213 19:02:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:02:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:02:00 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221213 19:02:00 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 19:02:00 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 19:02:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0022 |         209.7170 |         -42.6577 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0013 |         204.7106 |         -42.2476 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0002 |         204.2418 |         -42.4646 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0047 |         204.4244 |         -42.3497 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0043 |         204.5549 |         -42.5026 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0041 |         204.5042 |         -42.7100 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |          -0.0024 |         204.2676 |         -42.4661 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0003 |         204.0278 |         -42.3934 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0026 |         207.0476 |         -42.5686 |
[32m[20221213 19:02:01 @agent_ppo2.py:185][0m |           0.0010 |         203.8455 |         -42.6125 |
[32m[20221213 19:02:01 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 700.00
[32m[20221213 19:02:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 19:02:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.00
[32m[20221213 19:02:01 @agent_ppo2.py:143][0m Total time:      13.48 min
[32m[20221213 19:02:01 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221213 19:02:01 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 19:02:02 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:02:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |           0.0092 |          10.3712 |         -42.6317 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |          -0.0007 |           3.6420 |         -42.2611 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |           0.0036 |           3.0124 |         -42.1382 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |          -0.0053 |           2.7842 |         -41.8597 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |          -0.0009 |           2.6847 |         -41.6840 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |          -0.0026 |           2.6087 |         -41.4907 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |          -0.0040 |           2.5651 |         -41.3355 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |           0.0041 |           2.5318 |         -41.1519 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |          -0.0054 |           2.5063 |         -41.0241 |
[32m[20221213 19:02:02 @agent_ppo2.py:185][0m |           0.0053 |           2.4916 |         -40.9274 |
[32m[20221213 19:02:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.40
[32m[20221213 19:02:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:02:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:02:03 @agent_ppo2.py:143][0m Total time:      13.50 min
[32m[20221213 19:02:03 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 19:02:03 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 19:02:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |          -0.0001 |         198.6846 |         -40.3891 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |           0.0020 |         198.3348 |         -40.3898 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |           0.0005 |         196.9744 |         -40.2182 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |          -0.0027 |         196.5278 |         -40.2994 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |          -0.0013 |         196.5828 |         -39.6939 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |          -0.0023 |         196.4441 |         -40.1204 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |           0.0030 |         198.7959 |         -39.6746 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |          -0.0037 |         196.1215 |         -39.5352 |
[32m[20221213 19:02:03 @agent_ppo2.py:185][0m |          -0.0009 |         196.0693 |         -39.6340 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |          -0.0065 |         196.0214 |         -38.9956 |
[32m[20221213 19:02:04 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:02:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 981.00
[32m[20221213 19:02:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 985.00
[32m[20221213 19:02:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:02:04 @agent_ppo2.py:143][0m Total time:      13.52 min
[32m[20221213 19:02:04 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221213 19:02:04 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 19:02:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |           0.0070 |         210.8102 |         -39.9938 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |          -0.0027 |         200.5170 |         -39.7165 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |           0.0011 |         199.8745 |         -39.2410 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |           0.0027 |         199.7991 |         -39.4893 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |           0.0006 |         199.6815 |         -39.5976 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |           0.0010 |         199.3888 |         -39.7232 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |           0.0019 |         199.7689 |         -39.8792 |
[32m[20221213 19:02:04 @agent_ppo2.py:185][0m |          -0.0016 |         199.2475 |         -40.0925 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |          -0.0046 |         199.1923 |         -40.0128 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0043 |         199.0691 |         -39.6544 |
[32m[20221213 19:02:05 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.60
[32m[20221213 19:02:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.00
[32m[20221213 19:02:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 19:02:05 @agent_ppo2.py:143][0m Total time:      13.54 min
[32m[20221213 19:02:05 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 19:02:05 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 19:02:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:02:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0059 |           7.7907 |         -39.4969 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |          -0.0051 |           3.8234 |         -39.4259 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0016 |           3.5240 |         -39.7542 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0042 |           3.3657 |         -39.6464 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0040 |           3.2983 |         -39.6026 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0007 |           3.2178 |         -39.3969 |
[32m[20221213 19:02:05 @agent_ppo2.py:185][0m |           0.0010 |           3.2673 |         -39.3765 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |          -0.0050 |           3.1500 |         -39.4540 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |          -0.0014 |           3.0951 |         -39.5086 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |           0.0002 |           3.1047 |         -39.3606 |
[32m[20221213 19:02:06 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.80
[32m[20221213 19:02:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 19:02:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:02:06 @agent_ppo2.py:143][0m Total time:      13.56 min
[32m[20221213 19:02:06 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221213 19:02:06 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 19:02:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |           0.0036 |           4.0070 |         -40.3615 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |          -0.0008 |           2.4864 |         -40.1745 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |           0.0069 |           2.3779 |         -40.1052 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |           0.0069 |           2.3317 |         -39.9364 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |          -0.0016 |           2.2946 |         -39.9438 |
[32m[20221213 19:02:06 @agent_ppo2.py:185][0m |           0.0078 |           2.2568 |         -39.9984 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0016 |           2.2342 |         -39.8994 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0040 |           2.2208 |         -39.7739 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |           0.0004 |           2.2041 |         -39.5621 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0016 |           2.1958 |         -39.6478 |
[32m[20221213 19:02:07 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.60
[32m[20221213 19:02:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:02:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:02:07 @agent_ppo2.py:143][0m Total time:      13.57 min
[32m[20221213 19:02:07 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 19:02:07 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 19:02:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0012 |           5.2856 |         -39.1819 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |           0.0006 |           2.4750 |         -38.6261 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0033 |           2.2690 |         -38.6818 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0067 |           2.1932 |         -38.3838 |
[32m[20221213 19:02:07 @agent_ppo2.py:185][0m |          -0.0053 |           2.1580 |         -38.2542 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |          -0.0102 |           2.1288 |         -37.9514 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |          -0.0089 |           2.0961 |         -38.0637 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |          -0.0043 |           2.0902 |         -38.0525 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |          -0.0092 |           2.0717 |         -37.9756 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |          -0.0076 |           2.0556 |         -37.9567 |
[32m[20221213 19:02:08 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.40
[32m[20221213 19:02:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:02:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:02:08 @agent_ppo2.py:143][0m Total time:      13.59 min
[32m[20221213 19:02:08 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221213 19:02:08 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 19:02:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |           0.0028 |          28.1516 |         -37.5206 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |          -0.0005 |          21.7000 |         -37.1971 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |           0.0013 |          21.2720 |         -37.1143 |
[32m[20221213 19:02:08 @agent_ppo2.py:185][0m |           0.0000 |          21.4845 |         -37.0888 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |           0.0023 |          20.9872 |         -36.8942 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |           0.0032 |          20.8668 |         -37.2434 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |           0.0020 |          21.5201 |         -37.0931 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |          -0.0015 |          20.8228 |         -36.8219 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |           0.0002 |          20.6818 |         -36.8654 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |           0.0004 |          20.6785 |         -36.7072 |
[32m[20221213 19:02:09 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.00
[32m[20221213 19:02:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:02:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 19:02:09 @agent_ppo2.py:143][0m Total time:      13.61 min
[32m[20221213 19:02:09 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 19:02:09 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 19:02:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |           0.0011 |           4.3447 |         -37.1502 |
[32m[20221213 19:02:09 @agent_ppo2.py:185][0m |          -0.0021 |           2.1324 |         -37.3002 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |           0.0034 |           2.0399 |         -37.0887 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |           0.0059 |           1.9945 |         -37.0823 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |           0.0005 |           1.9779 |         -36.8006 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |          -0.0008 |           1.9558 |         -37.0201 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |          -0.0017 |           1.9457 |         -36.8095 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |           0.0023 |           1.9370 |         -36.8117 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |           0.0124 |           1.9197 |         -36.6800 |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |          -0.0025 |           1.9110 |         -36.5099 |
[32m[20221213 19:02:10 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.00
[32m[20221213 19:02:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:02:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:02:10 @agent_ppo2.py:143][0m Total time:      13.63 min
[32m[20221213 19:02:10 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221213 19:02:10 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 19:02:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:10 @agent_ppo2.py:185][0m |           0.0138 |          52.9181 |         -37.3956 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0077 |          53.3495 |         -37.7377 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0111 |          46.1460 |         -38.2617 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0014 |          45.0028 |         -38.0922 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |          -0.0050 |          44.5413 |         -38.4215 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0211 |          44.0435 |         -38.4712 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |          -0.0039 |          44.1972 |         -38.8619 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0069 |          43.6375 |         -38.8454 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0054 |          44.5733 |         -39.1512 |
[32m[20221213 19:02:11 @agent_ppo2.py:185][0m |           0.0007 |          43.8980 |         -39.1961 |
[32m[20221213 19:02:11 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 151.20
[32m[20221213 19:02:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 19:02:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.00
[32m[20221213 19:02:11 @agent_ppo2.py:143][0m Total time:      13.65 min
[32m[20221213 19:02:11 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 19:02:11 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 19:02:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |           0.0056 |           2.9019 |         -39.6469 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |           0.0010 |           2.1698 |         -39.4001 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |          -0.0024 |           2.0711 |         -39.0429 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |          -0.0037 |           2.0110 |         -39.1649 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |           0.0079 |           1.9727 |         -38.8146 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |          -0.0002 |           1.9474 |         -38.8259 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |          -0.0029 |           1.9253 |         -38.3358 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |           0.0064 |           1.9127 |         -38.4134 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |           0.0016 |           1.9027 |         -38.0436 |
[32m[20221213 19:02:12 @agent_ppo2.py:185][0m |          -0.0023 |           1.8944 |         -38.0703 |
[32m[20221213 19:02:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.20
[32m[20221213 19:02:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 19:02:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:02:12 @agent_ppo2.py:143][0m Total time:      13.67 min
[32m[20221213 19:02:12 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221213 19:02:12 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 19:02:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:02:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |           0.0017 |          15.9490 |         -37.5081 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |           0.0011 |          13.7857 |         -37.6034 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0012 |          13.5973 |         -37.5779 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0026 |          13.5107 |         -37.2931 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0047 |          13.4692 |         -37.4418 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |           0.0010 |          13.4535 |         -37.0906 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0024 |          13.4626 |         -37.2076 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0064 |          13.4573 |         -37.4036 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0017 |          13.3505 |         -37.2118 |
[32m[20221213 19:02:13 @agent_ppo2.py:185][0m |          -0.0022 |          13.5127 |         -37.3349 |
[32m[20221213 19:02:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.80
[32m[20221213 19:02:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:02:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 990.00
[32m[20221213 19:02:13 @agent_ppo2.py:143][0m Total time:      13.68 min
[32m[20221213 19:02:13 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 19:02:13 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
[32m[20221213 19:02:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |          -0.0002 |         190.5460 |         -37.2237 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |          -0.0031 |         188.8644 |         -36.9370 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |           0.0037 |         189.0388 |         -36.6318 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |           0.0001 |         188.0502 |         -36.2933 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |           0.0006 |         188.0120 |         -37.1536 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |          -0.0027 |         187.9964 |         -37.1153 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |          -0.0018 |         188.1348 |         -37.1024 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |           0.0034 |         189.5849 |         -37.2618 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |           0.0157 |         213.7095 |         -37.0455 |
[32m[20221213 19:02:14 @agent_ppo2.py:185][0m |           0.0351 |         231.9015 |         -36.2362 |
[32m[20221213 19:02:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:02:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:02:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 19:02:15 @agent_ppo2.py:143][0m Total time:      13.70 min
[32m[20221213 19:02:15 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221213 19:02:15 @agent_ppo2.py:121][0m #------------------------ Iteration 739 --------------------------#
[32m[20221213 19:02:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0010 |         198.9152 |         -35.5449 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0043 |         188.2299 |         -35.8127 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0040 |         187.7961 |         -35.5006 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0074 |         187.1533 |         -35.4147 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0017 |         188.4720 |         -35.5113 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0042 |         186.8537 |         -35.2786 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0047 |         186.7043 |         -35.2785 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0058 |         186.7112 |         -35.2055 |
[32m[20221213 19:02:15 @agent_ppo2.py:185][0m |          -0.0049 |         186.6179 |         -34.9564 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0050 |         186.5682 |         -35.2601 |
[32m[20221213 19:02:16 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 879.40
[32m[20221213 19:02:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 884.00
[32m[20221213 19:02:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 19:02:16 @agent_ppo2.py:143][0m Total time:      13.72 min
[32m[20221213 19:02:16 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 19:02:16 @agent_ppo2.py:121][0m #------------------------ Iteration 740 --------------------------#
[32m[20221213 19:02:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:02:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0060 |         210.6309 |         -35.5830 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0013 |         203.9215 |         -35.6443 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0006 |         203.1756 |         -36.0218 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0008 |         202.4065 |         -35.9743 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |           0.0006 |         202.7781 |         -35.9937 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0005 |         201.8064 |         -36.2612 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |          -0.0052 |         201.7735 |         -36.3023 |
[32m[20221213 19:02:16 @agent_ppo2.py:185][0m |           0.0008 |         202.3575 |         -36.4441 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |          -0.0040 |         201.6044 |         -36.8808 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |          -0.0045 |         201.2325 |         -36.9398 |
[32m[20221213 19:02:17 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.20
[32m[20221213 19:02:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 19:02:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:02:17 @agent_ppo2.py:143][0m Total time:      13.74 min
[32m[20221213 19:02:17 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221213 19:02:17 @agent_ppo2.py:121][0m #------------------------ Iteration 741 --------------------------#
[32m[20221213 19:02:17 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |           0.0183 |          10.6550 |         -37.6574 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |           0.0014 |           4.5720 |         -37.5516 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |           0.0044 |           4.0551 |         -38.3447 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |           0.0006 |           3.8283 |         -38.9071 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |          -0.0001 |           3.6110 |         -38.6169 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |          -0.0046 |           3.4839 |         -38.9109 |
[32m[20221213 19:02:17 @agent_ppo2.py:185][0m |          -0.0028 |           3.3988 |         -38.8977 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |          -0.0005 |           3.3577 |         -39.1449 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |           0.0059 |           3.3122 |         -39.0886 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |           0.0004 |           3.2636 |         -39.2264 |
[32m[20221213 19:02:18 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.40
[32m[20221213 19:02:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 19:02:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 19:02:18 @agent_ppo2.py:143][0m Total time:      13.76 min
[32m[20221213 19:02:18 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 19:02:18 @agent_ppo2.py:121][0m #------------------------ Iteration 742 --------------------------#
[32m[20221213 19:02:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |           0.0020 |           4.1609 |         -39.6196 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |          -0.0031 |           2.1789 |         -39.2703 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |          -0.0021 |           2.0886 |         -38.9781 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |          -0.0078 |           2.0330 |         -38.6558 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |          -0.0004 |           2.0075 |         -38.2157 |
[32m[20221213 19:02:18 @agent_ppo2.py:185][0m |           0.0044 |           1.9864 |         -38.0156 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |           0.0001 |           1.9808 |         -38.0196 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |          -0.0013 |           1.9634 |         -37.7401 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |          -0.0002 |           1.9544 |         -37.5743 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |          -0.0001 |           1.9556 |         -37.3473 |
[32m[20221213 19:02:19 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.60
[32m[20221213 19:02:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 67.00
[32m[20221213 19:02:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 19:02:19 @agent_ppo2.py:143][0m Total time:      13.77 min
[32m[20221213 19:02:19 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221213 19:02:19 @agent_ppo2.py:121][0m #------------------------ Iteration 743 --------------------------#
[32m[20221213 19:02:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |           0.0034 |          19.7549 |         -36.6254 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |           0.0008 |          14.4837 |         -36.7023 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |           0.0039 |          13.9926 |         -36.4430 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |           0.0040 |          13.8024 |         -36.5772 |
[32m[20221213 19:02:19 @agent_ppo2.py:185][0m |          -0.0042 |          13.7732 |         -36.2916 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |           0.0012 |          13.6481 |         -36.0936 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |           0.0025 |          14.3077 |         -36.0076 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |          -0.0009 |          13.7178 |         -35.9372 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |           0.0012 |          13.6019 |         -36.1846 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |          -0.0007 |          13.6746 |         -36.0021 |
[32m[20221213 19:02:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.00
[32m[20221213 19:02:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:02:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 19:02:20 @agent_ppo2.py:143][0m Total time:      13.79 min
[32m[20221213 19:02:20 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 19:02:20 @agent_ppo2.py:121][0m #------------------------ Iteration 744 --------------------------#
[32m[20221213 19:02:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |           0.0010 |         195.7831 |         -35.1208 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |           0.0027 |         190.2197 |         -35.2030 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |          -0.0007 |         189.5197 |         -34.5640 |
[32m[20221213 19:02:20 @agent_ppo2.py:185][0m |           0.0018 |         189.5170 |         -34.5887 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0010 |         189.3094 |         -34.4753 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0017 |         189.1630 |         -34.2532 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0011 |         189.0619 |         -34.0749 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0002 |         188.7865 |         -33.6688 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0000 |         188.6464 |         -33.8816 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0031 |         194.7837 |         -33.5297 |
[32m[20221213 19:02:21 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.80
[32m[20221213 19:02:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.00
[32m[20221213 19:02:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.00
[32m[20221213 19:02:21 @agent_ppo2.py:143][0m Total time:      13.81 min
[32m[20221213 19:02:21 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221213 19:02:21 @agent_ppo2.py:121][0m #------------------------ Iteration 745 --------------------------#
[32m[20221213 19:02:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0025 |          95.2100 |         -32.6804 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0029 |          91.7496 |         -32.7890 |
[32m[20221213 19:02:21 @agent_ppo2.py:185][0m |          -0.0010 |          91.0446 |         -32.9837 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |          -0.0061 |          91.1050 |         -33.0262 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |          -0.0016 |          90.7764 |         -33.2686 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |           0.0004 |          90.3973 |         -33.4649 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |          -0.0014 |          90.3548 |         -33.5659 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |           0.0007 |          90.8021 |         -33.8699 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |          -0.0030 |          89.8808 |         -33.7979 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |          -0.0049 |          89.8499 |         -33.6304 |
[32m[20221213 19:02:22 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.20
[32m[20221213 19:02:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.00
[32m[20221213 19:02:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:02:22 @agent_ppo2.py:143][0m Total time:      13.83 min
[32m[20221213 19:02:22 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 19:02:22 @agent_ppo2.py:121][0m #------------------------ Iteration 746 --------------------------#
[32m[20221213 19:02:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |          -0.0032 |         201.8968 |         -35.5401 |
[32m[20221213 19:02:22 @agent_ppo2.py:185][0m |           0.0009 |         198.2560 |         -35.4214 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |           0.0003 |         198.8049 |         -35.5149 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |          -0.0023 |         197.8245 |         -35.6329 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |          -0.0000 |         197.7296 |         -35.3954 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |          -0.0020 |         197.6663 |         -35.5107 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |          -0.0012 |         197.3979 |         -35.3840 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |           0.0038 |         199.9186 |         -35.5022 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |           0.0102 |         215.1424 |         -35.2839 |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |           0.0005 |         197.4251 |         -34.8217 |
[32m[20221213 19:02:23 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:02:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.80
[32m[20221213 19:02:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.00
[32m[20221213 19:02:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:02:23 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221213 19:02:23 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221213 19:02:23 @agent_ppo2.py:121][0m #------------------------ Iteration 747 --------------------------#
[32m[20221213 19:02:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:02:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:23 @agent_ppo2.py:185][0m |           0.0115 |          16.9673 |         -34.3083 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |           0.0013 |           6.0357 |         -34.7531 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |           0.0082 |           5.4575 |         -34.8057 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |          -0.0006 |           5.2434 |         -34.7641 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |          -0.0005 |           5.0700 |         -34.7890 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |          -0.0055 |           4.9084 |         -34.8476 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |          -0.0015 |           5.1219 |         -34.9178 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |           0.0087 |           4.7599 |         -34.7226 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |          -0.0055 |           4.7380 |         -34.4790 |
[32m[20221213 19:02:24 @agent_ppo2.py:185][0m |           0.0018 |           4.6952 |         -34.6784 |
[32m[20221213 19:02:24 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.40
[32m[20221213 19:02:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 19:02:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 19:02:24 @agent_ppo2.py:143][0m Total time:      13.86 min
[32m[20221213 19:02:24 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 19:02:24 @agent_ppo2.py:121][0m #------------------------ Iteration 748 --------------------------#
[32m[20221213 19:02:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0031 |         236.5600 |         -35.2309 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |          -0.0003 |         228.8937 |         -35.8924 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0004 |         227.2129 |         -36.1841 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0033 |         227.7447 |         -36.3880 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0136 |         260.5628 |         -36.9331 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0007 |         226.9156 |         -37.2679 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0004 |         226.5417 |         -37.5627 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |          -0.0027 |         226.5842 |         -37.8477 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |          -0.0019 |         226.3293 |         -38.2276 |
[32m[20221213 19:02:25 @agent_ppo2.py:185][0m |           0.0010 |         226.1625 |         -38.2708 |
[32m[20221213 19:02:25 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:02:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.60
[32m[20221213 19:02:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:02:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 19:02:25 @agent_ppo2.py:143][0m Total time:      13.88 min
[32m[20221213 19:02:25 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221213 19:02:25 @agent_ppo2.py:121][0m #------------------------ Iteration 749 --------------------------#
[32m[20221213 19:02:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |           0.0027 |         152.3588 |         -39.5098 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |          -0.0032 |         146.1189 |         -39.0334 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |           0.0015 |         144.5593 |         -38.6119 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |           0.0053 |         145.8270 |         -38.3143 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |          -0.0023 |         143.8567 |         -37.9905 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |          -0.0017 |         142.7026 |         -37.6197 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |          -0.0027 |         142.3411 |         -37.1292 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |          -0.0040 |         141.8741 |         -37.2038 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |          -0.0001 |         146.2178 |         -36.8249 |
[32m[20221213 19:02:26 @agent_ppo2.py:185][0m |           0.0014 |         141.4953 |         -36.5998 |
[32m[20221213 19:02:26 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.00
[32m[20221213 19:02:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 19:02:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:02:26 @agent_ppo2.py:143][0m Total time:      13.90 min
[32m[20221213 19:02:26 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 19:02:26 @agent_ppo2.py:121][0m #------------------------ Iteration 750 --------------------------#
[32m[20221213 19:02:27 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:02:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |           0.0024 |           3.7432 |         -36.8058 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |           0.0014 |           2.3569 |         -37.0906 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |          -0.0007 |           2.2439 |         -37.2499 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |           0.0016 |           2.1880 |         -37.3729 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |          -0.0018 |           2.1466 |         -37.6218 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |          -0.0072 |           2.1219 |         -37.5472 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |          -0.0050 |           2.0917 |         -37.6758 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |           0.0065 |           2.0676 |         -37.6264 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |          -0.0026 |           2.0491 |         -37.2961 |
[32m[20221213 19:02:27 @agent_ppo2.py:185][0m |          -0.0040 |           2.0369 |         -37.3526 |
[32m[20221213 19:02:27 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.40
[32m[20221213 19:02:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:02:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:02:28 @agent_ppo2.py:143][0m Total time:      13.92 min
[32m[20221213 19:02:28 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221213 19:02:28 @agent_ppo2.py:121][0m #------------------------ Iteration 751 --------------------------#
[32m[20221213 19:02:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |           0.0127 |          27.7362 |         -37.1333 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |           0.0027 |          17.5625 |         -37.2251 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |           0.0023 |          16.9974 |         -36.8587 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |          -0.0012 |          16.8067 |         -36.4606 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |          -0.0076 |          16.6204 |         -36.4957 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |          -0.0012 |          16.6101 |         -36.2911 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |          -0.0012 |          16.5481 |         -36.2651 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |          -0.0024 |          16.4343 |         -35.8815 |
[32m[20221213 19:02:28 @agent_ppo2.py:185][0m |          -0.0014 |          16.4521 |         -35.9506 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |          -0.0036 |          16.3917 |         -35.7656 |
[32m[20221213 19:02:29 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.20
[32m[20221213 19:02:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 19:02:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:02:29 @agent_ppo2.py:143][0m Total time:      13.94 min
[32m[20221213 19:02:29 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 19:02:29 @agent_ppo2.py:121][0m #------------------------ Iteration 752 --------------------------#
[32m[20221213 19:02:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |           0.0010 |          71.4729 |         -34.6482 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |          -0.0060 |          67.1828 |         -34.4807 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |          -0.0013 |          65.5356 |         -34.3617 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |           0.0059 |          64.8221 |         -33.9109 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |          -0.0015 |          63.8293 |         -33.9325 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |           0.0022 |          63.6777 |         -34.3335 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |          -0.0012 |          63.2554 |         -34.0995 |
[32m[20221213 19:02:29 @agent_ppo2.py:185][0m |          -0.0001 |          63.3124 |         -33.8285 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0049 |          62.7060 |         -34.0798 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0015 |          62.5815 |         -33.7040 |
[32m[20221213 19:02:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:02:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.00
[32m[20221213 19:02:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.00
[32m[20221213 19:02:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:02:30 @agent_ppo2.py:143][0m Total time:      13.96 min
[32m[20221213 19:02:30 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221213 19:02:30 @agent_ppo2.py:121][0m #------------------------ Iteration 753 --------------------------#
[32m[20221213 19:02:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |           0.0103 |         108.7751 |         -33.4645 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0030 |         104.0510 |         -33.4494 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0041 |         102.7762 |         -33.4525 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0007 |         101.9269 |         -33.3528 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0040 |         101.3723 |         -33.5479 |
[32m[20221213 19:02:30 @agent_ppo2.py:185][0m |          -0.0030 |         100.8358 |         -33.9859 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |          -0.0076 |         101.2969 |         -33.9723 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |          -0.0021 |         100.2590 |         -34.0347 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |          -0.0067 |          99.9569 |         -34.2772 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |          -0.0021 |          99.6252 |         -34.1872 |
[32m[20221213 19:02:31 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:02:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.00
[32m[20221213 19:02:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.00
[32m[20221213 19:02:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:02:31 @agent_ppo2.py:143][0m Total time:      13.97 min
[32m[20221213 19:02:31 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 19:02:31 @agent_ppo2.py:121][0m #------------------------ Iteration 754 --------------------------#
[32m[20221213 19:02:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |           0.0068 |          22.5864 |         -37.2391 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |           0.0117 |           8.1261 |         -37.2453 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |           0.0006 |           7.2634 |         -37.3267 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |           0.0008 |           6.8032 |         -37.3849 |
[32m[20221213 19:02:31 @agent_ppo2.py:185][0m |           0.0028 |           6.5541 |         -37.4052 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |           0.0045 |           6.4487 |         -37.4972 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |           0.0067 |           6.4050 |         -37.5711 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |           0.0061 |           6.2116 |         -37.3792 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |          -0.0012 |           6.0345 |         -37.4300 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |           0.0012 |           5.9259 |         -37.6668 |
[32m[20221213 19:02:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:02:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.40
[32m[20221213 19:02:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 19:02:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:02:32 @agent_ppo2.py:143][0m Total time:      13.99 min
[32m[20221213 19:02:32 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221213 19:02:32 @agent_ppo2.py:121][0m #------------------------ Iteration 755 --------------------------#
[32m[20221213 19:02:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |           0.0030 |           8.8589 |         -38.6011 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |          -0.0022 |           4.8569 |         -38.8392 |
[32m[20221213 19:02:32 @agent_ppo2.py:185][0m |          -0.0015 |           4.4885 |         -38.6718 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |          -0.0043 |           4.2852 |         -38.5638 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |          -0.0049 |           4.1632 |         -38.8483 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |          -0.0091 |           4.1445 |         -38.6351 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |          -0.0051 |           4.0668 |         -38.5518 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |          -0.0025 |           4.0346 |         -38.4047 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |          -0.0006 |           4.0259 |         -38.6760 |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |           0.0053 |           3.9614 |         -38.6610 |
[32m[20221213 19:02:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:02:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.20
[32m[20221213 19:02:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:02:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 19:02:33 @agent_ppo2.py:143][0m Total time:      14.01 min
[32m[20221213 19:02:33 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 19:02:33 @agent_ppo2.py:121][0m #------------------------ Iteration 756 --------------------------#
[32m[20221213 19:02:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:33 @agent_ppo2.py:185][0m |           0.0004 |           3.3703 |         -36.5809 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |           0.0081 |           2.0434 |         -36.6573 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |          -0.0011 |           1.9857 |         -36.7592 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |           0.0040 |           1.9604 |         -36.8129 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |           0.0058 |           1.9433 |         -36.6640 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |          -0.0026 |           1.9310 |         -36.6129 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |           0.0034 |           1.9229 |         -36.4995 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |           0.0069 |           1.9158 |         -36.3569 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |          -0.0012 |           1.9114 |         -36.3427 |
[32m[20221213 19:02:34 @agent_ppo2.py:185][0m |          -0.0059 |           1.9104 |         -36.6640 |
[32m[20221213 19:02:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:02:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.00
[32m[20221213 19:02:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:02:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:02:34 @agent_ppo2.py:143][0m Total time:      14.03 min
[32m[20221213 19:02:34 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221213 19:02:34 @agent_ppo2.py:121][0m #------------------------ Iteration 757 --------------------------#
[32m[20221213 19:02:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:02:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |           0.0084 |           3.7646 |         -38.3009 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |           0.0067 |           2.1595 |         -38.0116 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0040 |           2.0957 |         -37.9545 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |           0.0084 |           2.0674 |         -38.0655 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0047 |           2.0504 |         -38.1943 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0063 |           2.0351 |         -38.4158 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0045 |           2.0237 |         -38.3081 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0028 |           2.0165 |         -38.2037 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0008 |           2.0076 |         -38.0229 |
[32m[20221213 19:02:35 @agent_ppo2.py:185][0m |          -0.0047 |           1.9948 |         -38.4367 |
[32m[20221213 19:02:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:02:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.80
[32m[20221213 19:02:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 19:02:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 19:02:35 @agent_ppo2.py:143][0m Total time:      14.05 min
[32m[20221213 19:02:35 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 19:02:35 @agent_ppo2.py:121][0m #------------------------ Iteration 758 --------------------------#
[32m[20221213 19:02:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |          -0.0040 |         180.4415 |         -38.5681 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |           0.0091 |         177.7727 |         -38.4588 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |          -0.0014 |         174.5712 |         -39.0977 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |           0.0006 |         174.6877 |         -39.6267 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |           0.0126 |         174.0668 |         -39.1036 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |           0.0100 |         199.8533 |         -40.1707 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |           0.0112 |         174.3589 |         -39.1316 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |           0.0007 |         174.4461 |         -40.6773 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |          -0.0040 |         174.3456 |         -40.6555 |
[32m[20221213 19:02:36 @agent_ppo2.py:185][0m |          -0.0011 |         174.4769 |         -40.9362 |
[32m[20221213 19:02:36 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.40
[32m[20221213 19:02:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 889.00
[32m[20221213 19:02:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 19:02:37 @agent_ppo2.py:143][0m Total time:      14.07 min
[32m[20221213 19:02:37 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221213 19:02:37 @agent_ppo2.py:121][0m #------------------------ Iteration 759 --------------------------#
[32m[20221213 19:02:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |           0.0025 |          16.1823 |         -42.6255 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0012 |          11.4951 |         -42.5602 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0013 |          11.1501 |         -42.4556 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0022 |          10.9756 |         -42.5241 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0056 |          10.8608 |         -42.1276 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |           0.0025 |          10.8147 |         -41.6591 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |           0.0042 |          11.4459 |         -41.4918 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0024 |          10.8607 |         -41.3703 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0067 |          10.7088 |         -41.5340 |
[32m[20221213 19:02:37 @agent_ppo2.py:185][0m |          -0.0042 |          10.6607 |         -41.3785 |
[32m[20221213 19:02:37 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.60
[32m[20221213 19:02:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:02:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 19:02:38 @agent_ppo2.py:143][0m Total time:      14.09 min
[32m[20221213 19:02:38 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 19:02:38 @agent_ppo2.py:121][0m #------------------------ Iteration 760 --------------------------#
[32m[20221213 19:02:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:02:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0025 |         187.7307 |         -40.1908 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0027 |         183.1860 |         -39.6559 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0029 |         182.2391 |         -39.7678 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0153 |         203.2914 |         -40.1164 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0041 |         181.7236 |         -40.2089 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0026 |         182.1921 |         -40.2670 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |           0.0037 |         181.5889 |         -40.3139 |
[32m[20221213 19:02:38 @agent_ppo2.py:185][0m |          -0.0011 |         181.4377 |         -40.7331 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |           0.0026 |         182.3146 |         -40.5145 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |           0.0007 |         181.4749 |         -40.3792 |
[32m[20221213 19:02:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.00
[32m[20221213 19:02:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.00
[32m[20221213 19:02:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 19:02:39 @agent_ppo2.py:143][0m Total time:      14.10 min
[32m[20221213 19:02:39 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221213 19:02:39 @agent_ppo2.py:121][0m #------------------------ Iteration 761 --------------------------#
[32m[20221213 19:02:39 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |           0.0116 |          19.0637 |         -41.8593 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |           0.0095 |          14.1227 |         -42.6651 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |           0.0040 |          13.8777 |         -43.0717 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |          -0.0015 |          13.2360 |         -42.9535 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |           0.0059 |          13.2124 |         -43.1256 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |          -0.0034 |          13.1674 |         -43.4686 |
[32m[20221213 19:02:39 @agent_ppo2.py:185][0m |          -0.0065 |          13.2047 |         -43.5315 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |           0.0002 |          13.1458 |         -43.7526 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |           0.0066 |          13.2795 |         -43.6869 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |           0.0008 |          13.1695 |         -43.4577 |
[32m[20221213 19:02:40 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.00
[32m[20221213 19:02:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 19:02:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 19:02:40 @agent_ppo2.py:143][0m Total time:      14.12 min
[32m[20221213 19:02:40 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 19:02:40 @agent_ppo2.py:121][0m #------------------------ Iteration 762 --------------------------#
[32m[20221213 19:02:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |           0.0051 |          14.1994 |         -42.6547 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |           0.0012 |           3.9066 |         -42.2756 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |          -0.0045 |           3.4419 |         -42.3154 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |          -0.0069 |           3.4050 |         -41.9376 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |          -0.0011 |           3.2206 |         -41.7609 |
[32m[20221213 19:02:40 @agent_ppo2.py:185][0m |           0.0025 |           3.1555 |         -41.7129 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |          -0.0069 |           3.1177 |         -41.6463 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |          -0.0037 |           3.1106 |         -41.3019 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |          -0.0015 |           3.0827 |         -41.4226 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |           0.0010 |           3.0838 |         -41.4474 |
[32m[20221213 19:02:41 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.40
[32m[20221213 19:02:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 19:02:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:02:41 @agent_ppo2.py:143][0m Total time:      14.14 min
[32m[20221213 19:02:41 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221213 19:02:41 @agent_ppo2.py:121][0m #------------------------ Iteration 763 --------------------------#
[32m[20221213 19:02:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |           0.0099 |         206.0040 |         -41.4597 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |           0.0217 |         187.5489 |         -38.8630 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |           0.0047 |         190.7515 |         -41.8657 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |           0.0004 |         187.6881 |         -41.6245 |
[32m[20221213 19:02:41 @agent_ppo2.py:185][0m |          -0.0008 |         187.2255 |         -41.8614 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |          -0.0044 |         187.2236 |         -41.5153 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0135 |         206.7323 |         -41.6985 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0003 |         187.5492 |         -41.5131 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0003 |         186.9845 |         -41.6782 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0108 |         197.9273 |         -41.4980 |
[32m[20221213 19:02:42 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.20
[32m[20221213 19:02:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.00
[32m[20221213 19:02:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:02:42 @agent_ppo2.py:143][0m Total time:      14.16 min
[32m[20221213 19:02:42 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 19:02:42 @agent_ppo2.py:121][0m #------------------------ Iteration 764 --------------------------#
[32m[20221213 19:02:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0044 |          10.1020 |         -41.6214 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |          -0.0046 |           4.2410 |         -41.1412 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0035 |           4.2876 |         -41.1766 |
[32m[20221213 19:02:42 @agent_ppo2.py:185][0m |           0.0015 |           3.9948 |         -40.8055 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |          -0.0044 |           3.9287 |         -40.5917 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |           0.0038 |           3.9066 |         -40.7240 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |           0.0050 |           3.8761 |         -40.5645 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |          -0.0010 |           3.8713 |         -40.5869 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |          -0.0062 |           3.8557 |         -40.4608 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |          -0.0014 |           3.8526 |         -40.5454 |
[32m[20221213 19:02:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.40
[32m[20221213 19:02:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:02:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 19:02:43 @agent_ppo2.py:143][0m Total time:      14.18 min
[32m[20221213 19:02:43 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221213 19:02:43 @agent_ppo2.py:121][0m #------------------------ Iteration 765 --------------------------#
[32m[20221213 19:02:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |           0.0057 |           7.2196 |         -40.5311 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |          -0.0017 |           4.0432 |         -40.2786 |
[32m[20221213 19:02:43 @agent_ppo2.py:185][0m |          -0.0023 |           3.9198 |         -40.1704 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0011 |           3.8375 |         -40.1173 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0015 |           3.7709 |         -40.2005 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0059 |           3.7454 |         -39.9889 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0051 |           3.7188 |         -40.0094 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0029 |           3.6767 |         -39.8791 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0023 |           3.7250 |         -40.0719 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |          -0.0099 |           3.6595 |         -39.8891 |
[32m[20221213 19:02:44 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.40
[32m[20221213 19:02:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.00
[32m[20221213 19:02:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.00
[32m[20221213 19:02:44 @agent_ppo2.py:143][0m Total time:      14.19 min
[32m[20221213 19:02:44 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 19:02:44 @agent_ppo2.py:121][0m #------------------------ Iteration 766 --------------------------#
[32m[20221213 19:02:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |           0.0015 |          92.2825 |         -39.4805 |
[32m[20221213 19:02:44 @agent_ppo2.py:185][0m |           0.0016 |          88.2339 |         -39.5448 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |          -0.0009 |          87.6284 |         -39.3126 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |           0.0004 |          87.3265 |         -39.3392 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |           0.0003 |          87.1630 |         -39.2248 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |          -0.0016 |          86.8891 |         -38.8953 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |          -0.0015 |          87.0065 |         -38.8493 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |          -0.0022 |          87.1712 |         -39.2174 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |          -0.0001 |          86.2972 |         -38.8873 |
[32m[20221213 19:02:45 @agent_ppo2.py:185][0m |          -0.0024 |          86.3806 |         -38.9710 |
[32m[20221213 19:02:45 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.80
[32m[20221213 19:02:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 19:02:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 19:02:45 @agent_ppo2.py:143][0m Total time:      14.21 min
[32m[20221213 19:02:45 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221213 19:02:45 @agent_ppo2.py:121][0m #------------------------ Iteration 767 --------------------------#
[32m[20221213 19:02:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:02:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0015 |         193.8335 |         -39.2167 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |          -0.0013 |         193.1555 |         -39.2516 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |          -0.0001 |         193.0628 |         -39.4515 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0079 |         193.4189 |         -39.0039 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0130 |         212.8956 |         -39.5715 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0035 |         192.9771 |         -38.9953 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |          -0.0008 |         192.5474 |         -39.3604 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0062 |         194.2903 |         -39.2303 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0112 |         207.0491 |         -39.6601 |
[32m[20221213 19:02:46 @agent_ppo2.py:185][0m |           0.0002 |         192.5649 |         -39.4053 |
[32m[20221213 19:02:46 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:02:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:02:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:02:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 19:02:46 @agent_ppo2.py:143][0m Total time:      14.23 min
[32m[20221213 19:02:46 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 19:02:46 @agent_ppo2.py:121][0m #------------------------ Iteration 768 --------------------------#
[32m[20221213 19:02:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0034 |         100.2568 |         -40.0245 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0020 |          97.5351 |         -39.8898 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0030 |          96.8273 |         -39.5036 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |          -0.0032 |          97.0453 |         -40.2240 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0055 |          96.5222 |         -40.1217 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |          -0.0024 |          95.8803 |         -40.2036 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0047 |          97.5722 |         -39.9620 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0025 |          95.6620 |         -40.1244 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |          -0.0007 |          95.4425 |         -40.0798 |
[32m[20221213 19:02:47 @agent_ppo2.py:185][0m |           0.0044 |          95.0138 |         -39.8795 |
[32m[20221213 19:02:47 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:02:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.00
[32m[20221213 19:02:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.00
[32m[20221213 19:02:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 19:02:47 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221213 19:02:47 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221213 19:02:47 @agent_ppo2.py:121][0m #------------------------ Iteration 769 --------------------------#
[32m[20221213 19:02:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |           0.0014 |         104.2639 |         -39.7379 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |          -0.0008 |          99.6985 |         -39.9867 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |           0.0013 |          98.8890 |         -40.3317 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |           0.0062 |         102.5522 |         -40.3348 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |          -0.0008 |          97.5848 |         -40.3453 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |           0.0184 |         121.0535 |         -40.6028 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |           0.0002 |          97.0069 |         -40.6678 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |          -0.0029 |          97.1712 |         -40.6456 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |          -0.0107 |          96.9698 |         -40.7113 |
[32m[20221213 19:02:48 @agent_ppo2.py:185][0m |           0.0052 |          97.0759 |         -40.6080 |
[32m[20221213 19:02:48 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.80
[32m[20221213 19:02:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 19:02:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 19:02:48 @agent_ppo2.py:143][0m Total time:      14.27 min
[32m[20221213 19:02:48 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 19:02:48 @agent_ppo2.py:121][0m #------------------------ Iteration 770 --------------------------#
[32m[20221213 19:02:49 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:02:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |           0.0079 |         205.3177 |         -41.3905 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |          -0.0014 |         204.6167 |         -42.5464 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |           0.0025 |         204.5077 |         -42.4323 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |          -0.0005 |         204.9524 |         -43.1672 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |           0.0001 |         205.2627 |         -43.8071 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |          -0.0022 |         204.4221 |         -44.0586 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |           0.0091 |         211.3128 |         -44.0250 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |           0.0014 |         204.9900 |         -44.3600 |
[32m[20221213 19:02:49 @agent_ppo2.py:185][0m |           0.0016 |         205.0574 |         -44.8547 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |          -0.0040 |         204.5557 |         -45.2281 |
[32m[20221213 19:02:50 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:02:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:02:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 19:02:50 @agent_ppo2.py:143][0m Total time:      14.29 min
[32m[20221213 19:02:50 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221213 19:02:50 @agent_ppo2.py:121][0m #------------------------ Iteration 771 --------------------------#
[32m[20221213 19:02:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |           0.0094 |          11.1238 |         -46.8041 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |           0.0096 |           7.6008 |         -46.6442 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |           0.0104 |           7.0972 |         -46.7789 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |           0.0018 |           6.8303 |         -46.9301 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |          -0.0036 |           6.6630 |         -46.8723 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |           0.0052 |           6.5742 |         -46.9783 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |          -0.0063 |           6.4908 |         -47.2205 |
[32m[20221213 19:02:50 @agent_ppo2.py:185][0m |           0.0035 |           6.4250 |         -46.9815 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0015 |           6.3825 |         -47.1515 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0008 |           6.3596 |         -46.8699 |
[32m[20221213 19:02:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:02:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.80
[32m[20221213 19:02:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 19:02:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:02:51 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221213 19:02:51 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 19:02:51 @agent_ppo2.py:121][0m #------------------------ Iteration 772 --------------------------#
[32m[20221213 19:02:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0137 |         226.1788 |         -47.0611 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0088 |         208.1998 |         -47.0027 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0013 |         208.0232 |         -47.9916 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |          -0.0003 |         207.8760 |         -47.5829 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0016 |         207.6927 |         -47.5970 |
[32m[20221213 19:02:51 @agent_ppo2.py:185][0m |           0.0006 |         208.9255 |         -48.1633 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |           0.0025 |         207.6888 |         -48.2537 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |           0.0018 |         207.9114 |         -48.3442 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |           0.0040 |         207.5353 |         -48.4883 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |          -0.0000 |         207.5992 |         -48.4827 |
[32m[20221213 19:02:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:02:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:02:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:02:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 19:02:52 @agent_ppo2.py:143][0m Total time:      14.32 min
[32m[20221213 19:02:52 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221213 19:02:52 @agent_ppo2.py:121][0m #------------------------ Iteration 773 --------------------------#
[32m[20221213 19:02:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |           0.0155 |         196.7116 |         -48.8880 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |          -0.0022 |         177.7471 |         -49.0133 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |          -0.0037 |         177.0335 |         -48.7142 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |          -0.0015 |         176.9282 |         -48.4837 |
[32m[20221213 19:02:52 @agent_ppo2.py:185][0m |           0.0128 |         198.0974 |         -48.3978 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |           0.0078 |         188.5062 |         -48.2328 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |          -0.0020 |         175.8924 |         -48.3407 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |          -0.0031 |         176.1961 |         -48.3308 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |          -0.0044 |         175.6107 |         -48.2865 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |           0.0136 |         203.0771 |         -47.5431 |
[32m[20221213 19:02:53 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:02:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.40
[32m[20221213 19:02:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.00
[32m[20221213 19:02:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:02:53 @agent_ppo2.py:143][0m Total time:      14.34 min
[32m[20221213 19:02:53 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 19:02:53 @agent_ppo2.py:121][0m #------------------------ Iteration 774 --------------------------#
[32m[20221213 19:02:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |           0.0123 |         218.2991 |         -45.2625 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |           0.0004 |         214.6960 |         -47.1823 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |           0.0004 |         214.1327 |         -47.0414 |
[32m[20221213 19:02:53 @agent_ppo2.py:185][0m |          -0.0014 |         213.8649 |         -47.3889 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |           0.0106 |         224.5817 |         -47.9381 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |          -0.0022 |         213.7195 |         -48.2030 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |           0.0080 |         224.5735 |         -48.0335 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |           0.0007 |         215.3756 |         -48.5290 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |          -0.0020 |         213.3436 |         -48.7470 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |          -0.0004 |         213.3464 |         -48.5323 |
[32m[20221213 19:02:54 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:02:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.20
[32m[20221213 19:02:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 930.00
[32m[20221213 19:02:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:02:54 @agent_ppo2.py:143][0m Total time:      14.36 min
[32m[20221213 19:02:54 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221213 19:02:54 @agent_ppo2.py:121][0m #------------------------ Iteration 775 --------------------------#
[32m[20221213 19:02:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |           0.0124 |          13.8319 |         -50.0016 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |           0.0004 |           6.6096 |         -49.3759 |
[32m[20221213 19:02:54 @agent_ppo2.py:185][0m |          -0.0025 |           5.9611 |         -49.6202 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |           0.0013 |           5.6747 |         -48.8847 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |           0.0048 |           5.5144 |         -48.6938 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |           0.0009 |           5.2580 |         -47.9597 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |           0.0036 |           5.2131 |         -47.9559 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |          -0.0056 |           5.1835 |         -48.1522 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |          -0.0082 |           5.1321 |         -47.7392 |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |          -0.0043 |           5.1179 |         -47.6887 |
[32m[20221213 19:02:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:02:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.40
[32m[20221213 19:02:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 48.00
[32m[20221213 19:02:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 19:02:55 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221213 19:02:55 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 19:02:55 @agent_ppo2.py:121][0m #------------------------ Iteration 776 --------------------------#
[32m[20221213 19:02:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:55 @agent_ppo2.py:185][0m |           0.0122 |          22.7175 |         -45.7999 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |           0.0162 |          11.9148 |         -45.6701 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |           0.0017 |          11.3137 |         -45.4634 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |          -0.0024 |          11.1482 |         -45.4432 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |           0.0103 |          10.8102 |         -45.3644 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |          -0.0011 |          11.5514 |         -45.7694 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |           0.0076 |          11.5811 |         -45.3887 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |           0.0018 |          10.6103 |         -45.3295 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |          -0.0021 |          10.4725 |         -45.3648 |
[32m[20221213 19:02:56 @agent_ppo2.py:185][0m |          -0.0027 |          10.4495 |         -45.2061 |
[32m[20221213 19:02:56 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:02:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.00
[32m[20221213 19:02:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 19:02:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:02:56 @agent_ppo2.py:143][0m Total time:      14.40 min
[32m[20221213 19:02:56 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221213 19:02:56 @agent_ppo2.py:121][0m #------------------------ Iteration 777 --------------------------#
[32m[20221213 19:02:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:02:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0003 |         123.6399 |         -44.2432 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0001 |         115.6560 |         -44.5156 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |           0.0022 |         113.1170 |         -44.7874 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0019 |         111.5995 |         -45.1025 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0003 |         110.6306 |         -44.7673 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |           0.0021 |         109.2860 |         -45.0366 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0020 |         109.3062 |         -45.3856 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0040 |         108.1811 |         -45.2372 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |           0.0023 |         107.8496 |         -45.5035 |
[32m[20221213 19:02:57 @agent_ppo2.py:185][0m |          -0.0029 |         107.4261 |         -45.4808 |
[32m[20221213 19:02:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:02:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.80
[32m[20221213 19:02:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.00
[32m[20221213 19:02:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 19:02:57 @agent_ppo2.py:143][0m Total time:      14.42 min
[32m[20221213 19:02:57 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 19:02:57 @agent_ppo2.py:121][0m #------------------------ Iteration 778 --------------------------#
[32m[20221213 19:02:58 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:02:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |           0.0054 |         226.0284 |         -48.1322 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |           0.0066 |         217.7333 |         -47.9698 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |          -0.0038 |         216.7507 |         -48.9354 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |           0.0018 |         215.9395 |         -49.1658 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |           0.0046 |         219.3141 |         -49.5869 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |           0.0006 |         213.9641 |         -49.3314 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |          -0.0020 |         213.4870 |         -50.1143 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |          -0.0001 |         213.1117 |         -49.9682 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |           0.0033 |         212.7788 |         -49.8838 |
[32m[20221213 19:02:58 @agent_ppo2.py:185][0m |          -0.0011 |         212.5996 |         -49.7695 |
[32m[20221213 19:02:58 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:02:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 664.00
[32m[20221213 19:02:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.00
[32m[20221213 19:02:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.00
[32m[20221213 19:02:59 @agent_ppo2.py:143][0m Total time:      14.44 min
[32m[20221213 19:02:59 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221213 19:02:59 @agent_ppo2.py:121][0m #------------------------ Iteration 779 --------------------------#
[32m[20221213 19:02:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:02:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:02:59 @agent_ppo2.py:185][0m |           0.0022 |         149.2067 |         -49.9118 |
[32m[20221213 19:02:59 @agent_ppo2.py:185][0m |          -0.0028 |         139.2020 |         -50.1188 |
[32m[20221213 19:02:59 @agent_ppo2.py:185][0m |          -0.0012 |         136.4220 |         -50.1600 |
[32m[20221213 19:02:59 @agent_ppo2.py:185][0m |           0.0020 |         135.7412 |         -49.9942 |
[32m[20221213 19:02:59 @agent_ppo2.py:185][0m |           0.0025 |         134.8684 |         -50.0058 |
[32m[20221213 19:02:59 @agent_ppo2.py:185][0m |           0.0001 |         133.8786 |         -50.1926 |
[32m[20221213 19:03:00 @agent_ppo2.py:185][0m |           0.0084 |         155.3340 |         -49.9730 |
[32m[20221213 19:03:00 @agent_ppo2.py:185][0m |          -0.0006 |         134.0399 |         -49.9017 |
[32m[20221213 19:03:00 @agent_ppo2.py:185][0m |          -0.0021 |         133.1674 |         -49.6643 |
[32m[20221213 19:03:00 @agent_ppo2.py:185][0m |           0.0019 |         132.4389 |         -49.5091 |
[32m[20221213 19:03:00 @agent_ppo2.py:130][0m Policy update time: 1.32 s
[32m[20221213 19:03:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.40
[32m[20221213 19:03:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 667.00
[32m[20221213 19:03:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 19:03:00 @agent_ppo2.py:143][0m Total time:      14.46 min
[32m[20221213 19:03:00 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 19:03:00 @agent_ppo2.py:121][0m #------------------------ Iteration 780 --------------------------#
[32m[20221213 19:03:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 19:03:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |           0.0102 |         207.7085 |         -50.6710 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |           0.0030 |         178.7570 |         -49.8619 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |          -0.0005 |         177.0463 |         -50.6026 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |          -0.0022 |         175.9064 |         -50.8210 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |           0.0001 |         175.3052 |         -50.7955 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |           0.0107 |         179.3413 |         -51.0113 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |          -0.0006 |         175.1728 |         -50.4841 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |           0.0028 |         175.0480 |         -50.4356 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |           0.0028 |         174.5908 |         -50.2859 |
[32m[20221213 19:03:01 @agent_ppo2.py:185][0m |          -0.0030 |         174.6911 |         -50.4687 |
[32m[20221213 19:03:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:03:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.60
[32m[20221213 19:03:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.00
[32m[20221213 19:03:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 19:03:01 @agent_ppo2.py:143][0m Total time:      14.48 min
[32m[20221213 19:03:01 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221213 19:03:01 @agent_ppo2.py:121][0m #------------------------ Iteration 781 --------------------------#
[32m[20221213 19:03:02 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |           0.0015 |          60.9328 |         -47.5984 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |          -0.0058 |          54.5649 |         -46.7176 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |          -0.0048 |          53.1776 |         -46.9294 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |           0.0036 |          52.1848 |         -46.4714 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |           0.0006 |          51.4395 |         -46.9042 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |           0.0018 |          50.7180 |         -46.7377 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |          -0.0007 |          50.0487 |         -46.3434 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |           0.0019 |          49.7414 |         -46.3192 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |          -0.0020 |          49.8292 |         -46.2293 |
[32m[20221213 19:03:02 @agent_ppo2.py:185][0m |           0.0001 |          49.1034 |         -46.0507 |
[32m[20221213 19:03:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.00
[32m[20221213 19:03:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 658.00
[32m[20221213 19:03:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:03:03 @agent_ppo2.py:143][0m Total time:      14.50 min
[32m[20221213 19:03:03 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 19:03:03 @agent_ppo2.py:121][0m #------------------------ Iteration 782 --------------------------#
[32m[20221213 19:03:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |           0.0160 |          31.3928 |         -47.6226 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |          -0.0016 |          12.1190 |         -47.1461 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |           0.0026 |           9.9892 |         -47.3497 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |          -0.0030 |           9.4111 |         -47.6167 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |          -0.0026 |           9.0371 |         -47.7101 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |           0.0138 |           8.8595 |         -47.5650 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |           0.0087 |           9.0129 |         -47.3482 |
[32m[20221213 19:03:03 @agent_ppo2.py:185][0m |           0.0069 |           8.6566 |         -47.3564 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |          -0.0021 |           8.5328 |         -47.4444 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |          -0.0032 |           8.4825 |         -47.4505 |
[32m[20221213 19:03:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:03:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.60
[32m[20221213 19:03:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 19:03:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 19:03:04 @agent_ppo2.py:143][0m Total time:      14.52 min
[32m[20221213 19:03:04 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221213 19:03:04 @agent_ppo2.py:121][0m #------------------------ Iteration 783 --------------------------#
[32m[20221213 19:03:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |           0.0031 |          14.5840 |         -48.8842 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |          -0.0022 |           8.4310 |         -49.1919 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |           0.0045 |           8.8833 |         -49.1739 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |          -0.0029 |           7.8954 |         -48.9103 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |           0.0014 |           7.7496 |         -49.0818 |
[32m[20221213 19:03:04 @agent_ppo2.py:185][0m |          -0.0074 |           7.6585 |         -49.1195 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |          -0.0063 |           7.5911 |         -49.2868 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |          -0.0058 |           7.5567 |         -49.2616 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |           0.0028 |           7.4854 |         -49.2035 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |          -0.0085 |           7.4752 |         -49.6209 |
[32m[20221213 19:03:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:03:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.60
[32m[20221213 19:03:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 19:03:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 19:03:05 @agent_ppo2.py:143][0m Total time:      14.54 min
[32m[20221213 19:03:05 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 19:03:05 @agent_ppo2.py:121][0m #------------------------ Iteration 784 --------------------------#
[32m[20221213 19:03:05 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |          -0.0015 |         223.5347 |         -49.2668 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |           0.0086 |         243.3217 |         -49.1718 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |           0.0069 |         225.0679 |         -48.9950 |
[32m[20221213 19:03:05 @agent_ppo2.py:185][0m |          -0.0007 |         216.8330 |         -48.1840 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |           0.0111 |         226.8036 |         -47.8089 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |          -0.0028 |         216.3984 |         -47.8095 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |          -0.0029 |         216.0254 |         -47.8360 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |           0.0122 |         232.3892 |         -47.5272 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |           0.0029 |         219.0211 |         -47.2504 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |           0.0030 |         215.1238 |         -47.0193 |
[32m[20221213 19:03:06 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.40
[32m[20221213 19:03:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.00
[32m[20221213 19:03:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:03:06 @agent_ppo2.py:143][0m Total time:      14.56 min
[32m[20221213 19:03:06 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221213 19:03:06 @agent_ppo2.py:121][0m #------------------------ Iteration 785 --------------------------#
[32m[20221213 19:03:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |          -0.0056 |           8.3807 |         -46.0810 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |           0.0042 |           6.8922 |         -45.7041 |
[32m[20221213 19:03:06 @agent_ppo2.py:185][0m |          -0.0018 |           6.7452 |         -45.6620 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0052 |           6.6211 |         -45.9752 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |           0.0014 |           6.7172 |         -45.9560 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0029 |           6.5118 |         -45.9589 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0042 |           6.4796 |         -45.6386 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0058 |           6.4633 |         -45.8256 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0013 |           6.5910 |         -45.8233 |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0049 |           6.4377 |         -45.9586 |
[32m[20221213 19:03:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.80
[32m[20221213 19:03:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 19:03:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:03:07 @agent_ppo2.py:143][0m Total time:      14.58 min
[32m[20221213 19:03:07 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 19:03:07 @agent_ppo2.py:121][0m #------------------------ Iteration 786 --------------------------#
[32m[20221213 19:03:07 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:07 @agent_ppo2.py:185][0m |          -0.0027 |           9.5877 |         -47.0182 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0030 |           6.1605 |         -46.2554 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0041 |           6.0450 |         -45.5217 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0103 |           5.8463 |         -45.1539 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0007 |           5.7654 |         -44.6928 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0036 |           5.8378 |         -44.2695 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0101 |           5.7269 |         -44.2143 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0129 |           5.6565 |         -43.9190 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0064 |           5.9628 |         -43.7160 |
[32m[20221213 19:03:08 @agent_ppo2.py:185][0m |          -0.0086 |           5.6826 |         -43.4746 |
[32m[20221213 19:03:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.00
[32m[20221213 19:03:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:03:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:03:08 @agent_ppo2.py:143][0m Total time:      14.60 min
[32m[20221213 19:03:08 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221213 19:03:08 @agent_ppo2.py:121][0m #------------------------ Iteration 787 --------------------------#
[32m[20221213 19:03:08 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:03:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |           0.0121 |         179.1268 |         -41.4786 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |           0.0131 |         172.3287 |         -40.8298 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |           0.0025 |         169.2786 |         -41.7587 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |          -0.0007 |         168.4569 |         -41.5193 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |           0.0041 |         169.1191 |         -41.8046 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |           0.0018 |         167.4062 |         -41.4256 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |          -0.0006 |         167.4491 |         -41.8873 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |          -0.0015 |         167.3799 |         -41.9109 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |           0.0022 |         168.2430 |         -41.9707 |
[32m[20221213 19:03:09 @agent_ppo2.py:185][0m |          -0.0028 |         166.4847 |         -41.9100 |
[32m[20221213 19:03:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.20
[32m[20221213 19:03:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 19:03:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 19:03:09 @agent_ppo2.py:143][0m Total time:      14.62 min
[32m[20221213 19:03:09 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 19:03:09 @agent_ppo2.py:121][0m #------------------------ Iteration 788 --------------------------#
[32m[20221213 19:03:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |           0.0076 |           9.1958 |         -42.6635 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0007 |           5.5449 |         -42.7294 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0012 |           5.0121 |         -43.1183 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |           0.0016 |           4.7869 |         -43.5286 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |           0.0035 |           4.6950 |         -43.5052 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0004 |           4.5771 |         -43.5097 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0082 |           4.5555 |         -43.0605 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0005 |           4.6201 |         -43.2661 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0041 |           4.4575 |         -43.2156 |
[32m[20221213 19:03:10 @agent_ppo2.py:185][0m |          -0.0065 |           4.4450 |         -43.3005 |
[32m[20221213 19:03:10 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.80
[32m[20221213 19:03:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:03:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:03:11 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221213 19:03:11 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221213 19:03:11 @agent_ppo2.py:121][0m #------------------------ Iteration 789 --------------------------#
[32m[20221213 19:03:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |           0.0001 |         206.3379 |         -44.7036 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0015 |         201.6813 |         -44.7659 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0046 |         200.6251 |         -45.3135 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |           0.0024 |         200.2672 |         -45.7080 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0012 |         200.4620 |         -45.7132 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0010 |         201.1275 |         -46.2601 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0036 |         200.3337 |         -46.3713 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0027 |         200.1609 |         -46.3613 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0010 |         200.3683 |         -46.3588 |
[32m[20221213 19:03:11 @agent_ppo2.py:185][0m |          -0.0019 |         201.0631 |         -46.5749 |
[32m[20221213 19:03:11 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.00
[32m[20221213 19:03:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 19:03:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:03:12 @agent_ppo2.py:143][0m Total time:      14.65 min
[32m[20221213 19:03:12 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 19:03:12 @agent_ppo2.py:121][0m #------------------------ Iteration 790 --------------------------#
[32m[20221213 19:03:12 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:03:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |           0.0012 |           7.1790 |         -47.0632 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |           0.0065 |           4.4834 |         -46.9470 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |          -0.0031 |           4.2894 |         -47.1951 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |           0.0074 |           4.1922 |         -46.7912 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |           0.0010 |           4.1410 |         -47.0465 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |          -0.0027 |           4.1582 |         -47.2743 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |          -0.0027 |           4.0768 |         -47.0852 |
[32m[20221213 19:03:12 @agent_ppo2.py:185][0m |          -0.0019 |           4.0639 |         -46.7389 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |          -0.0015 |           4.0354 |         -47.6165 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |          -0.0066 |           4.0002 |         -47.3953 |
[32m[20221213 19:03:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:03:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.00
[32m[20221213 19:03:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 19:03:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:03:13 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221213 19:03:13 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221213 19:03:13 @agent_ppo2.py:121][0m #------------------------ Iteration 791 --------------------------#
[32m[20221213 19:03:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |           0.0084 |          15.7842 |         -47.8799 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |           0.0013 |           7.0112 |         -47.8264 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |           0.0020 |           6.7999 |         -47.4960 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |          -0.0018 |           6.6859 |         -47.2457 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |           0.0027 |           6.6035 |         -47.2381 |
[32m[20221213 19:03:13 @agent_ppo2.py:185][0m |           0.0041 |           6.9207 |         -46.9801 |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |           0.0037 |           7.0881 |         -47.1923 |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |          -0.0028 |           6.5267 |         -46.4801 |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |           0.0104 |           6.5120 |         -46.7875 |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |          -0.0048 |           6.4895 |         -46.5376 |
[32m[20221213 19:03:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 19:03:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.20
[32m[20221213 19:03:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.00
[32m[20221213 19:03:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 19:03:14 @agent_ppo2.py:143][0m Total time:      14.69 min
[32m[20221213 19:03:14 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 19:03:14 @agent_ppo2.py:121][0m #------------------------ Iteration 792 --------------------------#
[32m[20221213 19:03:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:03:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |          -0.0025 |         188.7728 |         -47.0125 |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |          -0.0000 |         184.4923 |         -46.8352 |
[32m[20221213 19:03:14 @agent_ppo2.py:185][0m |           0.0169 |         200.7587 |         -45.9852 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |           0.0037 |         182.0510 |         -45.5742 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |          -0.0036 |         181.6705 |         -46.1839 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |           0.0037 |         186.1392 |         -45.7100 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |           0.0075 |         194.4749 |         -45.4786 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |          -0.0031 |         181.5248 |         -45.7505 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |          -0.0048 |         181.3812 |         -45.4519 |
[32m[20221213 19:03:15 @agent_ppo2.py:185][0m |          -0.0047 |         180.8842 |         -44.6203 |
[32m[20221213 19:03:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 19:03:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 697.80
[32m[20221213 19:03:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.00
[32m[20221213 19:03:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:03:15 @agent_ppo2.py:143][0m Total time:      14.71 min
[32m[20221213 19:03:15 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221213 19:03:15 @agent_ppo2.py:121][0m #------------------------ Iteration 793 --------------------------#
[32m[20221213 19:03:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |           0.0088 |           9.4469 |         -43.4686 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |          -0.0039 |           6.8241 |         -43.5269 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |           0.0037 |           6.9710 |         -43.8082 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |           0.0055 |           6.7586 |         -44.1566 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |           0.0028 |           6.7176 |         -44.5596 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |           0.0000 |           6.6876 |         -44.3391 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |          -0.0046 |           6.6991 |         -44.7807 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |          -0.0013 |           6.7590 |         -44.3096 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |          -0.0008 |           6.6452 |         -44.4620 |
[32m[20221213 19:03:16 @agent_ppo2.py:185][0m |          -0.0012 |           6.6885 |         -44.9748 |
[32m[20221213 19:03:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.20
[32m[20221213 19:03:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 19:03:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.00
[32m[20221213 19:03:16 @agent_ppo2.py:143][0m Total time:      14.73 min
[32m[20221213 19:03:16 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 19:03:16 @agent_ppo2.py:121][0m #------------------------ Iteration 794 --------------------------#
[32m[20221213 19:03:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:03:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |           0.0028 |          10.4115 |         -45.6587 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |          -0.0013 |           6.8359 |         -45.5949 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |          -0.0038 |           6.2891 |         -45.5141 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |           0.0018 |           6.1527 |         -45.3111 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |           0.0018 |           6.0769 |         -45.2731 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |          -0.0026 |           5.9654 |         -45.2719 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |          -0.0006 |           5.9438 |         -45.1643 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |          -0.0058 |           5.9003 |         -45.1082 |
[32m[20221213 19:03:17 @agent_ppo2.py:185][0m |          -0.0021 |           5.9109 |         -44.8723 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |          -0.0041 |           5.8397 |         -45.0869 |
[32m[20221213 19:03:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:03:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.00
[32m[20221213 19:03:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 38.00
[32m[20221213 19:03:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 19:03:18 @agent_ppo2.py:143][0m Total time:      14.75 min
[32m[20221213 19:03:18 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221213 19:03:18 @agent_ppo2.py:121][0m #------------------------ Iteration 795 --------------------------#
[32m[20221213 19:03:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |           0.0025 |           8.7149 |         -45.6435 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |          -0.0004 |           6.3869 |         -44.8857 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |           0.0007 |           6.0884 |         -44.0820 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |          -0.0093 |           5.8736 |         -43.6307 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |          -0.0073 |           5.7750 |         -42.8492 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |          -0.0130 |           5.7157 |         -42.8048 |
[32m[20221213 19:03:18 @agent_ppo2.py:185][0m |          -0.0094 |           5.6347 |         -42.4550 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |          -0.0060 |           5.5989 |         -42.7730 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |          -0.0085 |           5.5836 |         -42.3548 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |          -0.0105 |           5.5328 |         -42.4117 |
[32m[20221213 19:03:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:03:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.60
[32m[20221213 19:03:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:03:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:03:19 @agent_ppo2.py:143][0m Total time:      14.77 min
[32m[20221213 19:03:19 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 19:03:19 @agent_ppo2.py:121][0m #------------------------ Iteration 796 --------------------------#
[32m[20221213 19:03:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |           0.0011 |          20.8825 |         -42.5356 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |          -0.0043 |          17.6028 |         -42.5027 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |           0.0056 |          16.9376 |         -43.1959 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |          -0.0001 |          16.5805 |         -42.8356 |
[32m[20221213 19:03:19 @agent_ppo2.py:185][0m |          -0.0073 |          17.1729 |         -42.9399 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |          -0.0123 |          16.5455 |         -43.3284 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |          -0.0064 |          15.8176 |         -42.8541 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |          -0.0078 |          15.4930 |         -43.2462 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |          -0.0049 |          15.5885 |         -43.1423 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |          -0.0070 |          15.2901 |         -43.4052 |
[32m[20221213 19:03:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.20
[32m[20221213 19:03:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.00
[32m[20221213 19:03:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:03:20 @agent_ppo2.py:143][0m Total time:      14.79 min
[32m[20221213 19:03:20 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221213 19:03:20 @agent_ppo2.py:121][0m #------------------------ Iteration 797 --------------------------#
[32m[20221213 19:03:20 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:03:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |           0.0041 |           9.0217 |         -44.9485 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |          -0.0029 |           5.4536 |         -44.8443 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |           0.0031 |           5.3784 |         -44.4791 |
[32m[20221213 19:03:20 @agent_ppo2.py:185][0m |           0.0023 |           5.1836 |         -44.1635 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |           0.0006 |           5.3961 |         -44.7683 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |           0.0022 |           5.0840 |         -44.2164 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |          -0.0055 |           5.1286 |         -44.5922 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |          -0.0001 |           5.0125 |         -44.5035 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |          -0.0054 |           5.0736 |         -44.5345 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |          -0.0089 |           4.9740 |         -44.6246 |
[32m[20221213 19:03:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.00
[32m[20221213 19:03:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 19:03:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 19:03:21 @agent_ppo2.py:143][0m Total time:      14.81 min
[32m[20221213 19:03:21 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 19:03:21 @agent_ppo2.py:121][0m #------------------------ Iteration 798 --------------------------#
[32m[20221213 19:03:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |           0.0016 |           4.6206 |         -44.9424 |
[32m[20221213 19:03:21 @agent_ppo2.py:185][0m |          -0.0009 |           4.0174 |         -45.3461 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |           0.0031 |           3.9612 |         -45.2575 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |          -0.0052 |           3.9657 |         -45.6081 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |          -0.0014 |           3.9363 |         -45.8348 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |           0.0122 |           4.2110 |         -45.7573 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |          -0.0020 |           3.9329 |         -45.7822 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |          -0.0047 |           3.9570 |         -45.5907 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |           0.0024 |           3.9136 |         -45.7919 |
[32m[20221213 19:03:22 @agent_ppo2.py:185][0m |           0.0018 |           3.9268 |         -45.9056 |
[32m[20221213 19:03:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:03:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.80
[32m[20221213 19:03:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:03:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 19:03:22 @agent_ppo2.py:143][0m Total time:      14.83 min
[32m[20221213 19:03:22 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221213 19:03:22 @agent_ppo2.py:121][0m #------------------------ Iteration 799 --------------------------#
[32m[20221213 19:03:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:03:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |           0.0062 |           5.3191 |         -47.2081 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |           0.0073 |           4.0204 |         -47.1525 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0083 |           3.8477 |         -46.9206 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0010 |           3.8275 |         -46.8096 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0115 |           3.7412 |         -46.6223 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0102 |           3.7020 |         -46.5986 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0050 |           3.7902 |         -46.6522 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0071 |           3.6513 |         -46.0513 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0112 |           3.6715 |         -46.0155 |
[32m[20221213 19:03:23 @agent_ppo2.py:185][0m |          -0.0047 |           3.6266 |         -45.9672 |
[32m[20221213 19:03:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:03:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.20
[32m[20221213 19:03:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:03:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.00
[32m[20221213 19:03:23 @agent_ppo2.py:143][0m Total time:      14.85 min
[32m[20221213 19:03:23 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 19:03:23 @agent_ppo2.py:121][0m #------------------------ Iteration 800 --------------------------#
[32m[20221213 19:03:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:03:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |           0.0036 |           4.2687 |         -46.6911 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0035 |           3.4435 |         -47.0824 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |           0.0006 |           3.4092 |         -47.4886 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0017 |           3.3716 |         -47.4567 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0061 |           3.3727 |         -47.7349 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0045 |           3.3452 |         -48.1347 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0034 |           3.3519 |         -48.2141 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0017 |           3.3276 |         -48.1323 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0083 |           3.2969 |         -48.4620 |
[32m[20221213 19:03:24 @agent_ppo2.py:185][0m |          -0.0063 |           3.2774 |         -48.5718 |
[32m[20221213 19:03:24 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.80
[32m[20221213 19:03:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 19:03:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:03:25 @agent_ppo2.py:143][0m Total time:      14.87 min
[32m[20221213 19:03:25 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221213 19:03:25 @agent_ppo2.py:121][0m #------------------------ Iteration 801 --------------------------#
[32m[20221213 19:03:25 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |           0.0048 |         100.5082 |         -48.8749 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |           0.0004 |          92.7428 |         -49.7586 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |           0.0020 |          91.4960 |         -49.7293 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |          -0.0024 |          91.2531 |         -50.0057 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |           0.0089 |          92.0322 |         -50.1186 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |          -0.0008 |          93.2320 |         -49.9039 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |           0.0002 |          91.4290 |         -50.1816 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |          -0.0026 |          91.1448 |         -50.3177 |
[32m[20221213 19:03:25 @agent_ppo2.py:185][0m |          -0.0049 |          90.2383 |         -50.3144 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |          -0.0043 |          91.0722 |         -50.1955 |
[32m[20221213 19:03:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.80
[32m[20221213 19:03:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 892.00
[32m[20221213 19:03:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.00
[32m[20221213 19:03:26 @agent_ppo2.py:143][0m Total time:      14.89 min
[32m[20221213 19:03:26 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 19:03:26 @agent_ppo2.py:121][0m #------------------------ Iteration 802 --------------------------#
[32m[20221213 19:03:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |           0.0038 |           7.3679 |         -50.5729 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |          -0.0069 |           5.5172 |         -50.1618 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |           0.0031 |           5.3150 |         -49.7909 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |           0.0037 |           5.1749 |         -49.4678 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |          -0.0058 |           5.0997 |         -49.3711 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |          -0.0002 |           5.0399 |         -49.0447 |
[32m[20221213 19:03:26 @agent_ppo2.py:185][0m |          -0.0025 |           4.9935 |         -48.5437 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |          -0.0080 |           4.9362 |         -48.6717 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |          -0.0069 |           4.8944 |         -48.6648 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |          -0.0058 |           4.8568 |         -48.5546 |
[32m[20221213 19:03:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.00
[32m[20221213 19:03:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 19:03:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:03:27 @agent_ppo2.py:143][0m Total time:      14.91 min
[32m[20221213 19:03:27 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221213 19:03:27 @agent_ppo2.py:121][0m #------------------------ Iteration 803 --------------------------#
[32m[20221213 19:03:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |           0.0129 |         155.9729 |         -49.4468 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |          -0.0047 |         141.4218 |         -49.1387 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |           0.0059 |         140.5605 |         -48.6347 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |          -0.0011 |         140.4380 |         -49.2001 |
[32m[20221213 19:03:27 @agent_ppo2.py:185][0m |          -0.0019 |         140.1596 |         -48.5730 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0026 |         139.8251 |         -48.9121 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0012 |         139.7245 |         -49.1938 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0007 |         139.4919 |         -48.6908 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0142 |         160.0183 |         -48.5560 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |          -0.0021 |         139.5232 |         -48.9545 |
[32m[20221213 19:03:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:03:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 928.60
[32m[20221213 19:03:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 930.00
[32m[20221213 19:03:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.00
[32m[20221213 19:03:28 @agent_ppo2.py:143][0m Total time:      14.93 min
[32m[20221213 19:03:28 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 19:03:28 @agent_ppo2.py:121][0m #------------------------ Iteration 804 --------------------------#
[32m[20221213 19:03:28 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0031 |         153.8855 |         -47.7827 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0025 |         149.7760 |         -48.1140 |
[32m[20221213 19:03:28 @agent_ppo2.py:185][0m |           0.0008 |         149.4937 |         -48.2362 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |           0.0061 |         149.2778 |         -48.2235 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |           0.0009 |         148.8393 |         -48.2293 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |           0.0167 |         164.5314 |         -49.4686 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |           0.0001 |         150.2352 |         -49.7365 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |          -0.0014 |         148.7050 |         -49.9581 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |          -0.0041 |         148.8273 |         -49.7292 |
[32m[20221213 19:03:29 @agent_ppo2.py:185][0m |           0.0064 |         148.6671 |         -50.0091 |
[32m[20221213 19:03:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 19:03:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.00
[32m[20221213 19:03:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.00
[32m[20221213 19:03:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.00
[32m[20221213 19:03:29 @agent_ppo2.py:143][0m Total time:      14.95 min
[32m[20221213 19:03:29 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221213 19:03:29 @agent_ppo2.py:121][0m #------------------------ Iteration 805 --------------------------#
[32m[20221213 19:03:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |           0.0106 |           6.9389 |         -51.3288 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |           0.0043 |           2.9040 |         -50.4456 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |           0.0001 |           2.7768 |         -50.4698 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |          -0.0030 |           2.8888 |         -50.7247 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |          -0.0027 |           2.7331 |         -50.3478 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |           0.0002 |           2.7004 |         -50.3486 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |          -0.0076 |           2.7402 |         -49.8771 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |          -0.0014 |           2.6759 |         -49.8697 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |          -0.0045 |           2.6717 |         -49.2049 |
[32m[20221213 19:03:30 @agent_ppo2.py:185][0m |           0.0020 |           2.8037 |         -49.3543 |
[32m[20221213 19:03:30 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.00
[32m[20221213 19:03:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:03:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:03:30 @agent_ppo2.py:143][0m Total time:      14.97 min
[32m[20221213 19:03:30 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 19:03:30 @agent_ppo2.py:121][0m #------------------------ Iteration 806 --------------------------#
[32m[20221213 19:03:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |           0.0073 |           2.8171 |         -50.0279 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |          -0.0001 |           2.5324 |         -50.1223 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |          -0.0041 |           2.5208 |         -50.3189 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |           0.0013 |           2.5073 |         -50.2925 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |           0.0023 |           2.5028 |         -49.4120 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |           0.0032 |           2.4804 |         -50.1593 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |           0.0007 |           2.4718 |         -50.3678 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |          -0.0032 |           2.4695 |         -50.3302 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |          -0.0061 |           2.4620 |         -50.5185 |
[32m[20221213 19:03:31 @agent_ppo2.py:185][0m |          -0.0033 |           2.4615 |         -50.4155 |
[32m[20221213 19:03:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:03:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.20
[32m[20221213 19:03:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.00
[32m[20221213 19:03:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:03:32 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221213 19:03:32 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221213 19:03:32 @agent_ppo2.py:121][0m #------------------------ Iteration 807 --------------------------#
[32m[20221213 19:03:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:03:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |           0.0021 |           5.5519 |         -49.6174 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |           0.0025 |           4.6812 |         -49.2931 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |           0.0004 |           4.5681 |         -49.6745 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |          -0.0026 |           4.5645 |         -49.4524 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |          -0.0039 |           4.5285 |         -49.4525 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |          -0.0027 |           4.5105 |         -49.3716 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |           0.0001 |           4.4959 |         -49.4759 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |           0.0005 |           4.4955 |         -50.1493 |
[32m[20221213 19:03:32 @agent_ppo2.py:185][0m |          -0.0074 |           4.4793 |         -49.7914 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |          -0.0045 |           4.4679 |         -49.9959 |
[32m[20221213 19:03:33 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.00
[32m[20221213 19:03:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:03:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 681.00
[32m[20221213 19:03:33 @agent_ppo2.py:143][0m Total time:      15.00 min
[32m[20221213 19:03:33 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 19:03:33 @agent_ppo2.py:121][0m #------------------------ Iteration 808 --------------------------#
[32m[20221213 19:03:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |           0.0097 |           5.6787 |         -51.0595 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |           0.0184 |           4.3622 |         -51.6163 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |           0.0126 |           4.0971 |         -51.8524 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |           0.0050 |           4.0291 |         -52.3726 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |           0.0023 |           3.9901 |         -51.9655 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |           0.0001 |           3.9728 |         -52.4393 |
[32m[20221213 19:03:33 @agent_ppo2.py:185][0m |          -0.0043 |           3.9495 |         -52.2353 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |           0.0010 |           3.9324 |         -52.1853 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |          -0.0030 |           3.9114 |         -52.0810 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |           0.0011 |           3.9229 |         -52.4666 |
[32m[20221213 19:03:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:03:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.00
[32m[20221213 19:03:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 44.00
[32m[20221213 19:03:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 19:03:34 @agent_ppo2.py:143][0m Total time:      15.02 min
[32m[20221213 19:03:34 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221213 19:03:34 @agent_ppo2.py:121][0m #------------------------ Iteration 809 --------------------------#
[32m[20221213 19:03:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |           0.0026 |          36.7423 |         -52.8298 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |           0.0008 |          33.6925 |         -52.1424 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |          -0.0026 |          33.3729 |         -52.0957 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |          -0.0041 |          33.1372 |         -51.7914 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |          -0.0007 |          32.9106 |         -51.3552 |
[32m[20221213 19:03:34 @agent_ppo2.py:185][0m |          -0.0060 |          32.9796 |         -51.1642 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |           0.0017 |          32.9016 |         -50.8526 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |          -0.0014 |          34.2895 |         -50.5794 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |          -0.0020 |          33.0033 |         -50.6986 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |           0.0047 |          32.6392 |         -50.5489 |
[32m[20221213 19:03:35 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.60
[32m[20221213 19:03:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.00
[32m[20221213 19:03:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 19:03:35 @agent_ppo2.py:143][0m Total time:      15.04 min
[32m[20221213 19:03:35 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 19:03:35 @agent_ppo2.py:121][0m #------------------------ Iteration 810 --------------------------#
[32m[20221213 19:03:35 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:03:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |          -0.0001 |         159.9085 |         -50.7607 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |          -0.0007 |         154.8758 |         -50.5240 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |           0.0033 |         155.4460 |         -50.3772 |
[32m[20221213 19:03:35 @agent_ppo2.py:185][0m |           0.0096 |         161.8311 |         -50.2765 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |           0.0004 |         154.9745 |         -50.4495 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |          -0.0000 |         154.0518 |         -49.9255 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |          -0.0000 |         153.6448 |         -50.2523 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |          -0.0044 |         153.7871 |         -49.8996 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |           0.0012 |         153.6261 |         -49.4862 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |           0.0025 |         153.9697 |         -49.3118 |
[32m[20221213 19:03:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:03:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 19:03:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 19:03:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.00
[32m[20221213 19:03:36 @agent_ppo2.py:143][0m Total time:      15.06 min
[32m[20221213 19:03:36 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221213 19:03:36 @agent_ppo2.py:121][0m #------------------------ Iteration 811 --------------------------#
[32m[20221213 19:03:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |          -0.0035 |         164.7617 |         -49.1319 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |           0.0050 |         162.8140 |         -48.0042 |
[32m[20221213 19:03:36 @agent_ppo2.py:185][0m |           0.0018 |         162.9787 |         -49.8113 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0146 |         181.7790 |         -49.7683 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0031 |         162.9171 |         -50.4937 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0019 |         162.8627 |         -50.8228 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0181 |         184.2891 |         -50.9719 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0005 |         163.2929 |         -51.3827 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0064 |         163.0474 |         -51.1317 |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0010 |         162.7796 |         -51.4407 |
[32m[20221213 19:03:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:03:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:03:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.00
[32m[20221213 19:03:37 @agent_ppo2.py:143][0m Total time:      15.08 min
[32m[20221213 19:03:37 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 19:03:37 @agent_ppo2.py:121][0m #------------------------ Iteration 812 --------------------------#
[32m[20221213 19:03:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:37 @agent_ppo2.py:185][0m |           0.0169 |           6.8051 |         -52.1744 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |           0.0036 |           5.3290 |         -50.0055 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0159 |           5.2098 |         -50.0800 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0198 |           5.1651 |         -50.2212 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0203 |           5.0944 |         -50.5711 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0201 |           5.1072 |         -50.9504 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0146 |           5.1874 |         -51.5164 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0160 |           5.3109 |         -51.3653 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0120 |           5.0909 |         -51.3938 |
[32m[20221213 19:03:38 @agent_ppo2.py:185][0m |          -0.0228 |           4.9525 |         -51.8648 |
[32m[20221213 19:03:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 19:03:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.00
[32m[20221213 19:03:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:03:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 19:03:38 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221213 19:03:38 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221213 19:03:38 @agent_ppo2.py:121][0m #------------------------ Iteration 813 --------------------------#
[32m[20221213 19:03:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0029 |         149.1638 |         -58.1778 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0003 |         147.1433 |         -58.2485 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0006 |         145.2874 |         -58.1235 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0061 |         149.0078 |         -58.2278 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0001 |         144.2990 |         -58.3164 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |          -0.0023 |         143.9668 |         -58.0122 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0045 |         143.3520 |         -57.4539 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0019 |         143.0621 |         -57.4976 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0023 |         142.8457 |         -57.4219 |
[32m[20221213 19:03:39 @agent_ppo2.py:185][0m |           0.0020 |         142.8217 |         -57.6807 |
[32m[20221213 19:03:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 19:03:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.80
[32m[20221213 19:03:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.00
[32m[20221213 19:03:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:03:40 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 19:03:40 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 19:03:40 @agent_ppo2.py:121][0m #------------------------ Iteration 814 --------------------------#
[32m[20221213 19:03:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |           0.0192 |          10.6037 |         -55.5666 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |           0.0063 |           4.5611 |         -55.2158 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |           0.0044 |           4.0691 |         -54.6569 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |          -0.0022 |           3.7841 |         -54.5979 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |          -0.0058 |           3.6033 |         -54.8691 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |          -0.0019 |           3.4936 |         -54.7573 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |          -0.0127 |           3.3740 |         -54.7679 |
[32m[20221213 19:03:40 @agent_ppo2.py:185][0m |          -0.0072 |           3.3175 |         -54.5284 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |          -0.0066 |           3.2662 |         -54.7892 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |          -0.0127 |           3.2287 |         -54.8481 |
[32m[20221213 19:03:41 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 19:03:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.80
[32m[20221213 19:03:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:03:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:03:41 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221213 19:03:41 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221213 19:03:41 @agent_ppo2.py:121][0m #------------------------ Iteration 815 --------------------------#
[32m[20221213 19:03:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |           0.0111 |         160.4928 |         -56.7641 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |          -0.0005 |         153.7997 |         -57.2412 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |          -0.0025 |         153.3931 |         -57.7528 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |           0.0029 |         152.4004 |         -57.7508 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |           0.0190 |         158.2366 |         -57.6935 |
[32m[20221213 19:03:41 @agent_ppo2.py:185][0m |           0.0051 |         165.7189 |         -58.8461 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |           0.0050 |         153.8501 |         -58.8916 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |           0.0026 |         153.5399 |         -59.0093 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |           0.0058 |         152.6000 |         -58.5463 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |           0.0014 |         152.6639 |         -58.9746 |
[32m[20221213 19:03:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:03:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.00
[32m[20221213 19:03:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:03:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.00
[32m[20221213 19:03:42 @agent_ppo2.py:143][0m Total time:      15.16 min
[32m[20221213 19:03:42 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 19:03:42 @agent_ppo2.py:121][0m #------------------------ Iteration 816 --------------------------#
[32m[20221213 19:03:42 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:03:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |           0.0049 |         160.7553 |         -59.6350 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |          -0.0003 |         155.5798 |         -60.3858 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |          -0.0008 |         155.2484 |         -60.2383 |
[32m[20221213 19:03:42 @agent_ppo2.py:185][0m |          -0.0025 |         154.7850 |         -60.3423 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |          -0.0015 |         154.9430 |         -60.2813 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |          -0.0001 |         154.3687 |         -60.4355 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |          -0.0021 |         154.2440 |         -60.6402 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |           0.0027 |         154.3152 |         -60.6447 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |           0.0060 |         156.6243 |         -60.6213 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |           0.0005 |         153.9662 |         -60.0939 |
[32m[20221213 19:03:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:03:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.60
[32m[20221213 19:03:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 19:03:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.00
[32m[20221213 19:03:43 @agent_ppo2.py:143][0m Total time:      15.18 min
[32m[20221213 19:03:43 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221213 19:03:43 @agent_ppo2.py:121][0m #------------------------ Iteration 817 --------------------------#
[32m[20221213 19:03:43 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:03:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |           0.0161 |           8.6333 |         -61.8409 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |           0.0045 |           3.1788 |         -61.3644 |
[32m[20221213 19:03:43 @agent_ppo2.py:185][0m |          -0.0052 |           2.9690 |         -61.3073 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |          -0.0039 |           2.8839 |         -61.0429 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |           0.0005 |           2.8444 |         -60.3289 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |          -0.0051 |           2.8172 |         -60.3262 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |          -0.0002 |           2.8287 |         -59.7728 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |           0.0011 |           2.8169 |         -59.4368 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |           0.0020 |           2.7836 |         -59.0750 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |          -0.0043 |           2.7703 |         -58.8715 |
[32m[20221213 19:03:44 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.40
[32m[20221213 19:03:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:03:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:03:44 @agent_ppo2.py:143][0m Total time:      15.20 min
[32m[20221213 19:03:44 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 19:03:44 @agent_ppo2.py:121][0m #------------------------ Iteration 818 --------------------------#
[32m[20221213 19:03:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |           0.0039 |         164.0183 |         -57.2034 |
[32m[20221213 19:03:44 @agent_ppo2.py:185][0m |           0.0044 |         161.2517 |         -56.3814 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |           0.0005 |         160.8397 |         -57.7424 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |           0.0085 |         161.2403 |         -57.1614 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |          -0.0004 |         160.7352 |         -57.0127 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |          -0.0044 |         160.4089 |         -58.1514 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |          -0.0006 |         160.1965 |         -58.1545 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |           0.0069 |         162.8964 |         -57.6344 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |           0.0077 |         172.4724 |         -57.7528 |
[32m[20221213 19:03:45 @agent_ppo2.py:185][0m |          -0.0013 |         160.2342 |         -57.8003 |
[32m[20221213 19:03:45 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.00
[32m[20221213 19:03:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.00
[32m[20221213 19:03:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 19:03:45 @agent_ppo2.py:143][0m Total time:      15.21 min
[32m[20221213 19:03:45 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221213 19:03:45 @agent_ppo2.py:121][0m #------------------------ Iteration 819 --------------------------#
[32m[20221213 19:03:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |           0.0077 |           9.7222 |         -59.2723 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |           0.0031 |           3.1900 |         -59.2141 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |          -0.0020 |           3.0123 |         -59.4469 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |           0.0002 |           2.9404 |         -59.4321 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |          -0.0047 |           2.9236 |         -59.1322 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |          -0.0031 |           2.8893 |         -59.4708 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |           0.0004 |           2.9051 |         -59.2913 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |          -0.0073 |           2.8532 |         -59.0026 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |           0.0012 |           2.8492 |         -59.4129 |
[32m[20221213 19:03:46 @agent_ppo2.py:185][0m |           0.0090 |           2.8334 |         -59.1471 |
[32m[20221213 19:03:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:03:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.00
[32m[20221213 19:03:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:03:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 19:03:46 @agent_ppo2.py:143][0m Total time:      15.23 min
[32m[20221213 19:03:46 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 19:03:46 @agent_ppo2.py:121][0m #------------------------ Iteration 820 --------------------------#
[32m[20221213 19:03:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:03:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0007 |         185.8851 |         -57.7933 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0133 |         188.8043 |         -56.8520 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0061 |         181.8701 |         -57.6095 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0024 |         183.1047 |         -58.0799 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0041 |         181.8233 |         -58.2601 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |          -0.0004 |         181.5202 |         -58.7460 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0011 |         181.3779 |         -58.7787 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |          -0.0012 |         181.3276 |         -58.6336 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |          -0.0012 |         181.4419 |         -58.6142 |
[32m[20221213 19:03:47 @agent_ppo2.py:185][0m |           0.0093 |         192.3422 |         -58.8382 |
[32m[20221213 19:03:47 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:03:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.00
[32m[20221213 19:03:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 19:03:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:03:47 @agent_ppo2.py:143][0m Total time:      15.25 min
[32m[20221213 19:03:47 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221213 19:03:47 @agent_ppo2.py:121][0m #------------------------ Iteration 821 --------------------------#
[32m[20221213 19:03:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |           0.0001 |         200.6641 |         -60.1217 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |           0.0068 |         200.9648 |         -59.5926 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |           0.0013 |         197.5205 |         -59.0510 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |           0.0097 |         209.5657 |         -59.1144 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |           0.0061 |         197.2898 |         -57.0196 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |          -0.0026 |         196.3203 |         -57.9761 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |          -0.0015 |         196.0575 |         -57.9953 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |          -0.0003 |         195.9899 |         -57.8531 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |           0.0042 |         195.6046 |         -57.2613 |
[32m[20221213 19:03:48 @agent_ppo2.py:185][0m |          -0.0015 |         196.4463 |         -56.9396 |
[32m[20221213 19:03:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:03:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:03:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:03:49 @agent_ppo2.py:143][0m Total time:      15.27 min
[32m[20221213 19:03:49 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 19:03:49 @agent_ppo2.py:121][0m #------------------------ Iteration 822 --------------------------#
[32m[20221213 19:03:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |           0.0074 |         216.7958 |         -55.0917 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |           0.0015 |         202.2621 |         -54.2655 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |          -0.0045 |         201.8461 |         -54.5627 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |           0.0231 |         235.6108 |         -54.4362 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |           0.0019 |         201.6252 |         -53.2098 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |          -0.0033 |         200.9599 |         -54.5184 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |           0.0098 |         230.3528 |         -53.8227 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |          -0.0035 |         201.2315 |         -53.4306 |
[32m[20221213 19:03:49 @agent_ppo2.py:185][0m |          -0.0000 |         203.2183 |         -53.7591 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |          -0.0030 |         200.6290 |         -53.5872 |
[32m[20221213 19:03:50 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:03:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.80
[32m[20221213 19:03:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:03:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.00
[32m[20221213 19:03:50 @agent_ppo2.py:143][0m Total time:      15.29 min
[32m[20221213 19:03:50 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221213 19:03:50 @agent_ppo2.py:121][0m #------------------------ Iteration 823 --------------------------#
[32m[20221213 19:03:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |          -0.0007 |         186.5749 |         -55.9705 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |          -0.0005 |         176.7154 |         -55.2329 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |           0.0162 |         203.5782 |         -55.4289 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |           0.0002 |         173.4862 |         -54.8007 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |          -0.0023 |         172.0801 |         -54.8221 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |           0.0123 |         201.6315 |         -54.7407 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |          -0.0016 |         171.1181 |         -54.3544 |
[32m[20221213 19:03:50 @agent_ppo2.py:185][0m |          -0.0009 |         170.6787 |         -54.1161 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |           0.0012 |         170.0766 |         -54.0317 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |          -0.0019 |         169.7224 |         -54.2924 |
[32m[20221213 19:03:51 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.60
[32m[20221213 19:03:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:03:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.00
[32m[20221213 19:03:51 @agent_ppo2.py:143][0m Total time:      15.31 min
[32m[20221213 19:03:51 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 19:03:51 @agent_ppo2.py:121][0m #------------------------ Iteration 824 --------------------------#
[32m[20221213 19:03:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |           0.0125 |         196.3676 |         -51.5358 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |           0.0058 |         173.3410 |         -51.4107 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |          -0.0002 |         171.1222 |         -52.3150 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |           0.0025 |         170.4578 |         -52.3433 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |           0.0013 |         169.7632 |         -52.7872 |
[32m[20221213 19:03:51 @agent_ppo2.py:185][0m |           0.0134 |         199.2540 |         -53.4078 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |          -0.0001 |         168.7944 |         -53.1691 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |          -0.0029 |         168.5206 |         -53.4399 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |          -0.0011 |         168.3386 |         -53.0888 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |          -0.0032 |         168.0392 |         -53.6112 |
[32m[20221213 19:03:52 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.00
[32m[20221213 19:03:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 19:03:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 41.00
[32m[20221213 19:03:52 @agent_ppo2.py:143][0m Total time:      15.32 min
[32m[20221213 19:03:52 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221213 19:03:52 @agent_ppo2.py:121][0m #------------------------ Iteration 825 --------------------------#
[32m[20221213 19:03:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |           0.0002 |         213.5580 |         -57.1477 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |           0.0002 |         210.7419 |         -56.7050 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |           0.0035 |         216.0838 |         -56.8372 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |           0.0016 |         209.8038 |         -56.4173 |
[32m[20221213 19:03:52 @agent_ppo2.py:185][0m |          -0.0023 |         209.1277 |         -56.7997 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |           0.0004 |         209.5289 |         -57.1875 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |           0.0007 |         208.9864 |         -56.6469 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |          -0.0004 |         208.7946 |         -56.6996 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |          -0.0002 |         208.9100 |         -56.3108 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |           0.0023 |         208.6945 |         -56.7769 |
[32m[20221213 19:03:53 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.00
[32m[20221213 19:03:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.00
[32m[20221213 19:03:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.00
[32m[20221213 19:03:53 @agent_ppo2.py:143][0m Total time:      15.34 min
[32m[20221213 19:03:53 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 19:03:53 @agent_ppo2.py:121][0m #------------------------ Iteration 826 --------------------------#
[32m[20221213 19:03:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |           0.0011 |         225.1530 |         -56.3978 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |           0.0000 |         218.6790 |         -56.3914 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |           0.0039 |         217.2212 |         -56.0405 |
[32m[20221213 19:03:53 @agent_ppo2.py:185][0m |          -0.0017 |         216.7862 |         -56.2425 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0002 |         216.5330 |         -56.4857 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |          -0.0039 |         216.4737 |         -56.4324 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0071 |         220.6204 |         -56.7190 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |          -0.0017 |         216.0897 |         -56.4607 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0004 |         216.2982 |         -56.5653 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0083 |         233.6825 |         -56.6146 |
[32m[20221213 19:03:54 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.80
[32m[20221213 19:03:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 19:03:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.00
[32m[20221213 19:03:54 @agent_ppo2.py:143][0m Total time:      15.36 min
[32m[20221213 19:03:54 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221213 19:03:54 @agent_ppo2.py:121][0m #------------------------ Iteration 827 --------------------------#
[32m[20221213 19:03:54 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:03:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0061 |          54.3291 |         -56.5055 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0046 |          12.4964 |         -56.3591 |
[32m[20221213 19:03:54 @agent_ppo2.py:185][0m |           0.0010 |          10.5516 |         -56.2021 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0122 |           9.8620 |         -55.7794 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0027 |           9.2878 |         -55.4071 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0056 |           9.0809 |         -55.6155 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0010 |           9.2199 |         -55.5554 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0009 |           8.7785 |         -55.0537 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |          -0.0025 |           8.6901 |         -55.0587 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0024 |           8.5671 |         -54.9557 |
[32m[20221213 19:03:55 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.60
[32m[20221213 19:03:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:03:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.00
[32m[20221213 19:03:55 @agent_ppo2.py:143][0m Total time:      15.38 min
[32m[20221213 19:03:55 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 19:03:55 @agent_ppo2.py:121][0m #------------------------ Iteration 828 --------------------------#
[32m[20221213 19:03:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0004 |         219.2994 |         -54.1424 |
[32m[20221213 19:03:55 @agent_ppo2.py:185][0m |           0.0002 |         215.1369 |         -53.5607 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |          -0.0004 |         214.1151 |         -53.9615 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |           0.0059 |         218.7796 |         -54.1309 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |           0.0018 |         213.3305 |         -53.8720 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |          -0.0023 |         212.3662 |         -53.9622 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |          -0.0035 |         212.2957 |         -54.2054 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |           0.0023 |         214.9566 |         -54.3276 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |          -0.0012 |         211.4645 |         -54.4353 |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |          -0.0005 |         215.4669 |         -54.1614 |
[32m[20221213 19:03:56 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.40
[32m[20221213 19:03:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 19:03:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 19:03:56 @agent_ppo2.py:143][0m Total time:      15.40 min
[32m[20221213 19:03:56 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221213 19:03:56 @agent_ppo2.py:121][0m #------------------------ Iteration 829 --------------------------#
[32m[20221213 19:03:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:56 @agent_ppo2.py:185][0m |           0.0277 |         245.0163 |         -53.2171 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |          -0.0017 |         213.8544 |         -55.9568 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0002 |         213.2755 |         -55.8052 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |          -0.0019 |         212.7055 |         -56.1054 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0082 |         222.3599 |         -56.3421 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0012 |         212.5069 |         -56.6338 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0002 |         212.6536 |         -56.7624 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0077 |         215.5662 |         -56.8607 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0077 |         212.4767 |         -56.6297 |
[32m[20221213 19:03:57 @agent_ppo2.py:185][0m |           0.0023 |         213.4174 |         -57.8028 |
[32m[20221213 19:03:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:03:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.20
[32m[20221213 19:03:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.00
[32m[20221213 19:03:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.00
[32m[20221213 19:03:57 @agent_ppo2.py:143][0m Total time:      15.41 min
[32m[20221213 19:03:57 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 19:03:57 @agent_ppo2.py:121][0m #------------------------ Iteration 830 --------------------------#
[32m[20221213 19:03:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:03:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |           0.0114 |          67.0106 |         -58.1130 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |           0.0111 |          38.6085 |         -57.7899 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |           0.0072 |          35.2385 |         -58.6595 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |          -0.0015 |          33.7060 |         -58.4163 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |           0.0025 |          32.8860 |         -58.5898 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |          -0.0033 |          32.4365 |         -58.5900 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |          -0.0021 |          32.1716 |         -58.2632 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |          -0.0048 |          31.9771 |         -58.4918 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |           0.0010 |          31.9232 |         -58.3583 |
[32m[20221213 19:03:58 @agent_ppo2.py:185][0m |           0.0023 |          31.6850 |         -58.6943 |
[32m[20221213 19:03:58 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:03:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.80
[32m[20221213 19:03:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 19:03:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:03:58 @agent_ppo2.py:143][0m Total time:      15.43 min
[32m[20221213 19:03:58 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221213 19:03:58 @agent_ppo2.py:121][0m #------------------------ Iteration 831 --------------------------#
[32m[20221213 19:03:59 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:03:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |           0.0166 |          36.6247 |         -60.5828 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0005 |          26.7796 |         -60.5978 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |           0.0019 |          26.7854 |         -60.8559 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0049 |          26.3322 |         -61.0010 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0007 |          26.3021 |         -60.8196 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0019 |          26.3545 |         -60.9663 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0082 |          26.1764 |         -61.1518 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0075 |          26.1576 |         -61.0027 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0041 |          26.1894 |         -60.9954 |
[32m[20221213 19:03:59 @agent_ppo2.py:185][0m |          -0.0033 |          26.1421 |         -61.1642 |
[32m[20221213 19:03:59 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:04:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.00
[32m[20221213 19:04:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:04:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:04:00 @agent_ppo2.py:143][0m Total time:      15.45 min
[32m[20221213 19:04:00 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 19:04:00 @agent_ppo2.py:121][0m #------------------------ Iteration 832 --------------------------#
[32m[20221213 19:04:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0038 |          26.5063 |         -60.6500 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |          -0.0039 |          22.5417 |         -60.7750 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0109 |          22.7833 |         -60.7036 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0026 |          21.9367 |         -60.4280 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0087 |          21.8590 |         -60.5168 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0017 |          21.8208 |         -59.6634 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0033 |          21.7459 |         -59.4516 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0008 |          21.7012 |         -59.7532 |
[32m[20221213 19:04:00 @agent_ppo2.py:185][0m |           0.0012 |          21.9496 |         -59.6848 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |          -0.0045 |          21.6700 |         -59.6895 |
[32m[20221213 19:04:01 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.80
[32m[20221213 19:04:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 19:04:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:04:01 @agent_ppo2.py:143][0m Total time:      15.47 min
[32m[20221213 19:04:01 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221213 19:04:01 @agent_ppo2.py:121][0m #------------------------ Iteration 833 --------------------------#
[32m[20221213 19:04:01 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |           0.0078 |          11.5977 |         -58.8551 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |          -0.0012 |           7.9008 |         -59.5483 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |           0.0032 |           7.7576 |         -59.5942 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |           0.0109 |           7.6752 |         -59.7720 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |           0.0059 |           7.6547 |         -60.4128 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |           0.0047 |           7.5892 |         -60.4307 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |           0.0093 |           7.5871 |         -60.7165 |
[32m[20221213 19:04:01 @agent_ppo2.py:185][0m |          -0.0010 |           7.5456 |         -61.0965 |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |          -0.0008 |           7.5051 |         -61.1525 |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |          -0.0018 |           7.5605 |         -60.8345 |
[32m[20221213 19:04:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.60
[32m[20221213 19:04:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:04:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:04:02 @agent_ppo2.py:143][0m Total time:      15.49 min
[32m[20221213 19:04:02 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 19:04:02 @agent_ppo2.py:121][0m #------------------------ Iteration 834 --------------------------#
[32m[20221213 19:04:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 19:04:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |           0.0153 |          12.1467 |         -61.4711 |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |           0.0083 |           7.0303 |         -62.6161 |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |          -0.0059 |           6.6927 |         -62.7180 |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |          -0.0076 |           6.4415 |         -62.7612 |
[32m[20221213 19:04:02 @agent_ppo2.py:185][0m |           0.0010 |           6.2910 |         -62.5396 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |          -0.0033 |           6.2319 |         -63.0161 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |          -0.0060 |           6.2362 |         -63.0138 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |          -0.0032 |           6.1506 |         -62.9570 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |           0.0043 |           6.4223 |         -62.7459 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |          -0.0030 |           6.2220 |         -62.9174 |
[32m[20221213 19:04:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:04:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.40
[32m[20221213 19:04:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 19:04:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:04:03 @agent_ppo2.py:143][0m Total time:      15.51 min
[32m[20221213 19:04:03 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221213 19:04:03 @agent_ppo2.py:121][0m #------------------------ Iteration 835 --------------------------#
[32m[20221213 19:04:03 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |           0.0139 |           7.3912 |         -65.5890 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |          -0.0037 |           4.5851 |         -65.0072 |
[32m[20221213 19:04:03 @agent_ppo2.py:185][0m |          -0.0086 |           4.4671 |         -65.1656 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0054 |           4.3868 |         -64.5703 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0058 |           4.3723 |         -63.9354 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0034 |           4.3261 |         -64.2339 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0056 |           4.2963 |         -63.9856 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0086 |           4.2484 |         -64.1047 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0024 |           4.3871 |         -64.0780 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0036 |           4.2432 |         -63.6373 |
[32m[20221213 19:04:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.60
[32m[20221213 19:04:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 19:04:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:04:04 @agent_ppo2.py:143][0m Total time:      15.53 min
[32m[20221213 19:04:04 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 19:04:04 @agent_ppo2.py:121][0m #------------------------ Iteration 836 --------------------------#
[32m[20221213 19:04:04 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |           0.0028 |         200.5135 |         -63.2884 |
[32m[20221213 19:04:04 @agent_ppo2.py:185][0m |          -0.0001 |         194.7027 |         -63.2228 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |           0.0154 |         194.9494 |         -63.2589 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |           0.0017 |         194.4362 |         -63.7663 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |          -0.0008 |         194.4087 |         -63.8180 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |          -0.0002 |         194.2837 |         -64.1543 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |          -0.0044 |         194.0861 |         -64.2992 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |           0.0047 |         194.3621 |         -63.7525 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |          -0.0019 |         194.2144 |         -64.9010 |
[32m[20221213 19:04:05 @agent_ppo2.py:185][0m |           0.0081 |         205.8241 |         -64.8149 |
[32m[20221213 19:04:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.20
[32m[20221213 19:04:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 883.00
[32m[20221213 19:04:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:04:05 @agent_ppo2.py:143][0m Total time:      15.55 min
[32m[20221213 19:04:05 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221213 19:04:05 @agent_ppo2.py:121][0m #------------------------ Iteration 837 --------------------------#
[32m[20221213 19:04:05 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0050 |         204.4118 |         -64.0407 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |          -0.0011 |         198.0189 |         -63.3485 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0015 |         198.6564 |         -63.0280 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0015 |         197.5716 |         -62.4523 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0073 |         218.1473 |         -62.7890 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0058 |         197.4893 |         -62.1372 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |          -0.0056 |         197.4282 |         -62.0560 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0002 |         197.4979 |         -62.1344 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |          -0.0032 |         197.3674 |         -61.9140 |
[32m[20221213 19:04:06 @agent_ppo2.py:185][0m |           0.0122 |         213.4472 |         -61.4501 |
[32m[20221213 19:04:06 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.60
[32m[20221213 19:04:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 19:04:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 19:04:06 @agent_ppo2.py:143][0m Total time:      15.57 min
[32m[20221213 19:04:06 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 19:04:06 @agent_ppo2.py:121][0m #------------------------ Iteration 838 --------------------------#
[32m[20221213 19:04:06 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0014 |          24.2740 |         -63.0427 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0008 |          20.2629 |         -62.1458 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0010 |          19.6229 |         -61.3225 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0063 |          20.2284 |         -60.8440 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0003 |          19.1698 |         -61.0246 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |          -0.0021 |          19.1181 |         -60.8513 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0038 |          20.0955 |         -60.8499 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |           0.0051 |          19.0112 |         -60.4112 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |          -0.0034 |          18.8917 |         -60.5578 |
[32m[20221213 19:04:07 @agent_ppo2.py:185][0m |          -0.0033 |          18.8420 |         -60.4307 |
[32m[20221213 19:04:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.80
[32m[20221213 19:04:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.00
[32m[20221213 19:04:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 19:04:07 @agent_ppo2.py:143][0m Total time:      15.58 min
[32m[20221213 19:04:07 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221213 19:04:07 @agent_ppo2.py:121][0m #------------------------ Iteration 839 --------------------------#
[32m[20221213 19:04:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |           0.0227 |         197.9421 |         -56.8428 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |           0.0084 |         204.1192 |         -58.1304 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |           0.0044 |         192.6631 |         -57.1161 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |          -0.0005 |         192.4947 |         -58.1652 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |          -0.0045 |         192.2490 |         -59.0086 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |          -0.0037 |         192.1871 |         -59.2565 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |           0.0206 |         219.3103 |         -59.0168 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |          -0.0004 |         192.9333 |         -58.3641 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |           0.0001 |         191.6577 |         -58.7509 |
[32m[20221213 19:04:08 @agent_ppo2.py:185][0m |          -0.0059 |         191.7648 |         -59.1904 |
[32m[20221213 19:04:08 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:04:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.60
[32m[20221213 19:04:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 19:04:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:04:09 @agent_ppo2.py:143][0m Total time:      15.60 min
[32m[20221213 19:04:09 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 19:04:09 @agent_ppo2.py:121][0m #------------------------ Iteration 840 --------------------------#
[32m[20221213 19:04:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:04:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |           0.0091 |           8.8282 |         -58.9656 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |           0.0082 |           4.4938 |         -58.4840 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |           0.0014 |           4.3435 |         -58.3778 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |           0.0047 |           4.3069 |         -57.5649 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |          -0.0012 |           4.2611 |         -57.2544 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |          -0.0027 |           4.2898 |         -56.8972 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |          -0.0134 |           4.2572 |         -57.3414 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |          -0.0151 |           4.1799 |         -57.5549 |
[32m[20221213 19:04:09 @agent_ppo2.py:185][0m |          -0.0164 |           4.1706 |         -57.6669 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |           0.0007 |           4.4816 |         -57.5939 |
[32m[20221213 19:04:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.60
[32m[20221213 19:04:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:04:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.00
[32m[20221213 19:04:10 @agent_ppo2.py:143][0m Total time:      15.62 min
[32m[20221213 19:04:10 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221213 19:04:10 @agent_ppo2.py:121][0m #------------------------ Iteration 841 --------------------------#
[32m[20221213 19:04:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |           0.0052 |           7.5328 |         -57.5392 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |           0.0014 |           4.6513 |         -56.9091 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |           0.0091 |           4.4445 |         -57.0129 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |           0.0009 |           4.3271 |         -57.0068 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |          -0.0044 |           4.2885 |         -57.1745 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |           0.0021 |           4.2391 |         -57.2639 |
[32m[20221213 19:04:10 @agent_ppo2.py:185][0m |          -0.0039 |           4.1949 |         -56.8816 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |          -0.0050 |           4.1955 |         -56.7338 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |          -0.0044 |           4.1610 |         -56.7092 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |          -0.0006 |           4.1257 |         -56.7870 |
[32m[20221213 19:04:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:04:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.60
[32m[20221213 19:04:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:04:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 19:04:11 @agent_ppo2.py:143][0m Total time:      15.64 min
[32m[20221213 19:04:11 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 19:04:11 @agent_ppo2.py:121][0m #------------------------ Iteration 842 --------------------------#
[32m[20221213 19:04:11 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |           0.0053 |         191.5227 |         -57.9794 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |          -0.0022 |         188.8673 |         -58.0326 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |          -0.0005 |         188.4047 |         -57.5481 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |           0.0003 |         188.3107 |         -57.7347 |
[32m[20221213 19:04:11 @agent_ppo2.py:185][0m |           0.0061 |         188.1753 |         -57.6395 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |          -0.0004 |         188.0923 |         -57.3020 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |          -0.0028 |         188.0120 |         -56.5717 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |          -0.0020 |         187.9458 |         -57.2607 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |           0.0074 |         191.8973 |         -56.8222 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |           0.0022 |         188.1398 |         -56.6498 |
[32m[20221213 19:04:12 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.40
[32m[20221213 19:04:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.00
[32m[20221213 19:04:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:04:12 @agent_ppo2.py:143][0m Total time:      15.66 min
[32m[20221213 19:04:12 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221213 19:04:12 @agent_ppo2.py:121][0m #------------------------ Iteration 843 --------------------------#
[32m[20221213 19:04:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |           0.0015 |         198.7568 |         -55.5801 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |          -0.0022 |         195.9560 |         -56.0439 |
[32m[20221213 19:04:12 @agent_ppo2.py:185][0m |          -0.0054 |         195.9755 |         -56.6292 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |           0.0010 |         195.2579 |         -57.0301 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |          -0.0000 |         195.7878 |         -56.6543 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |           0.0025 |         195.3532 |         -57.3487 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |          -0.0012 |         195.2824 |         -57.0525 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |          -0.0021 |         195.2388 |         -57.8385 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |           0.0008 |         195.0499 |         -58.0777 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |          -0.0019 |         194.9410 |         -58.7632 |
[32m[20221213 19:04:13 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.80
[32m[20221213 19:04:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 19:04:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.00
[32m[20221213 19:04:13 @agent_ppo2.py:143][0m Total time:      15.68 min
[32m[20221213 19:04:13 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 19:04:13 @agent_ppo2.py:121][0m #------------------------ Iteration 844 --------------------------#
[32m[20221213 19:04:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |           0.0181 |           7.9626 |         -59.8338 |
[32m[20221213 19:04:13 @agent_ppo2.py:185][0m |           0.0300 |           3.7641 |         -58.6018 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |           0.0179 |           3.6521 |         -57.7708 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |           0.0038 |           3.5667 |         -57.6830 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |           0.0056 |           3.5969 |         -57.0904 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |          -0.0023 |           3.5369 |         -57.4695 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |          -0.0011 |           3.5169 |         -57.2281 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |          -0.0067 |           3.5029 |         -57.2886 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |          -0.0076 |           3.5174 |         -56.9071 |
[32m[20221213 19:04:14 @agent_ppo2.py:185][0m |          -0.0079 |           3.5019 |         -57.0304 |
[32m[20221213 19:04:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 19:04:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.00
[32m[20221213 19:04:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:04:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:04:14 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 19:04:14 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221213 19:04:14 @agent_ppo2.py:121][0m #------------------------ Iteration 845 --------------------------#
[32m[20221213 19:04:14 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |           0.0118 |           6.9718 |         -57.7605 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |           0.0027 |           3.8282 |         -57.0648 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0035 |           3.6597 |         -57.0933 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0024 |           3.6032 |         -56.6644 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0017 |           3.5557 |         -56.3076 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |           0.0003 |           3.5419 |         -56.0501 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0047 |           3.4956 |         -56.1955 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0022 |           3.4767 |         -56.2619 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0069 |           3.4597 |         -55.6243 |
[32m[20221213 19:04:15 @agent_ppo2.py:185][0m |          -0.0015 |           3.4168 |         -55.4691 |
[32m[20221213 19:04:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.00
[32m[20221213 19:04:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 19:04:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 19:04:15 @agent_ppo2.py:143][0m Total time:      15.72 min
[32m[20221213 19:04:15 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 19:04:15 @agent_ppo2.py:121][0m #------------------------ Iteration 846 --------------------------#
[32m[20221213 19:04:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |           0.0025 |           4.2332 |         -53.9333 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |           0.0023 |           2.6644 |         -53.7370 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |          -0.0024 |           2.5881 |         -53.1019 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |          -0.0017 |           2.5288 |         -53.0427 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |           0.0093 |           2.4927 |         -52.8303 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |           0.0055 |           2.4721 |         -52.7172 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |           0.0018 |           2.4557 |         -52.8204 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |          -0.0024 |           2.4141 |         -52.3903 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |          -0.0078 |           2.4121 |         -52.6847 |
[32m[20221213 19:04:16 @agent_ppo2.py:185][0m |          -0.0043 |           2.3992 |         -52.4224 |
[32m[20221213 19:04:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.80
[32m[20221213 19:04:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:04:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 19:04:17 @agent_ppo2.py:143][0m Total time:      15.74 min
[32m[20221213 19:04:17 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221213 19:04:17 @agent_ppo2.py:121][0m #------------------------ Iteration 847 --------------------------#
[32m[20221213 19:04:17 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |           0.0051 |         193.1468 |         -52.8745 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |          -0.0001 |         182.2216 |         -52.7285 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |           0.0041 |         181.8912 |         -53.3139 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |           0.0097 |         186.7296 |         -53.1843 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |           0.0066 |         181.5474 |         -53.0431 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |          -0.0020 |         181.5288 |         -53.3202 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |           0.0017 |         190.5857 |         -53.3730 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |           0.0009 |         181.6414 |         -53.5142 |
[32m[20221213 19:04:17 @agent_ppo2.py:185][0m |          -0.0065 |         181.7398 |         -52.7166 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0012 |         182.5924 |         -53.3261 |
[32m[20221213 19:04:18 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.40
[32m[20221213 19:04:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.00
[32m[20221213 19:04:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 893.00
[32m[20221213 19:04:18 @agent_ppo2.py:143][0m Total time:      15.75 min
[32m[20221213 19:04:18 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 19:04:18 @agent_ppo2.py:121][0m #------------------------ Iteration 848 --------------------------#
[32m[20221213 19:04:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0041 |           4.3961 |         -54.5328 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0060 |           2.7962 |         -54.5526 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0026 |           2.6129 |         -54.5399 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0039 |           2.5379 |         -54.8099 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0145 |           2.4859 |         -54.4614 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0094 |           2.4605 |         -54.3423 |
[32m[20221213 19:04:18 @agent_ppo2.py:185][0m |           0.0073 |           2.4476 |         -54.7589 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |          -0.0082 |           2.4160 |         -55.1023 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |          -0.0032 |           2.4009 |         -54.4540 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |          -0.0082 |           2.3941 |         -55.0040 |
[32m[20221213 19:04:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.60
[32m[20221213 19:04:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 19:04:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:04:19 @agent_ppo2.py:143][0m Total time:      15.77 min
[32m[20221213 19:04:19 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221213 19:04:19 @agent_ppo2.py:121][0m #------------------------ Iteration 849 --------------------------#
[32m[20221213 19:04:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |          -0.0040 |           2.3312 |         -53.7088 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |          -0.0019 |           1.9992 |         -53.7015 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |           0.0028 |           1.9771 |         -53.2394 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |           0.0040 |           1.9718 |         -53.3985 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |           0.0020 |           1.9632 |         -53.3794 |
[32m[20221213 19:04:19 @agent_ppo2.py:185][0m |          -0.0050 |           1.9607 |         -53.0581 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |          -0.0023 |           1.9621 |         -53.1233 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |          -0.0002 |           1.9671 |         -53.3994 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |          -0.0036 |           1.9571 |         -52.9523 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |           0.0027 |           1.9554 |         -52.8828 |
[32m[20221213 19:04:20 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.40
[32m[20221213 19:04:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:04:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:04:20 @agent_ppo2.py:143][0m Total time:      15.79 min
[32m[20221213 19:04:20 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 19:04:20 @agent_ppo2.py:121][0m #------------------------ Iteration 850 --------------------------#
[32m[20221213 19:04:20 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |           0.0055 |           3.1106 |         -55.3412 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |           0.0021 |           2.2902 |         -55.2319 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |           0.0094 |           2.2274 |         -55.0561 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |          -0.0020 |           2.1888 |         -55.3929 |
[32m[20221213 19:04:20 @agent_ppo2.py:185][0m |          -0.0011 |           2.1695 |         -55.3077 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |          -0.0038 |           2.1566 |         -54.7550 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |          -0.0004 |           2.1422 |         -54.8389 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |          -0.0054 |           2.1446 |         -54.9267 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |          -0.0044 |           2.1269 |         -54.7830 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |          -0.0070 |           2.1282 |         -54.9344 |
[32m[20221213 19:04:21 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:04:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.40
[32m[20221213 19:04:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.00
[32m[20221213 19:04:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:04:21 @agent_ppo2.py:143][0m Total time:      15.81 min
[32m[20221213 19:04:21 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221213 19:04:21 @agent_ppo2.py:121][0m #------------------------ Iteration 851 --------------------------#
[32m[20221213 19:04:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |           0.0051 |         119.0550 |         -54.3521 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |           0.0010 |         115.4378 |         -54.1735 |
[32m[20221213 19:04:21 @agent_ppo2.py:185][0m |           0.0014 |         114.4692 |         -53.8437 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |           0.0008 |         113.7979 |         -53.9858 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |          -0.0016 |         113.2839 |         -53.8658 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |           0.0060 |         112.9824 |         -52.9829 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |          -0.0016 |         113.4191 |         -53.7636 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |          -0.0006 |         113.0807 |         -53.8736 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |           0.0113 |         140.2249 |         -53.7224 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |           0.0039 |         112.8319 |         -53.3550 |
[32m[20221213 19:04:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.20
[32m[20221213 19:04:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.00
[32m[20221213 19:04:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 19:04:22 @agent_ppo2.py:143][0m Total time:      15.83 min
[32m[20221213 19:04:22 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 19:04:22 @agent_ppo2.py:121][0m #------------------------ Iteration 852 --------------------------#
[32m[20221213 19:04:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |           0.0063 |          21.6484 |         -54.5991 |
[32m[20221213 19:04:22 @agent_ppo2.py:185][0m |           0.0062 |          17.3240 |         -54.2543 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |          -0.0026 |          15.6425 |         -53.3573 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |          -0.0007 |          15.2913 |         -53.5817 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |           0.0062 |          16.0805 |         -52.9853 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |          -0.0039 |          15.1387 |         -52.8072 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |          -0.0074 |          14.8961 |         -52.4594 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |          -0.0030 |          14.7427 |         -52.6622 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |           0.0065 |          14.7989 |         -52.4439 |
[32m[20221213 19:04:23 @agent_ppo2.py:185][0m |           0.0010 |          14.8590 |         -51.9963 |
[32m[20221213 19:04:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.40
[32m[20221213 19:04:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 72.00
[32m[20221213 19:04:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:04:23 @agent_ppo2.py:143][0m Total time:      15.85 min
[32m[20221213 19:04:23 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221213 19:04:23 @agent_ppo2.py:121][0m #------------------------ Iteration 853 --------------------------#
[32m[20221213 19:04:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0087 |           2.7020 |         -50.9990 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0078 |           2.0801 |         -51.6280 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0028 |           2.0281 |         -51.8793 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0007 |           1.9985 |         -52.2132 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0088 |           1.9777 |         -51.6465 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0016 |           1.9629 |         -52.2002 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0070 |           1.9535 |         -52.6981 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0014 |           1.9380 |         -52.1150 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |          -0.0001 |           1.9276 |         -52.3150 |
[32m[20221213 19:04:24 @agent_ppo2.py:185][0m |           0.0007 |           1.9224 |         -52.2749 |
[32m[20221213 19:04:24 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.80
[32m[20221213 19:04:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 19:04:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:04:24 @agent_ppo2.py:143][0m Total time:      15.87 min
[32m[20221213 19:04:24 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 19:04:24 @agent_ppo2.py:121][0m #------------------------ Iteration 854 --------------------------#
[32m[20221213 19:04:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0008 |          16.0921 |         -51.8501 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0032 |          13.8235 |         -51.5402 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |           0.0082 |          15.1392 |         -51.5478 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0007 |          13.4665 |         -50.9523 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0037 |          13.4394 |         -51.0590 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |           0.0009 |          13.2019 |         -50.7704 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0026 |          13.1016 |         -51.4356 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0050 |          13.0494 |         -51.4590 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0047 |          12.9474 |         -51.1736 |
[32m[20221213 19:04:25 @agent_ppo2.py:185][0m |          -0.0049 |          12.8843 |         -51.1867 |
[32m[20221213 19:04:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.80
[32m[20221213 19:04:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 73.00
[32m[20221213 19:04:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 19:04:25 @agent_ppo2.py:143][0m Total time:      15.88 min
[32m[20221213 19:04:25 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221213 19:04:25 @agent_ppo2.py:121][0m #------------------------ Iteration 855 --------------------------#
[32m[20221213 19:04:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |           0.0024 |         188.0037 |         -50.6331 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |           0.0033 |         180.4349 |         -50.4232 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |           0.0015 |         179.7035 |         -50.1434 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |           0.0019 |         179.5465 |         -50.2648 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |          -0.0026 |         179.7670 |         -50.7495 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |           0.0019 |         179.1904 |         -50.4538 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |          -0.0040 |         179.2898 |         -50.6127 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |          -0.0006 |         179.2487 |         -50.8481 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |          -0.0002 |         179.9590 |         -51.0883 |
[32m[20221213 19:04:26 @agent_ppo2.py:185][0m |           0.0062 |         179.0151 |         -49.5272 |
[32m[20221213 19:04:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.00
[32m[20221213 19:04:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:04:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.00
[32m[20221213 19:04:27 @agent_ppo2.py:143][0m Total time:      15.90 min
[32m[20221213 19:04:27 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 19:04:27 @agent_ppo2.py:121][0m #------------------------ Iteration 856 --------------------------#
[32m[20221213 19:04:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |           0.0031 |         100.7651 |         -52.3685 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |          -0.0003 |          93.2896 |         -52.7086 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |           0.0044 |          91.8776 |         -52.7390 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |          -0.0001 |          90.2349 |         -53.0070 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |          -0.0015 |          89.7999 |         -53.4126 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |           0.0018 |          89.0925 |         -53.3719 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |          -0.0017 |          88.7884 |         -53.9302 |
[32m[20221213 19:04:27 @agent_ppo2.py:185][0m |           0.0003 |          88.7620 |         -54.3347 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |          -0.0022 |          88.1721 |         -54.3517 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |          -0.0040 |          88.2927 |         -54.7368 |
[32m[20221213 19:04:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:04:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.60
[32m[20221213 19:04:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 19:04:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:04:28 @agent_ppo2.py:143][0m Total time:      15.92 min
[32m[20221213 19:04:28 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221213 19:04:28 @agent_ppo2.py:121][0m #------------------------ Iteration 857 --------------------------#
[32m[20221213 19:04:28 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |           0.0035 |         205.8993 |         -55.2074 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |           0.0144 |         222.2151 |         -55.7213 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |           0.0002 |         199.2694 |         -55.7211 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |          -0.0014 |         198.5578 |         -56.1798 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |          -0.0005 |         198.6119 |         -55.9831 |
[32m[20221213 19:04:28 @agent_ppo2.py:185][0m |           0.0102 |         214.8534 |         -56.0366 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0038 |         198.4597 |         -55.0676 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0003 |         198.5935 |         -56.2212 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0001 |         198.3919 |         -56.3520 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |          -0.0048 |         198.2212 |         -56.0223 |
[32m[20221213 19:04:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:04:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.00
[32m[20221213 19:04:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 19:04:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:04:29 @agent_ppo2.py:143][0m Total time:      15.94 min
[32m[20221213 19:04:29 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 19:04:29 @agent_ppo2.py:121][0m #------------------------ Iteration 858 --------------------------#
[32m[20221213 19:04:29 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0103 |          64.8698 |         -57.7664 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0040 |          56.1573 |         -57.6123 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0002 |          54.7023 |         -58.0493 |
[32m[20221213 19:04:29 @agent_ppo2.py:185][0m |           0.0047 |          53.9495 |         -57.8928 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |           0.0046 |          53.4169 |         -57.9421 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |           0.0050 |          52.9100 |         -57.9765 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |           0.0019 |          53.0479 |         -58.7638 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |          -0.0015 |          52.3518 |         -58.4722 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |          -0.0032 |          52.1924 |         -58.4769 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |           0.0012 |          52.0861 |         -57.9808 |
[32m[20221213 19:04:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.20
[32m[20221213 19:04:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 19:04:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.00
[32m[20221213 19:04:30 @agent_ppo2.py:143][0m Total time:      15.96 min
[32m[20221213 19:04:30 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221213 19:04:30 @agent_ppo2.py:121][0m #------------------------ Iteration 859 --------------------------#
[32m[20221213 19:04:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |          -0.0044 |         209.5597 |         -60.5301 |
[32m[20221213 19:04:30 @agent_ppo2.py:185][0m |          -0.0021 |         206.7767 |         -60.8450 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |           0.0010 |         204.7818 |         -61.2222 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |           0.0033 |         204.1541 |         -61.0832 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |           0.0054 |         204.1720 |         -60.7398 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |          -0.0017 |         203.9092 |         -61.6655 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |           0.0085 |         211.8568 |         -61.7113 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |           0.0007 |         203.8536 |         -61.6840 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |          -0.0033 |         203.9964 |         -61.8735 |
[32m[20221213 19:04:31 @agent_ppo2.py:185][0m |           0.0011 |         204.3406 |         -62.3827 |
[32m[20221213 19:04:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:04:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.00
[32m[20221213 19:04:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 19:04:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.00
[32m[20221213 19:04:31 @agent_ppo2.py:143][0m Total time:      15.98 min
[32m[20221213 19:04:31 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 19:04:31 @agent_ppo2.py:121][0m #------------------------ Iteration 860 --------------------------#
[32m[20221213 19:04:31 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |           0.0042 |          17.4554 |         -62.5639 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |           0.0037 |          11.5605 |         -62.3725 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |           0.0055 |          11.2224 |         -62.6882 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |           0.0015 |          11.0261 |         -62.8899 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |          -0.0026 |          10.8783 |         -63.0291 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |           0.0018 |          10.8086 |         -62.4189 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |          -0.0005 |          10.7328 |         -62.6413 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |          -0.0035 |          10.6595 |         -62.7646 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |          -0.0015 |          10.6093 |         -62.5131 |
[32m[20221213 19:04:32 @agent_ppo2.py:185][0m |           0.0015 |          10.5872 |         -62.3182 |
[32m[20221213 19:04:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 19:04:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.80
[32m[20221213 19:04:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 19:04:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:04:32 @agent_ppo2.py:143][0m Total time:      16.00 min
[32m[20221213 19:04:32 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221213 19:04:32 @agent_ppo2.py:121][0m #------------------------ Iteration 861 --------------------------#
[32m[20221213 19:04:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |           0.0032 |         142.2586 |         -63.0158 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |           0.0101 |         139.3324 |         -63.5368 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |          -0.0022 |         135.2071 |         -63.7475 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |           0.0049 |         135.7181 |         -63.8601 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |          -0.0001 |         133.6241 |         -63.4477 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |           0.0080 |         146.7144 |         -64.1540 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |           0.0053 |         133.1337 |         -64.3165 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |           0.0004 |         132.7262 |         -64.8273 |
[32m[20221213 19:04:33 @agent_ppo2.py:185][0m |          -0.0053 |         132.4295 |         -64.9029 |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |           0.0048 |         136.9321 |         -64.6228 |
[32m[20221213 19:04:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 19:04:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.40
[32m[20221213 19:04:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.00
[32m[20221213 19:04:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:04:34 @agent_ppo2.py:143][0m Total time:      16.02 min
[32m[20221213 19:04:34 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 19:04:34 @agent_ppo2.py:121][0m #------------------------ Iteration 862 --------------------------#
[32m[20221213 19:04:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |           0.0089 |         228.1855 |         -67.1704 |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |           0.0144 |         228.5610 |         -65.8426 |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |          -0.0037 |         206.0810 |         -66.4666 |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |          -0.0025 |         205.5835 |         -66.5450 |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |           0.0043 |         205.2387 |         -66.0746 |
[32m[20221213 19:04:34 @agent_ppo2.py:185][0m |          -0.0040 |         204.8154 |         -66.4392 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |           0.0010 |         205.2623 |         -66.0334 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |           0.0006 |         206.2933 |         -66.1624 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |          -0.0015 |         204.2245 |         -65.9835 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |          -0.0005 |         204.3130 |         -65.7420 |
[32m[20221213 19:04:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:04:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 849.20
[32m[20221213 19:04:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.00
[32m[20221213 19:04:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:04:35 @agent_ppo2.py:143][0m Total time:      16.04 min
[32m[20221213 19:04:35 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221213 19:04:35 @agent_ppo2.py:121][0m #------------------------ Iteration 863 --------------------------#
[32m[20221213 19:04:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |          -0.0004 |         213.8799 |         -64.7518 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |          -0.0015 |         210.8200 |         -65.0071 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |           0.0213 |         233.4540 |         -66.0289 |
[32m[20221213 19:04:35 @agent_ppo2.py:185][0m |           0.0051 |         210.1046 |         -65.4894 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |           0.0038 |         210.0811 |         -66.0879 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |          -0.0027 |         209.5232 |         -66.3814 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |           0.0009 |         209.7790 |         -65.9164 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |           0.0007 |         209.2455 |         -66.2281 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |           0.0024 |         209.6479 |         -66.0727 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |          -0.0007 |         209.2779 |         -66.4797 |
[32m[20221213 19:04:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:04:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.60
[32m[20221213 19:04:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 888.00
[32m[20221213 19:04:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 19:04:36 @agent_ppo2.py:143][0m Total time:      16.06 min
[32m[20221213 19:04:36 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 19:04:36 @agent_ppo2.py:121][0m #------------------------ Iteration 864 --------------------------#
[32m[20221213 19:04:36 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |           0.0103 |          16.5567 |         -66.8757 |
[32m[20221213 19:04:36 @agent_ppo2.py:185][0m |           0.0141 |           5.4404 |         -66.1739 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |           0.0045 |           4.8672 |         -66.4567 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |           0.0006 |           4.5348 |         -65.7313 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |          -0.0031 |           4.3091 |         -65.2145 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |          -0.0092 |           4.1577 |         -65.0888 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |          -0.0038 |           4.1145 |         -64.5932 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |          -0.0051 |           4.0531 |         -64.9374 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |          -0.0066 |           3.9577 |         -64.5324 |
[32m[20221213 19:04:37 @agent_ppo2.py:185][0m |          -0.0092 |           3.9324 |         -63.9337 |
[32m[20221213 19:04:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 19:04:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.20
[32m[20221213 19:04:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:04:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:04:37 @agent_ppo2.py:143][0m Total time:      16.08 min
[32m[20221213 19:04:37 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221213 19:04:37 @agent_ppo2.py:121][0m #------------------------ Iteration 865 --------------------------#
[32m[20221213 19:04:37 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |           0.0034 |          10.0238 |         -62.3168 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0006 |           4.0090 |         -62.2782 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0062 |           3.5729 |         -61.7380 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0054 |           3.3280 |         -61.0676 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0004 |           3.1624 |         -60.8199 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0075 |           3.0051 |         -60.4866 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0027 |           2.9303 |         -60.1827 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0046 |           2.8718 |         -59.9938 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0108 |           2.7348 |         -59.9855 |
[32m[20221213 19:04:38 @agent_ppo2.py:185][0m |          -0.0114 |           2.6802 |         -59.5708 |
[32m[20221213 19:04:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.20
[32m[20221213 19:04:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:04:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.00
[32m[20221213 19:04:38 @agent_ppo2.py:143][0m Total time:      16.10 min
[32m[20221213 19:04:38 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 19:04:38 @agent_ppo2.py:121][0m #------------------------ Iteration 866 --------------------------#
[32m[20221213 19:04:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0035 |           2.3820 |         -59.0077 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0016 |           1.9484 |         -59.2355 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0018 |           1.9073 |         -58.8830 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0021 |           1.8918 |         -59.0443 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0006 |           1.8834 |         -59.0514 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0009 |           1.8736 |         -59.1073 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0004 |           1.8691 |         -59.1573 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |          -0.0064 |           1.8675 |         -58.9412 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |           0.0014 |           1.8640 |         -59.3837 |
[32m[20221213 19:04:39 @agent_ppo2.py:185][0m |          -0.0027 |           1.8625 |         -59.0263 |
[32m[20221213 19:04:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:04:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.80
[32m[20221213 19:04:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:04:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.00
[32m[20221213 19:04:40 @agent_ppo2.py:143][0m Total time:      16.12 min
[32m[20221213 19:04:40 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221213 19:04:40 @agent_ppo2.py:121][0m #------------------------ Iteration 867 --------------------------#
[32m[20221213 19:04:40 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0022 |           2.6413 |         -59.2234 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0044 |           2.0668 |         -59.0010 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0036 |           2.0359 |         -59.3767 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0056 |           2.0110 |         -58.9475 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0033 |           2.0005 |         -59.1280 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0003 |           1.9931 |         -59.2168 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |          -0.0020 |           1.9871 |         -58.8900 |
[32m[20221213 19:04:40 @agent_ppo2.py:185][0m |           0.0004 |           1.9847 |         -59.1417 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |          -0.0034 |           1.9834 |         -59.2970 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0014 |           1.9784 |         -59.2180 |
[32m[20221213 19:04:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:04:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 69.40
[32m[20221213 19:04:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.00
[32m[20221213 19:04:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 19:04:41 @agent_ppo2.py:143][0m Total time:      16.14 min
[32m[20221213 19:04:41 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 19:04:41 @agent_ppo2.py:121][0m #------------------------ Iteration 868 --------------------------#
[32m[20221213 19:04:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0045 |         183.2911 |         -59.4565 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0039 |         182.2540 |         -60.2317 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0043 |         181.6075 |         -60.8408 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0211 |         196.7347 |         -60.4590 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |          -0.0018 |         181.7410 |         -61.2563 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0008 |         181.5157 |         -61.5863 |
[32m[20221213 19:04:41 @agent_ppo2.py:185][0m |           0.0042 |         194.4449 |         -62.2262 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |           0.0153 |         183.2104 |         -60.5233 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |           0.0110 |         182.5231 |         -61.0497 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |           0.0047 |         181.2274 |         -62.0392 |
[32m[20221213 19:04:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:04:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:04:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 19:04:42 @agent_ppo2.py:143][0m Total time:      16.16 min
[32m[20221213 19:04:42 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221213 19:04:42 @agent_ppo2.py:121][0m #------------------------ Iteration 869 --------------------------#
[32m[20221213 19:04:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |          -0.0006 |         189.2167 |         -64.6305 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |           0.0008 |         183.9925 |         -65.3292 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |          -0.0007 |         183.2057 |         -65.1922 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |          -0.0019 |         182.4860 |         -64.9007 |
[32m[20221213 19:04:42 @agent_ppo2.py:185][0m |          -0.0011 |         182.5558 |         -65.1419 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |           0.0099 |         183.0469 |         -64.6240 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |           0.0094 |         189.5438 |         -64.9920 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |          -0.0032 |         182.3646 |         -64.9766 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |          -0.0034 |         182.1312 |         -65.1420 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |          -0.0031 |         182.5381 |         -65.0918 |
[32m[20221213 19:04:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:04:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.60
[32m[20221213 19:04:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 19:04:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:04:43 @agent_ppo2.py:143][0m Total time:      16.18 min
[32m[20221213 19:04:43 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 19:04:43 @agent_ppo2.py:121][0m #------------------------ Iteration 870 --------------------------#
[32m[20221213 19:04:43 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |           0.0133 |          43.7922 |         -66.0703 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |           0.0049 |          28.3107 |         -66.4523 |
[32m[20221213 19:04:43 @agent_ppo2.py:185][0m |           0.0036 |          27.3073 |         -66.7171 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0101 |          26.8769 |         -67.2511 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0098 |          26.6166 |         -67.4978 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0087 |          26.9515 |         -67.7868 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0081 |          26.0829 |         -67.7027 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0016 |          25.9395 |         -67.7587 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0025 |          26.2280 |         -68.0922 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0041 |          25.8887 |         -68.2084 |
[32m[20221213 19:04:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:04:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 73.80
[32m[20221213 19:04:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 76.00
[32m[20221213 19:04:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:04:44 @agent_ppo2.py:143][0m Total time:      16.20 min
[32m[20221213 19:04:44 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221213 19:04:44 @agent_ppo2.py:121][0m #------------------------ Iteration 871 --------------------------#
[32m[20221213 19:04:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |           0.0046 |          70.3228 |         -68.1089 |
[32m[20221213 19:04:44 @agent_ppo2.py:185][0m |          -0.0038 |          62.5559 |         -67.9459 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |          -0.0063 |          61.6491 |         -68.2830 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |           0.0033 |          61.0434 |         -68.3800 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |          -0.0015 |          60.5332 |         -68.4681 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |           0.0025 |          60.2894 |         -68.4411 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |          -0.0017 |          60.1518 |         -68.0097 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |           0.0000 |          59.8136 |         -68.4537 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |          -0.0023 |          59.7741 |         -68.5992 |
[32m[20221213 19:04:45 @agent_ppo2.py:185][0m |          -0.0013 |          60.3969 |         -68.5479 |
[32m[20221213 19:04:45 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.80
[32m[20221213 19:04:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 19:04:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:04:45 @agent_ppo2.py:143][0m Total time:      16.21 min
[32m[20221213 19:04:45 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 19:04:45 @agent_ppo2.py:121][0m #------------------------ Iteration 872 --------------------------#
[32m[20221213 19:04:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |           0.0097 |         104.4023 |         -69.2693 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |           0.0030 |          96.8389 |         -69.2265 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |           0.0001 |          95.7544 |         -69.5587 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |          -0.0005 |          95.6980 |         -69.9222 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |           0.0034 |          94.7913 |         -70.1379 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |          -0.0037 |          94.3934 |         -70.6048 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |          -0.0049 |          94.1778 |         -70.7894 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |           0.0028 |          93.6988 |         -70.7723 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |          -0.0021 |          94.1677 |         -70.8346 |
[32m[20221213 19:04:46 @agent_ppo2.py:185][0m |           0.0111 |         106.7634 |         -71.0425 |
[32m[20221213 19:04:46 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.20
[32m[20221213 19:04:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.00
[32m[20221213 19:04:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:04:46 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221213 19:04:46 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221213 19:04:46 @agent_ppo2.py:121][0m #------------------------ Iteration 873 --------------------------#
[32m[20221213 19:04:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |           0.0085 |           8.3392 |         -73.4868 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |           0.0017 |           4.4549 |         -72.6118 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |           0.0042 |           3.8535 |         -72.7550 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |          -0.0000 |           3.4943 |         -72.7348 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |           0.0069 |           3.3453 |         -72.5351 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |          -0.0086 |           3.1646 |         -71.7143 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |          -0.0054 |           3.0890 |         -71.9621 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |          -0.0097 |           3.0158 |         -71.9431 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |          -0.0139 |           2.9880 |         -72.0807 |
[32m[20221213 19:04:47 @agent_ppo2.py:185][0m |          -0.0040 |           2.9518 |         -72.0926 |
[32m[20221213 19:04:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.60
[32m[20221213 19:04:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:04:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.00
[32m[20221213 19:04:47 @agent_ppo2.py:143][0m Total time:      16.25 min
[32m[20221213 19:04:47 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 19:04:47 @agent_ppo2.py:121][0m #------------------------ Iteration 874 --------------------------#
[32m[20221213 19:04:48 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0035 |         203.4255 |         -72.0175 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |          -0.0028 |         197.1712 |         -71.9340 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0033 |         196.8934 |         -71.3950 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0031 |         196.6907 |         -70.6284 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0028 |         196.6581 |         -70.2361 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0064 |         201.8660 |         -70.7133 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0048 |         202.8929 |         -70.6299 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0006 |         196.6709 |         -69.7759 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |           0.0014 |         196.9600 |         -69.0524 |
[32m[20221213 19:04:48 @agent_ppo2.py:185][0m |          -0.0011 |         196.6243 |         -69.6895 |
[32m[20221213 19:04:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.80
[32m[20221213 19:04:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 19:04:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:04:49 @agent_ppo2.py:143][0m Total time:      16.27 min
[32m[20221213 19:04:49 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221213 19:04:49 @agent_ppo2.py:121][0m #------------------------ Iteration 875 --------------------------#
[32m[20221213 19:04:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |           0.0021 |          71.3899 |         -68.3811 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |           0.0044 |          64.8377 |         -68.5154 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |          -0.0017 |          63.8472 |         -69.1485 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |          -0.0030 |          63.2501 |         -69.8454 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |          -0.0004 |          64.5953 |         -69.7618 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |          -0.0003 |          62.8458 |         -70.2934 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |          -0.0046 |          62.6539 |         -70.3992 |
[32m[20221213 19:04:49 @agent_ppo2.py:185][0m |           0.0017 |          62.3733 |         -70.8943 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0024 |          62.2676 |         -70.9683 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0011 |          62.1429 |         -71.4944 |
[32m[20221213 19:04:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.60
[32m[20221213 19:04:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 19:04:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.00
[32m[20221213 19:04:50 @agent_ppo2.py:143][0m Total time:      16.29 min
[32m[20221213 19:04:50 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 19:04:50 @agent_ppo2.py:121][0m #------------------------ Iteration 876 --------------------------#
[32m[20221213 19:04:50 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0009 |         209.0606 |         -75.3993 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0033 |         205.5969 |         -75.7785 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0150 |         224.8818 |         -76.3921 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0017 |         205.3767 |         -75.7101 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |          -0.0007 |         205.3717 |         -75.5746 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0031 |         207.9338 |         -76.5678 |
[32m[20221213 19:04:50 @agent_ppo2.py:185][0m |           0.0011 |         205.2093 |         -75.0325 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |           0.0149 |         217.2795 |         -75.4130 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |           0.0025 |         205.2547 |         -76.2993 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |          -0.0035 |         205.2213 |         -75.6256 |
[32m[20221213 19:04:51 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:04:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.00
[32m[20221213 19:04:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 888.00
[32m[20221213 19:04:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.00
[32m[20221213 19:04:51 @agent_ppo2.py:143][0m Total time:      16.31 min
[32m[20221213 19:04:51 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221213 19:04:51 @agent_ppo2.py:121][0m #------------------------ Iteration 877 --------------------------#
[32m[20221213 19:04:51 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:04:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |          -0.0012 |         178.0471 |         -77.0177 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |           0.0016 |         175.8423 |         -77.2968 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |          -0.0019 |         174.5452 |         -77.4229 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |           0.0026 |         174.1787 |         -77.2559 |
[32m[20221213 19:04:51 @agent_ppo2.py:185][0m |           0.0032 |         173.7230 |         -77.5807 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |           0.0018 |         173.9051 |         -77.1169 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |           0.0019 |         173.3083 |         -77.0930 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |           0.0002 |         174.6291 |         -77.5097 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |          -0.0037 |         173.0869 |         -77.7960 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |           0.0019 |         174.2381 |         -78.0024 |
[32m[20221213 19:04:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:04:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 613.80
[32m[20221213 19:04:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.00
[32m[20221213 19:04:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.00
[32m[20221213 19:04:52 @agent_ppo2.py:143][0m Total time:      16.33 min
[32m[20221213 19:04:52 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 19:04:52 @agent_ppo2.py:121][0m #------------------------ Iteration 878 --------------------------#
[32m[20221213 19:04:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |           0.0018 |         146.7339 |         -77.8711 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |          -0.0008 |         138.0259 |         -78.3569 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |          -0.0031 |         136.7555 |         -77.5612 |
[32m[20221213 19:04:52 @agent_ppo2.py:185][0m |           0.0008 |         136.0935 |         -77.1822 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |           0.0004 |         135.9259 |         -76.9548 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |          -0.0036 |         135.7093 |         -76.7310 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |           0.0005 |         135.3872 |         -76.7310 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |          -0.0013 |         135.7069 |         -76.0441 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |          -0.0002 |         136.7138 |         -76.0291 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |           0.0043 |         136.0791 |         -75.7819 |
[32m[20221213 19:04:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:04:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.00
[32m[20221213 19:04:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 19:04:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 19:04:53 @agent_ppo2.py:143][0m Total time:      16.34 min
[32m[20221213 19:04:53 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221213 19:04:53 @agent_ppo2.py:121][0m #------------------------ Iteration 879 --------------------------#
[32m[20221213 19:04:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |           0.0029 |          26.2437 |         -75.7193 |
[32m[20221213 19:04:53 @agent_ppo2.py:185][0m |          -0.0010 |          19.9410 |         -75.3821 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |          -0.0038 |          19.2498 |         -75.4262 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |           0.0013 |          19.3886 |         -75.1061 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |          -0.0022 |          18.6114 |         -74.6671 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |           0.0026 |          19.1827 |         -74.7219 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |           0.0017 |          18.3861 |         -74.3250 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |           0.0011 |          18.4188 |         -74.0574 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |          -0.0007 |          17.9091 |         -74.1727 |
[32m[20221213 19:04:54 @agent_ppo2.py:185][0m |          -0.0120 |          17.7046 |         -74.2629 |
[32m[20221213 19:04:54 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.80
[32m[20221213 19:04:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.00
[32m[20221213 19:04:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 19:04:54 @agent_ppo2.py:143][0m Total time:      16.36 min
[32m[20221213 19:04:54 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 19:04:54 @agent_ppo2.py:121][0m #------------------------ Iteration 880 --------------------------#
[32m[20221213 19:04:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:04:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |           0.0078 |          13.0705 |         -72.2458 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |           0.0050 |           4.4971 |         -72.3885 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |          -0.0052 |           4.1803 |         -72.2328 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |           0.0130 |           4.0663 |         -72.1447 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |           0.0003 |           3.9486 |         -71.7629 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |           0.0037 |           4.0211 |         -71.5069 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |          -0.0039 |           3.8767 |         -71.5001 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |          -0.0078 |           3.7878 |         -71.6220 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |          -0.0104 |           3.7494 |         -71.7176 |
[32m[20221213 19:04:55 @agent_ppo2.py:185][0m |           0.0008 |           3.7251 |         -71.3771 |
[32m[20221213 19:04:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.80
[32m[20221213 19:04:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:04:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 19:04:55 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221213 19:04:55 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221213 19:04:55 @agent_ppo2.py:121][0m #------------------------ Iteration 881 --------------------------#
[32m[20221213 19:04:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |           0.0028 |          61.5612 |         -70.7209 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |          -0.0009 |          56.6863 |         -71.1661 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |           0.0046 |          55.5463 |         -71.3196 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |          -0.0023 |          55.2507 |         -71.1671 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |           0.0073 |          55.5064 |         -71.4415 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |          -0.0031 |          55.2486 |         -71.8979 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |          -0.0022 |          55.7995 |         -72.0435 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |           0.0055 |          55.1146 |         -71.6859 |
[32m[20221213 19:04:56 @agent_ppo2.py:185][0m |           0.0019 |          59.4580 |         -71.5897 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |          -0.0006 |          54.5142 |         -71.1014 |
[32m[20221213 19:04:57 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 19:04:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.00
[32m[20221213 19:04:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.00
[32m[20221213 19:04:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:04:57 @agent_ppo2.py:143][0m Total time:      16.40 min
[32m[20221213 19:04:57 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 19:04:57 @agent_ppo2.py:121][0m #------------------------ Iteration 882 --------------------------#
[32m[20221213 19:04:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |          -0.0011 |          56.5320 |         -72.6398 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |          -0.0010 |          50.6065 |         -71.7148 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |          -0.0061 |          49.8129 |         -71.7768 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |           0.0019 |          49.6184 |         -71.1229 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |           0.0019 |          48.9722 |         -70.8576 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |           0.0014 |          48.7682 |         -70.6686 |
[32m[20221213 19:04:57 @agent_ppo2.py:185][0m |           0.0020 |          48.5707 |         -70.4957 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |           0.0025 |          49.6045 |         -70.1371 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |          -0.0007 |          48.1891 |         -69.2708 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |          -0.0063 |          48.0608 |         -68.9710 |
[32m[20221213 19:04:58 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:04:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.60
[32m[20221213 19:04:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.00
[32m[20221213 19:04:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.00
[32m[20221213 19:04:58 @agent_ppo2.py:143][0m Total time:      16.42 min
[32m[20221213 19:04:58 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221213 19:04:58 @agent_ppo2.py:121][0m #------------------------ Iteration 883 --------------------------#
[32m[20221213 19:04:58 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:04:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |          -0.0006 |         191.1241 |         -68.9332 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |           0.0152 |         201.4086 |         -69.3650 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |           0.0127 |         188.5962 |         -68.5353 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |          -0.0031 |         186.9870 |         -69.2379 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |           0.0003 |         186.5941 |         -69.5826 |
[32m[20221213 19:04:58 @agent_ppo2.py:185][0m |          -0.0007 |         186.5680 |         -69.9192 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0108 |         188.5521 |         -68.8954 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0108 |         199.5849 |         -69.0557 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0001 |         186.5794 |         -69.5759 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0001 |         186.4100 |         -69.3715 |
[32m[20221213 19:04:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:04:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.40
[32m[20221213 19:04:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 890.00
[32m[20221213 19:04:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 19:04:59 @agent_ppo2.py:143][0m Total time:      16.44 min
[32m[20221213 19:04:59 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 19:04:59 @agent_ppo2.py:121][0m #------------------------ Iteration 884 --------------------------#
[32m[20221213 19:04:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:04:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0065 |          10.9015 |         -68.4806 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0135 |          10.2956 |         -69.1098 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |           0.0089 |          10.0160 |         -68.7963 |
[32m[20221213 19:04:59 @agent_ppo2.py:185][0m |          -0.0005 |           9.3627 |         -68.6276 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |          -0.0032 |           9.2548 |         -68.3476 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |          -0.0076 |           9.2124 |         -68.4969 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |           0.0063 |           9.1839 |         -68.6348 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |           0.0003 |           9.1610 |         -68.7357 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |           0.0105 |           9.0809 |         -67.8979 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |           0.0092 |           9.6515 |         -68.6440 |
[32m[20221213 19:05:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:05:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.40
[32m[20221213 19:05:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 67.00
[32m[20221213 19:05:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 996.00
[32m[20221213 19:05:00 @agent_ppo2.py:143][0m Total time:      16.46 min
[32m[20221213 19:05:00 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221213 19:05:00 @agent_ppo2.py:121][0m #------------------------ Iteration 885 --------------------------#
[32m[20221213 19:05:00 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |          -0.0024 |         138.6627 |         -68.7180 |
[32m[20221213 19:05:00 @agent_ppo2.py:185][0m |          -0.0011 |         132.8066 |         -68.8993 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |           0.0004 |         132.2572 |         -69.5416 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |           0.0002 |         131.0130 |         -69.9338 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0020 |         130.4821 |         -69.8411 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0023 |         129.8418 |         -70.3874 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0031 |         129.7853 |         -70.4046 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0019 |         130.2373 |         -70.8799 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0106 |         130.3581 |         -71.1644 |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0033 |         128.7266 |         -70.1798 |
[32m[20221213 19:05:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:05:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.40
[32m[20221213 19:05:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 929.00
[32m[20221213 19:05:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.00
[32m[20221213 19:05:01 @agent_ppo2.py:143][0m Total time:      16.48 min
[32m[20221213 19:05:01 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 19:05:01 @agent_ppo2.py:121][0m #------------------------ Iteration 886 --------------------------#
[32m[20221213 19:05:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:05:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:01 @agent_ppo2.py:185][0m |          -0.0006 |         193.4458 |         -69.8863 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0023 |         190.5372 |         -70.2818 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0019 |         189.6565 |         -69.9087 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |           0.0063 |         192.0767 |         -70.0290 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0018 |         189.1698 |         -70.0805 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0014 |         188.6753 |         -70.0518 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0001 |         188.9377 |         -69.6007 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0012 |         187.9511 |         -69.6154 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |          -0.0030 |         188.5279 |         -69.5856 |
[32m[20221213 19:05:02 @agent_ppo2.py:185][0m |           0.0011 |         188.8396 |         -69.9292 |
[32m[20221213 19:05:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:05:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 973.60
[32m[20221213 19:05:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.00
[32m[20221213 19:05:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 19:05:02 @agent_ppo2.py:143][0m Total time:      16.50 min
[32m[20221213 19:05:02 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221213 19:05:02 @agent_ppo2.py:121][0m #------------------------ Iteration 887 --------------------------#
[32m[20221213 19:05:02 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0055 |          10.2753 |         -70.5647 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0061 |           4.2523 |         -70.8592 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |          -0.0005 |           3.6727 |         -70.8926 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0021 |           3.4255 |         -70.2037 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0013 |           3.2793 |         -70.6620 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0027 |           3.2084 |         -70.3645 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0017 |           3.1516 |         -70.5272 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |           0.0038 |           3.1571 |         -69.8653 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |          -0.0027 |           3.0926 |         -69.9913 |
[32m[20221213 19:05:03 @agent_ppo2.py:185][0m |          -0.0091 |           3.0716 |         -70.0617 |
[32m[20221213 19:05:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:05:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.40
[32m[20221213 19:05:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 19:05:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:05:03 @agent_ppo2.py:143][0m Total time:      16.52 min
[32m[20221213 19:05:03 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 19:05:03 @agent_ppo2.py:121][0m #------------------------ Iteration 888 --------------------------#
[32m[20221213 19:05:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:05:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0025 |           3.5900 |         -69.0430 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0038 |           2.7940 |         -68.8716 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0137 |           2.7786 |         -69.1117 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0044 |           2.7585 |         -69.5842 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0041 |           2.7664 |         -69.5081 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0131 |           2.7569 |         -69.6158 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0098 |           2.7489 |         -69.9183 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |          -0.0047 |           2.7414 |         -69.8390 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |          -0.0052 |           2.7634 |         -69.8892 |
[32m[20221213 19:05:04 @agent_ppo2.py:185][0m |           0.0038 |           2.7491 |         -69.4493 |
[32m[20221213 19:05:04 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:05:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.20
[32m[20221213 19:05:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.00
[32m[20221213 19:05:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.00
[32m[20221213 19:05:05 @agent_ppo2.py:143][0m Total time:      16.54 min
[32m[20221213 19:05:05 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221213 19:05:05 @agent_ppo2.py:121][0m #------------------------ Iteration 889 --------------------------#
[32m[20221213 19:05:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:05:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |           0.0005 |         103.2434 |         -70.8466 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |          -0.0037 |          97.4245 |         -70.3953 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |           0.0034 |          96.9822 |         -69.9890 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |          -0.0043 |          96.4930 |         -69.7402 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |          -0.0006 |          95.7217 |         -69.9854 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |          -0.0045 |          96.0059 |         -69.1139 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |           0.0009 |          95.2148 |         -69.0188 |
[32m[20221213 19:05:05 @agent_ppo2.py:185][0m |          -0.0042 |          95.2993 |         -68.6263 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |          -0.0018 |          96.1693 |         -68.2667 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |          -0.0038 |          94.5693 |         -67.8394 |
[32m[20221213 19:05:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:05:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.00
[32m[20221213 19:05:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 19:05:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:05:06 @agent_ppo2.py:143][0m Total time:      16.56 min
[32m[20221213 19:05:06 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 19:05:06 @agent_ppo2.py:121][0m #------------------------ Iteration 890 --------------------------#
[32m[20221213 19:05:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:05:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |           0.0040 |           7.5869 |         -66.0582 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |           0.0061 |           2.9632 |         -65.1979 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |          -0.0007 |           2.6951 |         -65.5195 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |          -0.0019 |           2.6001 |         -65.4234 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |          -0.0055 |           2.5363 |         -65.0813 |
[32m[20221213 19:05:06 @agent_ppo2.py:185][0m |          -0.0035 |           2.5217 |         -65.2316 |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |          -0.0058 |           2.5037 |         -65.2764 |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |          -0.0057 |           2.4793 |         -65.0934 |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |          -0.0037 |           2.4642 |         -64.5707 |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |          -0.0035 |           2.4526 |         -64.6243 |
[32m[20221213 19:05:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:05:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.20
[32m[20221213 19:05:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:05:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.00
[32m[20221213 19:05:07 @agent_ppo2.py:143][0m Total time:      16.57 min
[32m[20221213 19:05:07 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221213 19:05:07 @agent_ppo2.py:121][0m #------------------------ Iteration 891 --------------------------#
[32m[20221213 19:05:07 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:05:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |           0.0052 |          13.0951 |         -65.8009 |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |           0.0052 |           8.4880 |         -65.5711 |
[32m[20221213 19:05:07 @agent_ppo2.py:185][0m |           0.0023 |           7.6861 |         -66.1156 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |           0.0047 |           7.2259 |         -65.8591 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |          -0.0031 |           6.8993 |         -66.3709 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |           0.0021 |           6.6747 |         -66.3486 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |           0.0037 |           6.6973 |         -66.5583 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |          -0.0056 |           6.4389 |         -66.5323 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |          -0.0022 |           6.3338 |         -66.5446 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |          -0.0053 |           6.2763 |         -66.9191 |
[32m[20221213 19:05:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:05:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.00
[32m[20221213 19:05:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:05:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.00
[32m[20221213 19:05:08 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221213 19:05:08 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 19:05:08 @agent_ppo2.py:121][0m #------------------------ Iteration 892 --------------------------#
[32m[20221213 19:05:08 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |           0.0037 |           6.1461 |         -67.8490 |
[32m[20221213 19:05:08 @agent_ppo2.py:185][0m |           0.0060 |           3.5728 |         -68.0590 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |           0.0027 |           3.4241 |         -67.7973 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0001 |           3.3550 |         -67.7073 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0062 |           3.2913 |         -67.3184 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0053 |           3.2787 |         -67.1177 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0063 |           3.2589 |         -66.9573 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0013 |           3.2450 |         -66.5760 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0045 |           3.2258 |         -66.4684 |
[32m[20221213 19:05:09 @agent_ppo2.py:185][0m |          -0.0043 |           3.2330 |         -66.3183 |
[32m[20221213 19:05:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:05:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.40
[32m[20221213 19:05:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 19:05:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 19:05:09 @agent_ppo2.py:143][0m Total time:      16.61 min
[32m[20221213 19:05:09 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221213 19:05:09 @agent_ppo2.py:121][0m #------------------------ Iteration 893 --------------------------#
[32m[20221213 19:05:09 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |          -0.0054 |         173.4402 |         -66.1536 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |           0.0030 |         165.1011 |         -65.9588 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |          -0.0013 |         164.9998 |         -66.2459 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |           0.0046 |         164.4267 |         -65.6813 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |          -0.0001 |         163.8189 |         -66.5268 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |          -0.0011 |         163.5684 |         -66.4591 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |           0.0068 |         164.4250 |         -66.4841 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |           0.0065 |         167.4825 |         -66.5616 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |          -0.0022 |         163.3345 |         -66.7845 |
[32m[20221213 19:05:10 @agent_ppo2.py:185][0m |          -0.0025 |         163.5123 |         -67.1260 |
[32m[20221213 19:05:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:05:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.00
[32m[20221213 19:05:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 19:05:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.00
[32m[20221213 19:05:11 @agent_ppo2.py:143][0m Total time:      16.63 min
[32m[20221213 19:05:11 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 19:05:11 @agent_ppo2.py:121][0m #------------------------ Iteration 894 --------------------------#
[32m[20221213 19:05:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:05:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |           0.0006 |         170.5539 |         -67.0193 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0015 |         165.5552 |         -68.1204 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0033 |         165.2799 |         -68.4139 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0042 |         164.9017 |         -68.9149 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0032 |         164.8342 |         -68.9711 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0059 |         164.6984 |         -69.1336 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0018 |         164.7189 |         -69.8097 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |           0.0056 |         168.8911 |         -69.8643 |
[32m[20221213 19:05:11 @agent_ppo2.py:185][0m |          -0.0006 |         165.3214 |         -70.7082 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |          -0.0034 |         164.7523 |         -70.2647 |
[32m[20221213 19:05:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:05:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.00
[32m[20221213 19:05:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 19:05:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:05:12 @agent_ppo2.py:143][0m Total time:      16.65 min
[32m[20221213 19:05:12 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221213 19:05:12 @agent_ppo2.py:121][0m #------------------------ Iteration 895 --------------------------#
[32m[20221213 19:05:12 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |           0.0064 |          13.7346 |         -73.2390 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |           0.0066 |           7.5160 |         -72.8158 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |           0.0008 |           7.0234 |         -72.4863 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |          -0.0010 |           6.7341 |         -72.4188 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |          -0.0013 |           6.5992 |         -72.6045 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |          -0.0017 |           6.4846 |         -72.5893 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |          -0.0001 |           6.4217 |         -72.4906 |
[32m[20221213 19:05:12 @agent_ppo2.py:185][0m |          -0.0032 |           6.3721 |         -72.5657 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |           0.0049 |           6.7457 |         -72.3178 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |          -0.0003 |           6.2830 |         -72.4448 |
[32m[20221213 19:05:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.00
[32m[20221213 19:05:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:05:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:05:13 @agent_ppo2.py:143][0m Total time:      16.67 min
[32m[20221213 19:05:13 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 19:05:13 @agent_ppo2.py:121][0m #------------------------ Iteration 896 --------------------------#
[32m[20221213 19:05:13 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |           0.0004 |         184.8513 |         -69.7552 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |           0.0047 |         205.1157 |         -70.0380 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |          -0.0020 |         179.3090 |         -70.4287 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |           0.0095 |         180.1983 |         -70.1899 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |           0.0075 |         199.6760 |         -69.9619 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |           0.0030 |         179.1550 |         -70.0111 |
[32m[20221213 19:05:13 @agent_ppo2.py:185][0m |          -0.0052 |         178.1421 |         -70.5382 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |           0.0006 |         178.1215 |         -70.4853 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |          -0.0040 |         177.5409 |         -70.5778 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |          -0.0008 |         177.4833 |         -71.1054 |
[32m[20221213 19:05:14 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.60
[32m[20221213 19:05:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 19:05:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.00
[32m[20221213 19:05:14 @agent_ppo2.py:143][0m Total time:      16.69 min
[32m[20221213 19:05:14 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221213 19:05:14 @agent_ppo2.py:121][0m #------------------------ Iteration 897 --------------------------#
[32m[20221213 19:05:14 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |          -0.0019 |         199.2205 |         -72.3538 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |          -0.0027 |         194.5514 |         -71.8337 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |          -0.0024 |         194.0099 |         -71.3833 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |           0.0056 |         218.2902 |         -71.0101 |
[32m[20221213 19:05:14 @agent_ppo2.py:185][0m |          -0.0051 |         193.4723 |         -70.7207 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |           0.0050 |         197.3336 |         -70.3784 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |          -0.0020 |         192.6807 |         -69.6684 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |          -0.0010 |         192.4284 |         -69.3391 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |           0.0048 |         194.7412 |         -68.9687 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |           0.0005 |         192.8129 |         -68.2591 |
[32m[20221213 19:05:15 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:05:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.00
[32m[20221213 19:05:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 19:05:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:05:15 @agent_ppo2.py:143][0m Total time:      16.71 min
[32m[20221213 19:05:15 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 19:05:15 @agent_ppo2.py:121][0m #------------------------ Iteration 898 --------------------------#
[32m[20221213 19:05:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |           0.0078 |         185.6667 |         -66.8973 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |           0.0071 |         179.9076 |         -66.6929 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |          -0.0030 |         178.5749 |         -67.2548 |
[32m[20221213 19:05:15 @agent_ppo2.py:185][0m |          -0.0028 |         177.9133 |         -66.2298 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0001 |         177.6919 |         -66.6687 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0038 |         177.5517 |         -66.1206 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0033 |         177.1574 |         -65.9662 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0056 |         181.3474 |         -66.3970 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |          -0.0004 |         176.8094 |         -66.6383 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |          -0.0024 |         176.7786 |         -66.7208 |
[32m[20221213 19:05:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.00
[32m[20221213 19:05:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.00
[32m[20221213 19:05:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.00
[32m[20221213 19:05:16 @agent_ppo2.py:143][0m Total time:      16.73 min
[32m[20221213 19:05:16 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221213 19:05:16 @agent_ppo2.py:121][0m #------------------------ Iteration 899 --------------------------#
[32m[20221213 19:05:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0084 |          11.4712 |         -66.2857 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0109 |           5.9118 |         -66.7766 |
[32m[20221213 19:05:16 @agent_ppo2.py:185][0m |           0.0038 |           5.4119 |         -66.6184 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |           0.0046 |           5.2245 |         -66.7806 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |           0.0018 |           5.0123 |         -67.4340 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |          -0.0032 |           4.8777 |         -67.4734 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |          -0.0031 |           4.8440 |         -67.5160 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |          -0.0041 |           4.7305 |         -67.6121 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |          -0.0043 |           4.6843 |         -67.8270 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |          -0.0006 |           4.6241 |         -67.9868 |
[32m[20221213 19:05:17 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.80
[32m[20221213 19:05:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 19:05:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:05:17 @agent_ppo2.py:143][0m Total time:      16.74 min
[32m[20221213 19:05:17 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 19:05:17 @agent_ppo2.py:121][0m #------------------------ Iteration 900 --------------------------#
[32m[20221213 19:05:17 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |           0.0085 |          34.4548 |         -69.1687 |
[32m[20221213 19:05:17 @agent_ppo2.py:185][0m |           0.0126 |          13.4042 |         -69.0374 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |           0.0049 |          12.8447 |         -69.0951 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |           0.0071 |          12.5804 |         -69.2169 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |          -0.0021 |          12.5786 |         -69.1743 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |          -0.0006 |          12.3389 |         -69.5650 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |          -0.0056 |          12.2625 |         -69.0979 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |           0.0065 |          12.2701 |         -69.0369 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |          -0.0019 |          12.1750 |         -68.9547 |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |           0.0014 |          12.0750 |         -69.3669 |
[32m[20221213 19:05:18 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:05:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.80
[32m[20221213 19:05:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.00
[32m[20221213 19:05:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.00
[32m[20221213 19:05:18 @agent_ppo2.py:143][0m Total time:      16.76 min
[32m[20221213 19:05:18 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221213 19:05:18 @agent_ppo2.py:121][0m #------------------------ Iteration 901 --------------------------#
[32m[20221213 19:05:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:18 @agent_ppo2.py:185][0m |           0.0013 |          88.9631 |         -69.7332 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0011 |          81.0863 |         -70.1895 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0005 |          80.1345 |         -70.1890 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0043 |          79.6139 |         -70.3468 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0021 |          79.2530 |         -70.3019 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0034 |          79.4781 |         -70.2324 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0008 |          78.9237 |         -70.0807 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |           0.0007 |          78.9216 |         -70.2752 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0025 |          78.7277 |         -70.2055 |
[32m[20221213 19:05:19 @agent_ppo2.py:185][0m |          -0.0043 |          78.9447 |         -69.9021 |
[32m[20221213 19:05:19 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.00
[32m[20221213 19:05:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 19:05:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.00
[32m[20221213 19:05:19 @agent_ppo2.py:143][0m Total time:      16.78 min
[32m[20221213 19:05:19 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 19:05:19 @agent_ppo2.py:121][0m #------------------------ Iteration 902 --------------------------#
[32m[20221213 19:05:19 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0006 |         139.2289 |         -68.3589 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |          -0.0033 |         131.0051 |         -68.7911 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |          -0.0036 |         130.0898 |         -68.5504 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0016 |         129.0572 |         -69.2405 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0040 |         141.9973 |         -69.4021 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0040 |         128.2129 |         -68.7308 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0074 |         129.0865 |         -69.0655 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0010 |         127.3872 |         -69.5936 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |          -0.0003 |         127.4199 |         -69.2135 |
[32m[20221213 19:05:20 @agent_ppo2.py:185][0m |           0.0020 |         127.0569 |         -69.3243 |
[32m[20221213 19:05:20 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:05:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.00
[32m[20221213 19:05:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.00
[32m[20221213 19:05:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.00
[32m[20221213 19:05:20 @agent_ppo2.py:143][0m Total time:      16.80 min
[32m[20221213 19:05:20 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221213 19:05:20 @agent_ppo2.py:121][0m #------------------------ Iteration 903 --------------------------#
[32m[20221213 19:05:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0024 |         185.0008 |         -71.4934 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0046 |         190.7682 |         -72.1649 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |          -0.0009 |         181.9233 |         -72.4357 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0320 |         201.7451 |         -71.5538 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0068 |         182.5860 |         -72.3099 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0052 |         192.5964 |         -73.2020 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0004 |         181.5165 |         -73.4566 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0041 |         181.3429 |         -72.1937 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0017 |         183.2500 |         -73.6669 |
[32m[20221213 19:05:21 @agent_ppo2.py:185][0m |           0.0006 |         181.3308 |         -74.4243 |
[32m[20221213 19:05:21 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 869.20
[32m[20221213 19:05:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 918.00
[32m[20221213 19:05:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 19:05:21 @agent_ppo2.py:143][0m Total time:      16.82 min
[32m[20221213 19:05:21 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 19:05:21 @agent_ppo2.py:121][0m #------------------------ Iteration 904 --------------------------#
[32m[20221213 19:05:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0045 |          34.8036 |         -74.5001 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0049 |          30.9206 |         -74.2999 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |          -0.0008 |          30.5240 |         -73.4451 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0063 |          30.4479 |         -73.8802 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0048 |          30.4393 |         -73.4125 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0081 |          31.0938 |         -72.2680 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0045 |          29.7597 |         -73.1714 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0007 |          29.3405 |         -73.2955 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |          -0.0036 |          29.4990 |         -73.6090 |
[32m[20221213 19:05:22 @agent_ppo2.py:185][0m |           0.0124 |          30.4092 |         -72.5604 |
[32m[20221213 19:05:22 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.40
[32m[20221213 19:05:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.00
[32m[20221213 19:05:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.00
[32m[20221213 19:05:23 @agent_ppo2.py:143][0m Total time:      16.84 min
[32m[20221213 19:05:23 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221213 19:05:23 @agent_ppo2.py:121][0m #------------------------ Iteration 905 --------------------------#
[32m[20221213 19:05:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0012 |         182.0526 |         -75.3746 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0024 |         175.1952 |         -75.0076 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0023 |         176.0266 |         -75.2851 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0074 |         174.4361 |         -75.4472 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0096 |         174.3505 |         -75.0660 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0066 |         174.0022 |         -75.5880 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0093 |         173.6186 |         -75.7123 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0038 |         173.8096 |         -74.9031 |
[32m[20221213 19:05:23 @agent_ppo2.py:185][0m |          -0.0024 |         177.2346 |         -74.9689 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |          -0.0062 |         173.4602 |         -75.2107 |
[32m[20221213 19:05:24 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.40
[32m[20221213 19:05:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.00
[32m[20221213 19:05:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.00
[32m[20221213 19:05:24 @agent_ppo2.py:143][0m Total time:      16.85 min
[32m[20221213 19:05:24 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 19:05:24 @agent_ppo2.py:121][0m #------------------------ Iteration 906 --------------------------#
[32m[20221213 19:05:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |          -0.0011 |         117.0666 |         -73.6009 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |           0.0067 |         109.6348 |         -73.2772 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |           0.0091 |         107.1161 |         -73.4158 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |           0.0005 |         106.9651 |         -74.9989 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |          -0.0008 |         106.6669 |         -75.5523 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |          -0.0023 |         104.8529 |         -75.2828 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |          -0.0022 |         105.0183 |         -76.5776 |
[32m[20221213 19:05:24 @agent_ppo2.py:185][0m |          -0.0016 |         103.9026 |         -75.9906 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |           0.0006 |         104.2094 |         -76.4237 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |           0.0003 |         104.1065 |         -76.9963 |
[32m[20221213 19:05:25 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.80
[32m[20221213 19:05:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.00
[32m[20221213 19:05:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 19:05:25 @agent_ppo2.py:143][0m Total time:      16.87 min
[32m[20221213 19:05:25 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221213 19:05:25 @agent_ppo2.py:121][0m #------------------------ Iteration 907 --------------------------#
[32m[20221213 19:05:25 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |           0.0009 |         175.8494 |         -79.4141 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |          -0.0026 |         171.0205 |         -79.3196 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |          -0.0063 |         170.0108 |         -78.6108 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |          -0.0094 |         167.9074 |         -79.0545 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |          -0.0061 |         168.0972 |         -79.2490 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |          -0.0076 |         167.6227 |         -78.9563 |
[32m[20221213 19:05:25 @agent_ppo2.py:185][0m |          -0.0040 |         167.2680 |         -78.6785 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |          -0.0078 |         167.3526 |         -78.9822 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |          -0.0058 |         167.1212 |         -78.3702 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |          -0.0084 |         167.2275 |         -78.7693 |
[32m[20221213 19:05:26 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:05:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.40
[32m[20221213 19:05:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 19:05:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.00
[32m[20221213 19:05:26 @agent_ppo2.py:143][0m Total time:      16.89 min
[32m[20221213 19:05:26 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 19:05:26 @agent_ppo2.py:121][0m #------------------------ Iteration 908 --------------------------#
[32m[20221213 19:05:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |           0.0008 |         168.1608 |         -80.2616 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |           0.0047 |         166.2037 |         -80.5331 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |           0.0029 |         165.9140 |         -81.1402 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |           0.0018 |         165.9195 |         -82.3884 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |           0.0013 |         165.5246 |         -82.2803 |
[32m[20221213 19:05:26 @agent_ppo2.py:185][0m |           0.0015 |         165.8146 |         -82.0630 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0014 |         167.6102 |         -83.4376 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0071 |         166.3456 |         -82.3402 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |          -0.0065 |         165.5755 |         -83.2523 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0232 |         209.6013 |         -84.1488 |
[32m[20221213 19:05:27 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:05:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 965.00
[32m[20221213 19:05:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.00
[32m[20221213 19:05:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.00
[32m[20221213 19:05:27 @agent_ppo2.py:143][0m Total time:      16.91 min
[32m[20221213 19:05:27 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221213 19:05:27 @agent_ppo2.py:121][0m #------------------------ Iteration 909 --------------------------#
[32m[20221213 19:05:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0051 |         178.3027 |         -82.4980 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0056 |         170.3231 |         -82.9417 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |          -0.0023 |         169.5865 |         -84.9276 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0035 |         169.8653 |         -85.2460 |
[32m[20221213 19:05:27 @agent_ppo2.py:185][0m |           0.0056 |         170.3393 |         -85.0368 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0024 |         169.1538 |         -85.4355 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0015 |         171.7755 |         -86.4568 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0062 |         169.1502 |         -86.6207 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0064 |         177.7733 |         -86.9805 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0034 |         169.9724 |         -87.8557 |
[32m[20221213 19:05:28 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.40
[32m[20221213 19:05:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 19:05:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 19:05:28 @agent_ppo2.py:143][0m Total time:      16.93 min
[32m[20221213 19:05:28 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 19:05:28 @agent_ppo2.py:121][0m #------------------------ Iteration 910 --------------------------#
[32m[20221213 19:05:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:05:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0013 |         180.9917 |         -92.5727 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0041 |         178.8151 |         -92.6707 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0008 |         177.8634 |         -91.6890 |
[32m[20221213 19:05:28 @agent_ppo2.py:185][0m |           0.0015 |         177.2214 |         -92.0246 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |           0.0053 |         177.0585 |         -92.7747 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |          -0.0002 |         176.8213 |         -92.0299 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |          -0.0001 |         176.6760 |         -92.0949 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |           0.0036 |         176.7393 |         -92.6505 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |           0.0084 |         181.3058 |         -92.4757 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |          -0.0039 |         176.3956 |         -92.9660 |
[32m[20221213 19:05:29 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.20
[32m[20221213 19:05:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 19:05:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.00
[32m[20221213 19:05:29 @agent_ppo2.py:143][0m Total time:      16.94 min
[32m[20221213 19:05:29 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221213 19:05:29 @agent_ppo2.py:121][0m #------------------------ Iteration 911 --------------------------#
[32m[20221213 19:05:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |           0.0096 |         193.0924 |         -93.1184 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |           0.0079 |         184.5765 |         -93.5068 |
[32m[20221213 19:05:29 @agent_ppo2.py:185][0m |           0.0096 |         188.1812 |         -93.9976 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |           0.0102 |         181.4982 |         -94.4840 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |          -0.0010 |         181.5914 |         -95.5380 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |           0.0015 |         181.9466 |         -96.5747 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |          -0.0002 |         181.4734 |         -96.9058 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |           0.0028 |         181.3786 |         -97.1873 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |          -0.0024 |         181.5035 |         -96.9411 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |           0.0018 |         181.8382 |         -96.8266 |
[32m[20221213 19:05:30 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.60
[32m[20221213 19:05:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 19:05:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:05:30 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221213 19:05:30 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 19:05:30 @agent_ppo2.py:121][0m #------------------------ Iteration 912 --------------------------#
[32m[20221213 19:05:30 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |           0.0088 |         189.7342 |        -100.8961 |
[32m[20221213 19:05:30 @agent_ppo2.py:185][0m |           0.0002 |         186.2543 |        -101.8581 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |           0.0192 |         201.8842 |        -101.3353 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |           0.0006 |         186.0139 |        -101.8111 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |          -0.0032 |         185.1684 |        -101.8469 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |           0.0051 |         187.3651 |        -101.5773 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |           0.0007 |         185.4827 |        -101.2308 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |          -0.0021 |         185.0143 |        -102.3342 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |           0.0045 |         185.3234 |        -102.4633 |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |          -0.0005 |         184.8518 |        -102.9028 |
[32m[20221213 19:05:31 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.20
[32m[20221213 19:05:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.00
[32m[20221213 19:05:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 19:05:31 @agent_ppo2.py:143][0m Total time:      16.98 min
[32m[20221213 19:05:31 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221213 19:05:31 @agent_ppo2.py:121][0m #------------------------ Iteration 913 --------------------------#
[32m[20221213 19:05:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:31 @agent_ppo2.py:185][0m |          -0.0016 |         193.2072 |        -102.9008 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |           0.0119 |         212.2587 |        -103.6334 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |           0.0028 |         190.4000 |        -102.0723 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |          -0.0019 |         189.3331 |        -102.7327 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |          -0.0010 |         188.9080 |        -103.3044 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |           0.0072 |         189.9084 |        -102.9639 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |          -0.0021 |         188.6491 |        -103.9426 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |           0.0059 |         188.6919 |        -101.5798 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |           0.0069 |         191.1391 |        -104.2877 |
[32m[20221213 19:05:32 @agent_ppo2.py:185][0m |           0.0002 |         188.2641 |        -103.9076 |
[32m[20221213 19:05:32 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.40
[32m[20221213 19:05:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.00
[32m[20221213 19:05:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.00
[32m[20221213 19:05:32 @agent_ppo2.py:143][0m Total time:      17.00 min
[32m[20221213 19:05:32 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221213 19:05:32 @agent_ppo2.py:121][0m #------------------------ Iteration 914 --------------------------#
[32m[20221213 19:05:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |          -0.0011 |         197.5506 |        -107.1036 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |          -0.0023 |         193.1812 |        -106.8186 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |          -0.0002 |         192.7289 |        -107.0393 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |           0.0038 |         192.9784 |        -106.5424 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |          -0.0014 |         192.5588 |        -106.7775 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |           0.0006 |         192.8810 |        -106.7371 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |           0.0060 |         200.9651 |        -107.0416 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |           0.0176 |         216.8038 |        -107.1318 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |           0.0028 |         192.8352 |        -106.9094 |
[32m[20221213 19:05:33 @agent_ppo2.py:185][0m |           0.0025 |         195.6425 |        -107.2574 |
[32m[20221213 19:05:33 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.80
[32m[20221213 19:05:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 676.00
[32m[20221213 19:05:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:05:33 @agent_ppo2.py:143][0m Total time:      17.02 min
[32m[20221213 19:05:33 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221213 19:05:33 @agent_ppo2.py:121][0m #------------------------ Iteration 915 --------------------------#
[32m[20221213 19:05:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0161 |         219.4994 |        -107.1677 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0063 |         192.8977 |        -106.5920 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0042 |         192.6794 |        -106.6718 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0072 |         192.3714 |        -106.3355 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |          -0.0013 |         192.3562 |        -107.6177 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0010 |         192.0759 |        -106.3340 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0008 |         192.2334 |        -106.3214 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0034 |         192.0613 |        -106.1818 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0015 |         192.0334 |        -106.3246 |
[32m[20221213 19:05:34 @agent_ppo2.py:185][0m |           0.0022 |         192.0254 |        -106.2963 |
[32m[20221213 19:05:34 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:05:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.40
[32m[20221213 19:05:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 890.00
[32m[20221213 19:05:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:05:34 @agent_ppo2.py:143][0m Total time:      17.03 min
[32m[20221213 19:05:34 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221213 19:05:34 @agent_ppo2.py:121][0m #------------------------ Iteration 916 --------------------------#
[32m[20221213 19:05:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |          -0.0004 |         196.4503 |        -106.3998 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |          -0.0002 |         195.9024 |        -106.1011 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |           0.0029 |         195.2041 |        -105.4846 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |           0.0008 |         194.7136 |        -104.6633 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |           0.0033 |         194.4788 |        -104.7682 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |           0.0025 |         195.4471 |        -104.6320 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |           0.0058 |         198.7012 |        -105.5434 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |          -0.0003 |         193.4936 |        -106.0254 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |          -0.0000 |         193.3986 |        -104.7516 |
[32m[20221213 19:05:35 @agent_ppo2.py:185][0m |          -0.0024 |         193.6352 |        -104.7466 |
[32m[20221213 19:05:35 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.60
[32m[20221213 19:05:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:05:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 665.00
[32m[20221213 19:05:36 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221213 19:05:36 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221213 19:05:36 @agent_ppo2.py:121][0m #------------------------ Iteration 917 --------------------------#
[32m[20221213 19:05:36 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0002 |         205.2409 |        -105.6851 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0059 |         201.8880 |        -104.7249 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0031 |         201.6935 |        -104.5515 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0073 |         208.9868 |        -105.8240 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0022 |         202.0348 |        -105.9270 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |          -0.0009 |         201.5425 |        -106.4659 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0024 |         202.0623 |        -106.3142 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0189 |         216.8179 |        -107.1517 |
[32m[20221213 19:05:36 @agent_ppo2.py:185][0m |           0.0053 |         202.2937 |        -105.8711 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0137 |         231.3319 |        -107.1313 |
[32m[20221213 19:05:37 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.20
[32m[20221213 19:05:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.00
[32m[20221213 19:05:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 889.00
[32m[20221213 19:05:37 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 19:05:37 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221213 19:05:37 @agent_ppo2.py:121][0m #------------------------ Iteration 918 --------------------------#
[32m[20221213 19:05:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0172 |         160.5379 |        -104.7407 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0090 |         123.3899 |        -105.1336 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0082 |         121.9017 |        -105.1473 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0043 |         120.6477 |        -106.2397 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0055 |         120.0631 |        -105.7123 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0019 |         119.1730 |        -106.5292 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0014 |         119.2532 |        -107.2648 |
[32m[20221213 19:05:37 @agent_ppo2.py:185][0m |           0.0009 |         118.3410 |        -107.3338 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |          -0.0047 |         118.5056 |        -107.6211 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |          -0.0008 |         118.4671 |        -107.1563 |
[32m[20221213 19:05:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:05:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.80
[32m[20221213 19:05:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 887.00
[32m[20221213 19:05:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:05:38 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221213 19:05:38 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221213 19:05:38 @agent_ppo2.py:121][0m #------------------------ Iteration 919 --------------------------#
[32m[20221213 19:05:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:05:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |           0.0092 |          64.3766 |        -110.2725 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |           0.0131 |          45.8185 |        -109.8650 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |           0.0072 |          42.7378 |        -110.4984 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |          -0.0022 |          41.7390 |        -110.4274 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |          -0.0060 |          40.3565 |        -110.2903 |
[32m[20221213 19:05:38 @agent_ppo2.py:185][0m |          -0.0039 |          39.8077 |        -110.1628 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |           0.0057 |          39.2680 |        -109.7403 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |           0.0023 |          38.4343 |        -110.5318 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |          -0.0029 |          38.0292 |        -110.5854 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |          -0.0021 |          37.2300 |        -110.1381 |
[32m[20221213 19:05:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:05:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.40
[32m[20221213 19:05:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:05:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 50.00
[32m[20221213 19:05:39 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221213 19:05:39 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221213 19:05:39 @agent_ppo2.py:121][0m #------------------------ Iteration 920 --------------------------#
[32m[20221213 19:05:39 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |          -0.0022 |         200.3424 |        -111.7166 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |           0.0022 |         198.2886 |        -110.7545 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |           0.0038 |         198.1144 |        -109.9824 |
[32m[20221213 19:05:39 @agent_ppo2.py:185][0m |          -0.0048 |         197.7096 |        -111.1776 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0201 |         217.5896 |        -111.1357 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0049 |         199.1317 |        -110.1472 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0006 |         198.0500 |        -110.5444 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0012 |         197.7558 |        -110.5969 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0059 |         207.0482 |        -110.8611 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0088 |         205.9419 |        -110.7731 |
[32m[20221213 19:05:40 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.00
[32m[20221213 19:05:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 858.00
[32m[20221213 19:05:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:05:40 @agent_ppo2.py:143][0m Total time:      17.13 min
[32m[20221213 19:05:40 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221213 19:05:40 @agent_ppo2.py:121][0m #------------------------ Iteration 921 --------------------------#
[32m[20221213 19:05:40 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0134 |          41.9564 |        -107.7411 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0044 |          19.3338 |        -108.5162 |
[32m[20221213 19:05:40 @agent_ppo2.py:185][0m |           0.0138 |          17.7535 |        -109.3460 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0070 |          16.1461 |        -108.5547 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0046 |          15.7039 |        -108.9715 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0024 |          15.2338 |        -109.6025 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0082 |          14.9669 |        -109.8666 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0024 |          14.7764 |        -110.0658 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0021 |          14.7065 |        -109.8584 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |           0.0064 |          14.4781 |        -109.2483 |
[32m[20221213 19:05:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:05:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.60
[32m[20221213 19:05:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:05:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 19:05:41 @agent_ppo2.py:143][0m Total time:      17.14 min
[32m[20221213 19:05:41 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221213 19:05:41 @agent_ppo2.py:121][0m #------------------------ Iteration 922 --------------------------#
[32m[20221213 19:05:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |          -0.0024 |         190.9357 |        -108.0087 |
[32m[20221213 19:05:41 @agent_ppo2.py:185][0m |          -0.0014 |         188.4510 |        -108.1496 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |          -0.0027 |         188.7519 |        -108.1053 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0154 |         207.3727 |        -108.2017 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0046 |         189.5940 |        -108.0787 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0017 |         188.5647 |        -107.7725 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0177 |         213.3875 |        -107.8290 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0019 |         188.6452 |        -107.0310 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0006 |         187.8075 |        -106.9297 |
[32m[20221213 19:05:42 @agent_ppo2.py:185][0m |           0.0023 |         187.5610 |        -107.2552 |
[32m[20221213 19:05:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:05:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.00
[32m[20221213 19:05:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 893.00
[32m[20221213 19:05:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:05:42 @agent_ppo2.py:143][0m Total time:      17.16 min
[32m[20221213 19:05:42 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221213 19:05:42 @agent_ppo2.py:121][0m #------------------------ Iteration 923 --------------------------#
[32m[20221213 19:05:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0116 |         219.6900 |        -110.2234 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0021 |         193.6966 |        -108.3998 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0025 |         193.4134 |        -107.8051 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |          -0.0020 |         193.6622 |        -110.1631 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0056 |         197.8252 |        -110.5091 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |          -0.0017 |         193.2650 |        -110.8729 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0069 |         193.3360 |        -109.2163 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0078 |         193.1202 |        -109.3730 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0035 |         194.0400 |        -111.2458 |
[32m[20221213 19:05:43 @agent_ppo2.py:185][0m |           0.0011 |         193.1277 |        -110.0335 |
[32m[20221213 19:05:43 @agent_ppo2.py:130][0m Policy update time: 0.85 s
[32m[20221213 19:05:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 990.80
[32m[20221213 19:05:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:05:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.00
[32m[20221213 19:05:43 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221213 19:05:43 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221213 19:05:43 @agent_ppo2.py:121][0m #------------------------ Iteration 924 --------------------------#
[32m[20221213 19:05:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |           0.0072 |         164.6133 |        -111.1566 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |           0.0082 |         159.8892 |        -111.0611 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |          -0.0011 |         160.3039 |        -112.3319 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |           0.0013 |         158.1910 |        -111.6820 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |          -0.0040 |         158.0051 |        -112.0955 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |          -0.0004 |         157.4742 |        -111.5879 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |           0.0009 |         156.8453 |        -111.6822 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |           0.0019 |         156.4639 |        -111.1691 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |          -0.0034 |         156.6216 |        -111.2469 |
[32m[20221213 19:05:44 @agent_ppo2.py:185][0m |           0.0014 |         156.0790 |        -111.1972 |
[32m[20221213 19:05:44 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:05:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.20
[32m[20221213 19:05:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 19:05:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 987.00
[32m[20221213 19:05:44 @agent_ppo2.py:143][0m Total time:      17.20 min
[32m[20221213 19:05:44 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221213 19:05:44 @agent_ppo2.py:121][0m #------------------------ Iteration 925 --------------------------#
[32m[20221213 19:05:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |           0.0079 |          20.8697 |        -111.9402 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |           0.0138 |          11.7587 |        -111.0375 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |           0.0013 |          10.9310 |        -110.7719 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |           0.0026 |          10.4896 |        -111.0191 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |          -0.0007 |          10.4078 |        -110.8716 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |          -0.0004 |          10.1116 |        -111.5401 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |          -0.0007 |           9.8771 |        -111.3895 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |           0.0042 |           9.9117 |        -111.5382 |
[32m[20221213 19:05:45 @agent_ppo2.py:185][0m |          -0.0030 |           9.9211 |        -112.0960 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0068 |           9.7863 |        -111.2617 |
[32m[20221213 19:05:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 19:05:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.80
[32m[20221213 19:05:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:05:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.00
[32m[20221213 19:05:46 @agent_ppo2.py:143][0m Total time:      17.22 min
[32m[20221213 19:05:46 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221213 19:05:46 @agent_ppo2.py:121][0m #------------------------ Iteration 926 --------------------------#
[32m[20221213 19:05:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |          -0.0000 |         233.1847 |        -115.7461 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |          -0.0011 |         230.5789 |        -116.4163 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0106 |         235.9029 |        -114.0695 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0002 |         230.5986 |        -115.9694 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0002 |         230.0164 |        -115.6325 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0032 |         230.1708 |        -116.1334 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0082 |         229.7081 |        -113.7841 |
[32m[20221213 19:05:46 @agent_ppo2.py:185][0m |           0.0050 |         229.8251 |        -116.4093 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |           0.0020 |         229.8292 |        -117.7853 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |           0.0001 |         229.5263 |        -116.1541 |
[32m[20221213 19:05:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:05:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 879.80
[32m[20221213 19:05:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 886.00
[32m[20221213 19:05:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:05:47 @agent_ppo2.py:143][0m Total time:      17.24 min
[32m[20221213 19:05:47 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221213 19:05:47 @agent_ppo2.py:121][0m #------------------------ Iteration 927 --------------------------#
[32m[20221213 19:05:47 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |          -0.0000 |         241.8903 |        -113.2025 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |          -0.0012 |         239.2968 |        -113.4446 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |           0.0099 |         242.0217 |        -112.9690 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |          -0.0027 |         238.0962 |        -112.8298 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |           0.0013 |         238.1500 |        -113.7068 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |           0.0061 |         238.5978 |        -112.4623 |
[32m[20221213 19:05:47 @agent_ppo2.py:185][0m |          -0.0018 |         238.1513 |        -112.3709 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0056 |         242.8260 |        -112.3633 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0010 |         237.8089 |        -112.3233 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0089 |         249.2632 |        -112.3090 |
[32m[20221213 19:05:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:05:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.80
[32m[20221213 19:05:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.00
[32m[20221213 19:05:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:05:48 @agent_ppo2.py:143][0m Total time:      17.26 min
[32m[20221213 19:05:48 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221213 19:05:48 @agent_ppo2.py:121][0m #------------------------ Iteration 928 --------------------------#
[32m[20221213 19:05:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0045 |           9.7066 |        -116.0828 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0022 |           7.6701 |        -114.5970 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0042 |           7.7021 |        -115.2874 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |           0.0003 |           7.5668 |        -115.9259 |
[32m[20221213 19:05:48 @agent_ppo2.py:185][0m |          -0.0011 |           7.5218 |        -116.3544 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |          -0.0010 |           7.4586 |        -116.3615 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |          -0.0029 |           7.4554 |        -116.5180 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |          -0.0045 |           7.4369 |        -116.4951 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |           0.0109 |           7.4934 |        -117.3230 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |          -0.0008 |           7.4299 |        -117.0493 |
[32m[20221213 19:05:49 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:05:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.60
[32m[20221213 19:05:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:05:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.00
[32m[20221213 19:05:49 @agent_ppo2.py:143][0m Total time:      17.28 min
[32m[20221213 19:05:49 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221213 19:05:49 @agent_ppo2.py:121][0m #------------------------ Iteration 929 --------------------------#
[32m[20221213 19:05:49 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |           0.0079 |          12.1447 |        -113.6992 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |           0.0029 |           6.8222 |        -113.3523 |
[32m[20221213 19:05:49 @agent_ppo2.py:185][0m |           0.0011 |           6.5608 |        -113.8128 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |          -0.0045 |           6.4014 |        -113.2563 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |           0.0003 |           6.2464 |        -112.8588 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |          -0.0051 |           6.2399 |        -112.3839 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |          -0.0049 |           6.2079 |        -112.4489 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |          -0.0073 |           6.0944 |        -111.4026 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |          -0.0041 |           6.0601 |        -110.6957 |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |          -0.0054 |           6.0350 |        -110.1914 |
[32m[20221213 19:05:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 19:05:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.20
[32m[20221213 19:05:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 19:05:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 19:05:50 @agent_ppo2.py:143][0m Total time:      17.29 min
[32m[20221213 19:05:50 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221213 19:05:50 @agent_ppo2.py:121][0m #------------------------ Iteration 930 --------------------------#
[32m[20221213 19:05:50 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:05:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:50 @agent_ppo2.py:185][0m |           0.0084 |          18.8113 |        -110.5029 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0001 |          10.6621 |        -110.7513 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |           0.0047 |           9.7945 |        -110.6388 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |           0.0031 |           9.4038 |        -110.6499 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0001 |           9.1837 |        -110.7198 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0093 |           9.1171 |        -110.7826 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0004 |           8.9429 |        -110.7480 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0039 |           8.8283 |        -110.7685 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0059 |           8.7727 |        -110.7386 |
[32m[20221213 19:05:51 @agent_ppo2.py:185][0m |          -0.0021 |           8.6967 |        -110.6110 |
[32m[20221213 19:05:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 19:05:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.40
[32m[20221213 19:05:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.00
[32m[20221213 19:05:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.00
[32m[20221213 19:05:51 @agent_ppo2.py:143][0m Total time:      17.32 min
[32m[20221213 19:05:51 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221213 19:05:51 @agent_ppo2.py:121][0m #------------------------ Iteration 931 --------------------------#
[32m[20221213 19:05:51 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |           0.0010 |         143.2096 |        -108.9186 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0037 |         139.6750 |        -109.1641 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0079 |         138.6927 |        -110.9986 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0074 |         139.4358 |        -111.6439 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0073 |         138.1181 |        -112.1915 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0071 |         138.3004 |        -111.9168 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0074 |         138.2034 |        -112.3730 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0077 |         137.5966 |        -112.6041 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0098 |         138.0256 |        -112.9785 |
[32m[20221213 19:05:52 @agent_ppo2.py:185][0m |          -0.0030 |         139.9087 |        -113.2920 |
[32m[20221213 19:05:52 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:05:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.40
[32m[20221213 19:05:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.00
[32m[20221213 19:05:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:05:52 @agent_ppo2.py:143][0m Total time:      17.33 min
[32m[20221213 19:05:52 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221213 19:05:52 @agent_ppo2.py:121][0m #------------------------ Iteration 932 --------------------------#
[32m[20221213 19:05:53 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0044 |         188.9932 |        -114.8981 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0087 |         188.1099 |        -114.7096 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |          -0.0004 |         187.2154 |        -116.2443 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |          -0.0000 |         186.2431 |        -115.9082 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |          -0.0021 |         186.3501 |        -115.8630 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0020 |         186.0697 |        -115.4843 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0108 |         185.2463 |        -114.5220 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0031 |         188.5859 |        -115.2282 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0005 |         186.3799 |        -115.8492 |
[32m[20221213 19:05:53 @agent_ppo2.py:185][0m |           0.0107 |         211.7532 |        -115.7543 |
[32m[20221213 19:05:53 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:05:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.80
[32m[20221213 19:05:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 19:05:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:05:53 @agent_ppo2.py:143][0m Total time:      17.35 min
[32m[20221213 19:05:53 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221213 19:05:54 @agent_ppo2.py:121][0m #------------------------ Iteration 933 --------------------------#
[32m[20221213 19:05:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |           0.0073 |           5.3070 |        -116.5107 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |          -0.0012 |           3.4735 |        -117.4517 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |           0.0060 |           3.5376 |        -117.8920 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |          -0.0004 |           3.3632 |        -118.0191 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |          -0.0025 |           3.2955 |        -118.2046 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |           0.0080 |           3.2508 |        -117.8347 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |           0.0010 |           3.2951 |        -118.3855 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |          -0.0001 |           3.2177 |        -118.1981 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |          -0.0058 |           3.2050 |        -118.8409 |
[32m[20221213 19:05:54 @agent_ppo2.py:185][0m |           0.0028 |           3.1989 |        -119.0163 |
[32m[20221213 19:05:54 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:05:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.00
[32m[20221213 19:05:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.00
[32m[20221213 19:05:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.00
[32m[20221213 19:05:55 @agent_ppo2.py:143][0m Total time:      17.37 min
[32m[20221213 19:05:55 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221213 19:05:55 @agent_ppo2.py:121][0m #------------------------ Iteration 934 --------------------------#
[32m[20221213 19:05:55 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:05:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |           0.0007 |          11.2861 |        -119.8421 |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |          -0.0022 |           4.5746 |        -118.5442 |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |          -0.0063 |           4.0335 |        -118.1833 |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |          -0.0040 |           3.8146 |        -117.3552 |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |          -0.0077 |           3.6884 |        -116.9519 |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |          -0.0092 |           3.5960 |        -116.7242 |
[32m[20221213 19:05:55 @agent_ppo2.py:185][0m |          -0.0071 |           3.5819 |        -116.5613 |
[32m[20221213 19:05:56 @agent_ppo2.py:185][0m |          -0.0049 |           3.5262 |        -115.4807 |
[32m[20221213 19:05:56 @agent_ppo2.py:185][0m |          -0.0124 |           3.5184 |        -115.0104 |
[32m[20221213 19:05:56 @agent_ppo2.py:185][0m |          -0.0083 |           3.4593 |        -114.8528 |
[32m[20221213 19:05:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 19:05:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.20
[32m[20221213 19:05:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 19:05:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 119.00
[32m[20221213 19:05:56 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221213 19:05:56 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221213 19:05:56 @agent_ppo2.py:121][0m #------------------------ Iteration 935 --------------------------#
[32m[20221213 19:05:56 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:05:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:56 @agent_ppo2.py:185][0m |           0.0065 |          77.9513 |        -112.1542 |
[32m[20221213 19:05:56 @agent_ppo2.py:185][0m |           0.0050 |          65.1313 |        -111.4985 |
[32m[20221213 19:05:56 @agent_ppo2.py:185][0m |          -0.0014 |          63.9033 |        -111.6270 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |           0.0090 |          67.4648 |        -111.7900 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |          -0.0010 |          63.5069 |        -111.1828 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |           0.0060 |          62.9863 |        -111.6426 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |           0.0032 |          63.1022 |        -111.9574 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |           0.0055 |          63.8500 |        -111.9911 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |           0.0022 |          62.7717 |        -112.6119 |
[32m[20221213 19:05:57 @agent_ppo2.py:185][0m |           0.0047 |          67.7862 |        -112.9620 |
[32m[20221213 19:05:57 @agent_ppo2.py:130][0m Policy update time: 1.29 s
[32m[20221213 19:05:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.00
[32m[20221213 19:05:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.00
[32m[20221213 19:05:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.00
[32m[20221213 19:05:57 @agent_ppo2.py:143][0m Total time:      17.42 min
[32m[20221213 19:05:57 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221213 19:05:57 @agent_ppo2.py:121][0m #------------------------ Iteration 936 --------------------------#
[32m[20221213 19:05:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:05:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |          -0.0077 |          55.5602 |        -111.5979 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |          -0.0036 |          49.3478 |        -111.0920 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |          -0.0002 |          48.0273 |        -111.0014 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |           0.0120 |          55.1324 |        -111.2933 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |           0.0019 |          47.3439 |        -109.9650 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |          -0.0024 |          46.7874 |        -111.7756 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |          -0.0040 |          46.7278 |        -111.8186 |
[32m[20221213 19:05:58 @agent_ppo2.py:185][0m |          -0.0030 |          46.3589 |        -111.6017 |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |          -0.0037 |          46.3500 |        -111.9784 |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |          -0.0093 |          46.0767 |        -112.6000 |
[32m[20221213 19:05:59 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 19:05:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.40
[32m[20221213 19:05:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.00
[32m[20221213 19:05:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.00
[32m[20221213 19:05:59 @agent_ppo2.py:143][0m Total time:      17.44 min
[32m[20221213 19:05:59 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221213 19:05:59 @agent_ppo2.py:121][0m #------------------------ Iteration 937 --------------------------#
[32m[20221213 19:05:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 19:05:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |           0.0025 |          10.2293 |        -112.6273 |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |           0.0079 |           4.1694 |        -112.1902 |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |           0.0109 |           3.9603 |        -111.8587 |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |           0.0070 |           3.8771 |        -111.7369 |
[32m[20221213 19:05:59 @agent_ppo2.py:185][0m |           0.0026 |           3.9327 |        -112.0375 |
[32m[20221213 19:06:00 @agent_ppo2.py:185][0m |           0.0067 |           3.7898 |        -110.8975 |
[32m[20221213 19:06:00 @agent_ppo2.py:185][0m |          -0.0005 |           3.7423 |        -111.1278 |
[32m[20221213 19:06:00 @agent_ppo2.py:185][0m |           0.0016 |           3.7226 |        -110.7598 |
[32m[20221213 19:06:00 @agent_ppo2.py:185][0m |          -0.0012 |           3.7031 |        -110.3339 |
[32m[20221213 19:06:00 @agent_ppo2.py:185][0m |           0.0012 |           3.6851 |        -111.1651 |
[32m[20221213 19:06:00 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 19:06:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.60
[32m[20221213 19:06:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 66.00
[32m[20221213 19:06:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 994.00
[32m[20221213 19:06:00 @agent_ppo2.py:143][0m Total time:      17.46 min
[32m[20221213 19:06:00 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221213 19:06:00 @agent_ppo2.py:121][0m #------------------------ Iteration 938 --------------------------#
[32m[20221213 19:06:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:00 @agent_ppo2.py:185][0m |          -0.0016 |          68.3132 |        -113.3387 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0023 |          62.6810 |        -111.8546 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0035 |          61.7123 |        -112.4600 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0073 |          61.1964 |        -112.8234 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0008 |          60.9641 |        -113.6218 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0048 |          60.5642 |        -113.8289 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |           0.0004 |          60.7678 |        -114.0005 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0008 |          60.3593 |        -114.0446 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0012 |          60.0726 |        -113.7342 |
[32m[20221213 19:06:01 @agent_ppo2.py:185][0m |          -0.0031 |          60.2641 |        -113.6402 |
[32m[20221213 19:06:01 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 19:06:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.60
[32m[20221213 19:06:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.00
[32m[20221213 19:06:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.00
[32m[20221213 19:06:01 @agent_ppo2.py:143][0m Total time:      17.48 min
[32m[20221213 19:06:01 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221213 19:06:01 @agent_ppo2.py:121][0m #------------------------ Iteration 939 --------------------------#
[32m[20221213 19:06:02 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0059 |          57.8371 |        -116.0084 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0026 |          53.8724 |        -116.1105 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0016 |          52.7482 |        -115.7322 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0011 |          51.9324 |        -115.5420 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0021 |          51.7621 |        -114.7253 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0001 |          51.2166 |        -114.5520 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |           0.0032 |          51.0359 |        -114.5403 |
[32m[20221213 19:06:02 @agent_ppo2.py:185][0m |          -0.0015 |          50.9870 |        -114.6321 |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |          -0.0035 |          50.8048 |        -114.6728 |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |           0.0024 |          50.6054 |        -114.3493 |
[32m[20221213 19:06:03 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 19:06:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.00
[32m[20221213 19:06:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 911.00
[32m[20221213 19:06:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.00
[32m[20221213 19:06:03 @agent_ppo2.py:143][0m Total time:      17.51 min
[32m[20221213 19:06:03 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221213 19:06:03 @agent_ppo2.py:121][0m #------------------------ Iteration 940 --------------------------#
[32m[20221213 19:06:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 19:06:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |           0.0097 |          33.7684 |        -111.7501 |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |           0.0140 |          39.1672 |        -112.0847 |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |           0.0054 |          26.5259 |        -111.7358 |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |           0.0035 |          25.3064 |        -112.2769 |
[32m[20221213 19:06:03 @agent_ppo2.py:185][0m |           0.0061 |          25.0391 |        -112.4430 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0045 |          24.8318 |        -112.8992 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0017 |          24.7258 |        -113.7298 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0016 |          24.5799 |        -113.2700 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0020 |          24.7065 |        -113.6261 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0025 |          24.5119 |        -113.2495 |
[32m[20221213 19:06:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.00
[32m[20221213 19:06:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 19:06:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.00
[32m[20221213 19:06:04 @agent_ppo2.py:143][0m Total time:      17.53 min
[32m[20221213 19:06:04 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221213 19:06:04 @agent_ppo2.py:121][0m #------------------------ Iteration 941 --------------------------#
[32m[20221213 19:06:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0076 |         137.6688 |        -115.2399 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |          -0.0023 |         129.8754 |        -115.2099 |
[32m[20221213 19:06:04 @agent_ppo2.py:185][0m |           0.0195 |         143.1197 |        -116.5998 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |           0.0047 |         129.4163 |        -116.0842 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |          -0.0047 |         126.0914 |        -117.1039 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |          -0.0015 |         125.8510 |        -117.3502 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |          -0.0052 |         125.8298 |        -117.5534 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |           0.0050 |         126.4440 |        -117.3740 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |          -0.0036 |         125.3374 |        -117.2551 |
[32m[20221213 19:06:05 @agent_ppo2.py:185][0m |          -0.0029 |         124.9992 |        -117.8059 |
[32m[20221213 19:06:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.40
[32m[20221213 19:06:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 19:06:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 19:06:05 @agent_ppo2.py:143][0m Total time:      17.55 min
[32m[20221213 19:06:05 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221213 19:06:05 @agent_ppo2.py:121][0m #------------------------ Iteration 942 --------------------------#
[32m[20221213 19:06:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |          -0.0007 |         212.9567 |        -124.1765 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0002 |         207.1969 |        -123.1721 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0008 |         206.1973 |        -123.2249 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0063 |         208.9325 |        -122.9471 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |          -0.0029 |         205.5455 |        -123.3164 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0135 |         229.2402 |        -123.5634 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0005 |         205.1440 |        -122.4511 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |          -0.0035 |         204.8892 |        -123.1971 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0025 |         204.7259 |        -123.2674 |
[32m[20221213 19:06:06 @agent_ppo2.py:185][0m |           0.0001 |         205.2573 |        -123.3376 |
[32m[20221213 19:06:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 631.40
[32m[20221213 19:06:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.00
[32m[20221213 19:06:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 19:06:06 @agent_ppo2.py:143][0m Total time:      17.57 min
[32m[20221213 19:06:06 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221213 19:06:06 @agent_ppo2.py:121][0m #------------------------ Iteration 943 --------------------------#
[32m[20221213 19:06:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:06:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |          -0.0038 |          96.1251 |        -125.1892 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |          -0.0082 |          90.8672 |        -125.5559 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |           0.0014 |          90.2189 |        -125.5135 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |          -0.0052 |          89.8736 |        -124.6583 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |          -0.0046 |          90.4014 |        -126.7203 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |          -0.0074 |          90.3280 |        -126.9516 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |          -0.0091 |          90.2236 |        -127.0383 |
[32m[20221213 19:06:07 @agent_ppo2.py:185][0m |           0.0016 |          90.6646 |        -125.8709 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |           0.0008 |          96.6778 |        -127.2584 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |          -0.0052 |          89.7367 |        -125.7432 |
[32m[20221213 19:06:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:06:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.40
[32m[20221213 19:06:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.00
[32m[20221213 19:06:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.00
[32m[20221213 19:06:08 @agent_ppo2.py:143][0m Total time:      17.59 min
[32m[20221213 19:06:08 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221213 19:06:08 @agent_ppo2.py:121][0m #------------------------ Iteration 944 --------------------------#
[32m[20221213 19:06:08 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |          -0.0004 |          52.8987 |        -126.3994 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |          -0.0068 |          48.7470 |        -125.0352 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |           0.0013 |          49.1398 |        -124.3229 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |          -0.0050 |          47.9763 |        -124.7183 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |           0.0006 |          47.9358 |        -123.7892 |
[32m[20221213 19:06:08 @agent_ppo2.py:185][0m |           0.0001 |          47.6790 |        -124.0372 |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |          -0.0016 |          47.8014 |        -124.0796 |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |           0.0024 |          47.7278 |        -123.4184 |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |           0.0011 |          47.6015 |        -124.0704 |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |           0.0039 |          49.4391 |        -122.6336 |
[32m[20221213 19:06:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.60
[32m[20221213 19:06:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.00
[32m[20221213 19:06:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:06:09 @agent_ppo2.py:143][0m Total time:      17.61 min
[32m[20221213 19:06:09 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221213 19:06:09 @agent_ppo2.py:121][0m #------------------------ Iteration 945 --------------------------#
[32m[20221213 19:06:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |          -0.0049 |         119.0592 |        -123.4418 |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |          -0.0014 |         116.2101 |        -123.4762 |
[32m[20221213 19:06:09 @agent_ppo2.py:185][0m |          -0.0027 |         115.2164 |        -123.3027 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |          -0.0048 |         114.7976 |        -123.8907 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |           0.0016 |         114.9765 |        -122.6090 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |          -0.0004 |         114.3145 |        -122.6363 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |           0.0013 |         114.5712 |        -123.1842 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |          -0.0035 |         114.8591 |        -123.9735 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |           0.0010 |         113.9825 |        -124.0678 |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |          -0.0056 |         113.9000 |        -123.6433 |
[32m[20221213 19:06:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.80
[32m[20221213 19:06:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 19:06:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.00
[32m[20221213 19:06:10 @agent_ppo2.py:143][0m Total time:      17.63 min
[32m[20221213 19:06:10 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221213 19:06:10 @agent_ppo2.py:121][0m #------------------------ Iteration 946 --------------------------#
[32m[20221213 19:06:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:10 @agent_ppo2.py:185][0m |           0.0117 |          13.8114 |        -126.5348 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |           0.0078 |           6.0692 |        -126.8140 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |          -0.0031 |           5.3865 |        -126.9879 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |           0.0045 |           5.1418 |        -126.3070 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |           0.0018 |           5.0162 |        -126.6720 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |          -0.0015 |           4.8949 |        -126.7157 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |          -0.0008 |           4.8616 |        -127.2460 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |          -0.0044 |           4.7550 |        -127.4342 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |           0.0075 |           4.7518 |        -127.6610 |
[32m[20221213 19:06:11 @agent_ppo2.py:185][0m |          -0.0019 |           4.7355 |        -127.2984 |
[32m[20221213 19:06:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.40
[32m[20221213 19:06:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.00
[32m[20221213 19:06:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:06:11 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221213 19:06:11 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221213 19:06:11 @agent_ppo2.py:121][0m #------------------------ Iteration 947 --------------------------#
[32m[20221213 19:06:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 19:06:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |           0.0051 |           8.8897 |        -127.1158 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |           0.0067 |           4.6332 |        -125.6897 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |           0.0065 |           4.2610 |        -125.7828 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |          -0.0044 |           4.0040 |        -125.6780 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |           0.0020 |           3.9177 |        -125.6894 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |           0.0026 |           3.8129 |        -125.6501 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |          -0.0010 |           3.8365 |        -124.6952 |
[32m[20221213 19:06:12 @agent_ppo2.py:185][0m |           0.0081 |           3.7098 |        -125.1769 |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |           0.0008 |           3.6526 |        -123.3774 |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |          -0.0040 |           3.6270 |        -124.0920 |
[32m[20221213 19:06:13 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 19:06:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.80
[32m[20221213 19:06:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.00
[32m[20221213 19:06:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:06:13 @agent_ppo2.py:143][0m Total time:      17.67 min
[32m[20221213 19:06:13 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221213 19:06:13 @agent_ppo2.py:121][0m #------------------------ Iteration 948 --------------------------#
[32m[20221213 19:06:13 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |           0.0150 |          41.3575 |        -125.1915 |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |           0.0058 |          22.0279 |        -126.2868 |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |           0.0007 |          21.0498 |        -126.5569 |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |           0.0073 |          21.0528 |        -126.0736 |
[32m[20221213 19:06:13 @agent_ppo2.py:185][0m |           0.0104 |          20.1994 |        -125.9373 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |          -0.0016 |          20.0822 |        -125.8729 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |          -0.0062 |          20.1191 |        -126.6794 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |           0.0001 |          20.1317 |        -126.6033 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |           0.0022 |          19.7100 |        -126.9367 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |           0.0002 |          19.7595 |        -127.1097 |
[32m[20221213 19:06:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.40
[32m[20221213 19:06:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.00
[32m[20221213 19:06:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.00
[32m[20221213 19:06:14 @agent_ppo2.py:143][0m Total time:      17.69 min
[32m[20221213 19:06:14 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221213 19:06:14 @agent_ppo2.py:121][0m #------------------------ Iteration 949 --------------------------#
[32m[20221213 19:06:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |           0.0032 |          20.1766 |        -127.9434 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |           0.0095 |          14.7080 |        -128.3419 |
[32m[20221213 19:06:14 @agent_ppo2.py:185][0m |           0.0017 |          14.1926 |        -127.8076 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |          -0.0022 |          13.9411 |        -127.8803 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |           0.0007 |          13.7377 |        -128.4061 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |          -0.0056 |          13.5820 |        -126.9449 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |           0.0042 |          13.4532 |        -128.2185 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |           0.0010 |          14.7290 |        -128.2732 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |          -0.0061 |          13.3147 |        -127.6104 |
[32m[20221213 19:06:15 @agent_ppo2.py:185][0m |           0.0012 |          13.6466 |        -127.4017 |
[32m[20221213 19:06:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.60
[32m[20221213 19:06:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:06:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.00
[32m[20221213 19:06:15 @agent_ppo2.py:143][0m Total time:      17.71 min
[32m[20221213 19:06:15 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221213 19:06:15 @agent_ppo2.py:121][0m #------------------------ Iteration 950 --------------------------#
[32m[20221213 19:06:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:06:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0010 |         106.7266 |        -124.5517 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0038 |         102.4566 |        -124.4923 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0000 |         101.1957 |        -124.9756 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0026 |         100.5456 |        -125.6840 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0042 |         100.6937 |        -125.7833 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |          -0.0004 |         100.1475 |        -126.4389 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |          -0.0042 |          99.7477 |        -126.6140 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0015 |          99.7392 |        -126.4910 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0060 |         106.3017 |        -126.4968 |
[32m[20221213 19:06:16 @agent_ppo2.py:185][0m |           0.0028 |          99.4337 |        -126.7466 |
[32m[20221213 19:06:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.20
[32m[20221213 19:06:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.00
[32m[20221213 19:06:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 19:06:17 @agent_ppo2.py:143][0m Total time:      17.73 min
[32m[20221213 19:06:17 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221213 19:06:17 @agent_ppo2.py:121][0m #------------------------ Iteration 951 --------------------------#
[32m[20221213 19:06:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |           0.0150 |         219.6841 |        -131.4220 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |           0.0023 |         198.1735 |        -129.9947 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |           0.0005 |         198.1854 |        -130.4060 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |          -0.0020 |         197.4314 |        -131.1822 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |          -0.0001 |         197.6143 |        -131.2270 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |           0.0134 |         198.9771 |        -129.4878 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |           0.0016 |         196.9331 |        -129.6613 |
[32m[20221213 19:06:17 @agent_ppo2.py:185][0m |          -0.0007 |         197.0943 |        -131.8578 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |           0.0007 |         197.3104 |        -131.0135 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |           0.0024 |         197.0449 |        -131.0034 |
[32m[20221213 19:06:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:06:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:06:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:06:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:06:18 @agent_ppo2.py:143][0m Total time:      17.75 min
[32m[20221213 19:06:18 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221213 19:06:18 @agent_ppo2.py:121][0m #------------------------ Iteration 952 --------------------------#
[32m[20221213 19:06:18 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |           0.0137 |          11.4724 |        -130.9480 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |           0.0150 |           8.4783 |        -131.0372 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |           0.0097 |           8.2781 |        -131.3973 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |           0.0041 |           8.2400 |        -132.7697 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |          -0.0025 |           8.2364 |        -132.9433 |
[32m[20221213 19:06:18 @agent_ppo2.py:185][0m |          -0.0101 |           8.1723 |        -134.7188 |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |           0.0023 |           8.1314 |        -134.1893 |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |           0.0044 |           8.0980 |        -134.3973 |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |           0.0054 |           8.5210 |        -135.3563 |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |           0.0032 |           8.1062 |        -135.4623 |
[32m[20221213 19:06:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 19:06:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.40
[32m[20221213 19:06:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 19:06:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 884.00
[32m[20221213 19:06:19 @agent_ppo2.py:143][0m Total time:      17.78 min
[32m[20221213 19:06:19 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221213 19:06:19 @agent_ppo2.py:121][0m #------------------------ Iteration 953 --------------------------#
[32m[20221213 19:06:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |          -0.0004 |          97.6129 |        -135.7436 |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |           0.0002 |          94.0718 |        -136.1952 |
[32m[20221213 19:06:19 @agent_ppo2.py:185][0m |           0.0111 |         103.6397 |        -135.5449 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |           0.0019 |          93.3881 |        -134.8949 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |           0.0003 |          92.7990 |        -135.0098 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |           0.0049 |          95.1051 |        -135.8257 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |           0.0029 |          92.3055 |        -135.3450 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |          -0.0030 |          92.4939 |        -136.7161 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |           0.0019 |          92.2772 |        -136.5552 |
[32m[20221213 19:06:20 @agent_ppo2.py:185][0m |          -0.0063 |          92.0105 |        -136.5232 |
[32m[20221213 19:06:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.40
[32m[20221213 19:06:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 19:06:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.00
[32m[20221213 19:06:20 @agent_ppo2.py:143][0m Total time:      17.80 min
[32m[20221213 19:06:20 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221213 19:06:20 @agent_ppo2.py:121][0m #------------------------ Iteration 954 --------------------------#
[32m[20221213 19:06:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |           0.0097 |           6.3093 |        -137.2933 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |           0.0042 |           3.1566 |        -136.6482 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0017 |           3.0449 |        -135.6887 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0021 |           3.0118 |        -135.4726 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0049 |           2.9413 |        -135.5162 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0080 |           2.9307 |        -134.9346 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0033 |           2.9681 |        -134.7379 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0022 |           2.8864 |        -134.8603 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0119 |           2.8551 |        -134.5506 |
[32m[20221213 19:06:21 @agent_ppo2.py:185][0m |          -0.0126 |           2.8369 |        -134.0403 |
[32m[20221213 19:06:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.20
[32m[20221213 19:06:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.00
[32m[20221213 19:06:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 889.00
[32m[20221213 19:06:21 @agent_ppo2.py:143][0m Total time:      17.82 min
[32m[20221213 19:06:21 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221213 19:06:21 @agent_ppo2.py:121][0m #------------------------ Iteration 955 --------------------------#
[32m[20221213 19:06:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |           0.0005 |         211.4447 |        -131.9087 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |           0.0028 |         209.1993 |        -131.6170 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |          -0.0015 |         206.5322 |        -130.6609 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |          -0.0010 |         206.0406 |        -130.2928 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |           0.0001 |         206.0774 |        -129.6223 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |           0.0017 |         206.0168 |        -128.8205 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |          -0.0025 |         206.1235 |        -129.1992 |
[32m[20221213 19:06:22 @agent_ppo2.py:185][0m |           0.0026 |         206.0478 |        -126.6999 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |           0.0017 |         205.8198 |        -127.9043 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |           0.0012 |         205.8255 |        -128.7175 |
[32m[20221213 19:06:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.20
[32m[20221213 19:06:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 19:06:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:06:23 @agent_ppo2.py:143][0m Total time:      17.84 min
[32m[20221213 19:06:23 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221213 19:06:23 @agent_ppo2.py:121][0m #------------------------ Iteration 956 --------------------------#
[32m[20221213 19:06:23 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |           0.0001 |         211.1678 |        -124.8922 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |          -0.0009 |         208.2787 |        -125.0164 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |           0.0041 |         207.4699 |        -124.7666 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |          -0.0002 |         206.3785 |        -125.5296 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |           0.0013 |         206.4827 |        -125.1214 |
[32m[20221213 19:06:23 @agent_ppo2.py:185][0m |          -0.0008 |         206.1606 |        -125.2068 |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |          -0.0022 |         206.4473 |        -124.9673 |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |           0.0034 |         206.2728 |        -124.5181 |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |           0.0025 |         206.5563 |        -124.3804 |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |           0.0015 |         205.9352 |        -125.4830 |
[32m[20221213 19:06:24 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.60
[32m[20221213 19:06:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 886.00
[32m[20221213 19:06:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 885.00
[32m[20221213 19:06:24 @agent_ppo2.py:143][0m Total time:      17.86 min
[32m[20221213 19:06:24 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221213 19:06:24 @agent_ppo2.py:121][0m #------------------------ Iteration 957 --------------------------#
[32m[20221213 19:06:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 19:06:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |           0.0010 |         218.8455 |        -126.3718 |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |          -0.0012 |         213.4918 |        -126.5091 |
[32m[20221213 19:06:24 @agent_ppo2.py:185][0m |           0.0106 |         229.0123 |        -126.5903 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0082 |         223.7227 |        -126.7300 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0141 |         218.0995 |        -125.5540 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0017 |         212.3246 |        -127.0285 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0021 |         212.0916 |        -127.9046 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0022 |         212.1006 |        -125.3391 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0042 |         211.8411 |        -126.6534 |
[32m[20221213 19:06:25 @agent_ppo2.py:185][0m |           0.0031 |         211.7234 |        -127.5949 |
[32m[20221213 19:06:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.60
[32m[20221213 19:06:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.00
[32m[20221213 19:06:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.00
[32m[20221213 19:06:25 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221213 19:06:25 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221213 19:06:25 @agent_ppo2.py:121][0m #------------------------ Iteration 958 --------------------------#
[32m[20221213 19:06:25 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0137 |          13.2970 |        -129.3949 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |          -0.0001 |           7.7270 |        -127.9326 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0197 |           7.5182 |        -126.8637 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0058 |           7.5949 |        -127.2741 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0111 |           7.5253 |        -127.9595 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0171 |           7.9802 |        -128.0494 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0033 |           7.4348 |        -128.2071 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0078 |           7.3998 |        -127.2443 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0100 |           7.4724 |        -127.8338 |
[32m[20221213 19:06:26 @agent_ppo2.py:185][0m |           0.0032 |           7.3593 |        -128.2022 |
[32m[20221213 19:06:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.20
[32m[20221213 19:06:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 73.00
[32m[20221213 19:06:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.00
[32m[20221213 19:06:26 @agent_ppo2.py:143][0m Total time:      17.90 min
[32m[20221213 19:06:26 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221213 19:06:26 @agent_ppo2.py:121][0m #------------------------ Iteration 959 --------------------------#
[32m[20221213 19:06:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0058 |          13.8664 |        -128.3223 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0025 |           5.1704 |        -126.4165 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0094 |           4.6008 |        -126.2381 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0023 |           4.4496 |        -126.3517 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0069 |           4.3703 |        -125.3807 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0053 |           4.4422 |        -123.2725 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |          -0.0006 |           4.3365 |        -124.7709 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |           0.0029 |           4.2880 |        -124.6816 |
[32m[20221213 19:06:27 @agent_ppo2.py:185][0m |          -0.0039 |           4.2807 |        -123.9709 |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |          -0.0040 |           4.2838 |        -123.8487 |
[32m[20221213 19:06:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.60
[32m[20221213 19:06:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 19:06:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 19:06:28 @agent_ppo2.py:143][0m Total time:      17.92 min
[32m[20221213 19:06:28 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221213 19:06:28 @agent_ppo2.py:121][0m #------------------------ Iteration 960 --------------------------#
[32m[20221213 19:06:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:06:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |           0.0094 |           8.3222 |        -125.6166 |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |           0.0054 |           4.8283 |        -126.0524 |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |           0.0017 |           4.6654 |        -126.7247 |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |           0.0010 |           4.6134 |        -126.9556 |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |          -0.0022 |           4.6281 |        -127.3428 |
[32m[20221213 19:06:28 @agent_ppo2.py:185][0m |          -0.0074 |           4.5612 |        -126.7803 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |           0.0073 |           4.5020 |        -127.2612 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |           0.0029 |           4.5138 |        -128.1124 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |          -0.0027 |           4.4529 |        -127.5835 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |          -0.0026 |           4.4378 |        -127.1694 |
[32m[20221213 19:06:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.00
[32m[20221213 19:06:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 44.00
[32m[20221213 19:06:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.00
[32m[20221213 19:06:29 @agent_ppo2.py:143][0m Total time:      17.94 min
[32m[20221213 19:06:29 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221213 19:06:29 @agent_ppo2.py:121][0m #------------------------ Iteration 961 --------------------------#
[32m[20221213 19:06:29 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |          -0.0007 |         217.1123 |        -128.1181 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |           0.0029 |         214.1005 |        -129.4116 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |           0.0049 |         213.5256 |        -128.4706 |
[32m[20221213 19:06:29 @agent_ppo2.py:185][0m |           0.0035 |         213.1590 |        -128.6260 |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |           0.0028 |         213.4308 |        -129.3201 |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |           0.0106 |         230.5090 |        -129.7668 |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |           0.0013 |         213.5086 |        -129.9914 |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |           0.0002 |         212.7257 |        -130.9302 |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |          -0.0008 |         212.8737 |        -130.0825 |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |          -0.0039 |         212.9592 |        -131.5349 |
[32m[20221213 19:06:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.00
[32m[20221213 19:06:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 19:06:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 19:06:30 @agent_ppo2.py:143][0m Total time:      17.96 min
[32m[20221213 19:06:30 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221213 19:06:30 @agent_ppo2.py:121][0m #------------------------ Iteration 962 --------------------------#
[32m[20221213 19:06:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:30 @agent_ppo2.py:185][0m |          -0.0011 |         214.3603 |        -130.0832 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0000 |         209.1407 |        -129.7397 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0013 |         208.5276 |        -129.8434 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0022 |         208.1794 |        -130.3358 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |          -0.0038 |         208.3159 |        -129.8210 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0063 |         207.7212 |        -129.6428 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |          -0.0034 |         207.6097 |        -129.5783 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0038 |         207.6287 |        -128.9132 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0060 |         210.9964 |        -129.6817 |
[32m[20221213 19:06:31 @agent_ppo2.py:185][0m |           0.0000 |         207.3784 |        -128.9594 |
[32m[20221213 19:06:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 19:06:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.00
[32m[20221213 19:06:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 19:06:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 19:06:31 @agent_ppo2.py:143][0m Total time:      17.98 min
[32m[20221213 19:06:31 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221213 19:06:31 @agent_ppo2.py:121][0m #------------------------ Iteration 963 --------------------------#
[32m[20221213 19:06:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0118 |          14.6387 |        -127.9532 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0062 |           8.2456 |        -126.8650 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0042 |           8.0111 |        -126.6610 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0038 |           7.9128 |        -127.5345 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |          -0.0055 |           7.8179 |        -128.0245 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0017 |           7.7557 |        -127.5481 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |          -0.0007 |           7.7613 |        -127.3751 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0059 |           7.6887 |        -126.9904 |
[32m[20221213 19:06:32 @agent_ppo2.py:185][0m |           0.0071 |           7.6656 |        -126.6602 |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |          -0.0081 |           7.5923 |        -126.5900 |
[32m[20221213 19:06:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.40
[32m[20221213 19:06:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:06:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.00
[32m[20221213 19:06:33 @agent_ppo2.py:143][0m Total time:      18.00 min
[32m[20221213 19:06:33 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221213 19:06:33 @agent_ppo2.py:121][0m #------------------------ Iteration 964 --------------------------#
[32m[20221213 19:06:33 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |           0.0039 |         189.4715 |        -128.5691 |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |           0.0022 |         184.3254 |        -128.9693 |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |           0.0272 |         222.0257 |        -128.4117 |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |           0.0067 |         181.9806 |        -127.6233 |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |           0.0054 |         197.9318 |        -128.8983 |
[32m[20221213 19:06:33 @agent_ppo2.py:185][0m |           0.0187 |         202.3031 |        -128.2082 |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |           0.0012 |         180.3950 |        -128.6080 |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |          -0.0018 |         181.0375 |        -129.5717 |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |          -0.0044 |         180.5316 |        -129.1284 |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |           0.0149 |         199.9422 |        -128.8227 |
[32m[20221213 19:06:34 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 19:06:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.00
[32m[20221213 19:06:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.00
[32m[20221213 19:06:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:06:34 @agent_ppo2.py:143][0m Total time:      18.03 min
[32m[20221213 19:06:34 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221213 19:06:34 @agent_ppo2.py:121][0m #------------------------ Iteration 965 --------------------------#
[32m[20221213 19:06:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |          -0.0022 |         212.0133 |        -128.4150 |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |           0.0167 |         234.6222 |        -127.5781 |
[32m[20221213 19:06:34 @agent_ppo2.py:185][0m |          -0.0020 |         205.1824 |        -127.8501 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |           0.0032 |         205.4667 |        -127.8094 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |          -0.0030 |         203.9828 |        -126.3579 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |          -0.0024 |         204.3608 |        -127.3054 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |          -0.0059 |         204.0887 |        -126.1124 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |          -0.0018 |         204.1783 |        -124.4457 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |          -0.0046 |         204.0277 |        -125.6827 |
[32m[20221213 19:06:35 @agent_ppo2.py:185][0m |          -0.0009 |         203.8949 |        -125.0556 |
[32m[20221213 19:06:35 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 19:06:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.00
[32m[20221213 19:06:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 19:06:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 19:06:35 @agent_ppo2.py:143][0m Total time:      18.05 min
[32m[20221213 19:06:35 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221213 19:06:35 @agent_ppo2.py:121][0m #------------------------ Iteration 966 --------------------------#
[32m[20221213 19:06:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:06:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0119 |          31.0299 |        -127.2541 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0056 |          17.3648 |        -127.2781 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0051 |          16.5521 |        -127.1269 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0061 |          16.0044 |        -127.1989 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0023 |          15.7116 |        -127.0261 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |          -0.0001 |          15.4894 |        -127.1787 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0102 |          15.7672 |        -127.4192 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |           0.0024 |          15.3616 |        -127.4127 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |          -0.0005 |          15.0862 |        -127.5163 |
[32m[20221213 19:06:36 @agent_ppo2.py:185][0m |          -0.0029 |          14.9106 |        -127.4189 |
[32m[20221213 19:06:36 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 19:06:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.80
[32m[20221213 19:06:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 74.00
[32m[20221213 19:06:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.00
[32m[20221213 19:06:37 @agent_ppo2.py:143][0m Total time:      18.07 min
[32m[20221213 19:06:37 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221213 19:06:37 @agent_ppo2.py:121][0m #------------------------ Iteration 967 --------------------------#
[32m[20221213 19:06:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 19:06:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:37 @agent_ppo2.py:185][0m |           0.0194 |          14.1155 |        -125.8925 |
[32m[20221213 19:06:37 @agent_ppo2.py:185][0m |           0.0131 |           8.2269 |        -125.1495 |
[32m[20221213 19:06:37 @agent_ppo2.py:185][0m |           0.0074 |           7.6385 |        -125.9267 |
[32m[20221213 19:06:37 @agent_ppo2.py:185][0m |           0.0020 |           7.4577 |        -126.9033 |
[32m[20221213 19:06:37 @agent_ppo2.py:185][0m |          -0.0019 |           7.4512 |        -126.7597 |
[32m[20221213 19:06:37 @agent_ppo2.py:185][0m |           0.0064 |           7.6551 |        -126.9292 |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |          -0.0009 |           7.3252 |        -126.0031 |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |           0.0062 |           7.2296 |        -126.5609 |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |           0.0102 |           7.1420 |        -126.1313 |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |           0.0076 |           7.1494 |        -127.0062 |
[32m[20221213 19:06:38 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 19:06:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.20
[32m[20221213 19:06:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.00
[32m[20221213 19:06:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.00
[32m[20221213 19:06:38 @agent_ppo2.py:143][0m Total time:      18.09 min
[32m[20221213 19:06:38 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221213 19:06:38 @agent_ppo2.py:121][0m #------------------------ Iteration 968 --------------------------#
[32m[20221213 19:06:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |           0.0023 |         178.3592 |        -128.2775 |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |          -0.0042 |         171.1533 |        -128.6342 |
[32m[20221213 19:06:38 @agent_ppo2.py:185][0m |           0.0087 |         170.6462 |        -128.6437 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |           0.0012 |         169.3365 |        -127.4968 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |          -0.0016 |         169.2508 |        -127.1220 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |          -0.0044 |         171.4236 |        -126.6475 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |          -0.0010 |         169.1015 |        -126.7700 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |           0.0007 |         168.7863 |        -126.6299 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |          -0.0014 |         167.6672 |        -125.9154 |
[32m[20221213 19:06:39 @agent_ppo2.py:185][0m |           0.0034 |         168.6446 |        -124.6758 |
[32m[20221213 19:06:39 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 19:06:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.20
[32m[20221213 19:06:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 19:06:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:06:39 @agent_ppo2.py:143][0m Total time:      18.11 min
[32m[20221213 19:06:39 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221213 19:06:39 @agent_ppo2.py:121][0m #------------------------ Iteration 969 --------------------------#
[32m[20221213 19:06:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |           0.0072 |         214.3530 |        -124.1681 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |           0.0010 |         201.9991 |        -125.1484 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |          -0.0003 |         199.0452 |        -125.6699 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |           0.0010 |         197.7360 |        -126.1402 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |           0.0020 |         197.6292 |        -125.5135 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |          -0.0005 |         198.3943 |        -125.4808 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |           0.0018 |         198.1051 |        -125.4986 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |           0.0005 |         196.9396 |        -125.8823 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |          -0.0038 |         196.9923 |        -126.3382 |
[32m[20221213 19:06:40 @agent_ppo2.py:185][0m |          -0.0003 |         196.3228 |        -126.8185 |
[32m[20221213 19:06:40 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 19:06:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.00
[32m[20221213 19:06:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 19:06:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 636.00
[32m[20221213 19:06:40 @agent_ppo2.py:143][0m Total time:      18.13 min
[32m[20221213 19:06:40 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221213 19:06:40 @agent_ppo2.py:121][0m #------------------------ Iteration 970 --------------------------#
[32m[20221213 19:06:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 19:06:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0074 |         233.7039 |        -124.4108 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0040 |         232.3225 |        -124.7824 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0181 |         266.1345 |        -125.2998 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0009 |         231.7974 |        -124.3106 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0050 |         231.5534 |        -124.0081 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0019 |         231.1025 |        -124.7672 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0016 |         230.8983 |        -124.4417 |
[32m[20221213 19:06:41 @agent_ppo2.py:185][0m |           0.0022 |         230.9834 |        -125.2561 |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |           0.0046 |         232.6107 |        -124.3951 |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |           0.0013 |         231.0219 |        -125.3559 |
[32m[20221213 19:06:42 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.20
[32m[20221213 19:06:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 19:06:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 19:06:42 @agent_ppo2.py:143][0m Total time:      18.16 min
[32m[20221213 19:06:42 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221213 19:06:42 @agent_ppo2.py:121][0m #------------------------ Iteration 971 --------------------------#
[32m[20221213 19:06:42 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |           0.0096 |          98.1627 |        -126.8936 |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |           0.0001 |          87.0790 |        -128.1263 |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |          -0.0072 |          86.0274 |        -127.7781 |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |          -0.0066 |          85.2175 |        -128.3785 |
[32m[20221213 19:06:42 @agent_ppo2.py:185][0m |           0.0043 |          87.2366 |        -129.0304 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |          -0.0009 |          84.4765 |        -129.1351 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |           0.0004 |          84.3429 |        -128.9978 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |          -0.0009 |          84.1493 |        -128.8481 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |          -0.0102 |          84.3055 |        -129.2793 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |          -0.0065 |          84.0166 |        -130.0902 |
[32m[20221213 19:06:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 19:06:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.00
[32m[20221213 19:06:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.00
[32m[20221213 19:06:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 19:06:43 @agent_ppo2.py:143][0m Total time:      18.18 min
[32m[20221213 19:06:43 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221213 19:06:43 @agent_ppo2.py:121][0m #------------------------ Iteration 972 --------------------------#
[32m[20221213 19:06:43 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |           0.0075 |         249.3986 |        -135.1392 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |          -0.0025 |         242.5803 |        -135.5548 |
[32m[20221213 19:06:43 @agent_ppo2.py:185][0m |           0.0027 |         241.6207 |        -136.1319 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |           0.0024 |         241.1300 |        -136.2000 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |           0.0037 |         240.5561 |        -135.8428 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |           0.0016 |         240.3032 |        -137.2041 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |           0.0038 |         239.8150 |        -137.1573 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |           0.0011 |         239.5611 |        -138.2743 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |           0.0008 |         239.5124 |        -137.5319 |
[32m[20221213 19:06:44 @agent_ppo2.py:185][0m |          -0.0003 |         239.3558 |        -138.3117 |
[32m[20221213 19:06:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.40
[32m[20221213 19:06:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.00
[32m[20221213 19:06:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.00
[32m[20221213 19:06:44 @agent_ppo2.py:143][0m Total time:      18.20 min
[32m[20221213 19:06:44 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221213 19:06:44 @agent_ppo2.py:121][0m #------------------------ Iteration 973 --------------------------#
[32m[20221213 19:06:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |          -0.0021 |         240.1705 |        -137.5022 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0023 |         237.8649 |        -137.4197 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |          -0.0022 |         236.9472 |        -137.7043 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0067 |         238.7066 |        -137.4990 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0083 |         240.4515 |        -137.3915 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0015 |         237.7879 |        -138.2061 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0033 |         238.2753 |        -136.9107 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0102 |         258.5280 |        -138.4239 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0164 |         264.0672 |        -137.9212 |
[32m[20221213 19:06:45 @agent_ppo2.py:185][0m |           0.0037 |         236.7043 |        -138.8374 |
[32m[20221213 19:06:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.20
[32m[20221213 19:06:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 19:06:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 19:06:46 @agent_ppo2.py:143][0m Total time:      18.22 min
[32m[20221213 19:06:46 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221213 19:06:46 @agent_ppo2.py:121][0m #------------------------ Iteration 974 --------------------------#
[32m[20221213 19:06:46 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0119 |          56.2993 |        -140.6357 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0107 |          31.8830 |        -141.4194 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0110 |          30.8595 |        -141.0390 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0068 |          30.3514 |        -141.8378 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0047 |          30.1796 |        -141.6190 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0016 |          29.8843 |        -142.0033 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0015 |          30.4187 |        -142.5569 |
[32m[20221213 19:06:46 @agent_ppo2.py:185][0m |           0.0041 |          29.8637 |        -141.9806 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0032 |          29.4937 |        -142.3435 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0033 |          29.6130 |        -142.9907 |
[32m[20221213 19:06:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.80
[32m[20221213 19:06:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.00
[32m[20221213 19:06:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.00
[32m[20221213 19:06:47 @agent_ppo2.py:143][0m Total time:      18.24 min
[32m[20221213 19:06:47 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221213 19:06:47 @agent_ppo2.py:121][0m #------------------------ Iteration 975 --------------------------#
[32m[20221213 19:06:47 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0212 |         248.0653 |        -138.9746 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0085 |         231.9014 |        -138.5302 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0073 |         231.5229 |        -140.4442 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0152 |         231.6576 |        -138.6949 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |           0.0025 |         231.5089 |        -142.3957 |
[32m[20221213 19:06:47 @agent_ppo2.py:185][0m |          -0.0003 |         231.0295 |        -141.7356 |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |          -0.0003 |         230.7750 |        -141.6761 |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |          -0.0017 |         231.0988 |        -142.8854 |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |          -0.0017 |         230.6774 |        -143.0162 |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |           0.0045 |         231.0884 |        -141.2913 |
[32m[20221213 19:06:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 19:06:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.40
[32m[20221213 19:06:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.00
[32m[20221213 19:06:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:06:48 @agent_ppo2.py:143][0m Total time:      18.26 min
[32m[20221213 19:06:48 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221213 19:06:48 @agent_ppo2.py:121][0m #------------------------ Iteration 976 --------------------------#
[32m[20221213 19:06:48 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |          -0.0036 |         236.5877 |        -149.0780 |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |           0.0033 |         235.5618 |        -149.9265 |
[32m[20221213 19:06:48 @agent_ppo2.py:185][0m |           0.0000 |         235.8874 |        -150.6391 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |          -0.0012 |         235.7732 |        -150.2113 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |           0.0014 |         235.9020 |        -151.0059 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |           0.0020 |         235.6385 |        -151.5303 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |           0.0044 |         235.8995 |        -150.1839 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |          -0.0013 |         235.5369 |        -152.1552 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |           0.0034 |         235.9778 |        -152.1118 |
[32m[20221213 19:06:49 @agent_ppo2.py:185][0m |           0.0140 |         253.3898 |        -152.8300 |
[32m[20221213 19:06:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1000.00
[32m[20221213 19:06:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1000.00
[32m[20221213 19:06:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:06:49 @agent_ppo2.py:143][0m Total time:      18.28 min
[32m[20221213 19:06:49 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221213 19:06:49 @agent_ppo2.py:121][0m #------------------------ Iteration 977 --------------------------#
[32m[20221213 19:06:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:06:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |          -0.0002 |         230.1276 |        -153.8350 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |          -0.0033 |         228.4155 |        -153.3690 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |          -0.0019 |         226.8713 |        -153.9774 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |           0.0012 |         226.3749 |        -153.3451 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |          -0.0048 |         226.4030 |        -154.9824 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |           0.0004 |         226.4233 |        -153.6558 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |           0.0053 |         228.1771 |        -153.9831 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |           0.0015 |         225.8289 |        -153.7208 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |           0.0101 |         226.3095 |        -152.9523 |
[32m[20221213 19:06:50 @agent_ppo2.py:185][0m |           0.0049 |         229.5655 |        -154.6172 |
[32m[20221213 19:06:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.00
[32m[20221213 19:06:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 19:06:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 19:06:50 @agent_ppo2.py:143][0m Total time:      18.30 min
[32m[20221213 19:06:50 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221213 19:06:50 @agent_ppo2.py:121][0m #------------------------ Iteration 978 --------------------------#
[32m[20221213 19:06:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |           0.0034 |         232.8702 |        -152.9577 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |           0.0056 |         230.8191 |        -150.3637 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |          -0.0011 |         230.1201 |        -151.9472 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |           0.0036 |         236.9013 |        -152.1001 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |           0.0040 |         231.8365 |        -151.2327 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |           0.0111 |         239.7551 |        -150.2079 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |           0.0000 |         230.1127 |        -149.0784 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |          -0.0020 |         229.7528 |        -150.9743 |
[32m[20221213 19:06:51 @agent_ppo2.py:185][0m |          -0.0000 |         229.7581 |        -149.9995 |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |          -0.0021 |         229.7840 |        -149.7683 |
[32m[20221213 19:06:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:06:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.20
[32m[20221213 19:06:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 19:06:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.00
[32m[20221213 19:06:52 @agent_ppo2.py:143][0m Total time:      18.32 min
[32m[20221213 19:06:52 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221213 19:06:52 @agent_ppo2.py:121][0m #------------------------ Iteration 979 --------------------------#
[32m[20221213 19:06:52 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |          -0.0001 |         244.9117 |        -146.4409 |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |          -0.0014 |         243.8027 |        -146.2180 |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |           0.0019 |         243.2040 |        -145.4901 |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |          -0.0037 |         243.0495 |        -144.7335 |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |          -0.0043 |         242.9284 |        -144.3778 |
[32m[20221213 19:06:52 @agent_ppo2.py:185][0m |           0.0030 |         242.8155 |        -144.2394 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |          -0.0014 |         242.6730 |        -145.8271 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |           0.0105 |         256.6820 |        -145.6734 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |          -0.0025 |         242.4048 |        -146.0932 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |          -0.0003 |         242.3822 |        -145.2016 |
[32m[20221213 19:06:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:06:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.60
[32m[20221213 19:06:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:06:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:06:53 @agent_ppo2.py:143][0m Total time:      18.34 min
[32m[20221213 19:06:53 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221213 19:06:53 @agent_ppo2.py:121][0m #------------------------ Iteration 980 --------------------------#
[32m[20221213 19:06:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:06:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |          -0.0031 |         242.4815 |        -150.1413 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |          -0.0009 |         241.3268 |        -150.1470 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |           0.0100 |         241.0634 |        -148.6416 |
[32m[20221213 19:06:53 @agent_ppo2.py:185][0m |          -0.0019 |         240.8117 |        -149.4049 |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |          -0.0014 |         240.3707 |        -149.2383 |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |          -0.0010 |         240.6626 |        -150.3266 |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |          -0.0018 |         240.4218 |        -150.1154 |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |          -0.0026 |         240.1984 |        -149.8062 |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |          -0.0006 |         240.2239 |        -149.6406 |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |           0.0030 |         239.9476 |        -148.5561 |
[32m[20221213 19:06:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.60
[32m[20221213 19:06:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 19:06:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.00
[32m[20221213 19:06:54 @agent_ppo2.py:143][0m Total time:      18.36 min
[32m[20221213 19:06:54 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221213 19:06:54 @agent_ppo2.py:121][0m #------------------------ Iteration 981 --------------------------#
[32m[20221213 19:06:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:06:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:54 @agent_ppo2.py:185][0m |           0.0307 |          20.5405 |        -145.3077 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |           0.0131 |          12.1228 |        -143.6508 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |           0.0025 |          10.8690 |        -144.9875 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |          -0.0033 |          10.3829 |        -145.0211 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |           0.0002 |          10.0580 |        -145.7390 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |          -0.0016 |           9.9212 |        -144.0324 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |          -0.0034 |           9.7258 |        -143.8850 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |          -0.0053 |           9.6430 |        -143.0545 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |          -0.0067 |           9.5601 |        -142.9926 |
[32m[20221213 19:06:55 @agent_ppo2.py:185][0m |          -0.0070 |           9.5432 |        -142.7998 |
[32m[20221213 19:06:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 19:06:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.60
[32m[20221213 19:06:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 19:06:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 19:06:55 @agent_ppo2.py:143][0m Total time:      18.38 min
[32m[20221213 19:06:55 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221213 19:06:55 @agent_ppo2.py:121][0m #------------------------ Iteration 982 --------------------------#
[32m[20221213 19:06:56 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0139 |          48.3161 |        -137.0058 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0085 |          40.8038 |        -138.7978 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0076 |          39.6560 |        -138.0785 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0039 |          40.0285 |        -140.7704 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0031 |          39.7507 |        -140.5150 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0017 |          39.4509 |        -139.7012 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |           0.0064 |          40.1060 |        -140.5563 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |          -0.0004 |          39.2706 |        -140.7548 |
[32m[20221213 19:06:56 @agent_ppo2.py:185][0m |          -0.0002 |          39.1662 |        -141.1567 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0043 |          40.0290 |        -141.1864 |
[32m[20221213 19:06:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.80
[32m[20221213 19:06:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:06:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:06:57 @agent_ppo2.py:143][0m Total time:      18.40 min
[32m[20221213 19:06:57 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221213 19:06:57 @agent_ppo2.py:121][0m #------------------------ Iteration 983 --------------------------#
[32m[20221213 19:06:57 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0014 |         242.4904 |        -142.2571 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0046 |         238.0535 |        -143.2068 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0062 |         237.7354 |        -142.3034 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0033 |         237.8649 |        -143.3525 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0059 |         241.7683 |        -144.5587 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0015 |         237.4112 |        -144.3881 |
[32m[20221213 19:06:57 @agent_ppo2.py:185][0m |           0.0022 |         236.7091 |        -142.5965 |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |           0.0075 |         236.7935 |        -143.3096 |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |          -0.0003 |         236.8065 |        -144.0609 |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |          -0.0028 |         236.9504 |        -145.0163 |
[32m[20221213 19:06:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:06:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.00
[32m[20221213 19:06:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 19:06:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:06:58 @agent_ppo2.py:143][0m Total time:      18.42 min
[32m[20221213 19:06:58 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221213 19:06:58 @agent_ppo2.py:121][0m #------------------------ Iteration 984 --------------------------#
[32m[20221213 19:06:58 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |           0.0048 |          18.3301 |        -144.0251 |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |          -0.0009 |          12.3186 |        -142.6189 |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |           0.0048 |          11.8163 |        -141.5979 |
[32m[20221213 19:06:58 @agent_ppo2.py:185][0m |           0.0056 |          11.4397 |        -140.7153 |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |          -0.0029 |          11.2163 |        -140.4792 |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |           0.0120 |          11.2663 |        -139.6429 |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |          -0.0058 |          10.9718 |        -138.5049 |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |           0.0033 |          10.8609 |        -138.7099 |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |          -0.0042 |          10.6764 |        -137.3146 |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |           0.0008 |          10.5717 |        -138.2216 |
[32m[20221213 19:06:59 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:06:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.00
[32m[20221213 19:06:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 19:06:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 105.00
[32m[20221213 19:06:59 @agent_ppo2.py:143][0m Total time:      18.45 min
[32m[20221213 19:06:59 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221213 19:06:59 @agent_ppo2.py:121][0m #------------------------ Iteration 985 --------------------------#
[32m[20221213 19:06:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:06:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:06:59 @agent_ppo2.py:185][0m |           0.0063 |         241.2784 |        -134.3872 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0036 |         234.3300 |        -134.3763 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0031 |         232.9757 |        -132.9256 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0017 |         232.1911 |        -132.3323 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0031 |         231.7592 |        -131.6307 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |          -0.0002 |         231.7520 |        -130.3040 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0059 |         232.9851 |        -130.3996 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0006 |         231.5275 |        -129.4244 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |          -0.0006 |         231.5139 |        -128.3614 |
[32m[20221213 19:07:00 @agent_ppo2.py:185][0m |           0.0074 |         239.9118 |        -127.9653 |
[32m[20221213 19:07:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:07:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.00
[32m[20221213 19:07:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.00
[32m[20221213 19:07:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:07:00 @agent_ppo2.py:143][0m Total time:      18.47 min
[32m[20221213 19:07:00 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221213 19:07:00 @agent_ppo2.py:121][0m #------------------------ Iteration 986 --------------------------#
[32m[20221213 19:07:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0132 |          10.7316 |        -125.3822 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0029 |           7.4906 |        -124.5410 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0109 |           7.3145 |        -124.9598 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0028 |           7.2676 |        -124.3951 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0165 |           7.5819 |        -124.0893 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0050 |           7.2140 |        -123.1975 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |          -0.0021 |           7.1709 |        -122.9351 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |          -0.0030 |           7.1898 |        -122.3768 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0028 |           7.1549 |        -122.1612 |
[32m[20221213 19:07:01 @agent_ppo2.py:185][0m |           0.0058 |           7.1838 |        -121.8669 |
[32m[20221213 19:07:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:07:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.60
[32m[20221213 19:07:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:07:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.00
[32m[20221213 19:07:02 @agent_ppo2.py:143][0m Total time:      18.49 min
[32m[20221213 19:07:02 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221213 19:07:02 @agent_ppo2.py:121][0m #------------------------ Iteration 987 --------------------------#
[32m[20221213 19:07:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:07:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0005 |         121.4491 |        -122.5839 |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0034 |         111.9975 |        -123.0752 |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0024 |         110.7514 |        -122.9059 |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0032 |         111.1975 |        -124.2594 |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0035 |         110.6753 |        -123.7720 |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0031 |         110.5274 |        -123.8792 |
[32m[20221213 19:07:02 @agent_ppo2.py:185][0m |           0.0012 |         110.0468 |        -124.4903 |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |          -0.0015 |         110.0273 |        -124.0968 |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |           0.0001 |         109.6375 |        -123.4806 |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |          -0.0002 |         109.4956 |        -124.9617 |
[32m[20221213 19:07:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:07:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.20
[32m[20221213 19:07:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.00
[32m[20221213 19:07:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:07:03 @agent_ppo2.py:143][0m Total time:      18.51 min
[32m[20221213 19:07:03 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221213 19:07:03 @agent_ppo2.py:121][0m #------------------------ Iteration 988 --------------------------#
[32m[20221213 19:07:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |           0.0038 |          15.1757 |        -124.2868 |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |           0.0088 |           7.6624 |        -124.9926 |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |           0.0089 |           7.3800 |        -123.3448 |
[32m[20221213 19:07:03 @agent_ppo2.py:185][0m |          -0.0010 |           7.1162 |        -122.2224 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |           0.0060 |           7.0346 |        -122.6297 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |          -0.0007 |           7.0256 |        -122.2555 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |           0.0012 |           6.9241 |        -122.5224 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |          -0.0035 |           6.9212 |        -122.6460 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |          -0.0065 |           6.9168 |        -122.4128 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |           0.0053 |           6.8490 |        -121.1261 |
[32m[20221213 19:07:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:07:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.60
[32m[20221213 19:07:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.00
[32m[20221213 19:07:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.00
[32m[20221213 19:07:04 @agent_ppo2.py:143][0m Total time:      18.53 min
[32m[20221213 19:07:04 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221213 19:07:04 @agent_ppo2.py:121][0m #------------------------ Iteration 989 --------------------------#
[32m[20221213 19:07:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |           0.0023 |         195.2054 |        -119.5000 |
[32m[20221213 19:07:04 @agent_ppo2.py:185][0m |           0.0004 |         186.9929 |        -119.2914 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |          -0.0028 |         183.7694 |        -119.4426 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |          -0.0011 |         182.0695 |        -120.4938 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |           0.0057 |         181.0990 |        -120.3867 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |           0.0079 |         185.6384 |        -121.5766 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |          -0.0010 |         178.9538 |        -121.4516 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |           0.0011 |         178.7235 |        -120.6135 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |           0.0038 |         179.1632 |        -119.9664 |
[32m[20221213 19:07:05 @agent_ppo2.py:185][0m |          -0.0034 |         178.7551 |        -120.9419 |
[32m[20221213 19:07:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:07:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.40
[32m[20221213 19:07:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 892.00
[32m[20221213 19:07:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.00
[32m[20221213 19:07:05 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221213 19:07:05 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221213 19:07:05 @agent_ppo2.py:121][0m #------------------------ Iteration 990 --------------------------#
[32m[20221213 19:07:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 19:07:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |           0.0061 |         189.5379 |        -123.4998 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |           0.0052 |         184.2578 |        -122.5900 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |           0.0005 |         183.9410 |        -124.1720 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |          -0.0039 |         183.3005 |        -123.2797 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |           0.0125 |         182.8658 |        -121.6409 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |          -0.0025 |         182.7961 |        -123.7640 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |           0.0076 |         199.7881 |        -124.1969 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |          -0.0016 |         182.2818 |        -123.0841 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |          -0.0018 |         182.0445 |        -123.0589 |
[32m[20221213 19:07:06 @agent_ppo2.py:185][0m |          -0.0055 |         182.3328 |        -123.0813 |
[32m[20221213 19:07:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 19:07:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.60
[32m[20221213 19:07:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.00
[32m[20221213 19:07:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:07:07 @agent_ppo2.py:143][0m Total time:      18.57 min
[32m[20221213 19:07:07 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221213 19:07:07 @agent_ppo2.py:121][0m #------------------------ Iteration 991 --------------------------#
[32m[20221213 19:07:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:07:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0167 |         266.6807 |        -127.7615 |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0068 |         237.9763 |        -127.9004 |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0149 |         267.5397 |        -128.4981 |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0004 |         237.2880 |        -127.8746 |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0000 |         236.7303 |        -128.7287 |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0053 |         236.6999 |        -126.5896 |
[32m[20221213 19:07:07 @agent_ppo2.py:185][0m |           0.0002 |         236.3990 |        -128.9610 |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0045 |         236.7342 |        -128.3451 |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0125 |         242.1160 |        -126.7621 |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0090 |         236.6843 |        -128.3972 |
[32m[20221213 19:07:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:07:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.80
[32m[20221213 19:07:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 891.00
[32m[20221213 19:07:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:07:08 @agent_ppo2.py:143][0m Total time:      18.59 min
[32m[20221213 19:07:08 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221213 19:07:08 @agent_ppo2.py:121][0m #------------------------ Iteration 992 --------------------------#
[32m[20221213 19:07:08 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0143 |          19.5338 |        -129.3033 |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0144 |           8.8999 |        -129.5531 |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0075 |           7.6599 |        -129.1887 |
[32m[20221213 19:07:08 @agent_ppo2.py:185][0m |           0.0073 |           7.2599 |        -129.9094 |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |           0.0119 |           7.0873 |        -129.7429 |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |          -0.0033 |           6.9755 |        -129.9769 |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |          -0.0005 |           6.9025 |        -129.9426 |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |           0.0089 |           6.9240 |        -129.4664 |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |           0.0013 |           6.9630 |        -129.4373 |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |          -0.0060 |           6.7501 |        -129.1150 |
[32m[20221213 19:07:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 19:07:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 72.40
[32m[20221213 19:07:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 74.00
[32m[20221213 19:07:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:07:09 @agent_ppo2.py:143][0m Total time:      18.61 min
[32m[20221213 19:07:09 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221213 19:07:09 @agent_ppo2.py:121][0m #------------------------ Iteration 993 --------------------------#
[32m[20221213 19:07:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:09 @agent_ppo2.py:185][0m |           0.0080 |         139.1546 |        -128.9842 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0116 |         131.3573 |        -128.7638 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0001 |         130.8121 |        -129.8151 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0085 |         157.7785 |        -130.2779 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |          -0.0006 |         130.0678 |        -130.6613 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0057 |         129.6363 |        -130.6694 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0002 |         129.3361 |        -131.1076 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0006 |         129.7787 |        -130.9828 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0025 |         129.2252 |        -131.3967 |
[32m[20221213 19:07:10 @agent_ppo2.py:185][0m |           0.0009 |         128.9499 |        -131.0261 |
[32m[20221213 19:07:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 19:07:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.80
[32m[20221213 19:07:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 921.00
[32m[20221213 19:07:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 937.00
[32m[20221213 19:07:10 @agent_ppo2.py:143][0m Total time:      18.63 min
[32m[20221213 19:07:10 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221213 19:07:10 @agent_ppo2.py:121][0m #------------------------ Iteration 994 --------------------------#
[32m[20221213 19:07:10 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |           0.0030 |          11.9021 |        -132.0272 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |           0.0218 |           6.2251 |        -132.5689 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |          -0.0002 |           5.9639 |        -132.8594 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |           0.0148 |           6.2458 |        -132.5675 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |           0.0126 |           5.8228 |        -131.6917 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |           0.0003 |           5.8698 |        -132.3154 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |          -0.0027 |           5.6946 |        -132.8082 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |           0.0026 |           5.6542 |        -132.3252 |
[32m[20221213 19:07:11 @agent_ppo2.py:185][0m |          -0.0010 |           5.6253 |        -132.2118 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |           0.0100 |           5.6124 |        -131.9425 |
[32m[20221213 19:07:12 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 19:07:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.20
[32m[20221213 19:07:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 77.00
[32m[20221213 19:07:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:07:12 @agent_ppo2.py:143][0m Total time:      18.65 min
[32m[20221213 19:07:12 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221213 19:07:12 @agent_ppo2.py:121][0m #------------------------ Iteration 995 --------------------------#
[32m[20221213 19:07:12 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |           0.0064 |          11.7644 |        -132.3077 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |           0.0050 |           6.5829 |        -132.0983 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |           0.0061 |           6.0160 |        -130.7143 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |          -0.0013 |           5.8925 |        -130.0528 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |          -0.0067 |           5.8697 |        -129.8542 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |          -0.0105 |           5.8336 |        -128.9623 |
[32m[20221213 19:07:12 @agent_ppo2.py:185][0m |           0.0040 |           5.8228 |        -128.3632 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |          -0.0064 |           5.8094 |        -127.4113 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |          -0.0108 |           5.7589 |        -127.3574 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |          -0.0121 |           5.7611 |        -127.4338 |
[32m[20221213 19:07:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 19:07:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.60
[32m[20221213 19:07:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 19:07:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 19:07:13 @agent_ppo2.py:143][0m Total time:      18.67 min
[32m[20221213 19:07:13 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221213 19:07:13 @agent_ppo2.py:121][0m #------------------------ Iteration 996 --------------------------#
[32m[20221213 19:07:13 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |           0.0114 |           8.8883 |        -125.2575 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |           0.0021 |           5.3188 |        -124.6406 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |          -0.0001 |           5.1425 |        -125.1762 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |           0.0052 |           5.2069 |        -125.4390 |
[32m[20221213 19:07:13 @agent_ppo2.py:185][0m |           0.0107 |           4.9941 |        -124.9651 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |           0.0014 |           4.9037 |        -124.5689 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |          -0.0007 |           4.8509 |        -125.1682 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |          -0.0025 |           4.8302 |        -125.4727 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |          -0.0064 |           4.7575 |        -125.3681 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |           0.0113 |           4.7221 |        -125.1629 |
[32m[20221213 19:07:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:07:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.40
[32m[20221213 19:07:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 19:07:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 19:07:14 @agent_ppo2.py:143][0m Total time:      18.69 min
[32m[20221213 19:07:14 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221213 19:07:14 @agent_ppo2.py:121][0m #------------------------ Iteration 997 --------------------------#
[32m[20221213 19:07:14 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |           0.0059 |           6.7766 |        -125.1119 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |           0.0041 |           5.8434 |        -125.1179 |
[32m[20221213 19:07:14 @agent_ppo2.py:185][0m |          -0.0007 |           5.7246 |        -124.3812 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |           0.0033 |           5.7516 |        -123.7407 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |          -0.0013 |           5.7376 |        -122.6768 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |          -0.0015 |           5.7475 |        -122.7342 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |          -0.0048 |           5.7293 |        -122.2214 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |          -0.0035 |           5.6845 |        -121.8559 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |          -0.0051 |           5.6821 |        -121.4866 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |           0.0018 |           5.6740 |        -121.2998 |
[32m[20221213 19:07:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.40
[32m[20221213 19:07:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:07:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.00
[32m[20221213 19:07:15 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221213 19:07:15 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221213 19:07:15 @agent_ppo2.py:121][0m #------------------------ Iteration 998 --------------------------#
[32m[20221213 19:07:15 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |           0.0027 |           6.5936 |        -117.7581 |
[32m[20221213 19:07:15 @agent_ppo2.py:185][0m |          -0.0067 |           4.6584 |        -116.9564 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |          -0.0023 |           4.5862 |        -116.8341 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |           0.0023 |           4.5467 |        -114.6284 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |           0.0030 |           4.5409 |        -114.2699 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |          -0.0075 |           4.4903 |        -115.2606 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |          -0.0056 |           4.4589 |        -114.9584 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |          -0.0022 |           4.4656 |        -114.8122 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |          -0.0082 |           4.4676 |        -114.5087 |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |          -0.0028 |           4.4227 |        -114.8846 |
[32m[20221213 19:07:16 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.80
[32m[20221213 19:07:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 19:07:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:07:16 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221213 19:07:16 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221213 19:07:16 @agent_ppo2.py:121][0m #------------------------ Iteration 999 --------------------------#
[32m[20221213 19:07:16 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:16 @agent_ppo2.py:185][0m |           0.0042 |         188.8162 |        -115.2028 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |           0.0017 |         186.3446 |        -115.2883 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |          -0.0021 |         185.7334 |        -115.7767 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |          -0.0018 |         185.4674 |        -115.6941 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |           0.0040 |         185.6515 |        -114.5932 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |           0.0043 |         185.5605 |        -115.6663 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |           0.0015 |         186.9256 |        -116.9869 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |          -0.0036 |         185.4002 |        -116.5402 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |           0.0005 |         185.7660 |        -117.6264 |
[32m[20221213 19:07:17 @agent_ppo2.py:185][0m |          -0.0009 |         185.4132 |        -117.3750 |
[32m[20221213 19:07:17 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:07:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.40
[32m[20221213 19:07:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.00
[32m[20221213 19:07:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.00
[32m[20221213 19:07:17 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 1000.00
[32m[20221213 19:07:17 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221213 19:07:17 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221213 19:07:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221213 19:07:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:07:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0014 |         111.5563 |        -117.9665 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |           0.0098 |         111.1479 |        -116.2200 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0076 |         103.3050 |        -116.2364 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0069 |         103.4542 |        -115.7247 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0090 |         103.0401 |        -114.9831 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0050 |         102.9324 |        -114.3777 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0044 |         103.1474 |        -113.4852 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0078 |         102.6841 |        -112.7228 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0117 |         102.3849 |        -112.5969 |
[32m[20221213 19:07:18 @agent_ppo2.py:185][0m |          -0.0014 |         102.6133 |        -112.3900 |
[32m[20221213 19:07:18 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.00
[32m[20221213 19:07:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.00
[32m[20221213 19:07:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 19:07:18 @agent_ppo2.py:143][0m Total time:      18.77 min
[32m[20221213 19:07:18 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221213 19:07:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221213 19:07:18 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0075 |         120.2805 |        -109.4247 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0110 |         119.1657 |        -108.8731 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0088 |         118.1246 |        -108.8254 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0067 |         119.7483 |        -108.4875 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0117 |         118.4240 |        -108.5049 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0106 |         117.1235 |        -108.3662 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0049 |         122.2543 |        -108.0523 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0054 |         116.8955 |        -108.0789 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0105 |         116.7864 |        -108.2487 |
[32m[20221213 19:07:19 @agent_ppo2.py:185][0m |          -0.0130 |         116.6692 |        -108.4764 |
[32m[20221213 19:07:19 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.00
[32m[20221213 19:07:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.00
[32m[20221213 19:07:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 19:07:19 @agent_ppo2.py:143][0m Total time:      18.78 min
[32m[20221213 19:07:19 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221213 19:07:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221213 19:07:20 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0005 |         187.6964 |        -107.8851 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0033 |         183.8871 |        -107.3820 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0077 |         183.6703 |        -108.1623 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0003 |         183.1180 |        -108.5684 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0026 |         186.7027 |        -108.3520 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |          -0.0002 |         183.3793 |        -108.4368 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0114 |         198.5684 |        -108.0790 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0041 |         182.5022 |        -107.7497 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0092 |         182.7825 |        -106.2521 |
[32m[20221213 19:07:20 @agent_ppo2.py:185][0m |           0.0018 |         181.4451 |        -108.1415 |
[32m[20221213 19:07:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.80
[32m[20221213 19:07:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 19:07:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.00
[32m[20221213 19:07:21 @agent_ppo2.py:143][0m Total time:      18.80 min
[32m[20221213 19:07:21 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221213 19:07:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221213 19:07:21 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0049 |           7.8988 |        -108.7925 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0090 |           5.0954 |        -108.4063 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0030 |           4.9093 |        -109.8660 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0024 |           4.8115 |        -110.0125 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0076 |           4.7685 |        -110.6736 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0028 |           4.7148 |        -111.2668 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |          -0.0017 |           4.7011 |        -111.3472 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |          -0.0001 |           4.6763 |        -111.7242 |
[32m[20221213 19:07:21 @agent_ppo2.py:185][0m |           0.0024 |           4.6529 |        -111.1686 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0034 |           4.6594 |        -110.6898 |
[32m[20221213 19:07:22 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.60
[32m[20221213 19:07:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.00
[32m[20221213 19:07:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:07:22 @agent_ppo2.py:143][0m Total time:      18.82 min
[32m[20221213 19:07:22 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221213 19:07:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221213 19:07:22 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |           0.0099 |          91.6516 |        -113.5590 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0043 |          83.5743 |        -113.0130 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0065 |          83.3709 |        -113.3013 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0085 |          82.1868 |        -113.5077 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0055 |          81.7510 |        -114.6107 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0012 |          80.9432 |        -115.1330 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0060 |          81.1600 |        -115.0729 |
[32m[20221213 19:07:22 @agent_ppo2.py:185][0m |          -0.0075 |          81.1042 |        -115.2666 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |          -0.0048 |          80.7009 |        -114.7313 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |          -0.0064 |          80.9362 |        -115.1796 |
[32m[20221213 19:07:23 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.20
[32m[20221213 19:07:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.00
[32m[20221213 19:07:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 19:07:23 @agent_ppo2.py:143][0m Total time:      18.84 min
[32m[20221213 19:07:23 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221213 19:07:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221213 19:07:23 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |           0.0075 |           5.1744 |        -118.1542 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |           0.0038 |           4.4995 |        -117.2899 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |           0.0011 |           4.4283 |        -117.0298 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |          -0.0001 |           4.3995 |        -116.5574 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |           0.0012 |           4.3912 |        -116.5570 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |           0.0089 |           4.3728 |        -116.3515 |
[32m[20221213 19:07:23 @agent_ppo2.py:185][0m |           0.0012 |           4.3362 |        -116.8824 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |           0.0023 |           4.3562 |        -116.8962 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |           0.0025 |           4.3353 |        -116.6275 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |          -0.0003 |           4.3356 |        -116.0798 |
[32m[20221213 19:07:24 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.00
[32m[20221213 19:07:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:07:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.00
[32m[20221213 19:07:24 @agent_ppo2.py:143][0m Total time:      18.86 min
[32m[20221213 19:07:24 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221213 19:07:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221213 19:07:24 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |           0.0142 |         201.9638 |        -115.6210 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |           0.0033 |         188.9618 |        -114.3412 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |           0.0022 |         186.4265 |        -116.2143 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |           0.0023 |         185.5479 |        -115.2357 |
[32m[20221213 19:07:24 @agent_ppo2.py:185][0m |          -0.0018 |         184.7056 |        -116.7399 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |          -0.0048 |         184.6284 |        -117.1105 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |          -0.0014 |         183.6647 |        -117.3982 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |           0.0007 |         183.3619 |        -115.9451 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |          -0.0042 |         184.1937 |        -116.2382 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |          -0.0024 |         183.4357 |        -116.6218 |
[32m[20221213 19:07:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.00
[32m[20221213 19:07:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.00
[32m[20221213 19:07:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.00
[32m[20221213 19:07:25 @agent_ppo2.py:143][0m Total time:      18.88 min
[32m[20221213 19:07:25 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221213 19:07:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221213 19:07:25 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |           0.0092 |          13.8153 |        -118.8665 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |           0.0081 |           5.4609 |        -119.1488 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |          -0.0031 |           4.7196 |        -118.6889 |
[32m[20221213 19:07:25 @agent_ppo2.py:185][0m |          -0.0017 |           4.5058 |        -119.5156 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |           0.0012 |           4.3211 |        -119.3799 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |           0.0021 |           4.2203 |        -120.0875 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |          -0.0068 |           4.1746 |        -120.2521 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |           0.0041 |           4.1572 |        -120.4167 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |          -0.0059 |           4.0504 |        -120.9227 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |           0.0013 |           3.9994 |        -120.9620 |
[32m[20221213 19:07:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.80
[32m[20221213 19:07:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.00
[32m[20221213 19:07:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.00
[32m[20221213 19:07:26 @agent_ppo2.py:143][0m Total time:      18.89 min
[32m[20221213 19:07:26 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221213 19:07:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221213 19:07:26 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |           0.0011 |           6.9353 |        -120.9100 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |          -0.0009 |           4.7951 |        -120.0072 |
[32m[20221213 19:07:26 @agent_ppo2.py:185][0m |           0.0065 |           4.6476 |        -118.7394 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |           0.0022 |           4.5401 |        -118.2385 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |          -0.0032 |           4.5152 |        -117.7847 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |           0.0017 |           4.4818 |        -117.0847 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |          -0.0046 |           4.4692 |        -117.1104 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |          -0.0034 |           4.4615 |        -116.6963 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |          -0.0051 |           4.4468 |        -116.1766 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |           0.0004 |           4.5028 |        -115.8498 |
[32m[20221213 19:07:27 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.20
[32m[20221213 19:07:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 19:07:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.00
[32m[20221213 19:07:27 @agent_ppo2.py:143][0m Total time:      18.91 min
[32m[20221213 19:07:27 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221213 19:07:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221213 19:07:27 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |           0.0125 |         134.3730 |        -112.3330 |
[32m[20221213 19:07:27 @agent_ppo2.py:185][0m |           0.0006 |         128.3358 |        -112.7553 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |           0.0031 |         127.7448 |        -113.0702 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0054 |         127.6821 |        -113.4403 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0045 |         127.4239 |        -114.6887 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0027 |         127.3776 |        -114.8635 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0049 |         127.4101 |        -114.9273 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0036 |         126.7201 |        -114.9311 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0020 |         126.6447 |        -114.6208 |
[32m[20221213 19:07:28 @agent_ppo2.py:185][0m |          -0.0021 |         127.4264 |        -115.7743 |
[32m[20221213 19:07:28 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 19:07:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 967.60
[32m[20221213 19:07:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 983.00
[32m[20221213 19:07:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 19:07:28 @agent_ppo2.py:143][0m Total time:      18.93 min
[32m[20221213 19:07:28 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221213 19:07:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221213 19:07:28 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |           0.0020 |         145.5108 |        -119.8718 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |          -0.0019 |         141.3813 |        -119.4031 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |          -0.0025 |         140.9628 |        -120.1806 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |           0.0052 |         139.9343 |        -118.3567 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |           0.0001 |         139.2562 |        -118.7606 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |           0.0043 |         139.1641 |        -118.8753 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |           0.0053 |         143.5347 |        -118.9527 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |          -0.0054 |         138.3366 |        -118.6340 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |          -0.0061 |         138.6903 |        -119.4221 |
[32m[20221213 19:07:29 @agent_ppo2.py:185][0m |          -0.0105 |         138.1111 |        -118.9012 |
[32m[20221213 19:07:29 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.60
[32m[20221213 19:07:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 19:07:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.00
[32m[20221213 19:07:29 @agent_ppo2.py:143][0m Total time:      18.95 min
[32m[20221213 19:07:29 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221213 19:07:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221213 19:07:29 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0201 |           6.6607 |        -118.6384 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0034 |           3.2039 |        -119.2219 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0064 |           3.0824 |        -118.7866 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0078 |           3.0195 |        -117.9625 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0015 |           2.9822 |        -117.5879 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0062 |           2.9650 |        -116.7524 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |          -0.0058 |           2.9504 |        -115.7798 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0070 |           2.9287 |        -115.8195 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0013 |           2.9262 |        -114.9572 |
[32m[20221213 19:07:30 @agent_ppo2.py:185][0m |           0.0057 |           2.9034 |        -115.4623 |
[32m[20221213 19:07:30 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.60
[32m[20221213 19:07:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 19:07:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:07:30 @agent_ppo2.py:143][0m Total time:      18.97 min
[32m[20221213 19:07:30 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221213 19:07:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221213 19:07:31 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |          -0.0008 |         169.0727 |        -110.9334 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |           0.0067 |         162.5053 |        -109.7045 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |          -0.0035 |         161.6758 |        -110.8607 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |           0.0079 |         164.0273 |        -110.3498 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |           0.0137 |         184.5829 |        -110.4665 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |           0.0012 |         161.7252 |        -110.8705 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |           0.0002 |         160.6153 |        -110.9604 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |           0.0006 |         160.4158 |        -109.8832 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |          -0.0006 |         160.4160 |        -111.0200 |
[32m[20221213 19:07:31 @agent_ppo2.py:185][0m |          -0.0019 |         160.7066 |        -110.6427 |
[32m[20221213 19:07:31 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.60
[32m[20221213 19:07:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.00
[32m[20221213 19:07:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:07:32 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221213 19:07:32 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221213 19:07:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221213 19:07:32 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |           0.0087 |          12.5614 |        -111.3926 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |           0.0068 |           7.4536 |        -110.4485 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |           0.0009 |           7.0542 |        -109.2922 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |           0.0024 |           7.0538 |        -108.5754 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |           0.0031 |           6.7760 |        -107.7325 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |          -0.0053 |           6.6469 |        -107.3669 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |           0.0011 |           6.6546 |        -107.0108 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |          -0.0047 |           6.5720 |        -106.2686 |
[32m[20221213 19:07:32 @agent_ppo2.py:185][0m |          -0.0030 |           6.4652 |        -105.7461 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |           0.0049 |           6.4592 |        -106.2658 |
[32m[20221213 19:07:33 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.00
[32m[20221213 19:07:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:07:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 19:07:33 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221213 19:07:33 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221213 19:07:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221213 19:07:33 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |           0.0060 |         172.1568 |        -104.2097 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |           0.0019 |         168.0483 |        -103.9359 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |           0.0005 |         167.8668 |        -104.2568 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |          -0.0005 |         167.3808 |        -104.6036 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |           0.0003 |         168.4970 |        -104.1465 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |          -0.0051 |         168.2249 |        -104.0133 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |           0.0009 |         169.8825 |        -104.6798 |
[32m[20221213 19:07:33 @agent_ppo2.py:185][0m |          -0.0026 |         167.0290 |        -103.8605 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |          -0.0024 |         167.4871 |        -104.7797 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0016 |         168.2042 |        -104.0654 |
[32m[20221213 19:07:34 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.60
[32m[20221213 19:07:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 19:07:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 19:07:34 @agent_ppo2.py:143][0m Total time:      19.02 min
[32m[20221213 19:07:34 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221213 19:07:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221213 19:07:34 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0031 |           5.6901 |        -105.3948 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0027 |           3.1755 |        -104.3410 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0004 |           3.0785 |        -104.4902 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0009 |           3.0881 |        -104.2840 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0041 |           3.0162 |        -104.1020 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0014 |           2.9994 |        -104.1730 |
[32m[20221213 19:07:34 @agent_ppo2.py:185][0m |           0.0015 |           2.9803 |        -104.0458 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |           0.0016 |           2.9816 |        -104.4172 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |          -0.0056 |           2.9648 |        -104.4553 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |           0.0018 |           2.9589 |        -103.5747 |
[32m[20221213 19:07:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:07:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.40
[32m[20221213 19:07:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 73.00
[32m[20221213 19:07:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.00
[32m[20221213 19:07:35 @agent_ppo2.py:143][0m Total time:      19.04 min
[32m[20221213 19:07:35 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221213 19:07:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221213 19:07:35 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |           0.0019 |         168.4896 |        -102.7259 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |           0.0031 |         163.5847 |        -102.2162 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |           0.0023 |         161.8488 |        -101.7682 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |          -0.0015 |         160.6848 |        -101.6560 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |           0.0048 |         160.3224 |        -101.1546 |
[32m[20221213 19:07:35 @agent_ppo2.py:185][0m |          -0.0016 |         160.1250 |        -101.8152 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |           0.0101 |         176.9744 |        -101.1062 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |          -0.0018 |         161.3188 |        -100.7370 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |          -0.0003 |         159.9617 |        -100.7195 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |           0.0004 |         159.2612 |        -100.6320 |
[32m[20221213 19:07:36 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 597.20
[32m[20221213 19:07:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.00
[32m[20221213 19:07:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.00
[32m[20221213 19:07:36 @agent_ppo2.py:143][0m Total time:      19.06 min
[32m[20221213 19:07:36 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221213 19:07:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221213 19:07:36 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |           0.0106 |          13.6162 |         -98.9273 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |           0.0102 |           6.2634 |         -98.6916 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |           0.0021 |           5.9151 |         -99.5125 |
[32m[20221213 19:07:36 @agent_ppo2.py:185][0m |           0.0070 |           5.7748 |         -99.6987 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |           0.0067 |           5.7063 |        -100.1540 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |           0.0038 |           5.6399 |        -100.1584 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |          -0.0006 |           5.6245 |        -100.4542 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |          -0.0025 |           5.5948 |        -100.4184 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |          -0.0024 |           5.5593 |        -100.6462 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |          -0.0025 |           5.6496 |        -100.5929 |
[32m[20221213 19:07:37 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:07:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.80
[32m[20221213 19:07:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.00
[32m[20221213 19:07:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:07:37 @agent_ppo2.py:143][0m Total time:      19.08 min
[32m[20221213 19:07:37 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221213 19:07:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221213 19:07:37 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |          -0.0005 |           9.0946 |        -104.0031 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |           0.0077 |           5.5988 |        -103.8231 |
[32m[20221213 19:07:37 @agent_ppo2.py:185][0m |          -0.0015 |           5.4374 |        -104.2411 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |           0.0050 |           5.4018 |        -103.3655 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |           0.0016 |           5.4001 |        -103.5173 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |          -0.0056 |           5.3052 |        -103.3909 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |          -0.0091 |           5.3007 |        -103.1541 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |          -0.0007 |           5.3103 |        -103.1080 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |          -0.0068 |           5.2667 |        -102.8166 |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |          -0.0101 |           5.2741 |        -103.3644 |
[32m[20221213 19:07:38 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:07:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.60
[32m[20221213 19:07:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.00
[32m[20221213 19:07:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 19:07:38 @agent_ppo2.py:143][0m Total time:      19.10 min
[32m[20221213 19:07:38 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221213 19:07:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221213 19:07:38 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:38 @agent_ppo2.py:185][0m |          -0.0019 |          20.1438 |        -100.2440 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |          -0.0086 |           5.8549 |         -99.7498 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |           0.0009 |           5.3652 |         -99.1296 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |           0.0051 |           5.0075 |         -99.0445 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |           0.0027 |           4.8581 |         -99.2613 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |           0.0024 |           4.7400 |         -98.9963 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |           0.0021 |           4.6712 |         -98.9778 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |          -0.0021 |           4.6588 |         -98.9826 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |           0.0031 |           4.4937 |         -99.0087 |
[32m[20221213 19:07:39 @agent_ppo2.py:185][0m |          -0.0059 |           4.4229 |         -99.5636 |
[32m[20221213 19:07:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:07:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.00
[32m[20221213 19:07:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.00
[32m[20221213 19:07:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.00
[32m[20221213 19:07:39 @agent_ppo2.py:143][0m Total time:      19.11 min
[32m[20221213 19:07:39 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221213 19:07:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221213 19:07:39 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |           0.0046 |           6.9801 |         -99.8909 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |          -0.0022 |           3.6195 |         -99.8680 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |           0.0035 |           3.4291 |        -100.5396 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |           0.0030 |           3.3597 |        -100.8779 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |           0.0060 |           3.3267 |        -100.8898 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |          -0.0006 |           3.2963 |        -100.7635 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |           0.0010 |           3.2715 |        -101.0268 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |           0.0006 |           3.2529 |        -100.4391 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |          -0.0049 |           3.2339 |        -101.4532 |
[32m[20221213 19:07:40 @agent_ppo2.py:185][0m |          -0.0010 |           3.2330 |        -101.1746 |
[32m[20221213 19:07:40 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.20
[32m[20221213 19:07:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.00
[32m[20221213 19:07:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.00
[32m[20221213 19:07:40 @agent_ppo2.py:143][0m Total time:      19.13 min
[32m[20221213 19:07:40 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221213 19:07:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221213 19:07:41 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |           0.0175 |           7.7710 |        -101.9855 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |           0.0250 |           6.1178 |        -102.2014 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |           0.0180 |           6.0858 |        -100.8906 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |           0.0207 |           6.0660 |         -98.8157 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |           0.0085 |           6.0459 |        -103.1221 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |          -0.0051 |           6.0597 |        -104.1459 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |          -0.0058 |           6.0810 |        -104.1526 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |           0.0011 |           6.0520 |        -104.0445 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |          -0.0000 |           6.0625 |        -105.0655 |
[32m[20221213 19:07:41 @agent_ppo2.py:185][0m |          -0.0036 |           6.0326 |        -104.9817 |
[32m[20221213 19:07:41 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.60
[32m[20221213 19:07:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:07:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.00
[32m[20221213 19:07:41 @agent_ppo2.py:143][0m Total time:      19.15 min
[32m[20221213 19:07:41 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221213 19:07:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221213 19:07:42 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |           0.0132 |           5.5580 |        -107.1932 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |           0.0092 |           3.2411 |        -106.4709 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |           0.0054 |           3.1514 |        -106.4997 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |           0.0014 |           3.1799 |        -107.1638 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |           0.0051 |           3.0786 |        -106.7022 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |           0.0006 |           3.0474 |        -106.5064 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |          -0.0025 |           3.0418 |        -105.9338 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |          -0.0033 |           3.0118 |        -105.4484 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |          -0.0022 |           3.0000 |        -104.5686 |
[32m[20221213 19:07:42 @agent_ppo2.py:185][0m |          -0.0030 |           2.9847 |        -105.0321 |
[32m[20221213 19:07:42 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.40
[32m[20221213 19:07:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:07:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 19:07:43 @agent_ppo2.py:143][0m Total time:      19.17 min
[32m[20221213 19:07:43 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221213 19:07:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221213 19:07:43 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |           0.0121 |           7.7576 |        -104.4038 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |           0.0020 |           6.1277 |        -103.0510 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |          -0.0012 |           6.0422 |        -104.2971 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |          -0.0028 |           5.9637 |        -103.5566 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |          -0.0057 |           5.9215 |        -103.5373 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |          -0.0035 |           5.9049 |        -102.7677 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |          -0.0088 |           5.8927 |        -102.9552 |
[32m[20221213 19:07:43 @agent_ppo2.py:185][0m |           0.0007 |           6.0338 |        -102.9422 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |           0.0015 |           5.9384 |        -103.3840 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |          -0.0073 |           5.8478 |        -102.9616 |
[32m[20221213 19:07:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:07:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.60
[32m[20221213 19:07:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.00
[32m[20221213 19:07:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 884.00
[32m[20221213 19:07:44 @agent_ppo2.py:143][0m Total time:      19.19 min
[32m[20221213 19:07:44 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221213 19:07:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221213 19:07:44 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |           0.0012 |           6.1084 |        -101.7919 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |           0.0007 |           4.9233 |        -101.4650 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |           0.0092 |           4.9460 |        -100.8642 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |           0.0055 |           4.8110 |        -101.6947 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |           0.0020 |           4.8075 |        -101.8949 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |          -0.0027 |           4.7786 |        -101.6611 |
[32m[20221213 19:07:44 @agent_ppo2.py:185][0m |          -0.0038 |           4.7950 |        -101.9204 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0004 |           4.7781 |        -101.2361 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |           0.0027 |           4.7672 |        -101.1269 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |           0.0038 |           4.8998 |        -101.0696 |
[32m[20221213 19:07:45 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.40
[32m[20221213 19:07:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.00
[32m[20221213 19:07:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 19:07:45 @agent_ppo2.py:143][0m Total time:      19.21 min
[32m[20221213 19:07:45 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221213 19:07:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221213 19:07:45 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0076 |          72.6967 |        -102.8835 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0053 |          70.6510 |        -102.3795 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0054 |          69.2197 |        -102.2715 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0099 |          69.8365 |        -102.6854 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0070 |          68.2670 |        -102.6475 |
[32m[20221213 19:07:45 @agent_ppo2.py:185][0m |          -0.0085 |          68.8057 |        -102.2009 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |          -0.0044 |          69.0094 |        -102.6316 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |          -0.0104 |          68.6752 |        -102.1804 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |           0.0005 |          69.3232 |        -102.0999 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |          -0.0066 |          69.4548 |        -102.1696 |
[32m[20221213 19:07:46 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.20
[32m[20221213 19:07:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.00
[32m[20221213 19:07:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.00
[32m[20221213 19:07:46 @agent_ppo2.py:143][0m Total time:      19.22 min
[32m[20221213 19:07:46 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221213 19:07:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221213 19:07:46 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |           0.0032 |         148.7297 |        -102.9278 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |           0.0079 |         151.1825 |        -104.7146 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |           0.0031 |         143.9009 |        -104.9484 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |          -0.0045 |         141.8699 |        -105.4636 |
[32m[20221213 19:07:46 @agent_ppo2.py:185][0m |          -0.0018 |         142.0374 |        -104.9242 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |          -0.0003 |         141.8691 |        -104.7854 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |           0.0097 |         141.6732 |        -103.7750 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |           0.0004 |         145.1438 |        -104.9308 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |          -0.0013 |         141.2900 |        -104.7493 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |          -0.0022 |         141.0848 |        -104.7009 |
[32m[20221213 19:07:47 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.20
[32m[20221213 19:07:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 19:07:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 19:07:47 @agent_ppo2.py:143][0m Total time:      19.24 min
[32m[20221213 19:07:47 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221213 19:07:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221213 19:07:47 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |           0.0076 |         142.4945 |        -105.5075 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |          -0.0010 |         138.0748 |        -107.2985 |
[32m[20221213 19:07:47 @agent_ppo2.py:185][0m |          -0.0018 |         137.4698 |        -108.5518 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |           0.0027 |         137.0257 |        -108.5301 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |          -0.0046 |         136.8756 |        -109.0309 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |          -0.0042 |         137.2349 |        -110.0166 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |           0.0008 |         136.9946 |        -110.1014 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |          -0.0027 |         136.8254 |        -110.0243 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |          -0.0050 |         137.0915 |        -110.7671 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |           0.0014 |         136.6377 |        -111.6537 |
[32m[20221213 19:07:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:07:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.20
[32m[20221213 19:07:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 19:07:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 595.00
[32m[20221213 19:07:48 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221213 19:07:48 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221213 19:07:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221213 19:07:48 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |           0.0030 |         110.5856 |        -115.5541 |
[32m[20221213 19:07:48 @agent_ppo2.py:185][0m |           0.0002 |         105.7362 |        -115.6714 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0054 |         104.5448 |        -116.6746 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0004 |         104.0397 |        -115.9502 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0032 |         103.5838 |        -116.5386 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0053 |         103.5139 |        -117.1551 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0042 |         103.4118 |        -116.8282 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0059 |         103.0216 |        -116.4070 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0017 |         103.1873 |        -114.7847 |
[32m[20221213 19:07:49 @agent_ppo2.py:185][0m |          -0.0010 |         103.1353 |        -115.5398 |
[32m[20221213 19:07:49 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:07:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.40
[32m[20221213 19:07:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.00
[32m[20221213 19:07:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 19:07:49 @agent_ppo2.py:143][0m Total time:      19.28 min
[32m[20221213 19:07:49 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221213 19:07:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221213 19:07:49 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0105 |           5.0225 |        -116.3540 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0217 |           2.7808 |        -116.9321 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0040 |           2.6482 |        -117.5608 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |          -0.0031 |           2.5860 |        -117.7357 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0005 |           2.5514 |        -117.3885 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0108 |           2.5257 |        -117.6122 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0030 |           2.5028 |        -117.9362 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |          -0.0011 |           2.4861 |        -117.2814 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |          -0.0065 |           2.4789 |        -117.8202 |
[32m[20221213 19:07:50 @agent_ppo2.py:185][0m |           0.0034 |           2.4579 |        -117.6174 |
[32m[20221213 19:07:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 19:07:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.80
[32m[20221213 19:07:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.00
[32m[20221213 19:07:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 19:07:50 @agent_ppo2.py:143][0m Total time:      19.30 min
[32m[20221213 19:07:50 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221213 19:07:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221213 19:07:51 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 19:07:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |           0.0052 |           4.6576 |        -116.0085 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |           0.0096 |           3.9606 |        -113.8090 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |           0.0162 |           3.8920 |        -113.2383 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |          -0.0005 |           3.8327 |        -113.8685 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |           0.0074 |           3.8540 |        -113.4206 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |           0.0088 |           3.8131 |        -111.3318 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |          -0.0057 |           3.7978 |        -111.9020 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |           0.0006 |           3.7949 |        -111.6472 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |          -0.0022 |           3.8308 |        -112.4042 |
[32m[20221213 19:07:51 @agent_ppo2.py:185][0m |          -0.0045 |           3.8197 |        -111.4914 |
[32m[20221213 19:07:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 19:07:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.80
[32m[20221213 19:07:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 67.00
[32m[20221213 19:07:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.00
[32m[20221213 19:07:52 @agent_ppo2.py:143][0m Total time:      19.32 min
[32m[20221213 19:07:52 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221213 19:07:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221213 19:07:52 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |           0.0039 |           3.3234 |        -114.5624 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |           0.0094 |           2.7028 |        -113.2895 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |          -0.0047 |           2.6658 |        -112.6559 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |          -0.0032 |           2.6554 |        -111.5688 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |           0.0030 |           2.6524 |        -111.1666 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |           0.0076 |           2.6367 |        -109.8528 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |          -0.0030 |           2.6531 |        -109.9707 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |           0.0019 |           2.6313 |        -109.0095 |
[32m[20221213 19:07:52 @agent_ppo2.py:185][0m |           0.0020 |           2.6242 |        -108.3716 |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |           0.0017 |           2.6352 |        -108.1314 |
[32m[20221213 19:07:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 19:07:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.40
[32m[20221213 19:07:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.00
[32m[20221213 19:07:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.00
[32m[20221213 19:07:53 @agent_ppo2.py:143][0m Total time:      19.34 min
[32m[20221213 19:07:53 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221213 19:07:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221213 19:07:53 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |           0.0140 |         113.5841 |        -102.8337 |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |          -0.0014 |         100.8351 |        -103.7394 |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |          -0.0002 |          99.7098 |        -104.3830 |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |           0.0019 |         100.2990 |        -104.4878 |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |           0.0021 |          99.9340 |        -104.4839 |
[32m[20221213 19:07:53 @agent_ppo2.py:185][0m |           0.0007 |          99.1223 |        -105.2802 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0079 |         102.0333 |        -106.3366 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0028 |          98.7497 |        -106.5660 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |          -0.0006 |          98.9903 |        -106.5986 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |          -0.0005 |          98.7653 |        -107.4081 |
[32m[20221213 19:07:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 19:07:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.00
[32m[20221213 19:07:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.00
[32m[20221213 19:07:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:07:54 @agent_ppo2.py:143][0m Total time:      19.36 min
[32m[20221213 19:07:54 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221213 19:07:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221213 19:07:54 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0073 |           4.0243 |        -109.3434 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0076 |           3.4906 |        -108.8183 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0034 |           3.4173 |        -108.1979 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0021 |           3.3852 |        -107.8994 |
[32m[20221213 19:07:54 @agent_ppo2.py:185][0m |           0.0045 |           3.3705 |        -108.1838 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |          -0.0023 |           3.3627 |        -107.8863 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |           0.0039 |           3.3506 |        -108.0556 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |           0.0044 |           3.3501 |        -108.6168 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |           0.0019 |           3.3262 |        -108.7976 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |           0.0069 |           3.3234 |        -108.4454 |
[32m[20221213 19:07:55 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 19:07:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.60
[32m[20221213 19:07:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.00
[32m[20221213 19:07:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.00
[32m[20221213 19:07:55 @agent_ppo2.py:143][0m Total time:      19.38 min
[32m[20221213 19:07:55 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221213 19:07:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221213 19:07:55 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 19:07:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |           0.0028 |           5.4858 |        -110.1142 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |          -0.0009 |           3.2917 |        -110.1283 |
[32m[20221213 19:07:55 @agent_ppo2.py:185][0m |           0.0031 |           3.1597 |        -109.9243 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0085 |           3.0871 |        -110.1631 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0085 |           3.0372 |        -109.8434 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0088 |           2.9998 |        -109.9111 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0111 |           2.9888 |        -109.8356 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0054 |           2.9502 |        -109.1778 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0053 |           2.9235 |        -109.3261 |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |          -0.0035 |           2.9168 |        -109.2774 |
[32m[20221213 19:07:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 19:07:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.40
[32m[20221213 19:07:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.00
[32m[20221213 19:07:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.00
[32m[20221213 19:07:56 @agent_ppo2.py:143][0m Total time:      19.40 min
[32m[20221213 19:07:56 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221213 19:07:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221213 19:07:56 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:56 @agent_ppo2.py:185][0m |           0.0027 |           3.8026 |        -107.3860 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0035 |           3.4657 |        -106.1969 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0031 |           3.4413 |        -105.9173 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0005 |           3.4212 |        -104.9372 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0015 |           3.4147 |        -105.1082 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0009 |           3.4062 |        -104.1753 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0045 |           3.4107 |        -103.9193 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0008 |           3.3934 |        -103.6881 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |           0.0016 |           3.4128 |        -103.4916 |
[32m[20221213 19:07:57 @agent_ppo2.py:185][0m |          -0.0018 |           3.3928 |        -103.0131 |
[32m[20221213 19:07:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.80
[32m[20221213 19:07:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.00
[32m[20221213 19:07:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1000.00
[32m[20221213 19:07:57 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221213 19:07:57 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221213 19:07:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221213 19:07:57 @agent_ppo2.py:127][0m Sampling time: 0.11 s by 5 slaves
[32m[20221213 19:07:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0050 |           5.1390 |        -100.6393 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0085 |           3.4262 |        -100.4000 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0002 |           3.3770 |        -100.1207 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |          -0.0020 |           3.3561 |         -99.9038 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0083 |           3.3408 |         -98.8698 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |          -0.0041 |           3.3311 |         -98.2667 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0008 |           3.3167 |         -98.0306 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0080 |           3.3225 |         -96.9625 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |           0.0006 |           3.3076 |         -97.0079 |
[32m[20221213 19:07:58 @agent_ppo2.py:185][0m |          -0.0054 |           3.3037 |         -96.3192 |
[32m[20221213 19:07:58 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 19:07:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.60
[32m[20221213 19:07:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.00
[32m[20221213 19:07:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.00
[32m[20221213 19:07:58 @agent_ppo2.py:143][0m Total time:      19.43 min
[32m[20221213 19:07:58 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221213 19:07:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221213 19:07:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 19:07:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |           0.0083 |           4.4464 |         -94.7200 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |           0.0004 |           3.0435 |         -94.1174 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |          -0.0053 |           2.9773 |         -93.6565 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |           0.0024 |           2.9671 |         -93.0824 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |          -0.0022 |           2.8966 |         -93.2007 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |          -0.0011 |           2.8696 |         -92.6934 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |           0.0022 |           2.8475 |         -92.5340 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |          -0.0103 |           2.8291 |         -92.2667 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |          -0.0026 |           2.8111 |         -92.0427 |
[32m[20221213 19:07:59 @agent_ppo2.py:185][0m |          -0.0017 |           2.8008 |         -92.0803 |
[32m[20221213 19:07:59 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 19:07:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.00
[32m[20221213 19:07:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 43.00
[32m[20221213 19:07:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.00
[32m[20221213 19:07:59 @agent_ppo2.py:143][0m Total time:      19.45 min
[32m[20221213 19:07:59 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221213 19:07:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1038 --------------------------#
