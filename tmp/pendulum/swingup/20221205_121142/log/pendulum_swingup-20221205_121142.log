[32m[20221205 12:11:42 @logger.py:105][0m Log file set to ./tmp/pendulum/swingup/20221205_121142/log/pendulum_swingup-20221205_121142.log
[32m[20221205 12:11:42 @agent_ppo2.py:117][0m #------------------------ Iteration 0 --------------------------#
[32m[20221205 12:11:42 @agent_ppo2.py:123][0m Sampling time: 0.23 s by 1 slaves
[32m[20221205 12:11:42 @agent_ppo2.py:157][0m |      policy_loss |       value_loss |          entropy |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |           0.0002 |           0.0845 |          21.1277 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0030 |           0.0671 |          21.1133 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0029 |           0.0552 |          21.1026 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0025 |           0.0469 |          21.0969 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0031 |           0.0409 |          21.0870 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0034 |           0.0364 |          21.0855 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0030 |           0.0329 |          21.0795 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0027 |           0.0301 |          21.0603 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0047 |           0.0276 |          21.0775 |
[32m[20221205 12:11:42 @agent_ppo2.py:181][0m |          -0.0043 |           0.0254 |          21.0634 |
[32m[20221205 12:11:42 @agent_ppo2.py:126][0m Policy update time: 0.67 s
[32m[20221205 12:11:43 @agent_ppo2.py:134][0m Average TRAINING episode reward: 0.00
[32m[20221205 12:11:43 @agent_ppo2.py:135][0m Maximum TRAINING episode reward: 0.00
[32m[20221205 12:11:43 @agent_ppo2.py:136][0m Average EVALUATION episode reward: 0.00
[32m[20221205 12:11:43 @agent_ppo2.py:105][0m [4m[34mCRITICAL[0m Get the best episode reward: 0.00
[32m[20221205 12:11:43 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 0.00
[32m[20221205 12:11:43 @agent_ppo2.py:139][0m Total time:       0.02 min
[32m[20221205 12:11:43 @agent_ppo2.py:141][0m 2048 total steps have happened
[32m[20221205 12:11:43 @agent_ppo2.py:117][0m #------------------------ Iteration 1 --------------------------#
[32m[20221205 12:11:43 @agent_ppo2.py:123][0m Sampling time: 0.23 s by 1 slaves
[32m[20221205 12:11:43 @agent_ppo2.py:157][0m |      policy_loss |       value_loss |          entropy |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |           0.0002 |           0.0234 |          20.9663 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |          -0.0015 |           0.0216 |          20.9834 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |          -0.0017 |           0.0199 |          20.9711 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |           0.0000 |           0.0184 |          20.9657 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |          -0.0016 |           0.0169 |          20.9577 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |          -0.0012 |           0.0156 |          20.9613 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |          -0.0020 |           0.0143 |          20.9579 |
[32m[20221205 12:11:43 @agent_ppo2.py:181][0m |          -0.0027 |           0.0132 |          20.9556 |
[32m[20221205 12:11:44 @agent_ppo2.py:181][0m |          -0.0019 |           0.0121 |          20.9482 |
[32m[20221205 12:11:44 @agent_ppo2.py:181][0m |          -0.0020 |           0.0111 |          20.9415 |
[32m[20221205 12:11:44 @agent_ppo2.py:126][0m Policy update time: 0.65 s
