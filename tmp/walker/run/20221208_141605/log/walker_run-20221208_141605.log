[32m[20221208 14:16:05 @logger.py:105][0m Log file set to ./tmp/walker/run/20221208_141605/log/walker_run-20221208_141605.log
[32m[20221208 14:16:05 @agent_ppo2.py:115][0m #------------------------ Iteration 0 --------------------------#
[32m[20221208 14:16:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0037 |           0.1851 |           0.2305 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0163 |           0.0911 |           0.2297 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0189 |           0.0779 |           0.2301 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0216 |           0.0691 |           0.2297 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0223 |           0.0614 |           0.2293 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0231 |           0.0550 |           0.2290 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0255 |           0.0494 |           0.2285 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0259 |           0.0446 |           0.2282 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0267 |           0.0405 |           0.2280 |
[32m[20221208 14:16:06 @agent_ppo2.py:179][0m |          -0.0274 |           0.0369 |           0.2275 |
[32m[20221208 14:16:06 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:16:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.56
[32m[20221208 14:16:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 25.12
[32m[20221208 14:16:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.46
[32m[20221208 14:16:06 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 29.46
[32m[20221208 14:16:06 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 29.46
[32m[20221208 14:16:06 @agent_ppo2.py:137][0m Total time:       0.03 min
[32m[20221208 14:16:06 @agent_ppo2.py:139][0m 2048 total steps have happened
[32m[20221208 14:16:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1 --------------------------#
[32m[20221208 14:16:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:16:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |           0.0001 |           0.0589 |           0.2308 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0112 |           0.0375 |           0.2309 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0138 |           0.0349 |           0.2316 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0158 |           0.0328 |           0.2321 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0171 |           0.0311 |           0.2323 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0199 |           0.0299 |           0.2324 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0195 |           0.0286 |           0.2327 |
[32m[20221208 14:16:07 @agent_ppo2.py:179][0m |          -0.0218 |           0.0277 |           0.2330 |
[32m[20221208 14:16:08 @agent_ppo2.py:179][0m |          -0.0216 |           0.0266 |           0.2328 |
[32m[20221208 14:16:08 @agent_ppo2.py:179][0m |          -0.0241 |           0.0256 |           0.2335 |
[32m[20221208 14:16:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:16:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.86
[32m[20221208 14:16:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.72
[32m[20221208 14:16:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.22
[32m[20221208 14:16:08 @agent_ppo2.py:137][0m Total time:       0.05 min
[32m[20221208 14:16:08 @agent_ppo2.py:139][0m 4096 total steps have happened
[32m[20221208 14:16:08 @agent_ppo2.py:115][0m #------------------------ Iteration 2 --------------------------#
[32m[20221208 14:16:08 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |           0.0019 |           0.0577 |           0.2327 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0119 |           0.0375 |           0.2317 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0163 |           0.0363 |           0.2311 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0154 |           0.0356 |           0.2306 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0176 |           0.0350 |           0.2301 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0196 |           0.0347 |           0.2302 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0204 |           0.0343 |           0.2300 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0217 |           0.0335 |           0.2302 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0213 |           0.0332 |           0.2297 |
[32m[20221208 14:16:09 @agent_ppo2.py:179][0m |          -0.0240 |           0.0330 |           0.2293 |
[32m[20221208 14:16:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:16:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.31
[32m[20221208 14:16:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 27.96
[32m[20221208 14:16:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.77
[32m[20221208 14:16:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 31.77
[32m[20221208 14:16:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 31.77
[32m[20221208 14:16:10 @agent_ppo2.py:137][0m Total time:       0.08 min
[32m[20221208 14:16:10 @agent_ppo2.py:139][0m 6144 total steps have happened
[32m[20221208 14:16:10 @agent_ppo2.py:115][0m #------------------------ Iteration 3 --------------------------#
[32m[20221208 14:16:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |           0.0041 |           0.0524 |           0.2298 |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |          -0.0051 |           0.0402 |           0.2294 |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |          -0.0095 |           0.0394 |           0.2283 |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |          -0.0136 |           0.0389 |           0.2294 |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |          -0.0147 |           0.0393 |           0.2294 |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |          -0.0166 |           0.0389 |           0.2293 |
[32m[20221208 14:16:10 @agent_ppo2.py:179][0m |          -0.0180 |           0.0381 |           0.2294 |
[32m[20221208 14:16:11 @agent_ppo2.py:179][0m |          -0.0195 |           0.0379 |           0.2293 |
[32m[20221208 14:16:11 @agent_ppo2.py:179][0m |          -0.0207 |           0.0374 |           0.2296 |
[32m[20221208 14:16:11 @agent_ppo2.py:179][0m |          -0.0216 |           0.0370 |           0.2302 |
[32m[20221208 14:16:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:16:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.16
[32m[20221208 14:16:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.62
[32m[20221208 14:16:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.44
[32m[20221208 14:16:11 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 35.44
[32m[20221208 14:16:11 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 35.44
[32m[20221208 14:16:11 @agent_ppo2.py:137][0m Total time:       0.10 min
[32m[20221208 14:16:11 @agent_ppo2.py:139][0m 8192 total steps have happened
[32m[20221208 14:16:11 @agent_ppo2.py:115][0m #------------------------ Iteration 4 --------------------------#
[32m[20221208 14:16:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |           0.0015 |           0.0763 |           0.2369 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0110 |           0.0610 |           0.2359 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0141 |           0.0610 |           0.2356 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0167 |           0.0607 |           0.2359 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0192 |           0.0592 |           0.2358 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0190 |           0.0585 |           0.2355 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0218 |           0.0579 |           0.2352 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0212 |           0.0588 |           0.2352 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0223 |           0.0575 |           0.2354 |
[32m[20221208 14:16:12 @agent_ppo2.py:179][0m |          -0.0239 |           0.0571 |           0.2349 |
[32m[20221208 14:16:12 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:16:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 28.01
[32m[20221208 14:16:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.21
[32m[20221208 14:16:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.70
[32m[20221208 14:16:13 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 43.70
[32m[20221208 14:16:13 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 43.70
[32m[20221208 14:16:13 @agent_ppo2.py:137][0m Total time:       0.13 min
[32m[20221208 14:16:13 @agent_ppo2.py:139][0m 10240 total steps have happened
[32m[20221208 14:16:13 @agent_ppo2.py:115][0m #------------------------ Iteration 5 --------------------------#
[32m[20221208 14:16:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:13 @agent_ppo2.py:179][0m |           0.0006 |           0.0973 |           0.2381 |
[32m[20221208 14:16:13 @agent_ppo2.py:179][0m |          -0.0109 |           0.0840 |           0.2377 |
[32m[20221208 14:16:13 @agent_ppo2.py:179][0m |          -0.0155 |           0.0832 |           0.2375 |
[32m[20221208 14:16:13 @agent_ppo2.py:179][0m |          -0.0176 |           0.0819 |           0.2376 |
[32m[20221208 14:16:14 @agent_ppo2.py:179][0m |          -0.0191 |           0.0815 |           0.2370 |
[32m[20221208 14:16:14 @agent_ppo2.py:179][0m |          -0.0220 |           0.0809 |           0.2367 |
[32m[20221208 14:16:14 @agent_ppo2.py:179][0m |          -0.0237 |           0.0809 |           0.2368 |
[32m[20221208 14:16:14 @agent_ppo2.py:179][0m |          -0.0243 |           0.0817 |           0.2372 |
[32m[20221208 14:16:14 @agent_ppo2.py:179][0m |          -0.0255 |           0.0803 |           0.2366 |
[32m[20221208 14:16:14 @agent_ppo2.py:179][0m |          -0.0264 |           0.0798 |           0.2368 |
[32m[20221208 14:16:14 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:16:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 27.91
[32m[20221208 14:16:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 29.62
[32m[20221208 14:16:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.93
[32m[20221208 14:16:14 @agent_ppo2.py:137][0m Total time:       0.16 min
[32m[20221208 14:16:14 @agent_ppo2.py:139][0m 12288 total steps have happened
[32m[20221208 14:16:14 @agent_ppo2.py:115][0m #------------------------ Iteration 6 --------------------------#
[32m[20221208 14:16:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |           0.0022 |           0.0858 |           0.2406 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0105 |           0.0775 |           0.2409 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0140 |           0.0764 |           0.2417 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0166 |           0.0757 |           0.2421 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0170 |           0.0765 |           0.2427 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0174 |           0.0758 |           0.2429 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0206 |           0.0753 |           0.2435 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0216 |           0.0756 |           0.2440 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0215 |           0.0762 |           0.2434 |
[32m[20221208 14:16:15 @agent_ppo2.py:179][0m |          -0.0233 |           0.0752 |           0.2439 |
[32m[20221208 14:16:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:16:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 28.47
[32m[20221208 14:16:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.90
[32m[20221208 14:16:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 24.48
[32m[20221208 14:16:16 @agent_ppo2.py:137][0m Total time:       0.18 min
[32m[20221208 14:16:16 @agent_ppo2.py:139][0m 14336 total steps have happened
[32m[20221208 14:16:16 @agent_ppo2.py:115][0m #------------------------ Iteration 7 --------------------------#
[32m[20221208 14:16:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:16 @agent_ppo2.py:179][0m |           0.0005 |           0.1328 |           0.2422 |
[32m[20221208 14:16:16 @agent_ppo2.py:179][0m |          -0.0109 |           0.1170 |           0.2413 |
[32m[20221208 14:16:16 @agent_ppo2.py:179][0m |          -0.0169 |           0.1150 |           0.2412 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0174 |           0.1138 |           0.2419 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0209 |           0.1125 |           0.2409 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0207 |           0.1110 |           0.2419 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0229 |           0.1098 |           0.2424 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0261 |           0.1094 |           0.2425 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0254 |           0.1080 |           0.2422 |
[32m[20221208 14:16:17 @agent_ppo2.py:179][0m |          -0.0261 |           0.1078 |           0.2429 |
[32m[20221208 14:16:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:16:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.51
[32m[20221208 14:16:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 35.79
[32m[20221208 14:16:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.04
[32m[20221208 14:16:17 @agent_ppo2.py:137][0m Total time:       0.21 min
[32m[20221208 14:16:17 @agent_ppo2.py:139][0m 16384 total steps have happened
[32m[20221208 14:16:17 @agent_ppo2.py:115][0m #------------------------ Iteration 8 --------------------------#
[32m[20221208 14:16:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |           0.0006 |           0.1360 |           0.2479 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0135 |           0.1234 |           0.2491 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0164 |           0.1207 |           0.2493 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0189 |           0.1208 |           0.2497 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0211 |           0.1196 |           0.2506 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0226 |           0.1194 |           0.2501 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0222 |           0.1193 |           0.2512 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0245 |           0.1207 |           0.2511 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0259 |           0.1177 |           0.2516 |
[32m[20221208 14:16:18 @agent_ppo2.py:179][0m |          -0.0250 |           0.1178 |           0.2519 |
[32m[20221208 14:16:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:16:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.12
[32m[20221208 14:16:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 35.69
[32m[20221208 14:16:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.85
[32m[20221208 14:16:19 @agent_ppo2.py:137][0m Total time:       0.23 min
[32m[20221208 14:16:19 @agent_ppo2.py:139][0m 18432 total steps have happened
[32m[20221208 14:16:19 @agent_ppo2.py:115][0m #------------------------ Iteration 9 --------------------------#
[32m[20221208 14:16:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:19 @agent_ppo2.py:179][0m |          -0.0011 |           0.1540 |           0.2658 |
[32m[20221208 14:16:19 @agent_ppo2.py:179][0m |          -0.0130 |           0.1404 |           0.2646 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0178 |           0.1374 |           0.2649 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0213 |           0.1358 |           0.2651 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0232 |           0.1355 |           0.2657 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0233 |           0.1344 |           0.2661 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0260 |           0.1345 |           0.2655 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0263 |           0.1312 |           0.2662 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0283 |           0.1296 |           0.2664 |
[32m[20221208 14:16:20 @agent_ppo2.py:179][0m |          -0.0271 |           0.1307 |           0.2666 |
[32m[20221208 14:16:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.44
[32m[20221208 14:16:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 37.13
[32m[20221208 14:16:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.95
[32m[20221208 14:16:20 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 50.95
[32m[20221208 14:16:20 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 50.95
[32m[20221208 14:16:20 @agent_ppo2.py:137][0m Total time:       0.26 min
[32m[20221208 14:16:20 @agent_ppo2.py:139][0m 20480 total steps have happened
[32m[20221208 14:16:20 @agent_ppo2.py:115][0m #------------------------ Iteration 10 --------------------------#
[32m[20221208 14:16:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:16:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0029 |           0.2379 |           0.2626 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0137 |           0.2106 |           0.2625 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0178 |           0.2091 |           0.2623 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0202 |           0.2056 |           0.2628 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0216 |           0.2005 |           0.2627 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0244 |           0.1992 |           0.2628 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0263 |           0.1967 |           0.2627 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0257 |           0.1983 |           0.2634 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0282 |           0.1942 |           0.2633 |
[32m[20221208 14:16:21 @agent_ppo2.py:179][0m |          -0.0300 |           0.1954 |           0.2632 |
[32m[20221208 14:16:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.21
[32m[20221208 14:16:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.40
[32m[20221208 14:16:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.69
[32m[20221208 14:16:22 @agent_ppo2.py:137][0m Total time:       0.28 min
[32m[20221208 14:16:22 @agent_ppo2.py:139][0m 22528 total steps have happened
[32m[20221208 14:16:22 @agent_ppo2.py:115][0m #------------------------ Iteration 11 --------------------------#
[32m[20221208 14:16:22 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:22 @agent_ppo2.py:179][0m |          -0.0014 |           0.2017 |           0.2714 |
[32m[20221208 14:16:22 @agent_ppo2.py:179][0m |          -0.0144 |           0.1776 |           0.2691 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0172 |           0.1730 |           0.2695 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0201 |           0.1720 |           0.2690 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0209 |           0.1702 |           0.2687 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0224 |           0.1698 |           0.2685 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0233 |           0.1663 |           0.2684 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0254 |           0.1669 |           0.2681 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0258 |           0.1656 |           0.2683 |
[32m[20221208 14:16:23 @agent_ppo2.py:179][0m |          -0.0240 |           0.1665 |           0.2691 |
[32m[20221208 14:16:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.28
[32m[20221208 14:16:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.90
[32m[20221208 14:16:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.71
[32m[20221208 14:16:23 @agent_ppo2.py:137][0m Total time:       0.31 min
[32m[20221208 14:16:23 @agent_ppo2.py:139][0m 24576 total steps have happened
[32m[20221208 14:16:23 @agent_ppo2.py:115][0m #------------------------ Iteration 12 --------------------------#
[32m[20221208 14:16:24 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |           0.0011 |           0.2535 |           0.2734 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0097 |           0.2442 |           0.2728 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0151 |           0.2406 |           0.2729 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0183 |           0.2410 |           0.2733 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0211 |           0.2407 |           0.2731 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0223 |           0.2397 |           0.2726 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0239 |           0.2404 |           0.2723 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0244 |           0.2391 |           0.2726 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0235 |           0.2397 |           0.2723 |
[32m[20221208 14:16:24 @agent_ppo2.py:179][0m |          -0.0233 |           0.2395 |           0.2728 |
[32m[20221208 14:16:24 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.43
[32m[20221208 14:16:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.81
[32m[20221208 14:16:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.33
[32m[20221208 14:16:25 @agent_ppo2.py:137][0m Total time:       0.33 min
[32m[20221208 14:16:25 @agent_ppo2.py:139][0m 26624 total steps have happened
[32m[20221208 14:16:25 @agent_ppo2.py:115][0m #------------------------ Iteration 13 --------------------------#
[32m[20221208 14:16:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:25 @agent_ppo2.py:179][0m |          -0.0005 |           0.3611 |           0.2749 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0093 |           0.3354 |           0.2749 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0146 |           0.3303 |           0.2745 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0154 |           0.3281 |           0.2745 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0182 |           0.3230 |           0.2749 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0183 |           0.3212 |           0.2745 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0212 |           0.3163 |           0.2751 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0230 |           0.3144 |           0.2747 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0240 |           0.3095 |           0.2751 |
[32m[20221208 14:16:26 @agent_ppo2.py:179][0m |          -0.0241 |           0.3084 |           0.2742 |
[32m[20221208 14:16:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:16:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.17
[32m[20221208 14:16:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.44
[32m[20221208 14:16:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.83
[32m[20221208 14:16:26 @agent_ppo2.py:137][0m Total time:       0.36 min
[32m[20221208 14:16:26 @agent_ppo2.py:139][0m 28672 total steps have happened
[32m[20221208 14:16:26 @agent_ppo2.py:115][0m #------------------------ Iteration 14 --------------------------#
[32m[20221208 14:16:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |           0.0045 |           0.3908 |           0.2684 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0116 |           0.3610 |           0.2686 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0140 |           0.3534 |           0.2690 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0180 |           0.3507 |           0.2691 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0203 |           0.3488 |           0.2692 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0237 |           0.3465 |           0.2690 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0255 |           0.3417 |           0.2692 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0247 |           0.3440 |           0.2703 |
[32m[20221208 14:16:27 @agent_ppo2.py:179][0m |          -0.0279 |           0.3389 |           0.2704 |
[32m[20221208 14:16:28 @agent_ppo2.py:179][0m |          -0.0294 |           0.3369 |           0.2706 |
[32m[20221208 14:16:28 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:16:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.53
[32m[20221208 14:16:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.78
[32m[20221208 14:16:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.68
[32m[20221208 14:16:28 @agent_ppo2.py:137][0m Total time:       0.38 min
[32m[20221208 14:16:28 @agent_ppo2.py:139][0m 30720 total steps have happened
[32m[20221208 14:16:28 @agent_ppo2.py:115][0m #------------------------ Iteration 15 --------------------------#
[32m[20221208 14:16:28 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |           0.0052 |           0.3736 |           0.2737 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0042 |           0.3537 |           0.2748 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0103 |           0.3516 |           0.2747 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0120 |           0.3474 |           0.2748 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0136 |           0.3491 |           0.2746 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0144 |           0.3440 |           0.2752 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0173 |           0.3454 |           0.2757 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0160 |           0.3441 |           0.2754 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0202 |           0.3431 |           0.2753 |
[32m[20221208 14:16:29 @agent_ppo2.py:179][0m |          -0.0202 |           0.3443 |           0.2762 |
[32m[20221208 14:16:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:16:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.53
[32m[20221208 14:16:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.25
[32m[20221208 14:16:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.03
[32m[20221208 14:16:30 @agent_ppo2.py:137][0m Total time:       0.41 min
[32m[20221208 14:16:30 @agent_ppo2.py:139][0m 32768 total steps have happened
[32m[20221208 14:16:30 @agent_ppo2.py:115][0m #------------------------ Iteration 16 --------------------------#
[32m[20221208 14:16:30 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0004 |           0.4129 |           0.2847 |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0122 |           0.3929 |           0.2837 |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0155 |           0.3932 |           0.2843 |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0172 |           0.3898 |           0.2842 |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0198 |           0.3898 |           0.2854 |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0204 |           0.3852 |           0.2850 |
[32m[20221208 14:16:30 @agent_ppo2.py:179][0m |          -0.0213 |           0.3863 |           0.2857 |
[32m[20221208 14:16:31 @agent_ppo2.py:179][0m |          -0.0240 |           0.3841 |           0.2860 |
[32m[20221208 14:16:31 @agent_ppo2.py:179][0m |          -0.0233 |           0.3850 |           0.2865 |
[32m[20221208 14:16:31 @agent_ppo2.py:179][0m |          -0.0241 |           0.3825 |           0.2856 |
[32m[20221208 14:16:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:16:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.28
[32m[20221208 14:16:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.64
[32m[20221208 14:16:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.84
[32m[20221208 14:16:31 @agent_ppo2.py:137][0m Total time:       0.44 min
[32m[20221208 14:16:31 @agent_ppo2.py:139][0m 34816 total steps have happened
[32m[20221208 14:16:31 @agent_ppo2.py:115][0m #------------------------ Iteration 17 --------------------------#
[32m[20221208 14:16:32 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |           0.0032 |           0.4086 |           0.2886 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0118 |           0.3688 |           0.2879 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0152 |           0.3604 |           0.2884 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0194 |           0.3576 |           0.2887 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0189 |           0.3576 |           0.2870 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0223 |           0.3517 |           0.2894 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0243 |           0.3525 |           0.2889 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0268 |           0.3508 |           0.2893 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0262 |           0.3485 |           0.2904 |
[32m[20221208 14:16:32 @agent_ppo2.py:179][0m |          -0.0275 |           0.3537 |           0.2905 |
[32m[20221208 14:16:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:16:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.35
[32m[20221208 14:16:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.36
[32m[20221208 14:16:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.58
[32m[20221208 14:16:33 @agent_ppo2.py:137][0m Total time:       0.46 min
[32m[20221208 14:16:33 @agent_ppo2.py:139][0m 36864 total steps have happened
[32m[20221208 14:16:33 @agent_ppo2.py:115][0m #------------------------ Iteration 18 --------------------------#
[32m[20221208 14:16:33 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:33 @agent_ppo2.py:179][0m |          -0.0035 |           0.3962 |           0.3114 |
[32m[20221208 14:16:33 @agent_ppo2.py:179][0m |          -0.0132 |           0.3761 |           0.3101 |
[32m[20221208 14:16:33 @agent_ppo2.py:179][0m |          -0.0186 |           0.3735 |           0.3092 |
[32m[20221208 14:16:33 @agent_ppo2.py:179][0m |          -0.0187 |           0.3703 |           0.3086 |
[32m[20221208 14:16:33 @agent_ppo2.py:179][0m |          -0.0196 |           0.3710 |           0.3092 |
[32m[20221208 14:16:33 @agent_ppo2.py:179][0m |          -0.0217 |           0.3708 |           0.3093 |
[32m[20221208 14:16:34 @agent_ppo2.py:179][0m |          -0.0214 |           0.3758 |           0.3092 |
[32m[20221208 14:16:34 @agent_ppo2.py:179][0m |          -0.0232 |           0.3743 |           0.3100 |
[32m[20221208 14:16:34 @agent_ppo2.py:179][0m |          -0.0247 |           0.3778 |           0.3102 |
[32m[20221208 14:16:34 @agent_ppo2.py:179][0m |          -0.0252 |           0.3705 |           0.3104 |
[32m[20221208 14:16:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.85
[32m[20221208 14:16:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.67
[32m[20221208 14:16:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.22
[32m[20221208 14:16:34 @agent_ppo2.py:137][0m Total time:       0.49 min
[32m[20221208 14:16:34 @agent_ppo2.py:139][0m 38912 total steps have happened
[32m[20221208 14:16:34 @agent_ppo2.py:115][0m #------------------------ Iteration 19 --------------------------#
[32m[20221208 14:16:35 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |           0.0092 |           0.3902 |           0.3085 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0054 |           0.3856 |           0.3081 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0060 |           0.3802 |           0.3090 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0114 |           0.3785 |           0.3085 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0122 |           0.3792 |           0.3094 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0150 |           0.3802 |           0.3092 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0143 |           0.3774 |           0.3090 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0146 |           0.3762 |           0.3088 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0150 |           0.3783 |           0.3094 |
[32m[20221208 14:16:35 @agent_ppo2.py:179][0m |          -0.0163 |           0.3764 |           0.3081 |
[32m[20221208 14:16:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:16:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.53
[32m[20221208 14:16:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.22
[32m[20221208 14:16:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.82
[32m[20221208 14:16:36 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 53.82
[32m[20221208 14:16:36 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 53.82
[32m[20221208 14:16:36 @agent_ppo2.py:137][0m Total time:       0.51 min
[32m[20221208 14:16:36 @agent_ppo2.py:139][0m 40960 total steps have happened
[32m[20221208 14:16:36 @agent_ppo2.py:115][0m #------------------------ Iteration 20 --------------------------#
[32m[20221208 14:16:36 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:36 @agent_ppo2.py:179][0m |           0.0018 |           0.4097 |           0.2995 |
[32m[20221208 14:16:36 @agent_ppo2.py:179][0m |          -0.0066 |           0.4070 |           0.2981 |
[32m[20221208 14:16:36 @agent_ppo2.py:179][0m |          -0.0121 |           0.4034 |           0.2992 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0093 |           0.3996 |           0.2982 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0126 |           0.3987 |           0.2978 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0144 |           0.4012 |           0.2978 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0140 |           0.4024 |           0.2967 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0150 |           0.3991 |           0.2969 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0165 |           0.3961 |           0.2974 |
[32m[20221208 14:16:37 @agent_ppo2.py:179][0m |          -0.0175 |           0.3968 |           0.2969 |
[32m[20221208 14:16:37 @agent_ppo2.py:124][0m Policy update time: 0.84 s
[32m[20221208 14:16:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.96
[32m[20221208 14:16:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.65
[32m[20221208 14:16:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.05
[32m[20221208 14:16:38 @agent_ppo2.py:137][0m Total time:       0.55 min
[32m[20221208 14:16:38 @agent_ppo2.py:139][0m 43008 total steps have happened
[32m[20221208 14:16:38 @agent_ppo2.py:115][0m #------------------------ Iteration 21 --------------------------#
[32m[20221208 14:16:38 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:16:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:38 @agent_ppo2.py:179][0m |           0.0028 |           0.4211 |           0.3111 |
[32m[20221208 14:16:38 @agent_ppo2.py:179][0m |          -0.0061 |           0.4154 |           0.3093 |
[32m[20221208 14:16:38 @agent_ppo2.py:179][0m |          -0.0081 |           0.4158 |           0.3098 |
[32m[20221208 14:16:38 @agent_ppo2.py:179][0m |          -0.0103 |           0.4160 |           0.3092 |
[32m[20221208 14:16:39 @agent_ppo2.py:179][0m |          -0.0121 |           0.4173 |           0.3091 |
[32m[20221208 14:16:39 @agent_ppo2.py:179][0m |          -0.0124 |           0.4128 |           0.3091 |
[32m[20221208 14:16:39 @agent_ppo2.py:179][0m |          -0.0144 |           0.4119 |           0.3095 |
[32m[20221208 14:16:39 @agent_ppo2.py:179][0m |          -0.0151 |           0.4145 |           0.3088 |
[32m[20221208 14:16:39 @agent_ppo2.py:179][0m |          -0.0176 |           0.4127 |           0.3087 |
[32m[20221208 14:16:39 @agent_ppo2.py:179][0m |          -0.0149 |           0.4163 |           0.3082 |
[32m[20221208 14:16:39 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:16:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.86
[32m[20221208 14:16:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.42
[32m[20221208 14:16:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.88
[32m[20221208 14:16:39 @agent_ppo2.py:137][0m Total time:       0.57 min
[32m[20221208 14:16:39 @agent_ppo2.py:139][0m 45056 total steps have happened
[32m[20221208 14:16:39 @agent_ppo2.py:115][0m #------------------------ Iteration 22 --------------------------#
[32m[20221208 14:16:40 @agent_ppo2.py:121][0m Sampling time: 0.51 s by 1 slaves
[32m[20221208 14:16:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |           0.0007 |           0.4192 |           0.2953 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0067 |           0.4204 |           0.2961 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0081 |           0.4170 |           0.2944 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0097 |           0.4153 |           0.2936 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0104 |           0.4129 |           0.2930 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0106 |           0.4145 |           0.2932 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0130 |           0.4109 |           0.2922 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0126 |           0.4146 |           0.2916 |
[32m[20221208 14:16:40 @agent_ppo2.py:179][0m |          -0.0144 |           0.4140 |           0.2923 |
[32m[20221208 14:16:41 @agent_ppo2.py:179][0m |          -0.0158 |           0.4137 |           0.2919 |
[32m[20221208 14:16:41 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:16:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.30
[32m[20221208 14:16:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.13
[32m[20221208 14:16:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.42
[32m[20221208 14:16:41 @agent_ppo2.py:137][0m Total time:       0.60 min
[32m[20221208 14:16:41 @agent_ppo2.py:139][0m 47104 total steps have happened
[32m[20221208 14:16:41 @agent_ppo2.py:115][0m #------------------------ Iteration 23 --------------------------#
[32m[20221208 14:16:41 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |           0.0027 |           0.4680 |           0.2941 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0055 |           0.4551 |           0.2918 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0079 |           0.4535 |           0.2909 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0096 |           0.4507 |           0.2913 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0120 |           0.4518 |           0.2914 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0111 |           0.4498 |           0.2912 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0117 |           0.4500 |           0.2920 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0136 |           0.4473 |           0.2911 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0163 |           0.4440 |           0.2925 |
[32m[20221208 14:16:42 @agent_ppo2.py:179][0m |          -0.0164 |           0.4481 |           0.2917 |
[32m[20221208 14:16:42 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:16:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.09
[32m[20221208 14:16:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.49
[32m[20221208 14:16:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.08
[32m[20221208 14:16:43 @agent_ppo2.py:137][0m Total time:       0.63 min
[32m[20221208 14:16:43 @agent_ppo2.py:139][0m 49152 total steps have happened
[32m[20221208 14:16:43 @agent_ppo2.py:115][0m #------------------------ Iteration 24 --------------------------#
[32m[20221208 14:16:43 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:16:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:43 @agent_ppo2.py:179][0m |           0.0038 |           0.4628 |           0.2882 |
[32m[20221208 14:16:43 @agent_ppo2.py:179][0m |          -0.0068 |           0.4616 |           0.2895 |
[32m[20221208 14:16:43 @agent_ppo2.py:179][0m |          -0.0103 |           0.4581 |           0.2897 |
[32m[20221208 14:16:43 @agent_ppo2.py:179][0m |          -0.0093 |           0.4532 |           0.2902 |
[32m[20221208 14:16:43 @agent_ppo2.py:179][0m |          -0.0113 |           0.4557 |           0.2891 |
[32m[20221208 14:16:44 @agent_ppo2.py:179][0m |          -0.0101 |           0.4549 |           0.2896 |
[32m[20221208 14:16:44 @agent_ppo2.py:179][0m |          -0.0134 |           0.4517 |           0.2899 |
[32m[20221208 14:16:44 @agent_ppo2.py:179][0m |          -0.0134 |           0.4586 |           0.2904 |
[32m[20221208 14:16:44 @agent_ppo2.py:179][0m |          -0.0133 |           0.4531 |           0.2895 |
[32m[20221208 14:16:44 @agent_ppo2.py:179][0m |          -0.0157 |           0.4613 |           0.2894 |
[32m[20221208 14:16:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:16:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.41
[32m[20221208 14:16:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.79
[32m[20221208 14:16:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.55
[32m[20221208 14:16:44 @agent_ppo2.py:137][0m Total time:       0.65 min
[32m[20221208 14:16:44 @agent_ppo2.py:139][0m 51200 total steps have happened
[32m[20221208 14:16:44 @agent_ppo2.py:115][0m #------------------------ Iteration 25 --------------------------#
[32m[20221208 14:16:45 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:16:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |           0.0041 |           0.5202 |           0.3036 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0052 |           0.4871 |           0.3041 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0118 |           0.4814 |           0.3054 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0149 |           0.4760 |           0.3057 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0174 |           0.4885 |           0.3063 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0182 |           0.4790 |           0.3065 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0219 |           0.4769 |           0.3066 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0213 |           0.4721 |           0.3064 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0230 |           0.4719 |           0.3074 |
[32m[20221208 14:16:45 @agent_ppo2.py:179][0m |          -0.0247 |           0.4710 |           0.3081 |
[32m[20221208 14:16:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.12
[32m[20221208 14:16:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.46
[32m[20221208 14:16:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.05
[32m[20221208 14:16:46 @agent_ppo2.py:137][0m Total time:       0.68 min
[32m[20221208 14:16:46 @agent_ppo2.py:139][0m 53248 total steps have happened
[32m[20221208 14:16:46 @agent_ppo2.py:115][0m #------------------------ Iteration 26 --------------------------#
[32m[20221208 14:16:46 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:46 @agent_ppo2.py:179][0m |           0.0044 |           0.5448 |           0.3023 |
[32m[20221208 14:16:46 @agent_ppo2.py:179][0m |          -0.0073 |           0.5335 |           0.3020 |
[32m[20221208 14:16:46 @agent_ppo2.py:179][0m |          -0.0101 |           0.5260 |           0.3027 |
[32m[20221208 14:16:46 @agent_ppo2.py:179][0m |          -0.0126 |           0.5206 |           0.3027 |
[32m[20221208 14:16:47 @agent_ppo2.py:179][0m |          -0.0155 |           0.5187 |           0.3024 |
[32m[20221208 14:16:47 @agent_ppo2.py:179][0m |          -0.0177 |           0.5184 |           0.3028 |
[32m[20221208 14:16:47 @agent_ppo2.py:179][0m |          -0.0185 |           0.5180 |           0.3025 |
[32m[20221208 14:16:47 @agent_ppo2.py:179][0m |          -0.0176 |           0.5143 |           0.3023 |
[32m[20221208 14:16:47 @agent_ppo2.py:179][0m |          -0.0204 |           0.5142 |           0.3026 |
[32m[20221208 14:16:47 @agent_ppo2.py:179][0m |          -0.0201 |           0.5157 |           0.3022 |
[32m[20221208 14:16:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:16:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.74
[32m[20221208 14:16:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.18
[32m[20221208 14:16:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.10
[32m[20221208 14:16:47 @agent_ppo2.py:137][0m Total time:       0.71 min
[32m[20221208 14:16:47 @agent_ppo2.py:139][0m 55296 total steps have happened
[32m[20221208 14:16:47 @agent_ppo2.py:115][0m #------------------------ Iteration 27 --------------------------#
[32m[20221208 14:16:48 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |           0.0078 |           0.5256 |           0.3114 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0090 |           0.5132 |           0.3101 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0152 |           0.5111 |           0.3095 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0172 |           0.5067 |           0.3095 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0196 |           0.5096 |           0.3086 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0186 |           0.5065 |           0.3083 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0191 |           0.5053 |           0.3094 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0225 |           0.5031 |           0.3086 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0218 |           0.5017 |           0.3097 |
[32m[20221208 14:16:48 @agent_ppo2.py:179][0m |          -0.0238 |           0.4993 |           0.3100 |
[32m[20221208 14:16:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:16:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.19
[32m[20221208 14:16:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.21
[32m[20221208 14:16:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.15
[32m[20221208 14:16:49 @agent_ppo2.py:137][0m Total time:       0.73 min
[32m[20221208 14:16:49 @agent_ppo2.py:139][0m 57344 total steps have happened
[32m[20221208 14:16:49 @agent_ppo2.py:115][0m #------------------------ Iteration 28 --------------------------#
[32m[20221208 14:16:49 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:16:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:49 @agent_ppo2.py:179][0m |           0.0020 |           0.5180 |           0.3090 |
[32m[20221208 14:16:49 @agent_ppo2.py:179][0m |          -0.0059 |           0.5046 |           0.3078 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0123 |           0.5012 |           0.3075 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0122 |           0.4985 |           0.3086 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0130 |           0.4946 |           0.3099 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0139 |           0.4929 |           0.3092 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0167 |           0.4952 |           0.3106 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0194 |           0.4952 |           0.3097 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0205 |           0.4922 |           0.3103 |
[32m[20221208 14:16:50 @agent_ppo2.py:179][0m |          -0.0208 |           0.4969 |           0.3095 |
[32m[20221208 14:16:50 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 14:16:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.81
[32m[20221208 14:16:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.12
[32m[20221208 14:16:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.98
[32m[20221208 14:16:50 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 62.98
[32m[20221208 14:16:50 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 62.98
[32m[20221208 14:16:50 @agent_ppo2.py:137][0m Total time:       0.76 min
[32m[20221208 14:16:50 @agent_ppo2.py:139][0m 59392 total steps have happened
[32m[20221208 14:16:50 @agent_ppo2.py:115][0m #------------------------ Iteration 29 --------------------------#
[32m[20221208 14:16:51 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:16:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0023 |           0.5128 |           0.3228 |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0113 |           0.5045 |           0.3212 |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0148 |           0.5002 |           0.3215 |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0169 |           0.5008 |           0.3206 |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0189 |           0.4967 |           0.3205 |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0180 |           0.5037 |           0.3202 |
[32m[20221208 14:16:51 @agent_ppo2.py:179][0m |          -0.0202 |           0.4984 |           0.3209 |
[32m[20221208 14:16:52 @agent_ppo2.py:179][0m |          -0.0206 |           0.5002 |           0.3206 |
[32m[20221208 14:16:52 @agent_ppo2.py:179][0m |          -0.0210 |           0.4972 |           0.3201 |
[32m[20221208 14:16:52 @agent_ppo2.py:179][0m |          -0.0217 |           0.4946 |           0.3204 |
[32m[20221208 14:16:52 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:16:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.71
[32m[20221208 14:16:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.95
[32m[20221208 14:16:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.26
[32m[20221208 14:16:52 @agent_ppo2.py:137][0m Total time:       0.79 min
[32m[20221208 14:16:52 @agent_ppo2.py:139][0m 61440 total steps have happened
[32m[20221208 14:16:52 @agent_ppo2.py:115][0m #------------------------ Iteration 30 --------------------------#
[32m[20221208 14:16:53 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:16:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |           0.0017 |           0.5433 |           0.3184 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0085 |           0.5339 |           0.3168 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0122 |           0.5355 |           0.3184 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0145 |           0.5306 |           0.3182 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0166 |           0.5251 |           0.3167 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0195 |           0.5311 |           0.3177 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0185 |           0.5277 |           0.3182 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0198 |           0.5269 |           0.3173 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0219 |           0.5292 |           0.3171 |
[32m[20221208 14:16:53 @agent_ppo2.py:179][0m |          -0.0220 |           0.5262 |           0.3176 |
[32m[20221208 14:16:53 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 14:16:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.78
[32m[20221208 14:16:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.00
[32m[20221208 14:16:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.29
[32m[20221208 14:16:54 @agent_ppo2.py:137][0m Total time:       0.82 min
[32m[20221208 14:16:54 @agent_ppo2.py:139][0m 63488 total steps have happened
[32m[20221208 14:16:54 @agent_ppo2.py:115][0m #------------------------ Iteration 31 --------------------------#
[32m[20221208 14:16:54 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:16:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:54 @agent_ppo2.py:179][0m |           0.0011 |           0.5445 |           0.3144 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0071 |           0.5369 |           0.3121 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0090 |           0.5371 |           0.3126 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0116 |           0.5396 |           0.3107 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0114 |           0.5306 |           0.3117 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0100 |           0.5320 |           0.3105 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0138 |           0.5348 |           0.3103 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0140 |           0.5284 |           0.3096 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0171 |           0.5299 |           0.3108 |
[32m[20221208 14:16:55 @agent_ppo2.py:179][0m |          -0.0149 |           0.5352 |           0.3105 |
[32m[20221208 14:16:55 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:16:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.76
[32m[20221208 14:16:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.88
[32m[20221208 14:16:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.17
[32m[20221208 14:16:55 @agent_ppo2.py:137][0m Total time:       0.84 min
[32m[20221208 14:16:55 @agent_ppo2.py:139][0m 65536 total steps have happened
[32m[20221208 14:16:55 @agent_ppo2.py:115][0m #------------------------ Iteration 32 --------------------------#
[32m[20221208 14:16:56 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |           0.0012 |           0.5491 |           0.3054 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0089 |           0.5428 |           0.3038 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0093 |           0.5396 |           0.3024 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0121 |           0.5446 |           0.3019 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0132 |           0.5444 |           0.3009 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0144 |           0.5419 |           0.3016 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0169 |           0.5360 |           0.3011 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0173 |           0.5372 |           0.3011 |
[32m[20221208 14:16:56 @agent_ppo2.py:179][0m |          -0.0166 |           0.5409 |           0.3002 |
[32m[20221208 14:16:57 @agent_ppo2.py:179][0m |          -0.0164 |           0.5358 |           0.2996 |
[32m[20221208 14:16:57 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:16:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.67
[32m[20221208 14:16:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.23
[32m[20221208 14:16:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.92
[32m[20221208 14:16:57 @agent_ppo2.py:137][0m Total time:       0.87 min
[32m[20221208 14:16:57 @agent_ppo2.py:139][0m 67584 total steps have happened
[32m[20221208 14:16:57 @agent_ppo2.py:115][0m #------------------------ Iteration 33 --------------------------#
[32m[20221208 14:16:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:57 @agent_ppo2.py:179][0m |           0.0089 |           0.5654 |           0.3006 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0092 |           0.5269 |           0.2991 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0130 |           0.5196 |           0.2981 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0145 |           0.5188 |           0.2979 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0168 |           0.5134 |           0.2974 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0186 |           0.5079 |           0.2968 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0238 |           0.5061 |           0.2970 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0199 |           0.5019 |           0.2971 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0232 |           0.4969 |           0.2972 |
[32m[20221208 14:16:58 @agent_ppo2.py:179][0m |          -0.0244 |           0.5045 |           0.2970 |
[32m[20221208 14:16:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:16:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.87
[32m[20221208 14:16:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.10
[32m[20221208 14:16:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.06
[32m[20221208 14:16:58 @agent_ppo2.py:137][0m Total time:       0.89 min
[32m[20221208 14:16:58 @agent_ppo2.py:139][0m 69632 total steps have happened
[32m[20221208 14:16:58 @agent_ppo2.py:115][0m #------------------------ Iteration 34 --------------------------#
[32m[20221208 14:16:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:16:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |           0.0045 |           0.5764 |           0.3054 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0063 |           0.5422 |           0.3038 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0123 |           0.5296 |           0.3039 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0177 |           0.5285 |           0.3034 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0177 |           0.5262 |           0.3036 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0218 |           0.5222 |           0.3035 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0238 |           0.5175 |           0.3019 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0257 |           0.5177 |           0.3026 |
[32m[20221208 14:16:59 @agent_ppo2.py:179][0m |          -0.0275 |           0.5137 |           0.3026 |
[32m[20221208 14:17:00 @agent_ppo2.py:179][0m |          -0.0263 |           0.5143 |           0.3025 |
[32m[20221208 14:17:00 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.31
[32m[20221208 14:17:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.12
[32m[20221208 14:17:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.69
[32m[20221208 14:17:00 @agent_ppo2.py:137][0m Total time:       0.92 min
[32m[20221208 14:17:00 @agent_ppo2.py:139][0m 71680 total steps have happened
[32m[20221208 14:17:00 @agent_ppo2.py:115][0m #------------------------ Iteration 35 --------------------------#
[32m[20221208 14:17:00 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:00 @agent_ppo2.py:179][0m |           0.0041 |           0.5583 |           0.3015 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0050 |           0.5349 |           0.3017 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0079 |           0.5343 |           0.3016 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0108 |           0.5346 |           0.3019 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0136 |           0.5326 |           0.3022 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0143 |           0.5265 |           0.3012 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0148 |           0.5263 |           0.3028 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0169 |           0.5311 |           0.3026 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0150 |           0.5282 |           0.3028 |
[32m[20221208 14:17:01 @agent_ppo2.py:179][0m |          -0.0206 |           0.5200 |           0.3034 |
[32m[20221208 14:17:01 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.21
[32m[20221208 14:17:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.53
[32m[20221208 14:17:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.39
[32m[20221208 14:17:01 @agent_ppo2.py:137][0m Total time:       0.94 min
[32m[20221208 14:17:01 @agent_ppo2.py:139][0m 73728 total steps have happened
[32m[20221208 14:17:01 @agent_ppo2.py:115][0m #------------------------ Iteration 36 --------------------------#
[32m[20221208 14:17:02 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:17:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0003 |           0.5893 |           0.2979 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0104 |           0.5827 |           0.2966 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0149 |           0.5707 |           0.2970 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0178 |           0.5656 |           0.2968 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0205 |           0.5659 |           0.2977 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0198 |           0.5716 |           0.2976 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0240 |           0.5610 |           0.2982 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0243 |           0.5570 |           0.2989 |
[32m[20221208 14:17:02 @agent_ppo2.py:179][0m |          -0.0270 |           0.5575 |           0.2986 |
[32m[20221208 14:17:03 @agent_ppo2.py:179][0m |          -0.0253 |           0.5566 |           0.2987 |
[32m[20221208 14:17:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:17:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.28
[32m[20221208 14:17:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.68
[32m[20221208 14:17:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.21
[32m[20221208 14:17:03 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 63.21
[32m[20221208 14:17:03 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 63.21
[32m[20221208 14:17:03 @agent_ppo2.py:137][0m Total time:       0.97 min
[32m[20221208 14:17:03 @agent_ppo2.py:139][0m 75776 total steps have happened
[32m[20221208 14:17:03 @agent_ppo2.py:115][0m #------------------------ Iteration 37 --------------------------#
[32m[20221208 14:17:03 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |           0.0019 |           0.7630 |           0.3012 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0154 |           0.7316 |           0.2995 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0205 |           0.7159 |           0.2999 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0228 |           0.7142 |           0.2991 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0275 |           0.7016 |           0.3002 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0264 |           0.6912 |           0.2997 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0316 |           0.6925 |           0.2995 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0318 |           0.6796 |           0.3003 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0340 |           0.6723 |           0.3011 |
[32m[20221208 14:17:04 @agent_ppo2.py:179][0m |          -0.0356 |           0.6677 |           0.3007 |
[32m[20221208 14:17:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:17:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.03
[32m[20221208 14:17:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.84
[32m[20221208 14:17:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.17
[32m[20221208 14:17:04 @agent_ppo2.py:137][0m Total time:       0.99 min
[32m[20221208 14:17:04 @agent_ppo2.py:139][0m 77824 total steps have happened
[32m[20221208 14:17:04 @agent_ppo2.py:115][0m #------------------------ Iteration 38 --------------------------#
[32m[20221208 14:17:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |           0.0043 |           0.9464 |           0.3084 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0109 |           0.9088 |           0.3071 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0177 |           0.8746 |           0.3080 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0211 |           0.8743 |           0.3080 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0230 |           0.8581 |           0.3070 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0258 |           0.8480 |           0.3081 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0294 |           0.8493 |           0.3078 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0310 |           0.8614 |           0.3086 |
[32m[20221208 14:17:05 @agent_ppo2.py:179][0m |          -0.0336 |           0.8413 |           0.3078 |
[32m[20221208 14:17:06 @agent_ppo2.py:179][0m |          -0.0330 |           0.8420 |           0.3075 |
[32m[20221208 14:17:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.80
[32m[20221208 14:17:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.16
[32m[20221208 14:17:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.21
[32m[20221208 14:17:06 @agent_ppo2.py:137][0m Total time:       1.02 min
[32m[20221208 14:17:06 @agent_ppo2.py:139][0m 79872 total steps have happened
[32m[20221208 14:17:06 @agent_ppo2.py:115][0m #------------------------ Iteration 39 --------------------------#
[32m[20221208 14:17:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |           0.0012 |           0.6417 |           0.3127 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0104 |           0.6211 |           0.3114 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0182 |           0.6116 |           0.3129 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0190 |           0.5994 |           0.3125 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0216 |           0.6005 |           0.3119 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0261 |           0.6002 |           0.3125 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0274 |           0.5937 |           0.3132 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0287 |           0.5912 |           0.3127 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0310 |           0.5893 |           0.3135 |
[32m[20221208 14:17:07 @agent_ppo2.py:179][0m |          -0.0326 |           0.5885 |           0.3139 |
[32m[20221208 14:17:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.04
[32m[20221208 14:17:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.41
[32m[20221208 14:17:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.07
[32m[20221208 14:17:07 @agent_ppo2.py:137][0m Total time:       1.04 min
[32m[20221208 14:17:07 @agent_ppo2.py:139][0m 81920 total steps have happened
[32m[20221208 14:17:07 @agent_ppo2.py:115][0m #------------------------ Iteration 40 --------------------------#
[32m[20221208 14:17:08 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |           0.0032 |           0.5528 |           0.3233 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0095 |           0.5202 |           0.3232 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0124 |           0.5148 |           0.3227 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0161 |           0.5113 |           0.3221 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0184 |           0.5032 |           0.3222 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0196 |           0.5046 |           0.3224 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0208 |           0.5029 |           0.3230 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0220 |           0.5076 |           0.3228 |
[32m[20221208 14:17:08 @agent_ppo2.py:179][0m |          -0.0225 |           0.5002 |           0.3215 |
[32m[20221208 14:17:09 @agent_ppo2.py:179][0m |          -0.0208 |           0.5017 |           0.3222 |
[32m[20221208 14:17:09 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.68
[32m[20221208 14:17:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.43
[32m[20221208 14:17:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.41
[32m[20221208 14:17:09 @agent_ppo2.py:137][0m Total time:       1.07 min
[32m[20221208 14:17:09 @agent_ppo2.py:139][0m 83968 total steps have happened
[32m[20221208 14:17:09 @agent_ppo2.py:115][0m #------------------------ Iteration 41 --------------------------#
[32m[20221208 14:17:09 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |           0.0049 |           0.5121 |           0.3008 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0106 |           0.4865 |           0.3011 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0137 |           0.4834 |           0.3011 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0147 |           0.4786 |           0.3017 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0200 |           0.4715 |           0.3012 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0200 |           0.4649 |           0.3017 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0228 |           0.4613 |           0.2996 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0213 |           0.4572 |           0.2994 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0222 |           0.4553 |           0.3004 |
[32m[20221208 14:17:10 @agent_ppo2.py:179][0m |          -0.0250 |           0.4527 |           0.3006 |
[32m[20221208 14:17:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:17:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.72
[32m[20221208 14:17:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.69
[32m[20221208 14:17:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.53
[32m[20221208 14:17:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 65.53
[32m[20221208 14:17:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 65.53
[32m[20221208 14:17:10 @agent_ppo2.py:137][0m Total time:       1.09 min
[32m[20221208 14:17:10 @agent_ppo2.py:139][0m 86016 total steps have happened
[32m[20221208 14:17:10 @agent_ppo2.py:115][0m #------------------------ Iteration 42 --------------------------#
[32m[20221208 14:17:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |           0.0031 |           0.6190 |           0.3051 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0098 |           0.5651 |           0.3062 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0166 |           0.5553 |           0.3062 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0199 |           0.5577 |           0.3074 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0243 |           0.5525 |           0.3081 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0266 |           0.5487 |           0.3088 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0291 |           0.5423 |           0.3081 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0314 |           0.5402 |           0.3089 |
[32m[20221208 14:17:11 @agent_ppo2.py:179][0m |          -0.0323 |           0.5470 |           0.3090 |
[32m[20221208 14:17:12 @agent_ppo2.py:179][0m |          -0.0328 |           0.5395 |           0.3101 |
[32m[20221208 14:17:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.45
[32m[20221208 14:17:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.76
[32m[20221208 14:17:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.98
[32m[20221208 14:17:12 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 66.98
[32m[20221208 14:17:12 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 66.98
[32m[20221208 14:17:12 @agent_ppo2.py:137][0m Total time:       1.12 min
[32m[20221208 14:17:12 @agent_ppo2.py:139][0m 88064 total steps have happened
[32m[20221208 14:17:12 @agent_ppo2.py:115][0m #------------------------ Iteration 43 --------------------------#
[32m[20221208 14:17:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |           0.0045 |           0.6375 |           0.2999 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0082 |           0.5931 |           0.3001 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0144 |           0.5907 |           0.2988 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0177 |           0.5854 |           0.2993 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0230 |           0.5837 |           0.3001 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0242 |           0.5816 |           0.3001 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0271 |           0.5770 |           0.3007 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0298 |           0.5758 |           0.3008 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0294 |           0.5729 |           0.3012 |
[32m[20221208 14:17:13 @agent_ppo2.py:179][0m |          -0.0310 |           0.5740 |           0.3013 |
[32m[20221208 14:17:13 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.84
[32m[20221208 14:17:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.23
[32m[20221208 14:17:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.09
[32m[20221208 14:17:13 @agent_ppo2.py:137][0m Total time:       1.14 min
[32m[20221208 14:17:13 @agent_ppo2.py:139][0m 90112 total steps have happened
[32m[20221208 14:17:13 @agent_ppo2.py:115][0m #------------------------ Iteration 44 --------------------------#
[32m[20221208 14:17:14 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:17:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |           0.0058 |           0.4998 |           0.3306 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0042 |           0.4871 |           0.3314 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0074 |           0.4822 |           0.3324 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0126 |           0.4855 |           0.3329 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0120 |           0.4785 |           0.3318 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0158 |           0.4751 |           0.3324 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0146 |           0.4745 |           0.3321 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0174 |           0.4821 |           0.3321 |
[32m[20221208 14:17:14 @agent_ppo2.py:179][0m |          -0.0165 |           0.4775 |           0.3327 |
[32m[20221208 14:17:15 @agent_ppo2.py:179][0m |          -0.0187 |           0.4748 |           0.3320 |
[32m[20221208 14:17:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.35
[32m[20221208 14:17:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.22
[32m[20221208 14:17:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.97
[32m[20221208 14:17:15 @agent_ppo2.py:137][0m Total time:       1.17 min
[32m[20221208 14:17:15 @agent_ppo2.py:139][0m 92160 total steps have happened
[32m[20221208 14:17:15 @agent_ppo2.py:115][0m #------------------------ Iteration 45 --------------------------#
[32m[20221208 14:17:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |           0.0049 |           0.4948 |           0.3326 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0040 |           0.4687 |           0.3325 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0098 |           0.4684 |           0.3322 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0145 |           0.4725 |           0.3323 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0191 |           0.4655 |           0.3326 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0177 |           0.4594 |           0.3326 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0235 |           0.4621 |           0.3332 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0218 |           0.4566 |           0.3328 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0234 |           0.4611 |           0.3341 |
[32m[20221208 14:17:16 @agent_ppo2.py:179][0m |          -0.0273 |           0.4600 |           0.3327 |
[32m[20221208 14:17:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.48
[32m[20221208 14:17:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.34
[32m[20221208 14:17:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.33
[32m[20221208 14:17:16 @agent_ppo2.py:137][0m Total time:       1.19 min
[32m[20221208 14:17:16 @agent_ppo2.py:139][0m 94208 total steps have happened
[32m[20221208 14:17:16 @agent_ppo2.py:115][0m #------------------------ Iteration 46 --------------------------#
[32m[20221208 14:17:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |           0.0024 |           0.4905 |           0.3232 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0073 |           0.4589 |           0.3221 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0123 |           0.4508 |           0.3242 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0198 |           0.4451 |           0.3238 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0232 |           0.4459 |           0.3251 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0271 |           0.4362 |           0.3256 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0283 |           0.4329 |           0.3271 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0309 |           0.4285 |           0.3266 |
[32m[20221208 14:17:17 @agent_ppo2.py:179][0m |          -0.0331 |           0.4308 |           0.3276 |
[32m[20221208 14:17:18 @agent_ppo2.py:179][0m |          -0.0350 |           0.4392 |           0.3273 |
[32m[20221208 14:17:18 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.61
[32m[20221208 14:17:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.86
[32m[20221208 14:17:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.41
[32m[20221208 14:17:18 @agent_ppo2.py:137][0m Total time:       1.22 min
[32m[20221208 14:17:18 @agent_ppo2.py:139][0m 96256 total steps have happened
[32m[20221208 14:17:18 @agent_ppo2.py:115][0m #------------------------ Iteration 47 --------------------------#
[32m[20221208 14:17:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |           0.0076 |           0.6094 |           0.3378 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0060 |           0.5645 |           0.3377 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0134 |           0.5510 |           0.3377 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0173 |           0.5326 |           0.3372 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0219 |           0.5288 |           0.3388 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0231 |           0.5173 |           0.3386 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0270 |           0.5092 |           0.3396 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0257 |           0.4952 |           0.3398 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0284 |           0.4937 |           0.3400 |
[32m[20221208 14:17:19 @agent_ppo2.py:179][0m |          -0.0324 |           0.4838 |           0.3406 |
[32m[20221208 14:17:19 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.48
[32m[20221208 14:17:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 64.65
[32m[20221208 14:17:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.69
[32m[20221208 14:17:19 @agent_ppo2.py:137][0m Total time:       1.24 min
[32m[20221208 14:17:19 @agent_ppo2.py:139][0m 98304 total steps have happened
[32m[20221208 14:17:19 @agent_ppo2.py:115][0m #------------------------ Iteration 48 --------------------------#
[32m[20221208 14:17:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |           0.0042 |           1.2160 |           0.3425 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0095 |           1.1882 |           0.3414 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0156 |           1.1777 |           0.3395 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0168 |           1.1574 |           0.3389 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0191 |           1.1664 |           0.3387 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0224 |           1.1392 |           0.3396 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0261 |           1.1354 |           0.3393 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0248 |           1.1204 |           0.3395 |
[32m[20221208 14:17:20 @agent_ppo2.py:179][0m |          -0.0285 |           1.1207 |           0.3403 |
[32m[20221208 14:17:21 @agent_ppo2.py:179][0m |          -0.0278 |           1.1272 |           0.3392 |
[32m[20221208 14:17:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.66
[32m[20221208 14:17:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.69
[32m[20221208 14:17:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.73
[32m[20221208 14:17:21 @agent_ppo2.py:137][0m Total time:       1.27 min
[32m[20221208 14:17:21 @agent_ppo2.py:139][0m 100352 total steps have happened
[32m[20221208 14:17:21 @agent_ppo2.py:115][0m #------------------------ Iteration 49 --------------------------#
[32m[20221208 14:17:21 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |           0.0036 |           0.6878 |           0.3431 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0095 |           0.6526 |           0.3415 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0156 |           0.6446 |           0.3407 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0157 |           0.6418 |           0.3394 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0208 |           0.6350 |           0.3403 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0232 |           0.6298 |           0.3396 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0245 |           0.6254 |           0.3395 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0287 |           0.6244 |           0.3388 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0299 |           0.6216 |           0.3404 |
[32m[20221208 14:17:22 @agent_ppo2.py:179][0m |          -0.0335 |           0.6172 |           0.3407 |
[32m[20221208 14:17:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.72
[32m[20221208 14:17:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.09
[32m[20221208 14:17:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.14
[32m[20221208 14:17:22 @agent_ppo2.py:137][0m Total time:       1.29 min
[32m[20221208 14:17:22 @agent_ppo2.py:139][0m 102400 total steps have happened
[32m[20221208 14:17:22 @agent_ppo2.py:115][0m #------------------------ Iteration 50 --------------------------#
[32m[20221208 14:17:23 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |           0.0057 |           0.8529 |           0.3407 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0119 |           0.8034 |           0.3388 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0144 |           0.8001 |           0.3357 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0213 |           0.7875 |           0.3348 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0224 |           0.7821 |           0.3339 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0253 |           0.7751 |           0.3329 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0271 |           0.7725 |           0.3326 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0286 |           0.7684 |           0.3314 |
[32m[20221208 14:17:23 @agent_ppo2.py:179][0m |          -0.0307 |           0.7726 |           0.3309 |
[32m[20221208 14:17:24 @agent_ppo2.py:179][0m |          -0.0320 |           0.7669 |           0.3305 |
[32m[20221208 14:17:24 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.24
[32m[20221208 14:17:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.57
[32m[20221208 14:17:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.87
[32m[20221208 14:17:24 @agent_ppo2.py:137][0m Total time:       1.32 min
[32m[20221208 14:17:24 @agent_ppo2.py:139][0m 104448 total steps have happened
[32m[20221208 14:17:24 @agent_ppo2.py:115][0m #------------------------ Iteration 51 --------------------------#
[32m[20221208 14:17:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:24 @agent_ppo2.py:179][0m |           0.0033 |           1.3778 |           0.3200 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0137 |           1.3127 |           0.3196 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0184 |           1.3220 |           0.3191 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0247 |           1.2996 |           0.3210 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0287 |           1.2782 |           0.3214 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0321 |           1.2420 |           0.3220 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0353 |           1.2321 |           0.3234 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0375 |           1.2290 |           0.3243 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0389 |           1.2068 |           0.3255 |
[32m[20221208 14:17:25 @agent_ppo2.py:179][0m |          -0.0410 |           1.1928 |           0.3252 |
[32m[20221208 14:17:25 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:17:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.70
[32m[20221208 14:17:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.67
[32m[20221208 14:17:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.80
[32m[20221208 14:17:25 @agent_ppo2.py:137][0m Total time:       1.34 min
[32m[20221208 14:17:25 @agent_ppo2.py:139][0m 106496 total steps have happened
[32m[20221208 14:17:25 @agent_ppo2.py:115][0m #------------------------ Iteration 52 --------------------------#
[32m[20221208 14:17:26 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:17:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |           0.0018 |           0.8772 |           0.3220 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0133 |           0.8039 |           0.3213 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0160 |           0.7819 |           0.3214 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0247 |           0.7714 |           0.3225 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0294 |           0.7636 |           0.3241 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0294 |           0.7512 |           0.3240 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0335 |           0.7563 |           0.3249 |
[32m[20221208 14:17:26 @agent_ppo2.py:179][0m |          -0.0334 |           0.7393 |           0.3251 |
[32m[20221208 14:17:27 @agent_ppo2.py:179][0m |          -0.0369 |           0.7359 |           0.3248 |
[32m[20221208 14:17:27 @agent_ppo2.py:179][0m |          -0.0387 |           0.7313 |           0.3253 |
[32m[20221208 14:17:27 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:17:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.87
[32m[20221208 14:17:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.53
[32m[20221208 14:17:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.14
[32m[20221208 14:17:27 @agent_ppo2.py:137][0m Total time:       1.37 min
[32m[20221208 14:17:27 @agent_ppo2.py:139][0m 108544 total steps have happened
[32m[20221208 14:17:27 @agent_ppo2.py:115][0m #------------------------ Iteration 53 --------------------------#
[32m[20221208 14:17:28 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:17:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0026 |           1.3655 |           0.3265 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0084 |           1.3101 |           0.3252 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0191 |           1.2751 |           0.3270 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0259 |           1.2627 |           0.3271 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0302 |           1.2536 |           0.3279 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0328 |           1.2431 |           0.3279 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0364 |           1.2249 |           0.3272 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0395 |           1.2286 |           0.3272 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0406 |           1.2191 |           0.3287 |
[32m[20221208 14:17:28 @agent_ppo2.py:179][0m |          -0.0416 |           1.2088 |           0.3290 |
[32m[20221208 14:17:28 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:17:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.58
[32m[20221208 14:17:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.75
[32m[20221208 14:17:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.59
[32m[20221208 14:17:29 @agent_ppo2.py:137][0m Total time:       1.40 min
[32m[20221208 14:17:29 @agent_ppo2.py:139][0m 110592 total steps have happened
[32m[20221208 14:17:29 @agent_ppo2.py:115][0m #------------------------ Iteration 54 --------------------------#
[32m[20221208 14:17:29 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:17:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:29 @agent_ppo2.py:179][0m |           0.0064 |           0.9973 |           0.3510 |
[32m[20221208 14:17:29 @agent_ppo2.py:179][0m |          -0.0081 |           0.9482 |           0.3504 |
[32m[20221208 14:17:29 @agent_ppo2.py:179][0m |          -0.0164 |           0.9499 |           0.3515 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0178 |           0.9305 |           0.3514 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0220 |           0.9256 |           0.3525 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0239 |           0.9279 |           0.3531 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0273 |           0.9222 |           0.3535 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0272 |           0.9268 |           0.3546 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0287 |           0.9255 |           0.3541 |
[32m[20221208 14:17:30 @agent_ppo2.py:179][0m |          -0.0319 |           0.9152 |           0.3550 |
[32m[20221208 14:17:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:17:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.45
[32m[20221208 14:17:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.87
[32m[20221208 14:17:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.53
[32m[20221208 14:17:30 @agent_ppo2.py:137][0m Total time:       1.42 min
[32m[20221208 14:17:30 @agent_ppo2.py:139][0m 112640 total steps have happened
[32m[20221208 14:17:30 @agent_ppo2.py:115][0m #------------------------ Iteration 55 --------------------------#
[32m[20221208 14:17:31 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |           0.0050 |           1.0381 |           0.3469 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0019 |           1.0069 |           0.3441 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0110 |           1.0006 |           0.3448 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0154 |           1.0062 |           0.3434 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0171 |           0.9920 |           0.3419 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0207 |           0.9817 |           0.3423 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0219 |           0.9831 |           0.3420 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0229 |           0.9805 |           0.3421 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0237 |           0.9768 |           0.3418 |
[32m[20221208 14:17:31 @agent_ppo2.py:179][0m |          -0.0237 |           0.9707 |           0.3423 |
[32m[20221208 14:17:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:17:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.46
[32m[20221208 14:17:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.03
[32m[20221208 14:17:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.12
[32m[20221208 14:17:32 @agent_ppo2.py:137][0m Total time:       1.45 min
[32m[20221208 14:17:32 @agent_ppo2.py:139][0m 114688 total steps have happened
[32m[20221208 14:17:32 @agent_ppo2.py:115][0m #------------------------ Iteration 56 --------------------------#
[32m[20221208 14:17:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:32 @agent_ppo2.py:179][0m |           0.0082 |           0.7645 |           0.3337 |
[32m[20221208 14:17:32 @agent_ppo2.py:179][0m |          -0.0096 |           0.7388 |           0.3323 |
[32m[20221208 14:17:32 @agent_ppo2.py:179][0m |          -0.0092 |           0.7348 |           0.3323 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0167 |           0.7233 |           0.3318 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0174 |           0.7220 |           0.3334 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0206 |           0.7211 |           0.3319 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0238 |           0.7165 |           0.3329 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0248 |           0.7179 |           0.3330 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0270 |           0.7137 |           0.3325 |
[32m[20221208 14:17:33 @agent_ppo2.py:179][0m |          -0.0288 |           0.7117 |           0.3330 |
[32m[20221208 14:17:33 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.08
[32m[20221208 14:17:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.48
[32m[20221208 14:17:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.25
[32m[20221208 14:17:33 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 67.25
[32m[20221208 14:17:33 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 67.25
[32m[20221208 14:17:33 @agent_ppo2.py:137][0m Total time:       1.47 min
[32m[20221208 14:17:33 @agent_ppo2.py:139][0m 116736 total steps have happened
[32m[20221208 14:17:33 @agent_ppo2.py:115][0m #------------------------ Iteration 57 --------------------------#
[32m[20221208 14:17:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |           0.0067 |           1.3499 |           0.3294 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0042 |           1.3276 |           0.3275 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0114 |           1.3040 |           0.3307 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0201 |           1.3078 |           0.3313 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0254 |           1.2923 |           0.3320 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0257 |           1.2790 |           0.3331 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0302 |           1.2773 |           0.3337 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0320 |           1.2717 |           0.3355 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0302 |           1.2629 |           0.3349 |
[32m[20221208 14:17:34 @agent_ppo2.py:179][0m |          -0.0347 |           1.2563 |           0.3351 |
[32m[20221208 14:17:34 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.68
[32m[20221208 14:17:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.34
[32m[20221208 14:17:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.23
[32m[20221208 14:17:35 @agent_ppo2.py:137][0m Total time:       1.50 min
[32m[20221208 14:17:35 @agent_ppo2.py:139][0m 118784 total steps have happened
[32m[20221208 14:17:35 @agent_ppo2.py:115][0m #------------------------ Iteration 58 --------------------------#
[32m[20221208 14:17:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:35 @agent_ppo2.py:179][0m |           0.0080 |           1.0757 |           0.3460 |
[32m[20221208 14:17:35 @agent_ppo2.py:179][0m |          -0.0051 |           1.0463 |           0.3460 |
[32m[20221208 14:17:35 @agent_ppo2.py:179][0m |          -0.0129 |           1.0279 |           0.3466 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0181 |           1.0279 |           0.3473 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0234 |           1.0196 |           0.3483 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0254 |           1.0099 |           0.3495 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0284 |           1.0086 |           0.3494 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0283 |           1.0161 |           0.3479 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0328 |           1.0141 |           0.3494 |
[32m[20221208 14:17:36 @agent_ppo2.py:179][0m |          -0.0337 |           1.0044 |           0.3491 |
[32m[20221208 14:17:36 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.75
[32m[20221208 14:17:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.20
[32m[20221208 14:17:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.21
[32m[20221208 14:17:36 @agent_ppo2.py:137][0m Total time:       1.52 min
[32m[20221208 14:17:36 @agent_ppo2.py:139][0m 120832 total steps have happened
[32m[20221208 14:17:36 @agent_ppo2.py:115][0m #------------------------ Iteration 59 --------------------------#
[32m[20221208 14:17:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |           0.0122 |           1.2840 |           0.3509 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0073 |           1.2358 |           0.3508 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0162 |           1.2346 |           0.3503 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0209 |           1.2198 |           0.3489 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0211 |           1.2221 |           0.3489 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0251 |           1.2103 |           0.3484 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0267 |           1.1970 |           0.3489 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0302 |           1.2019 |           0.3488 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0309 |           1.1924 |           0.3502 |
[32m[20221208 14:17:37 @agent_ppo2.py:179][0m |          -0.0339 |           1.1934 |           0.3487 |
[32m[20221208 14:17:37 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.71
[32m[20221208 14:17:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.91
[32m[20221208 14:17:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.21
[32m[20221208 14:17:38 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 73.21
[32m[20221208 14:17:38 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 73.21
[32m[20221208 14:17:38 @agent_ppo2.py:137][0m Total time:       1.55 min
[32m[20221208 14:17:38 @agent_ppo2.py:139][0m 122880 total steps have happened
[32m[20221208 14:17:38 @agent_ppo2.py:115][0m #------------------------ Iteration 60 --------------------------#
[32m[20221208 14:17:38 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:38 @agent_ppo2.py:179][0m |           0.0064 |           1.0429 |           0.3659 |
[32m[20221208 14:17:38 @agent_ppo2.py:179][0m |          -0.0064 |           1.0322 |           0.3648 |
[32m[20221208 14:17:38 @agent_ppo2.py:179][0m |          -0.0137 |           1.0249 |           0.3666 |
[32m[20221208 14:17:38 @agent_ppo2.py:179][0m |          -0.0165 |           1.0185 |           0.3656 |
[32m[20221208 14:17:39 @agent_ppo2.py:179][0m |          -0.0184 |           1.0162 |           0.3661 |
[32m[20221208 14:17:39 @agent_ppo2.py:179][0m |          -0.0215 |           1.0259 |           0.3659 |
[32m[20221208 14:17:39 @agent_ppo2.py:179][0m |          -0.0235 |           1.0134 |           0.3652 |
[32m[20221208 14:17:39 @agent_ppo2.py:179][0m |          -0.0237 |           1.0089 |           0.3657 |
[32m[20221208 14:17:39 @agent_ppo2.py:179][0m |          -0.0255 |           1.0067 |           0.3659 |
[32m[20221208 14:17:39 @agent_ppo2.py:179][0m |          -0.0258 |           1.0041 |           0.3672 |
[32m[20221208 14:17:39 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.44
[32m[20221208 14:17:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.15
[32m[20221208 14:17:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.09
[32m[20221208 14:17:39 @agent_ppo2.py:137][0m Total time:       1.57 min
[32m[20221208 14:17:39 @agent_ppo2.py:139][0m 124928 total steps have happened
[32m[20221208 14:17:39 @agent_ppo2.py:115][0m #------------------------ Iteration 61 --------------------------#
[32m[20221208 14:17:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |           0.0091 |           1.1116 |           0.3514 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0093 |           1.1009 |           0.3522 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0088 |           1.0939 |           0.3516 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0128 |           1.0882 |           0.3534 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0159 |           1.0860 |           0.3514 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0156 |           1.0890 |           0.3513 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0168 |           1.0730 |           0.3507 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0208 |           1.0818 |           0.3529 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0232 |           1.0748 |           0.3523 |
[32m[20221208 14:17:40 @agent_ppo2.py:179][0m |          -0.0248 |           1.0823 |           0.3521 |
[32m[20221208 14:17:40 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.97
[32m[20221208 14:17:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.57
[32m[20221208 14:17:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.78
[32m[20221208 14:17:41 @agent_ppo2.py:137][0m Total time:       1.60 min
[32m[20221208 14:17:41 @agent_ppo2.py:139][0m 126976 total steps have happened
[32m[20221208 14:17:41 @agent_ppo2.py:115][0m #------------------------ Iteration 62 --------------------------#
[32m[20221208 14:17:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:41 @agent_ppo2.py:179][0m |           0.0064 |           1.0118 |           0.3628 |
[32m[20221208 14:17:41 @agent_ppo2.py:179][0m |          -0.0061 |           0.9234 |           0.3610 |
[32m[20221208 14:17:41 @agent_ppo2.py:179][0m |          -0.0136 |           0.8998 |           0.3604 |
[32m[20221208 14:17:41 @agent_ppo2.py:179][0m |          -0.0151 |           0.8931 |           0.3604 |
[32m[20221208 14:17:42 @agent_ppo2.py:179][0m |          -0.0206 |           0.8974 |           0.3600 |
[32m[20221208 14:17:42 @agent_ppo2.py:179][0m |          -0.0239 |           0.8863 |           0.3589 |
[32m[20221208 14:17:42 @agent_ppo2.py:179][0m |          -0.0260 |           0.8866 |           0.3587 |
[32m[20221208 14:17:42 @agent_ppo2.py:179][0m |          -0.0291 |           0.8785 |           0.3590 |
[32m[20221208 14:17:42 @agent_ppo2.py:179][0m |          -0.0294 |           0.8749 |           0.3586 |
[32m[20221208 14:17:42 @agent_ppo2.py:179][0m |          -0.0326 |           0.8683 |           0.3577 |
[32m[20221208 14:17:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.98
[32m[20221208 14:17:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.55
[32m[20221208 14:17:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.51
[32m[20221208 14:17:42 @agent_ppo2.py:137][0m Total time:       1.62 min
[32m[20221208 14:17:42 @agent_ppo2.py:139][0m 129024 total steps have happened
[32m[20221208 14:17:42 @agent_ppo2.py:115][0m #------------------------ Iteration 63 --------------------------#
[32m[20221208 14:17:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |           0.0023 |           1.0955 |           0.3397 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0140 |           1.0601 |           0.3383 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0191 |           1.0545 |           0.3379 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0243 |           1.0327 |           0.3380 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0272 |           1.0281 |           0.3384 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0296 |           1.0176 |           0.3374 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0317 |           1.0164 |           0.3381 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0350 |           1.0100 |           0.3384 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0370 |           1.0064 |           0.3385 |
[32m[20221208 14:17:43 @agent_ppo2.py:179][0m |          -0.0396 |           1.0061 |           0.3393 |
[32m[20221208 14:17:43 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.00
[32m[20221208 14:17:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.48
[32m[20221208 14:17:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.86
[32m[20221208 14:17:44 @agent_ppo2.py:137][0m Total time:       1.65 min
[32m[20221208 14:17:44 @agent_ppo2.py:139][0m 131072 total steps have happened
[32m[20221208 14:17:44 @agent_ppo2.py:115][0m #------------------------ Iteration 64 --------------------------#
[32m[20221208 14:17:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:44 @agent_ppo2.py:179][0m |           0.0100 |           1.6480 |           0.3327 |
[32m[20221208 14:17:44 @agent_ppo2.py:179][0m |          -0.0101 |           1.6096 |           0.3315 |
[32m[20221208 14:17:44 @agent_ppo2.py:179][0m |          -0.0128 |           1.5822 |           0.3291 |
[32m[20221208 14:17:44 @agent_ppo2.py:179][0m |          -0.0196 |           1.5756 |           0.3313 |
[32m[20221208 14:17:45 @agent_ppo2.py:179][0m |          -0.0232 |           1.5853 |           0.3295 |
[32m[20221208 14:17:45 @agent_ppo2.py:179][0m |          -0.0239 |           1.5751 |           0.3298 |
[32m[20221208 14:17:45 @agent_ppo2.py:179][0m |          -0.0285 |           1.5858 |           0.3299 |
[32m[20221208 14:17:45 @agent_ppo2.py:179][0m |          -0.0313 |           1.5400 |           0.3302 |
[32m[20221208 14:17:45 @agent_ppo2.py:179][0m |          -0.0338 |           1.5420 |           0.3308 |
[32m[20221208 14:17:45 @agent_ppo2.py:179][0m |          -0.0358 |           1.5411 |           0.3303 |
[32m[20221208 14:17:45 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:17:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.98
[32m[20221208 14:17:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.69
[32m[20221208 14:17:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.59
[32m[20221208 14:17:45 @agent_ppo2.py:137][0m Total time:       1.67 min
[32m[20221208 14:17:45 @agent_ppo2.py:139][0m 133120 total steps have happened
[32m[20221208 14:17:45 @agent_ppo2.py:115][0m #------------------------ Iteration 65 --------------------------#
[32m[20221208 14:17:46 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |           0.0073 |           0.7985 |           0.3293 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0101 |           0.7355 |           0.3271 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0136 |           0.7211 |           0.3259 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0153 |           0.7316 |           0.3261 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0161 |           0.7119 |           0.3243 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0241 |           0.6994 |           0.3245 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0257 |           0.6996 |           0.3235 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0272 |           0.7051 |           0.3243 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0268 |           0.7023 |           0.3236 |
[32m[20221208 14:17:46 @agent_ppo2.py:179][0m |          -0.0269 |           0.6947 |           0.3227 |
[32m[20221208 14:17:46 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.23
[32m[20221208 14:17:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.72
[32m[20221208 14:17:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.18
[32m[20221208 14:17:47 @agent_ppo2.py:137][0m Total time:       1.70 min
[32m[20221208 14:17:47 @agent_ppo2.py:139][0m 135168 total steps have happened
[32m[20221208 14:17:47 @agent_ppo2.py:115][0m #------------------------ Iteration 66 --------------------------#
[32m[20221208 14:17:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:47 @agent_ppo2.py:179][0m |           0.0015 |           0.8881 |           0.3351 |
[32m[20221208 14:17:47 @agent_ppo2.py:179][0m |          -0.0089 |           0.8599 |           0.3341 |
[32m[20221208 14:17:47 @agent_ppo2.py:179][0m |          -0.0124 |           0.8477 |           0.3336 |
[32m[20221208 14:17:47 @agent_ppo2.py:179][0m |          -0.0166 |           0.8463 |           0.3321 |
[32m[20221208 14:17:47 @agent_ppo2.py:179][0m |          -0.0207 |           0.8433 |           0.3314 |
[32m[20221208 14:17:48 @agent_ppo2.py:179][0m |          -0.0213 |           0.8413 |           0.3319 |
[32m[20221208 14:17:48 @agent_ppo2.py:179][0m |          -0.0254 |           0.8329 |           0.3317 |
[32m[20221208 14:17:48 @agent_ppo2.py:179][0m |          -0.0261 |           0.8317 |           0.3318 |
[32m[20221208 14:17:48 @agent_ppo2.py:179][0m |          -0.0301 |           0.8338 |           0.3323 |
[32m[20221208 14:17:48 @agent_ppo2.py:179][0m |          -0.0309 |           0.8295 |           0.3330 |
[32m[20221208 14:17:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:17:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.13
[32m[20221208 14:17:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.20
[32m[20221208 14:17:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.45
[32m[20221208 14:17:48 @agent_ppo2.py:137][0m Total time:       1.72 min
[32m[20221208 14:17:48 @agent_ppo2.py:139][0m 137216 total steps have happened
[32m[20221208 14:17:48 @agent_ppo2.py:115][0m #------------------------ Iteration 67 --------------------------#
[32m[20221208 14:17:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |           0.0066 |           1.4026 |           0.3152 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0067 |           1.3679 |           0.3130 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0147 |           1.3134 |           0.3114 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0207 |           1.3266 |           0.3108 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0247 |           1.2890 |           0.3109 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0260 |           1.2701 |           0.3111 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0308 |           1.2646 |           0.3109 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0318 |           1.2600 |           0.3114 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0339 |           1.2453 |           0.3114 |
[32m[20221208 14:17:49 @agent_ppo2.py:179][0m |          -0.0350 |           1.2455 |           0.3113 |
[32m[20221208 14:17:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.42
[32m[20221208 14:17:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.17
[32m[20221208 14:17:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.57
[32m[20221208 14:17:50 @agent_ppo2.py:137][0m Total time:       1.75 min
[32m[20221208 14:17:50 @agent_ppo2.py:139][0m 139264 total steps have happened
[32m[20221208 14:17:50 @agent_ppo2.py:115][0m #------------------------ Iteration 68 --------------------------#
[32m[20221208 14:17:50 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:50 @agent_ppo2.py:179][0m |           0.0031 |           0.7161 |           0.3088 |
[32m[20221208 14:17:50 @agent_ppo2.py:179][0m |          -0.0097 |           0.6824 |           0.3099 |
[32m[20221208 14:17:50 @agent_ppo2.py:179][0m |          -0.0138 |           0.6728 |           0.3080 |
[32m[20221208 14:17:50 @agent_ppo2.py:179][0m |          -0.0182 |           0.6646 |           0.3091 |
[32m[20221208 14:17:50 @agent_ppo2.py:179][0m |          -0.0214 |           0.6573 |           0.3091 |
[32m[20221208 14:17:51 @agent_ppo2.py:179][0m |          -0.0220 |           0.6541 |           0.3081 |
[32m[20221208 14:17:51 @agent_ppo2.py:179][0m |          -0.0258 |           0.6512 |           0.3082 |
[32m[20221208 14:17:51 @agent_ppo2.py:179][0m |          -0.0296 |           0.6550 |           0.3086 |
[32m[20221208 14:17:51 @agent_ppo2.py:179][0m |          -0.0291 |           0.6426 |           0.3085 |
[32m[20221208 14:17:51 @agent_ppo2.py:179][0m |          -0.0296 |           0.6433 |           0.3077 |
[32m[20221208 14:17:51 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.04
[32m[20221208 14:17:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.29
[32m[20221208 14:17:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.21
[32m[20221208 14:17:51 @agent_ppo2.py:137][0m Total time:       1.77 min
[32m[20221208 14:17:51 @agent_ppo2.py:139][0m 141312 total steps have happened
[32m[20221208 14:17:51 @agent_ppo2.py:115][0m #------------------------ Iteration 69 --------------------------#
[32m[20221208 14:17:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |           0.0074 |           0.8401 |           0.3073 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0077 |           0.8090 |           0.3054 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0128 |           0.7909 |           0.3060 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0212 |           0.7789 |           0.3041 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0263 |           0.7754 |           0.3045 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0274 |           0.7616 |           0.3051 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0291 |           0.7578 |           0.3051 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0342 |           0.7496 |           0.3056 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0349 |           0.7381 |           0.3065 |
[32m[20221208 14:17:52 @agent_ppo2.py:179][0m |          -0.0378 |           0.7385 |           0.3064 |
[32m[20221208 14:17:52 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.15
[32m[20221208 14:17:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.55
[32m[20221208 14:17:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.35
[32m[20221208 14:17:53 @agent_ppo2.py:137][0m Total time:       1.80 min
[32m[20221208 14:17:53 @agent_ppo2.py:139][0m 143360 total steps have happened
[32m[20221208 14:17:53 @agent_ppo2.py:115][0m #------------------------ Iteration 70 --------------------------#
[32m[20221208 14:17:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:53 @agent_ppo2.py:179][0m |           0.0042 |           0.8529 |           0.3096 |
[32m[20221208 14:17:53 @agent_ppo2.py:179][0m |          -0.0130 |           0.7944 |           0.3087 |
[32m[20221208 14:17:53 @agent_ppo2.py:179][0m |          -0.0174 |           0.7771 |           0.3087 |
[32m[20221208 14:17:53 @agent_ppo2.py:179][0m |          -0.0197 |           0.7649 |           0.3090 |
[32m[20221208 14:17:53 @agent_ppo2.py:179][0m |          -0.0287 |           0.7497 |           0.3091 |
[32m[20221208 14:17:53 @agent_ppo2.py:179][0m |          -0.0302 |           0.7409 |           0.3100 |
[32m[20221208 14:17:54 @agent_ppo2.py:179][0m |          -0.0316 |           0.7330 |           0.3086 |
[32m[20221208 14:17:54 @agent_ppo2.py:179][0m |          -0.0346 |           0.7323 |           0.3098 |
[32m[20221208 14:17:54 @agent_ppo2.py:179][0m |          -0.0377 |           0.7154 |           0.3106 |
[32m[20221208 14:17:54 @agent_ppo2.py:179][0m |          -0.0378 |           0.7113 |           0.3103 |
[32m[20221208 14:17:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.95
[32m[20221208 14:17:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.42
[32m[20221208 14:17:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.86
[32m[20221208 14:17:54 @agent_ppo2.py:137][0m Total time:       1.82 min
[32m[20221208 14:17:54 @agent_ppo2.py:139][0m 145408 total steps have happened
[32m[20221208 14:17:54 @agent_ppo2.py:115][0m #------------------------ Iteration 71 --------------------------#
[32m[20221208 14:17:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |           0.0131 |           0.9565 |           0.3284 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0015 |           0.9066 |           0.3269 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0121 |           0.9152 |           0.3272 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0168 |           0.8975 |           0.3266 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0229 |           0.9013 |           0.3272 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0299 |           0.8938 |           0.3269 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0284 |           0.8988 |           0.3271 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0294 |           0.8907 |           0.3270 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0355 |           0.8840 |           0.3275 |
[32m[20221208 14:17:55 @agent_ppo2.py:179][0m |          -0.0338 |           0.8758 |           0.3275 |
[32m[20221208 14:17:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.10
[32m[20221208 14:17:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.21
[32m[20221208 14:17:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.59
[32m[20221208 14:17:56 @agent_ppo2.py:137][0m Total time:       1.85 min
[32m[20221208 14:17:56 @agent_ppo2.py:139][0m 147456 total steps have happened
[32m[20221208 14:17:56 @agent_ppo2.py:115][0m #------------------------ Iteration 72 --------------------------#
[32m[20221208 14:17:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:56 @agent_ppo2.py:179][0m |           0.0079 |           1.0705 |           0.3196 |
[32m[20221208 14:17:56 @agent_ppo2.py:179][0m |          -0.0101 |           0.9782 |           0.3185 |
[32m[20221208 14:17:56 @agent_ppo2.py:179][0m |          -0.0203 |           0.9598 |           0.3195 |
[32m[20221208 14:17:56 @agent_ppo2.py:179][0m |          -0.0195 |           0.9460 |           0.3208 |
[32m[20221208 14:17:56 @agent_ppo2.py:179][0m |          -0.0290 |           0.9010 |           0.3203 |
[32m[20221208 14:17:56 @agent_ppo2.py:179][0m |          -0.0314 |           0.8895 |           0.3210 |
[32m[20221208 14:17:57 @agent_ppo2.py:179][0m |          -0.0348 |           0.8829 |           0.3213 |
[32m[20221208 14:17:57 @agent_ppo2.py:179][0m |          -0.0345 |           0.8712 |           0.3206 |
[32m[20221208 14:17:57 @agent_ppo2.py:179][0m |          -0.0406 |           0.8633 |           0.3212 |
[32m[20221208 14:17:57 @agent_ppo2.py:179][0m |          -0.0402 |           0.8519 |           0.3219 |
[32m[20221208 14:17:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:17:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.24
[32m[20221208 14:17:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.03
[32m[20221208 14:17:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.26
[32m[20221208 14:17:57 @agent_ppo2.py:137][0m Total time:       1.87 min
[32m[20221208 14:17:57 @agent_ppo2.py:139][0m 149504 total steps have happened
[32m[20221208 14:17:57 @agent_ppo2.py:115][0m #------------------------ Iteration 73 --------------------------#
[32m[20221208 14:17:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:17:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |           0.0067 |           0.9424 |           0.3203 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0082 |           0.8927 |           0.3190 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0125 |           0.8764 |           0.3201 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0207 |           0.8550 |           0.3197 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0241 |           0.8489 |           0.3192 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0286 |           0.8395 |           0.3203 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0319 |           0.8371 |           0.3203 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0304 |           0.8347 |           0.3206 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0361 |           0.8296 |           0.3213 |
[32m[20221208 14:17:58 @agent_ppo2.py:179][0m |          -0.0367 |           0.8273 |           0.3220 |
[32m[20221208 14:17:58 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:17:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.81
[32m[20221208 14:17:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.99
[32m[20221208 14:17:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.08
[32m[20221208 14:17:59 @agent_ppo2.py:137][0m Total time:       1.90 min
[32m[20221208 14:17:59 @agent_ppo2.py:139][0m 151552 total steps have happened
[32m[20221208 14:17:59 @agent_ppo2.py:115][0m #------------------------ Iteration 74 --------------------------#
[32m[20221208 14:17:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:17:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:17:59 @agent_ppo2.py:179][0m |           0.0041 |           0.5525 |           0.3340 |
[32m[20221208 14:17:59 @agent_ppo2.py:179][0m |          -0.0103 |           0.5386 |           0.3319 |
[32m[20221208 14:17:59 @agent_ppo2.py:179][0m |          -0.0154 |           0.5268 |           0.3327 |
[32m[20221208 14:17:59 @agent_ppo2.py:179][0m |          -0.0223 |           0.5270 |           0.3339 |
[32m[20221208 14:17:59 @agent_ppo2.py:179][0m |          -0.0271 |           0.5212 |           0.3339 |
[32m[20221208 14:18:00 @agent_ppo2.py:179][0m |          -0.0319 |           0.5094 |           0.3347 |
[32m[20221208 14:18:00 @agent_ppo2.py:179][0m |          -0.0342 |           0.5131 |           0.3350 |
[32m[20221208 14:18:00 @agent_ppo2.py:179][0m |          -0.0342 |           0.5052 |           0.3351 |
[32m[20221208 14:18:00 @agent_ppo2.py:179][0m |          -0.0373 |           0.5053 |           0.3344 |
[32m[20221208 14:18:00 @agent_ppo2.py:179][0m |          -0.0393 |           0.5009 |           0.3356 |
[32m[20221208 14:18:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:18:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.76
[32m[20221208 14:18:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.53
[32m[20221208 14:18:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.46
[32m[20221208 14:18:00 @agent_ppo2.py:137][0m Total time:       1.92 min
[32m[20221208 14:18:00 @agent_ppo2.py:139][0m 153600 total steps have happened
[32m[20221208 14:18:00 @agent_ppo2.py:115][0m #------------------------ Iteration 75 --------------------------#
[32m[20221208 14:18:01 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:18:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |           0.0079 |           0.7709 |           0.3235 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0100 |           0.7236 |           0.3247 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0163 |           0.6921 |           0.3244 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0221 |           0.6731 |           0.3256 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0253 |           0.6667 |           0.3253 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0281 |           0.6631 |           0.3257 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0302 |           0.6584 |           0.3267 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0324 |           0.6494 |           0.3255 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0367 |           0.6472 |           0.3272 |
[32m[20221208 14:18:01 @agent_ppo2.py:179][0m |          -0.0383 |           0.6409 |           0.3258 |
[32m[20221208 14:18:01 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:18:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.92
[32m[20221208 14:18:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.67
[32m[20221208 14:18:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.23
[32m[20221208 14:18:02 @agent_ppo2.py:137][0m Total time:       1.95 min
[32m[20221208 14:18:02 @agent_ppo2.py:139][0m 155648 total steps have happened
[32m[20221208 14:18:02 @agent_ppo2.py:115][0m #------------------------ Iteration 76 --------------------------#
[32m[20221208 14:18:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:02 @agent_ppo2.py:179][0m |           0.0107 |           1.2156 |           0.3331 |
[32m[20221208 14:18:02 @agent_ppo2.py:179][0m |          -0.0085 |           1.1577 |           0.3336 |
[32m[20221208 14:18:02 @agent_ppo2.py:179][0m |          -0.0126 |           1.1260 |           0.3317 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0233 |           1.1125 |           0.3343 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0289 |           1.1011 |           0.3339 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0296 |           1.0907 |           0.3340 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0330 |           1.0847 |           0.3328 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0345 |           1.0781 |           0.3334 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0378 |           1.0796 |           0.3328 |
[32m[20221208 14:18:03 @agent_ppo2.py:179][0m |          -0.0408 |           1.0654 |           0.3343 |
[32m[20221208 14:18:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:18:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.64
[32m[20221208 14:18:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.67
[32m[20221208 14:18:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.63
[32m[20221208 14:18:03 @agent_ppo2.py:137][0m Total time:       1.97 min
[32m[20221208 14:18:03 @agent_ppo2.py:139][0m 157696 total steps have happened
[32m[20221208 14:18:03 @agent_ppo2.py:115][0m #------------------------ Iteration 77 --------------------------#
[32m[20221208 14:18:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |           0.0064 |           1.5606 |           0.3554 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0124 |           1.4090 |           0.3562 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0202 |           1.3976 |           0.3556 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0254 |           1.3586 |           0.3575 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0307 |           1.3449 |           0.3592 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0347 |           1.3513 |           0.3593 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0375 |           1.3263 |           0.3597 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0406 |           1.3016 |           0.3612 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0429 |           1.3042 |           0.3611 |
[32m[20221208 14:18:04 @agent_ppo2.py:179][0m |          -0.0440 |           1.2893 |           0.3609 |
[32m[20221208 14:18:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.54
[32m[20221208 14:18:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.79
[32m[20221208 14:18:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.32
[32m[20221208 14:18:05 @agent_ppo2.py:137][0m Total time:       2.00 min
[32m[20221208 14:18:05 @agent_ppo2.py:139][0m 159744 total steps have happened
[32m[20221208 14:18:05 @agent_ppo2.py:115][0m #------------------------ Iteration 78 --------------------------#
[32m[20221208 14:18:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:05 @agent_ppo2.py:179][0m |           0.0000 |           1.3339 |           0.3449 |
[32m[20221208 14:18:05 @agent_ppo2.py:179][0m |          -0.0137 |           1.2532 |           0.3440 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0240 |           1.2068 |           0.3433 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0280 |           1.1764 |           0.3440 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0313 |           1.1449 |           0.3438 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0358 |           1.1187 |           0.3447 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0376 |           1.0994 |           0.3452 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0397 |           1.0901 |           0.3464 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0404 |           1.0661 |           0.3460 |
[32m[20221208 14:18:06 @agent_ppo2.py:179][0m |          -0.0431 |           1.0501 |           0.3468 |
[32m[20221208 14:18:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.88
[32m[20221208 14:18:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.95
[32m[20221208 14:18:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.45
[32m[20221208 14:18:06 @agent_ppo2.py:137][0m Total time:       2.02 min
[32m[20221208 14:18:06 @agent_ppo2.py:139][0m 161792 total steps have happened
[32m[20221208 14:18:06 @agent_ppo2.py:115][0m #------------------------ Iteration 79 --------------------------#
[32m[20221208 14:18:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |           0.0067 |           1.3444 |           0.3601 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0097 |           1.2535 |           0.3594 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0179 |           1.2489 |           0.3595 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0253 |           1.2187 |           0.3579 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0299 |           1.2142 |           0.3604 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0329 |           1.1987 |           0.3600 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0362 |           1.1866 |           0.3601 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0372 |           1.1998 |           0.3609 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0400 |           1.1852 |           0.3609 |
[32m[20221208 14:18:07 @agent_ppo2.py:179][0m |          -0.0436 |           1.1894 |           0.3611 |
[32m[20221208 14:18:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.41
[32m[20221208 14:18:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.25
[32m[20221208 14:18:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.94
[32m[20221208 14:18:08 @agent_ppo2.py:137][0m Total time:       2.05 min
[32m[20221208 14:18:08 @agent_ppo2.py:139][0m 163840 total steps have happened
[32m[20221208 14:18:08 @agent_ppo2.py:115][0m #------------------------ Iteration 80 --------------------------#
[32m[20221208 14:18:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:08 @agent_ppo2.py:179][0m |           0.0138 |           1.5701 |           0.3407 |
[32m[20221208 14:18:08 @agent_ppo2.py:179][0m |          -0.0132 |           1.4850 |           0.3424 |
[32m[20221208 14:18:08 @agent_ppo2.py:179][0m |          -0.0195 |           1.4436 |           0.3418 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0243 |           1.4128 |           0.3415 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0305 |           1.3919 |           0.3420 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0337 |           1.3686 |           0.3432 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0360 |           1.3649 |           0.3432 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0381 |           1.3701 |           0.3429 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0400 |           1.3322 |           0.3434 |
[32m[20221208 14:18:09 @agent_ppo2.py:179][0m |          -0.0430 |           1.3273 |           0.3430 |
[32m[20221208 14:18:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:18:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.26
[32m[20221208 14:18:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.97
[32m[20221208 14:18:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.19
[32m[20221208 14:18:09 @agent_ppo2.py:137][0m Total time:       2.07 min
[32m[20221208 14:18:09 @agent_ppo2.py:139][0m 165888 total steps have happened
[32m[20221208 14:18:09 @agent_ppo2.py:115][0m #------------------------ Iteration 81 --------------------------#
[32m[20221208 14:18:10 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:18:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |           0.0052 |           0.7734 |           0.3476 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0060 |           0.7242 |           0.3457 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0152 |           0.7016 |           0.3451 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0164 |           0.6924 |           0.3438 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0223 |           0.6792 |           0.3446 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0240 |           0.6769 |           0.3443 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0270 |           0.6728 |           0.3439 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0282 |           0.6606 |           0.3435 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0287 |           0.6582 |           0.3432 |
[32m[20221208 14:18:10 @agent_ppo2.py:179][0m |          -0.0346 |           0.6539 |           0.3434 |
[32m[20221208 14:18:10 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.28
[32m[20221208 14:18:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.49
[32m[20221208 14:18:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.81
[32m[20221208 14:18:11 @agent_ppo2.py:137][0m Total time:       2.10 min
[32m[20221208 14:18:11 @agent_ppo2.py:139][0m 167936 total steps have happened
[32m[20221208 14:18:11 @agent_ppo2.py:115][0m #------------------------ Iteration 82 --------------------------#
[32m[20221208 14:18:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:11 @agent_ppo2.py:179][0m |           0.0058 |           1.7074 |           0.3380 |
[32m[20221208 14:18:11 @agent_ppo2.py:179][0m |          -0.0121 |           1.5926 |           0.3378 |
[32m[20221208 14:18:11 @agent_ppo2.py:179][0m |          -0.0216 |           1.5588 |           0.3368 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0284 |           1.5140 |           0.3380 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0302 |           1.4974 |           0.3380 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0343 |           1.4887 |           0.3377 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0372 |           1.4906 |           0.3379 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0394 |           1.4637 |           0.3396 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0425 |           1.4394 |           0.3400 |
[32m[20221208 14:18:12 @agent_ppo2.py:179][0m |          -0.0422 |           1.4447 |           0.3397 |
[32m[20221208 14:18:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.22
[32m[20221208 14:18:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.20
[32m[20221208 14:18:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.23
[32m[20221208 14:18:12 @agent_ppo2.py:137][0m Total time:       2.12 min
[32m[20221208 14:18:12 @agent_ppo2.py:139][0m 169984 total steps have happened
[32m[20221208 14:18:12 @agent_ppo2.py:115][0m #------------------------ Iteration 83 --------------------------#
[32m[20221208 14:18:13 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:18:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |           0.0062 |           1.0387 |           0.3536 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0024 |           1.0013 |           0.3518 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0130 |           0.9811 |           0.3521 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0153 |           0.9738 |           0.3539 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0178 |           0.9634 |           0.3532 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0227 |           0.9509 |           0.3539 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0179 |           0.9504 |           0.3537 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0222 |           0.9526 |           0.3544 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0269 |           0.9548 |           0.3546 |
[32m[20221208 14:18:13 @agent_ppo2.py:179][0m |          -0.0277 |           0.9303 |           0.3550 |
[32m[20221208 14:18:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.04
[32m[20221208 14:18:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.88
[32m[20221208 14:18:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.86
[32m[20221208 14:18:14 @agent_ppo2.py:137][0m Total time:       2.15 min
[32m[20221208 14:18:14 @agent_ppo2.py:139][0m 172032 total steps have happened
[32m[20221208 14:18:14 @agent_ppo2.py:115][0m #------------------------ Iteration 84 --------------------------#
[32m[20221208 14:18:14 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:18:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:14 @agent_ppo2.py:179][0m |           0.0070 |           0.7830 |           0.3390 |
[32m[20221208 14:18:14 @agent_ppo2.py:179][0m |          -0.0103 |           0.7161 |           0.3394 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0191 |           0.6938 |           0.3366 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0260 |           0.6921 |           0.3371 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0304 |           0.6826 |           0.3365 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0333 |           0.6705 |           0.3369 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0383 |           0.6659 |           0.3364 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0385 |           0.6616 |           0.3361 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0407 |           0.6602 |           0.3359 |
[32m[20221208 14:18:15 @agent_ppo2.py:179][0m |          -0.0406 |           0.6521 |           0.3358 |
[32m[20221208 14:18:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.68
[32m[20221208 14:18:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.88
[32m[20221208 14:18:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.73
[32m[20221208 14:18:15 @agent_ppo2.py:137][0m Total time:       2.17 min
[32m[20221208 14:18:15 @agent_ppo2.py:139][0m 174080 total steps have happened
[32m[20221208 14:18:15 @agent_ppo2.py:115][0m #------------------------ Iteration 85 --------------------------#
[32m[20221208 14:18:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |           0.0056 |           1.4614 |           0.3454 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0148 |           1.4189 |           0.3471 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0171 |           1.3778 |           0.3464 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0235 |           1.3509 |           0.3481 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0269 |           1.3522 |           0.3477 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0309 |           1.3325 |           0.3485 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0328 |           1.3166 |           0.3505 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0334 |           1.3024 |           0.3503 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0346 |           1.3026 |           0.3507 |
[32m[20221208 14:18:16 @agent_ppo2.py:179][0m |          -0.0374 |           1.2894 |           0.3509 |
[32m[20221208 14:18:16 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:18:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.30
[32m[20221208 14:18:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.43
[32m[20221208 14:18:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.39
[32m[20221208 14:18:17 @agent_ppo2.py:137][0m Total time:       2.20 min
[32m[20221208 14:18:17 @agent_ppo2.py:139][0m 176128 total steps have happened
[32m[20221208 14:18:17 @agent_ppo2.py:115][0m #------------------------ Iteration 86 --------------------------#
[32m[20221208 14:18:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:17 @agent_ppo2.py:179][0m |           0.0086 |           0.6203 |           0.3473 |
[32m[20221208 14:18:17 @agent_ppo2.py:179][0m |          -0.0047 |           0.5688 |           0.3456 |
[32m[20221208 14:18:17 @agent_ppo2.py:179][0m |          -0.0199 |           0.5558 |           0.3457 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0209 |           0.5515 |           0.3447 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0248 |           0.5437 |           0.3461 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0284 |           0.5444 |           0.3450 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0304 |           0.5418 |           0.3466 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0357 |           0.5385 |           0.3465 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0349 |           0.5264 |           0.3463 |
[32m[20221208 14:18:18 @agent_ppo2.py:179][0m |          -0.0360 |           0.5299 |           0.3468 |
[32m[20221208 14:18:18 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:18:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.12
[32m[20221208 14:18:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.30
[32m[20221208 14:18:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.69
[32m[20221208 14:18:18 @agent_ppo2.py:137][0m Total time:       2.22 min
[32m[20221208 14:18:18 @agent_ppo2.py:139][0m 178176 total steps have happened
[32m[20221208 14:18:18 @agent_ppo2.py:115][0m #------------------------ Iteration 87 --------------------------#
[32m[20221208 14:18:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:18:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |           0.0082 |           1.2193 |           0.3440 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0126 |           1.1479 |           0.3462 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0213 |           1.1181 |           0.3444 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0256 |           1.1247 |           0.3436 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0330 |           1.1013 |           0.3440 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0382 |           1.0794 |           0.3451 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0389 |           1.0724 |           0.3449 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0416 |           1.0762 |           0.3453 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0439 |           1.0720 |           0.3446 |
[32m[20221208 14:18:19 @agent_ppo2.py:179][0m |          -0.0474 |           1.0578 |           0.3469 |
[32m[20221208 14:18:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.63
[32m[20221208 14:18:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.63
[32m[20221208 14:18:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.85
[32m[20221208 14:18:20 @agent_ppo2.py:137][0m Total time:       2.25 min
[32m[20221208 14:18:20 @agent_ppo2.py:139][0m 180224 total steps have happened
[32m[20221208 14:18:20 @agent_ppo2.py:115][0m #------------------------ Iteration 88 --------------------------#
[32m[20221208 14:18:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:20 @agent_ppo2.py:179][0m |           0.0185 |           1.5363 |           0.3605 |
[32m[20221208 14:18:20 @agent_ppo2.py:179][0m |          -0.0052 |           1.4658 |           0.3591 |
[32m[20221208 14:18:20 @agent_ppo2.py:179][0m |          -0.0167 |           1.4246 |           0.3617 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0266 |           1.4055 |           0.3634 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0312 |           1.3805 |           0.3629 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0353 |           1.3554 |           0.3631 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0407 |           1.3398 |           0.3636 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0412 |           1.3199 |           0.3646 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0433 |           1.3256 |           0.3639 |
[32m[20221208 14:18:21 @agent_ppo2.py:179][0m |          -0.0475 |           1.3048 |           0.3650 |
[32m[20221208 14:18:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.97
[32m[20221208 14:18:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.73
[32m[20221208 14:18:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.86
[32m[20221208 14:18:21 @agent_ppo2.py:137][0m Total time:       2.27 min
[32m[20221208 14:18:21 @agent_ppo2.py:139][0m 182272 total steps have happened
[32m[20221208 14:18:21 @agent_ppo2.py:115][0m #------------------------ Iteration 89 --------------------------#
[32m[20221208 14:18:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |           0.0085 |           1.1162 |           0.3455 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0068 |           1.0347 |           0.3432 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0228 |           1.0116 |           0.3447 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0202 |           0.9900 |           0.3440 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0283 |           0.9929 |           0.3435 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0324 |           0.9715 |           0.3460 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0376 |           0.9677 |           0.3464 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0385 |           0.9476 |           0.3461 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0392 |           0.9396 |           0.3466 |
[32m[20221208 14:18:22 @agent_ppo2.py:179][0m |          -0.0422 |           0.9477 |           0.3476 |
[32m[20221208 14:18:22 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:18:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.66
[32m[20221208 14:18:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.69
[32m[20221208 14:18:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.75
[32m[20221208 14:18:23 @agent_ppo2.py:137][0m Total time:       2.30 min
[32m[20221208 14:18:23 @agent_ppo2.py:139][0m 184320 total steps have happened
[32m[20221208 14:18:23 @agent_ppo2.py:115][0m #------------------------ Iteration 90 --------------------------#
[32m[20221208 14:18:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:23 @agent_ppo2.py:179][0m |           0.0072 |           1.1079 |           0.3714 |
[32m[20221208 14:18:23 @agent_ppo2.py:179][0m |          -0.0124 |           1.0607 |           0.3692 |
[32m[20221208 14:18:23 @agent_ppo2.py:179][0m |          -0.0235 |           1.0448 |           0.3699 |
[32m[20221208 14:18:23 @agent_ppo2.py:179][0m |          -0.0290 |           1.0320 |           0.3707 |
[32m[20221208 14:18:24 @agent_ppo2.py:179][0m |          -0.0308 |           1.0358 |           0.3698 |
[32m[20221208 14:18:24 @agent_ppo2.py:179][0m |          -0.0345 |           1.0257 |           0.3700 |
[32m[20221208 14:18:24 @agent_ppo2.py:179][0m |          -0.0374 |           1.0121 |           0.3702 |
[32m[20221208 14:18:24 @agent_ppo2.py:179][0m |          -0.0410 |           1.0074 |           0.3709 |
[32m[20221208 14:18:24 @agent_ppo2.py:179][0m |          -0.0433 |           1.0094 |           0.3715 |
[32m[20221208 14:18:24 @agent_ppo2.py:179][0m |          -0.0449 |           0.9966 |           0.3719 |
[32m[20221208 14:18:24 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:18:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.55
[32m[20221208 14:18:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.91
[32m[20221208 14:18:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.46
[32m[20221208 14:18:24 @agent_ppo2.py:137][0m Total time:       2.32 min
[32m[20221208 14:18:24 @agent_ppo2.py:139][0m 186368 total steps have happened
[32m[20221208 14:18:24 @agent_ppo2.py:115][0m #------------------------ Iteration 91 --------------------------#
[32m[20221208 14:18:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |           0.0092 |           1.3604 |           0.3670 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0072 |           1.3234 |           0.3646 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0177 |           1.2980 |           0.3655 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0260 |           1.2918 |           0.3670 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0287 |           1.2864 |           0.3675 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0306 |           1.2654 |           0.3685 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0361 |           1.2573 |           0.3681 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0368 |           1.2589 |           0.3685 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0406 |           1.2510 |           0.3684 |
[32m[20221208 14:18:25 @agent_ppo2.py:179][0m |          -0.0424 |           1.2534 |           0.3688 |
[32m[20221208 14:18:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.74
[32m[20221208 14:18:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.70
[32m[20221208 14:18:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.96
[32m[20221208 14:18:26 @agent_ppo2.py:137][0m Total time:       2.35 min
[32m[20221208 14:18:26 @agent_ppo2.py:139][0m 188416 total steps have happened
[32m[20221208 14:18:26 @agent_ppo2.py:115][0m #------------------------ Iteration 92 --------------------------#
[32m[20221208 14:18:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:26 @agent_ppo2.py:179][0m |           0.0138 |           1.2265 |           0.3722 |
[32m[20221208 14:18:26 @agent_ppo2.py:179][0m |          -0.0062 |           1.1824 |           0.3724 |
[32m[20221208 14:18:26 @agent_ppo2.py:179][0m |          -0.0228 |           1.1573 |           0.3755 |
[32m[20221208 14:18:26 @agent_ppo2.py:179][0m |          -0.0245 |           1.1541 |           0.3766 |
[32m[20221208 14:18:26 @agent_ppo2.py:179][0m |          -0.0323 |           1.1264 |           0.3775 |
[32m[20221208 14:18:27 @agent_ppo2.py:179][0m |          -0.0380 |           1.1117 |           0.3788 |
[32m[20221208 14:18:27 @agent_ppo2.py:179][0m |          -0.0378 |           1.1026 |           0.3791 |
[32m[20221208 14:18:27 @agent_ppo2.py:179][0m |          -0.0418 |           1.0889 |           0.3800 |
[32m[20221208 14:18:27 @agent_ppo2.py:179][0m |          -0.0410 |           1.0849 |           0.3805 |
[32m[20221208 14:18:27 @agent_ppo2.py:179][0m |          -0.0433 |           1.0790 |           0.3807 |
[32m[20221208 14:18:27 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:18:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.73
[32m[20221208 14:18:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.40
[32m[20221208 14:18:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.37
[32m[20221208 14:18:27 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 82.37
[32m[20221208 14:18:27 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 82.37
[32m[20221208 14:18:27 @agent_ppo2.py:137][0m Total time:       2.37 min
[32m[20221208 14:18:27 @agent_ppo2.py:139][0m 190464 total steps have happened
[32m[20221208 14:18:27 @agent_ppo2.py:115][0m #------------------------ Iteration 93 --------------------------#
[32m[20221208 14:18:28 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:18:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |           0.0093 |           0.8668 |           0.3869 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0109 |           0.8026 |           0.3862 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0192 |           0.7825 |           0.3879 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0214 |           0.7754 |           0.3866 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0249 |           0.7591 |           0.3866 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0288 |           0.7533 |           0.3866 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0317 |           0.7415 |           0.3857 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0329 |           0.7359 |           0.3851 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0356 |           0.7281 |           0.3867 |
[32m[20221208 14:18:28 @agent_ppo2.py:179][0m |          -0.0379 |           0.7233 |           0.3873 |
[32m[20221208 14:18:28 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:18:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.09
[32m[20221208 14:18:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.94
[32m[20221208 14:18:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.31
[32m[20221208 14:18:29 @agent_ppo2.py:137][0m Total time:       2.40 min
[32m[20221208 14:18:29 @agent_ppo2.py:139][0m 192512 total steps have happened
[32m[20221208 14:18:29 @agent_ppo2.py:115][0m #------------------------ Iteration 94 --------------------------#
[32m[20221208 14:18:29 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:18:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:29 @agent_ppo2.py:179][0m |           0.0096 |           0.7742 |           0.3777 |
[32m[20221208 14:18:29 @agent_ppo2.py:179][0m |          -0.0092 |           0.6940 |           0.3773 |
[32m[20221208 14:18:29 @agent_ppo2.py:179][0m |          -0.0200 |           0.6614 |           0.3753 |
[32m[20221208 14:18:29 @agent_ppo2.py:179][0m |          -0.0266 |           0.6385 |           0.3757 |
[32m[20221208 14:18:29 @agent_ppo2.py:179][0m |          -0.0288 |           0.6233 |           0.3748 |
[32m[20221208 14:18:30 @agent_ppo2.py:179][0m |          -0.0324 |           0.6096 |           0.3744 |
[32m[20221208 14:18:30 @agent_ppo2.py:179][0m |          -0.0358 |           0.6023 |           0.3751 |
[32m[20221208 14:18:30 @agent_ppo2.py:179][0m |          -0.0378 |           0.5867 |           0.3757 |
[32m[20221208 14:18:30 @agent_ppo2.py:179][0m |          -0.0400 |           0.5791 |           0.3769 |
[32m[20221208 14:18:30 @agent_ppo2.py:179][0m |          -0.0399 |           0.5686 |           0.3759 |
[32m[20221208 14:18:30 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.30
[32m[20221208 14:18:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.74
[32m[20221208 14:18:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.34
[32m[20221208 14:18:30 @agent_ppo2.py:137][0m Total time:       2.42 min
[32m[20221208 14:18:30 @agent_ppo2.py:139][0m 194560 total steps have happened
[32m[20221208 14:18:30 @agent_ppo2.py:115][0m #------------------------ Iteration 95 --------------------------#
[32m[20221208 14:18:31 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:18:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |           0.0138 |           1.0064 |           0.3588 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0117 |           0.9468 |           0.3556 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0197 |           0.9384 |           0.3537 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0243 |           0.9069 |           0.3545 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0302 |           0.8993 |           0.3544 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0356 |           0.8880 |           0.3549 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0403 |           0.8800 |           0.3557 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0427 |           0.8782 |           0.3549 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0420 |           0.8783 |           0.3544 |
[32m[20221208 14:18:31 @agent_ppo2.py:179][0m |          -0.0468 |           0.8559 |           0.3560 |
[32m[20221208 14:18:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.23
[32m[20221208 14:18:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.20
[32m[20221208 14:18:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.23
[32m[20221208 14:18:32 @agent_ppo2.py:137][0m Total time:       2.45 min
[32m[20221208 14:18:32 @agent_ppo2.py:139][0m 196608 total steps have happened
[32m[20221208 14:18:32 @agent_ppo2.py:115][0m #------------------------ Iteration 96 --------------------------#
[32m[20221208 14:18:32 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:18:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:32 @agent_ppo2.py:179][0m |           0.0062 |           0.6396 |           0.3686 |
[32m[20221208 14:18:32 @agent_ppo2.py:179][0m |          -0.0087 |           0.6057 |           0.3688 |
[32m[20221208 14:18:32 @agent_ppo2.py:179][0m |          -0.0133 |           0.5901 |           0.3690 |
[32m[20221208 14:18:32 @agent_ppo2.py:179][0m |          -0.0196 |           0.5890 |           0.3678 |
[32m[20221208 14:18:32 @agent_ppo2.py:179][0m |          -0.0221 |           0.5800 |           0.3687 |
[32m[20221208 14:18:33 @agent_ppo2.py:179][0m |          -0.0283 |           0.5710 |           0.3689 |
[32m[20221208 14:18:33 @agent_ppo2.py:179][0m |          -0.0291 |           0.5731 |           0.3694 |
[32m[20221208 14:18:33 @agent_ppo2.py:179][0m |          -0.0309 |           0.5634 |           0.3702 |
[32m[20221208 14:18:33 @agent_ppo2.py:179][0m |          -0.0333 |           0.5662 |           0.3709 |
[32m[20221208 14:18:33 @agent_ppo2.py:179][0m |          -0.0327 |           0.5594 |           0.3717 |
[32m[20221208 14:18:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.65
[32m[20221208 14:18:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.14
[32m[20221208 14:18:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.33
[32m[20221208 14:18:33 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 86.33
[32m[20221208 14:18:33 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 86.33
[32m[20221208 14:18:33 @agent_ppo2.py:137][0m Total time:       2.47 min
[32m[20221208 14:18:33 @agent_ppo2.py:139][0m 198656 total steps have happened
[32m[20221208 14:18:33 @agent_ppo2.py:115][0m #------------------------ Iteration 97 --------------------------#
[32m[20221208 14:18:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |           0.0080 |           0.8100 |           0.3714 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0099 |           0.7586 |           0.3684 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0201 |           0.7382 |           0.3709 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0276 |           0.7222 |           0.3704 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0332 |           0.7098 |           0.3698 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0359 |           0.6974 |           0.3715 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0373 |           0.6890 |           0.3715 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0404 |           0.6833 |           0.3716 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0436 |           0.6778 |           0.3712 |
[32m[20221208 14:18:34 @agent_ppo2.py:179][0m |          -0.0448 |           0.6740 |           0.3726 |
[32m[20221208 14:18:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.78
[32m[20221208 14:18:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.48
[32m[20221208 14:18:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.72
[32m[20221208 14:18:35 @agent_ppo2.py:137][0m Total time:       2.50 min
[32m[20221208 14:18:35 @agent_ppo2.py:139][0m 200704 total steps have happened
[32m[20221208 14:18:35 @agent_ppo2.py:115][0m #------------------------ Iteration 98 --------------------------#
[32m[20221208 14:18:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:35 @agent_ppo2.py:179][0m |           0.0084 |           1.6671 |           0.3830 |
[32m[20221208 14:18:35 @agent_ppo2.py:179][0m |          -0.0154 |           1.5150 |           0.3821 |
[32m[20221208 14:18:35 @agent_ppo2.py:179][0m |          -0.0215 |           1.4411 |           0.3828 |
[32m[20221208 14:18:35 @agent_ppo2.py:179][0m |          -0.0285 |           1.4205 |           0.3839 |
[32m[20221208 14:18:35 @agent_ppo2.py:179][0m |          -0.0311 |           1.3999 |           0.3863 |
[32m[20221208 14:18:36 @agent_ppo2.py:179][0m |          -0.0362 |           1.3561 |           0.3855 |
[32m[20221208 14:18:36 @agent_ppo2.py:179][0m |          -0.0394 |           1.3535 |           0.3850 |
[32m[20221208 14:18:36 @agent_ppo2.py:179][0m |          -0.0416 |           1.3244 |           0.3864 |
[32m[20221208 14:18:36 @agent_ppo2.py:179][0m |          -0.0421 |           1.3234 |           0.3868 |
[32m[20221208 14:18:36 @agent_ppo2.py:179][0m |          -0.0442 |           1.3033 |           0.3881 |
[32m[20221208 14:18:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.62
[32m[20221208 14:18:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.35
[32m[20221208 14:18:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.82
[32m[20221208 14:18:36 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 86.82
[32m[20221208 14:18:36 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 86.82
[32m[20221208 14:18:36 @agent_ppo2.py:137][0m Total time:       2.52 min
[32m[20221208 14:18:36 @agent_ppo2.py:139][0m 202752 total steps have happened
[32m[20221208 14:18:36 @agent_ppo2.py:115][0m #------------------------ Iteration 99 --------------------------#
[32m[20221208 14:18:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |           0.0075 |           0.4771 |           0.3955 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0120 |           0.4138 |           0.3938 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0220 |           0.3944 |           0.3924 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0314 |           0.3876 |           0.3917 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0327 |           0.3741 |           0.3927 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0362 |           0.3678 |           0.3912 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0379 |           0.3666 |           0.3896 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0429 |           0.3613 |           0.3905 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0470 |           0.3581 |           0.3902 |
[32m[20221208 14:18:37 @agent_ppo2.py:179][0m |          -0.0466 |           0.3591 |           0.3910 |
[32m[20221208 14:18:37 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 37.23
[32m[20221208 14:18:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.93
[32m[20221208 14:18:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.93
[32m[20221208 14:18:38 @agent_ppo2.py:137][0m Total time:       2.55 min
[32m[20221208 14:18:38 @agent_ppo2.py:139][0m 204800 total steps have happened
[32m[20221208 14:18:38 @agent_ppo2.py:115][0m #------------------------ Iteration 100 --------------------------#
[32m[20221208 14:18:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:38 @agent_ppo2.py:179][0m |           0.0067 |           1.9097 |           0.3734 |
[32m[20221208 14:18:38 @agent_ppo2.py:179][0m |          -0.0145 |           1.8585 |           0.3753 |
[32m[20221208 14:18:38 @agent_ppo2.py:179][0m |          -0.0215 |           1.8127 |           0.3763 |
[32m[20221208 14:18:38 @agent_ppo2.py:179][0m |          -0.0282 |           1.8177 |           0.3770 |
[32m[20221208 14:18:38 @agent_ppo2.py:179][0m |          -0.0332 |           1.7698 |           0.3792 |
[32m[20221208 14:18:39 @agent_ppo2.py:179][0m |          -0.0366 |           1.7546 |           0.3798 |
[32m[20221208 14:18:39 @agent_ppo2.py:179][0m |          -0.0383 |           1.7616 |           0.3803 |
[32m[20221208 14:18:39 @agent_ppo2.py:179][0m |          -0.0414 |           1.7324 |           0.3831 |
[32m[20221208 14:18:39 @agent_ppo2.py:179][0m |          -0.0444 |           1.7261 |           0.3833 |
[32m[20221208 14:18:39 @agent_ppo2.py:179][0m |          -0.0474 |           1.7113 |           0.3834 |
[32m[20221208 14:18:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.45
[32m[20221208 14:18:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.64
[32m[20221208 14:18:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.42
[32m[20221208 14:18:39 @agent_ppo2.py:137][0m Total time:       2.57 min
[32m[20221208 14:18:39 @agent_ppo2.py:139][0m 206848 total steps have happened
[32m[20221208 14:18:39 @agent_ppo2.py:115][0m #------------------------ Iteration 101 --------------------------#
[32m[20221208 14:18:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |           0.0127 |           2.0172 |           0.4063 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0132 |           1.9000 |           0.4056 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0231 |           1.8311 |           0.4089 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0292 |           1.8037 |           0.4083 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0306 |           1.7724 |           0.4112 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0383 |           1.7776 |           0.4106 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0384 |           1.7512 |           0.4126 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0413 |           1.7285 |           0.4120 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0437 |           1.7207 |           0.4134 |
[32m[20221208 14:18:40 @agent_ppo2.py:179][0m |          -0.0454 |           1.6926 |           0.4137 |
[32m[20221208 14:18:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.45
[32m[20221208 14:18:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.23
[32m[20221208 14:18:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.99
[32m[20221208 14:18:41 @agent_ppo2.py:137][0m Total time:       2.60 min
[32m[20221208 14:18:41 @agent_ppo2.py:139][0m 208896 total steps have happened
[32m[20221208 14:18:41 @agent_ppo2.py:115][0m #------------------------ Iteration 102 --------------------------#
[32m[20221208 14:18:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:41 @agent_ppo2.py:179][0m |           0.0065 |           1.3652 |           0.4156 |
[32m[20221208 14:18:41 @agent_ppo2.py:179][0m |          -0.0158 |           1.2276 |           0.4175 |
[32m[20221208 14:18:41 @agent_ppo2.py:179][0m |          -0.0265 |           1.1763 |           0.4169 |
[32m[20221208 14:18:41 @agent_ppo2.py:179][0m |          -0.0329 |           1.1455 |           0.4177 |
[32m[20221208 14:18:41 @agent_ppo2.py:179][0m |          -0.0388 |           1.1343 |           0.4193 |
[32m[20221208 14:18:42 @agent_ppo2.py:179][0m |          -0.0438 |           1.1117 |           0.4196 |
[32m[20221208 14:18:42 @agent_ppo2.py:179][0m |          -0.0447 |           1.0987 |           0.4204 |
[32m[20221208 14:18:42 @agent_ppo2.py:179][0m |          -0.0455 |           1.0905 |           0.4204 |
[32m[20221208 14:18:42 @agent_ppo2.py:179][0m |          -0.0505 |           1.0856 |           0.4224 |
[32m[20221208 14:18:42 @agent_ppo2.py:179][0m |          -0.0527 |           1.0652 |           0.4231 |
[32m[20221208 14:18:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:18:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.87
[32m[20221208 14:18:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.73
[32m[20221208 14:18:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.14
[32m[20221208 14:18:42 @agent_ppo2.py:137][0m Total time:       2.62 min
[32m[20221208 14:18:42 @agent_ppo2.py:139][0m 210944 total steps have happened
[32m[20221208 14:18:42 @agent_ppo2.py:115][0m #------------------------ Iteration 103 --------------------------#
[32m[20221208 14:18:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |           0.0154 |           1.4429 |           0.4252 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0024 |           1.3834 |           0.4247 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0231 |           1.3497 |           0.4277 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0263 |           1.3410 |           0.4272 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0340 |           1.3241 |           0.4298 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0395 |           1.3184 |           0.4288 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0442 |           1.2985 |           0.4312 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0461 |           1.2862 |           0.4300 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0528 |           1.2831 |           0.4322 |
[32m[20221208 14:18:43 @agent_ppo2.py:179][0m |          -0.0540 |           1.2707 |           0.4321 |
[32m[20221208 14:18:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:18:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.37
[32m[20221208 14:18:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.44
[32m[20221208 14:18:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.59
[32m[20221208 14:18:44 @agent_ppo2.py:137][0m Total time:       2.65 min
[32m[20221208 14:18:44 @agent_ppo2.py:139][0m 212992 total steps have happened
[32m[20221208 14:18:44 @agent_ppo2.py:115][0m #------------------------ Iteration 104 --------------------------#
[32m[20221208 14:18:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:44 @agent_ppo2.py:179][0m |           0.0067 |           1.5586 |           0.4320 |
[32m[20221208 14:18:44 @agent_ppo2.py:179][0m |          -0.0163 |           1.3341 |           0.4273 |
[32m[20221208 14:18:44 @agent_ppo2.py:179][0m |          -0.0221 |           1.2573 |           0.4274 |
[32m[20221208 14:18:44 @agent_ppo2.py:179][0m |          -0.0290 |           1.1898 |           0.4292 |
[32m[20221208 14:18:44 @agent_ppo2.py:179][0m |          -0.0340 |           1.1570 |           0.4288 |
[32m[20221208 14:18:45 @agent_ppo2.py:179][0m |          -0.0375 |           1.1254 |           0.4307 |
[32m[20221208 14:18:45 @agent_ppo2.py:179][0m |          -0.0386 |           1.1032 |           0.4289 |
[32m[20221208 14:18:45 @agent_ppo2.py:179][0m |          -0.0431 |           1.0649 |           0.4313 |
[32m[20221208 14:18:45 @agent_ppo2.py:179][0m |          -0.0449 |           1.0610 |           0.4321 |
[32m[20221208 14:18:45 @agent_ppo2.py:179][0m |          -0.0477 |           1.0292 |           0.4328 |
[32m[20221208 14:18:45 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.54
[32m[20221208 14:18:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.82
[32m[20221208 14:18:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.33
[32m[20221208 14:18:45 @agent_ppo2.py:137][0m Total time:       2.67 min
[32m[20221208 14:18:45 @agent_ppo2.py:139][0m 215040 total steps have happened
[32m[20221208 14:18:45 @agent_ppo2.py:115][0m #------------------------ Iteration 105 --------------------------#
[32m[20221208 14:18:46 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:18:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |           0.0103 |           1.7673 |           0.4283 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0087 |           1.7075 |           0.4260 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0195 |           1.6226 |           0.4252 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0294 |           1.6056 |           0.4277 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0330 |           1.5717 |           0.4289 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0381 |           1.5512 |           0.4296 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0403 |           1.5206 |           0.4298 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0438 |           1.5159 |           0.4286 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0463 |           1.5005 |           0.4319 |
[32m[20221208 14:18:46 @agent_ppo2.py:179][0m |          -0.0486 |           1.4806 |           0.4316 |
[32m[20221208 14:18:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.18
[32m[20221208 14:18:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.46
[32m[20221208 14:18:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.45
[32m[20221208 14:18:47 @agent_ppo2.py:137][0m Total time:       2.70 min
[32m[20221208 14:18:47 @agent_ppo2.py:139][0m 217088 total steps have happened
[32m[20221208 14:18:47 @agent_ppo2.py:115][0m #------------------------ Iteration 106 --------------------------#
[32m[20221208 14:18:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:47 @agent_ppo2.py:179][0m |           0.0059 |           1.8445 |           0.4413 |
[32m[20221208 14:18:47 @agent_ppo2.py:179][0m |          -0.0122 |           1.7771 |           0.4376 |
[32m[20221208 14:18:47 @agent_ppo2.py:179][0m |          -0.0231 |           1.7575 |           0.4376 |
[32m[20221208 14:18:47 @agent_ppo2.py:179][0m |          -0.0302 |           1.7421 |           0.4355 |
[32m[20221208 14:18:47 @agent_ppo2.py:179][0m |          -0.0309 |           1.7241 |           0.4360 |
[32m[20221208 14:18:48 @agent_ppo2.py:179][0m |          -0.0356 |           1.7080 |           0.4388 |
[32m[20221208 14:18:48 @agent_ppo2.py:179][0m |          -0.0405 |           1.6969 |           0.4379 |
[32m[20221208 14:18:48 @agent_ppo2.py:179][0m |          -0.0414 |           1.6970 |           0.4393 |
[32m[20221208 14:18:48 @agent_ppo2.py:179][0m |          -0.0458 |           1.6830 |           0.4396 |
[32m[20221208 14:18:48 @agent_ppo2.py:179][0m |          -0.0481 |           1.6819 |           0.4398 |
[32m[20221208 14:18:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.18
[32m[20221208 14:18:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.64
[32m[20221208 14:18:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.56
[32m[20221208 14:18:48 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 114.56
[32m[20221208 14:18:48 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 114.56
[32m[20221208 14:18:48 @agent_ppo2.py:137][0m Total time:       2.72 min
[32m[20221208 14:18:48 @agent_ppo2.py:139][0m 219136 total steps have happened
[32m[20221208 14:18:48 @agent_ppo2.py:115][0m #------------------------ Iteration 107 --------------------------#
[32m[20221208 14:18:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |           0.0075 |           1.9504 |           0.4323 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0170 |           1.7996 |           0.4320 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0280 |           1.7661 |           0.4310 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0311 |           1.7466 |           0.4318 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0364 |           1.7168 |           0.4312 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0394 |           1.7123 |           0.4316 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0397 |           1.7218 |           0.4321 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0433 |           1.6772 |           0.4339 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0484 |           1.6587 |           0.4349 |
[32m[20221208 14:18:49 @agent_ppo2.py:179][0m |          -0.0496 |           1.6576 |           0.4360 |
[32m[20221208 14:18:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.87
[32m[20221208 14:18:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.09
[32m[20221208 14:18:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.51
[32m[20221208 14:18:50 @agent_ppo2.py:137][0m Total time:       2.75 min
[32m[20221208 14:18:50 @agent_ppo2.py:139][0m 221184 total steps have happened
[32m[20221208 14:18:50 @agent_ppo2.py:115][0m #------------------------ Iteration 108 --------------------------#
[32m[20221208 14:18:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:50 @agent_ppo2.py:179][0m |           0.0085 |           2.4265 |           0.4415 |
[32m[20221208 14:18:50 @agent_ppo2.py:179][0m |          -0.0179 |           2.2900 |           0.4389 |
[32m[20221208 14:18:50 @agent_ppo2.py:179][0m |          -0.0279 |           2.2313 |           0.4396 |
[32m[20221208 14:18:50 @agent_ppo2.py:179][0m |          -0.0331 |           2.1963 |           0.4389 |
[32m[20221208 14:18:50 @agent_ppo2.py:179][0m |          -0.0380 |           2.1883 |           0.4427 |
[32m[20221208 14:18:51 @agent_ppo2.py:179][0m |          -0.0428 |           2.1542 |           0.4431 |
[32m[20221208 14:18:51 @agent_ppo2.py:179][0m |          -0.0447 |           2.1393 |           0.4437 |
[32m[20221208 14:18:51 @agent_ppo2.py:179][0m |          -0.0482 |           2.1293 |           0.4441 |
[32m[20221208 14:18:51 @agent_ppo2.py:179][0m |          -0.0511 |           2.1232 |           0.4456 |
[32m[20221208 14:18:51 @agent_ppo2.py:179][0m |          -0.0520 |           2.1012 |           0.4477 |
[32m[20221208 14:18:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.17
[32m[20221208 14:18:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.71
[32m[20221208 14:18:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.94
[32m[20221208 14:18:51 @agent_ppo2.py:137][0m Total time:       2.77 min
[32m[20221208 14:18:51 @agent_ppo2.py:139][0m 223232 total steps have happened
[32m[20221208 14:18:51 @agent_ppo2.py:115][0m #------------------------ Iteration 109 --------------------------#
[32m[20221208 14:18:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |           0.0070 |           1.5322 |           0.4523 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0174 |           1.4080 |           0.4522 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0282 |           1.3570 |           0.4541 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0329 |           1.3410 |           0.4549 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0371 |           1.3185 |           0.4537 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0426 |           1.2944 |           0.4558 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0416 |           1.3038 |           0.4573 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0449 |           1.2683 |           0.4575 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0478 |           1.2636 |           0.4598 |
[32m[20221208 14:18:52 @agent_ppo2.py:179][0m |          -0.0487 |           1.2526 |           0.4600 |
[32m[20221208 14:18:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:18:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.58
[32m[20221208 14:18:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.54
[32m[20221208 14:18:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.87
[32m[20221208 14:18:53 @agent_ppo2.py:137][0m Total time:       2.80 min
[32m[20221208 14:18:53 @agent_ppo2.py:139][0m 225280 total steps have happened
[32m[20221208 14:18:53 @agent_ppo2.py:115][0m #------------------------ Iteration 110 --------------------------#
[32m[20221208 14:18:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:53 @agent_ppo2.py:179][0m |           0.0106 |           1.2668 |           0.4634 |
[32m[20221208 14:18:53 @agent_ppo2.py:179][0m |          -0.0117 |           1.1515 |           0.4590 |
[32m[20221208 14:18:53 @agent_ppo2.py:179][0m |          -0.0233 |           1.1276 |           0.4589 |
[32m[20221208 14:18:53 @agent_ppo2.py:179][0m |          -0.0280 |           1.0845 |           0.4603 |
[32m[20221208 14:18:53 @agent_ppo2.py:179][0m |          -0.0344 |           1.0727 |           0.4599 |
[32m[20221208 14:18:54 @agent_ppo2.py:179][0m |          -0.0420 |           1.0473 |           0.4606 |
[32m[20221208 14:18:54 @agent_ppo2.py:179][0m |          -0.0442 |           1.0598 |           0.4617 |
[32m[20221208 14:18:54 @agent_ppo2.py:179][0m |          -0.0469 |           1.0308 |           0.4632 |
[32m[20221208 14:18:54 @agent_ppo2.py:179][0m |          -0.0506 |           1.0265 |           0.4619 |
[32m[20221208 14:18:54 @agent_ppo2.py:179][0m |          -0.0520 |           1.0285 |           0.4644 |
[32m[20221208 14:18:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.41
[32m[20221208 14:18:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.82
[32m[20221208 14:18:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.81
[32m[20221208 14:18:54 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 115.81
[32m[20221208 14:18:54 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 115.81
[32m[20221208 14:18:54 @agent_ppo2.py:137][0m Total time:       2.82 min
[32m[20221208 14:18:54 @agent_ppo2.py:139][0m 227328 total steps have happened
[32m[20221208 14:18:54 @agent_ppo2.py:115][0m #------------------------ Iteration 111 --------------------------#
[32m[20221208 14:18:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |           0.0084 |           1.0893 |           0.4655 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0134 |           0.9905 |           0.4623 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0200 |           0.9699 |           0.4636 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0282 |           0.9351 |           0.4622 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0335 |           0.9162 |           0.4640 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0372 |           0.8946 |           0.4648 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0402 |           0.8895 |           0.4651 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0421 |           0.8755 |           0.4657 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0430 |           0.8646 |           0.4668 |
[32m[20221208 14:18:55 @agent_ppo2.py:179][0m |          -0.0479 |           0.8616 |           0.4661 |
[32m[20221208 14:18:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:18:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.43
[32m[20221208 14:18:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.35
[32m[20221208 14:18:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.74
[32m[20221208 14:18:56 @agent_ppo2.py:137][0m Total time:       2.85 min
[32m[20221208 14:18:56 @agent_ppo2.py:139][0m 229376 total steps have happened
[32m[20221208 14:18:56 @agent_ppo2.py:115][0m #------------------------ Iteration 112 --------------------------#
[32m[20221208 14:18:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:18:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:56 @agent_ppo2.py:179][0m |           0.0060 |           2.4187 |           0.4681 |
[32m[20221208 14:18:56 @agent_ppo2.py:179][0m |          -0.0153 |           2.2669 |           0.4638 |
[32m[20221208 14:18:56 @agent_ppo2.py:179][0m |          -0.0238 |           2.1912 |           0.4647 |
[32m[20221208 14:18:56 @agent_ppo2.py:179][0m |          -0.0329 |           2.1595 |           0.4630 |
[32m[20221208 14:18:56 @agent_ppo2.py:179][0m |          -0.0411 |           2.1190 |           0.4630 |
[32m[20221208 14:18:56 @agent_ppo2.py:179][0m |          -0.0451 |           2.0840 |           0.4638 |
[32m[20221208 14:18:57 @agent_ppo2.py:179][0m |          -0.0471 |           2.0453 |           0.4662 |
[32m[20221208 14:18:57 @agent_ppo2.py:179][0m |          -0.0510 |           2.0260 |           0.4665 |
[32m[20221208 14:18:57 @agent_ppo2.py:179][0m |          -0.0527 |           2.0337 |           0.4670 |
[32m[20221208 14:18:57 @agent_ppo2.py:179][0m |          -0.0561 |           1.9866 |           0.4683 |
[32m[20221208 14:18:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.11
[32m[20221208 14:18:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.46
[32m[20221208 14:18:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.51
[32m[20221208 14:18:57 @agent_ppo2.py:137][0m Total time:       2.87 min
[32m[20221208 14:18:57 @agent_ppo2.py:139][0m 231424 total steps have happened
[32m[20221208 14:18:57 @agent_ppo2.py:115][0m #------------------------ Iteration 113 --------------------------#
[32m[20221208 14:18:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:18:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |           0.0151 |           2.9943 |           0.4804 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0147 |           2.7819 |           0.4767 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0230 |           2.6922 |           0.4771 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0320 |           2.6388 |           0.4781 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0372 |           2.6244 |           0.4802 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0433 |           2.5761 |           0.4793 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0473 |           2.5374 |           0.4804 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0512 |           2.5304 |           0.4828 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0546 |           2.4897 |           0.4815 |
[32m[20221208 14:18:58 @agent_ppo2.py:179][0m |          -0.0568 |           2.4991 |           0.4840 |
[32m[20221208 14:18:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:18:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.79
[32m[20221208 14:18:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.64
[32m[20221208 14:18:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.40
[32m[20221208 14:18:59 @agent_ppo2.py:137][0m Total time:       2.90 min
[32m[20221208 14:18:59 @agent_ppo2.py:139][0m 233472 total steps have happened
[32m[20221208 14:18:59 @agent_ppo2.py:115][0m #------------------------ Iteration 114 --------------------------#
[32m[20221208 14:18:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:18:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:18:59 @agent_ppo2.py:179][0m |           0.0130 |           2.5953 |           0.4633 |
[32m[20221208 14:18:59 @agent_ppo2.py:179][0m |          -0.0151 |           2.3745 |           0.4607 |
[32m[20221208 14:18:59 @agent_ppo2.py:179][0m |          -0.0234 |           2.3074 |           0.4600 |
[32m[20221208 14:18:59 @agent_ppo2.py:179][0m |          -0.0323 |           2.2475 |           0.4617 |
[32m[20221208 14:18:59 @agent_ppo2.py:179][0m |          -0.0405 |           2.2281 |           0.4635 |
[32m[20221208 14:18:59 @agent_ppo2.py:179][0m |          -0.0427 |           2.2016 |           0.4644 |
[32m[20221208 14:19:00 @agent_ppo2.py:179][0m |          -0.0459 |           2.1783 |           0.4659 |
[32m[20221208 14:19:00 @agent_ppo2.py:179][0m |          -0.0469 |           2.1673 |           0.4660 |
[32m[20221208 14:19:00 @agent_ppo2.py:179][0m |          -0.0498 |           2.1386 |           0.4670 |
[32m[20221208 14:19:00 @agent_ppo2.py:179][0m |          -0.0522 |           2.1288 |           0.4672 |
[32m[20221208 14:19:00 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.66
[32m[20221208 14:19:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.94
[32m[20221208 14:19:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.26
[32m[20221208 14:19:00 @agent_ppo2.py:137][0m Total time:       2.92 min
[32m[20221208 14:19:00 @agent_ppo2.py:139][0m 235520 total steps have happened
[32m[20221208 14:19:00 @agent_ppo2.py:115][0m #------------------------ Iteration 115 --------------------------#
[32m[20221208 14:19:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |           0.0137 |           2.4545 |           0.4715 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0045 |           2.2524 |           0.4662 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0227 |           2.1885 |           0.4673 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0342 |           2.1314 |           0.4663 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0377 |           2.1411 |           0.4664 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0455 |           2.0755 |           0.4665 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0460 |           2.0473 |           0.4680 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0485 |           2.0201 |           0.4682 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0545 |           2.0101 |           0.4690 |
[32m[20221208 14:19:01 @agent_ppo2.py:179][0m |          -0.0563 |           1.9906 |           0.4693 |
[32m[20221208 14:19:01 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.34
[32m[20221208 14:19:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.26
[32m[20221208 14:19:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.05
[32m[20221208 14:19:02 @agent_ppo2.py:137][0m Total time:       2.94 min
[32m[20221208 14:19:02 @agent_ppo2.py:139][0m 237568 total steps have happened
[32m[20221208 14:19:02 @agent_ppo2.py:115][0m #------------------------ Iteration 116 --------------------------#
[32m[20221208 14:19:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:19:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:02 @agent_ppo2.py:179][0m |           0.0136 |           2.3574 |           0.4979 |
[32m[20221208 14:19:02 @agent_ppo2.py:179][0m |          -0.0061 |           2.2121 |           0.4914 |
[32m[20221208 14:19:02 @agent_ppo2.py:179][0m |          -0.0235 |           2.1999 |           0.4946 |
[32m[20221208 14:19:02 @agent_ppo2.py:179][0m |          -0.0345 |           2.1528 |           0.4989 |
[32m[20221208 14:19:02 @agent_ppo2.py:179][0m |          -0.0370 |           2.1591 |           0.4998 |
[32m[20221208 14:19:02 @agent_ppo2.py:179][0m |          -0.0407 |           2.1320 |           0.5019 |
[32m[20221208 14:19:03 @agent_ppo2.py:179][0m |          -0.0450 |           2.1192 |           0.5029 |
[32m[20221208 14:19:03 @agent_ppo2.py:179][0m |          -0.0471 |           2.1154 |           0.5041 |
[32m[20221208 14:19:03 @agent_ppo2.py:179][0m |          -0.0518 |           2.0996 |           0.5066 |
[32m[20221208 14:19:03 @agent_ppo2.py:179][0m |          -0.0508 |           2.0735 |           0.5067 |
[32m[20221208 14:19:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.29
[32m[20221208 14:19:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.13
[32m[20221208 14:19:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.35
[32m[20221208 14:19:03 @agent_ppo2.py:137][0m Total time:       2.97 min
[32m[20221208 14:19:03 @agent_ppo2.py:139][0m 239616 total steps have happened
[32m[20221208 14:19:03 @agent_ppo2.py:115][0m #------------------------ Iteration 117 --------------------------#
[32m[20221208 14:19:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |           0.0137 |           1.4796 |           0.4879 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0094 |           1.3106 |           0.4848 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0273 |           1.2449 |           0.4846 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0385 |           1.1963 |           0.4840 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0429 |           1.1492 |           0.4861 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0507 |           1.1115 |           0.4877 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0501 |           1.0753 |           0.4888 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0540 |           1.0489 |           0.4892 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0568 |           1.0324 |           0.4902 |
[32m[20221208 14:19:04 @agent_ppo2.py:179][0m |          -0.0616 |           1.0094 |           0.4910 |
[32m[20221208 14:19:04 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.40
[32m[20221208 14:19:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.07
[32m[20221208 14:19:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.80
[32m[20221208 14:19:05 @agent_ppo2.py:137][0m Total time:       2.99 min
[32m[20221208 14:19:05 @agent_ppo2.py:139][0m 241664 total steps have happened
[32m[20221208 14:19:05 @agent_ppo2.py:115][0m #------------------------ Iteration 118 --------------------------#
[32m[20221208 14:19:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |           0.0225 |           2.3646 |           0.4996 |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |          -0.0044 |           2.2631 |           0.4982 |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |          -0.0182 |           2.2127 |           0.5025 |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |          -0.0246 |           2.1871 |           0.5033 |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |          -0.0287 |           2.1583 |           0.5064 |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |          -0.0318 |           2.1138 |           0.5068 |
[32m[20221208 14:19:05 @agent_ppo2.py:179][0m |          -0.0328 |           2.0907 |           0.5078 |
[32m[20221208 14:19:06 @agent_ppo2.py:179][0m |          -0.0384 |           2.0699 |           0.5105 |
[32m[20221208 14:19:06 @agent_ppo2.py:179][0m |          -0.0427 |           2.0760 |           0.5121 |
[32m[20221208 14:19:06 @agent_ppo2.py:179][0m |          -0.0412 |           2.0529 |           0.5121 |
[32m[20221208 14:19:06 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.69
[32m[20221208 14:19:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.54
[32m[20221208 14:19:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.13
[32m[20221208 14:19:06 @agent_ppo2.py:137][0m Total time:       3.02 min
[32m[20221208 14:19:06 @agent_ppo2.py:139][0m 243712 total steps have happened
[32m[20221208 14:19:06 @agent_ppo2.py:115][0m #------------------------ Iteration 119 --------------------------#
[32m[20221208 14:19:07 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |           0.0146 |           2.8097 |           0.5298 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0132 |           2.6858 |           0.5260 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0244 |           2.6256 |           0.5283 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0331 |           2.5794 |           0.5306 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0365 |           2.5453 |           0.5316 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0412 |           2.5027 |           0.5337 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0421 |           2.4683 |           0.5342 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0474 |           2.4538 |           0.5373 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0483 |           2.4375 |           0.5370 |
[32m[20221208 14:19:07 @agent_ppo2.py:179][0m |          -0.0495 |           2.4137 |           0.5385 |
[32m[20221208 14:19:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.13
[32m[20221208 14:19:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.77
[32m[20221208 14:19:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.70
[32m[20221208 14:19:08 @agent_ppo2.py:137][0m Total time:       3.04 min
[32m[20221208 14:19:08 @agent_ppo2.py:139][0m 245760 total steps have happened
[32m[20221208 14:19:08 @agent_ppo2.py:115][0m #------------------------ Iteration 120 --------------------------#
[32m[20221208 14:19:08 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:08 @agent_ppo2.py:179][0m |           0.0114 |           0.7676 |           0.5498 |
[32m[20221208 14:19:08 @agent_ppo2.py:179][0m |          -0.0053 |           0.6405 |           0.5468 |
[32m[20221208 14:19:08 @agent_ppo2.py:179][0m |          -0.0136 |           0.6130 |           0.5485 |
[32m[20221208 14:19:08 @agent_ppo2.py:179][0m |          -0.0205 |           0.5897 |           0.5476 |
[32m[20221208 14:19:08 @agent_ppo2.py:179][0m |          -0.0282 |           0.5762 |           0.5483 |
[32m[20221208 14:19:08 @agent_ppo2.py:179][0m |          -0.0292 |           0.5667 |           0.5477 |
[32m[20221208 14:19:09 @agent_ppo2.py:179][0m |          -0.0352 |           0.5643 |           0.5461 |
[32m[20221208 14:19:09 @agent_ppo2.py:179][0m |          -0.0350 |           0.5601 |           0.5491 |
[32m[20221208 14:19:09 @agent_ppo2.py:179][0m |          -0.0376 |           0.5542 |           0.5460 |
[32m[20221208 14:19:09 @agent_ppo2.py:179][0m |          -0.0398 |           0.5487 |           0.5474 |
[32m[20221208 14:19:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:19:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.64
[32m[20221208 14:19:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.02
[32m[20221208 14:19:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.98
[32m[20221208 14:19:09 @agent_ppo2.py:137][0m Total time:       3.07 min
[32m[20221208 14:19:09 @agent_ppo2.py:139][0m 247808 total steps have happened
[32m[20221208 14:19:09 @agent_ppo2.py:115][0m #------------------------ Iteration 121 --------------------------#
[32m[20221208 14:19:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:19:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |           0.0155 |           3.0714 |           0.5233 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0061 |           2.9392 |           0.5176 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0233 |           2.8873 |           0.5205 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0266 |           2.8910 |           0.5216 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0332 |           2.8335 |           0.5250 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0381 |           2.8444 |           0.5259 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0429 |           2.8410 |           0.5281 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0448 |           2.8255 |           0.5288 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0480 |           2.7997 |           0.5297 |
[32m[20221208 14:19:10 @agent_ppo2.py:179][0m |          -0.0504 |           2.7885 |           0.5325 |
[32m[20221208 14:19:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.24
[32m[20221208 14:19:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.94
[32m[20221208 14:19:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.25
[32m[20221208 14:19:11 @agent_ppo2.py:137][0m Total time:       3.09 min
[32m[20221208 14:19:11 @agent_ppo2.py:139][0m 249856 total steps have happened
[32m[20221208 14:19:11 @agent_ppo2.py:115][0m #------------------------ Iteration 122 --------------------------#
[32m[20221208 14:19:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |           0.0105 |           1.6336 |           0.5652 |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |          -0.0153 |           1.5354 |           0.5596 |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |          -0.0261 |           1.5013 |           0.5638 |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |          -0.0331 |           1.4792 |           0.5670 |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |          -0.0344 |           1.4409 |           0.5677 |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |          -0.0412 |           1.4382 |           0.5664 |
[32m[20221208 14:19:11 @agent_ppo2.py:179][0m |          -0.0411 |           1.4154 |           0.5661 |
[32m[20221208 14:19:12 @agent_ppo2.py:179][0m |          -0.0436 |           1.3951 |           0.5670 |
[32m[20221208 14:19:12 @agent_ppo2.py:179][0m |          -0.0482 |           1.3813 |           0.5695 |
[32m[20221208 14:19:12 @agent_ppo2.py:179][0m |          -0.0504 |           1.3699 |           0.5705 |
[32m[20221208 14:19:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.88
[32m[20221208 14:19:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.48
[32m[20221208 14:19:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.09
[32m[20221208 14:19:12 @agent_ppo2.py:137][0m Total time:       3.12 min
[32m[20221208 14:19:12 @agent_ppo2.py:139][0m 251904 total steps have happened
[32m[20221208 14:19:12 @agent_ppo2.py:115][0m #------------------------ Iteration 123 --------------------------#
[32m[20221208 14:19:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |           0.0119 |           2.6228 |           0.5559 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0105 |           2.5346 |           0.5531 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0284 |           2.4489 |           0.5534 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0350 |           2.4023 |           0.5552 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0419 |           2.3378 |           0.5563 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0451 |           2.3066 |           0.5558 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0470 |           2.2458 |           0.5577 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0493 |           2.2354 |           0.5576 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0535 |           2.2132 |           0.5584 |
[32m[20221208 14:19:13 @agent_ppo2.py:179][0m |          -0.0526 |           2.1588 |           0.5588 |
[32m[20221208 14:19:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.00
[32m[20221208 14:19:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.45
[32m[20221208 14:19:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.06
[32m[20221208 14:19:14 @agent_ppo2.py:137][0m Total time:       3.14 min
[32m[20221208 14:19:14 @agent_ppo2.py:139][0m 253952 total steps have happened
[32m[20221208 14:19:14 @agent_ppo2.py:115][0m #------------------------ Iteration 124 --------------------------#
[32m[20221208 14:19:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |           0.0134 |           2.5657 |           0.5808 |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |          -0.0114 |           2.4682 |           0.5799 |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |          -0.0242 |           2.4255 |           0.5818 |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |          -0.0321 |           2.4281 |           0.5844 |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |          -0.0355 |           2.3944 |           0.5839 |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |          -0.0427 |           2.3629 |           0.5855 |
[32m[20221208 14:19:14 @agent_ppo2.py:179][0m |          -0.0445 |           2.3414 |           0.5863 |
[32m[20221208 14:19:15 @agent_ppo2.py:179][0m |          -0.0492 |           2.3271 |           0.5870 |
[32m[20221208 14:19:15 @agent_ppo2.py:179][0m |          -0.0516 |           2.3049 |           0.5888 |
[32m[20221208 14:19:15 @agent_ppo2.py:179][0m |          -0.0548 |           2.3238 |           0.5907 |
[32m[20221208 14:19:15 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:19:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.58
[32m[20221208 14:19:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.98
[32m[20221208 14:19:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.12
[32m[20221208 14:19:15 @agent_ppo2.py:137][0m Total time:       3.17 min
[32m[20221208 14:19:15 @agent_ppo2.py:139][0m 256000 total steps have happened
[32m[20221208 14:19:15 @agent_ppo2.py:115][0m #------------------------ Iteration 125 --------------------------#
[32m[20221208 14:19:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |           0.0126 |           1.9553 |           0.5999 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0131 |           1.7785 |           0.5996 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0224 |           1.6913 |           0.6011 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0325 |           1.6393 |           0.6027 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0357 |           1.5866 |           0.6038 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0404 |           1.5490 |           0.6031 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0437 |           1.5241 |           0.6053 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0467 |           1.4898 |           0.6076 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0486 |           1.4754 |           0.6083 |
[32m[20221208 14:19:16 @agent_ppo2.py:179][0m |          -0.0512 |           1.4478 |           0.6079 |
[32m[20221208 14:19:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.81
[32m[20221208 14:19:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.58
[32m[20221208 14:19:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.23
[32m[20221208 14:19:17 @agent_ppo2.py:137][0m Total time:       3.19 min
[32m[20221208 14:19:17 @agent_ppo2.py:139][0m 258048 total steps have happened
[32m[20221208 14:19:17 @agent_ppo2.py:115][0m #------------------------ Iteration 126 --------------------------#
[32m[20221208 14:19:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |           0.0104 |           2.9249 |           0.6050 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0150 |           2.5008 |           0.6033 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0268 |           2.3419 |           0.6033 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0320 |           2.2379 |           0.6064 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0400 |           2.1517 |           0.6091 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0443 |           2.0864 |           0.6098 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0489 |           2.0457 |           0.6131 |
[32m[20221208 14:19:17 @agent_ppo2.py:179][0m |          -0.0491 |           1.9960 |           0.6138 |
[32m[20221208 14:19:18 @agent_ppo2.py:179][0m |          -0.0510 |           1.9699 |           0.6157 |
[32m[20221208 14:19:18 @agent_ppo2.py:179][0m |          -0.0550 |           1.9279 |           0.6186 |
[32m[20221208 14:19:18 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:19:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.02
[32m[20221208 14:19:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.42
[32m[20221208 14:19:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.80
[32m[20221208 14:19:18 @agent_ppo2.py:137][0m Total time:       3.22 min
[32m[20221208 14:19:18 @agent_ppo2.py:139][0m 260096 total steps have happened
[32m[20221208 14:19:18 @agent_ppo2.py:115][0m #------------------------ Iteration 127 --------------------------#
[32m[20221208 14:19:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |           0.0182 |           2.8304 |           0.6183 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0116 |           2.6234 |           0.6226 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0249 |           2.5648 |           0.6261 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0351 |           2.4931 |           0.6288 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0387 |           2.4588 |           0.6296 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0449 |           2.4730 |           0.6318 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0479 |           2.4164 |           0.6332 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0530 |           2.4055 |           0.6367 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0549 |           2.3839 |           0.6385 |
[32m[20221208 14:19:19 @agent_ppo2.py:179][0m |          -0.0571 |           2.3767 |           0.6372 |
[32m[20221208 14:19:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.00
[32m[20221208 14:19:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.34
[32m[20221208 14:19:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.86
[32m[20221208 14:19:20 @agent_ppo2.py:137][0m Total time:       3.24 min
[32m[20221208 14:19:20 @agent_ppo2.py:139][0m 262144 total steps have happened
[32m[20221208 14:19:20 @agent_ppo2.py:115][0m #------------------------ Iteration 128 --------------------------#
[32m[20221208 14:19:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:19:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |           0.0167 |           3.1309 |           0.6461 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0108 |           2.9758 |           0.6450 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0245 |           2.9227 |           0.6422 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0305 |           2.8840 |           0.6437 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0332 |           2.8338 |           0.6444 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0411 |           2.8100 |           0.6449 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0439 |           2.8273 |           0.6476 |
[32m[20221208 14:19:20 @agent_ppo2.py:179][0m |          -0.0442 |           2.7968 |           0.6458 |
[32m[20221208 14:19:21 @agent_ppo2.py:179][0m |          -0.0490 |           2.7772 |           0.6478 |
[32m[20221208 14:19:21 @agent_ppo2.py:179][0m |          -0.0497 |           2.7785 |           0.6491 |
[32m[20221208 14:19:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.20
[32m[20221208 14:19:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.03
[32m[20221208 14:19:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.19
[32m[20221208 14:19:21 @agent_ppo2.py:137][0m Total time:       3.27 min
[32m[20221208 14:19:21 @agent_ppo2.py:139][0m 264192 total steps have happened
[32m[20221208 14:19:21 @agent_ppo2.py:115][0m #------------------------ Iteration 129 --------------------------#
[32m[20221208 14:19:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:19:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |           0.0175 |           2.6990 |           0.6438 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0119 |           2.5300 |           0.6425 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0244 |           2.4979 |           0.6469 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0290 |           2.4378 |           0.6491 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0368 |           2.4091 |           0.6524 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0407 |           2.3691 |           0.6530 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0460 |           2.3573 |           0.6550 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0528 |           2.3815 |           0.6583 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0546 |           2.3200 |           0.6601 |
[32m[20221208 14:19:22 @agent_ppo2.py:179][0m |          -0.0533 |           2.3083 |           0.6610 |
[32m[20221208 14:19:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.74
[32m[20221208 14:19:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.21
[32m[20221208 14:19:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.96
[32m[20221208 14:19:22 @agent_ppo2.py:137][0m Total time:       3.29 min
[32m[20221208 14:19:22 @agent_ppo2.py:139][0m 266240 total steps have happened
[32m[20221208 14:19:22 @agent_ppo2.py:115][0m #------------------------ Iteration 130 --------------------------#
[32m[20221208 14:19:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:19:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |           0.0131 |           2.4930 |           0.6875 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0121 |           2.3743 |           0.6900 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0242 |           2.3106 |           0.6924 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0324 |           2.2855 |           0.6975 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0389 |           2.2596 |           0.6987 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0428 |           2.2193 |           0.7026 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0453 |           2.2064 |           0.7048 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0469 |           2.1812 |           0.7055 |
[32m[20221208 14:19:23 @agent_ppo2.py:179][0m |          -0.0506 |           2.1406 |           0.7085 |
[32m[20221208 14:19:24 @agent_ppo2.py:179][0m |          -0.0537 |           2.1298 |           0.7086 |
[32m[20221208 14:19:24 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.28
[32m[20221208 14:19:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.60
[32m[20221208 14:19:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.16
[32m[20221208 14:19:24 @agent_ppo2.py:137][0m Total time:       3.32 min
[32m[20221208 14:19:24 @agent_ppo2.py:139][0m 268288 total steps have happened
[32m[20221208 14:19:24 @agent_ppo2.py:115][0m #------------------------ Iteration 131 --------------------------#
[32m[20221208 14:19:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:19:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:24 @agent_ppo2.py:179][0m |           0.0193 |           2.8439 |           0.7245 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0072 |           2.7340 |           0.7189 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0234 |           2.6919 |           0.7298 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0315 |           2.6672 |           0.7323 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0377 |           2.6598 |           0.7344 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0366 |           2.6215 |           0.7341 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0434 |           2.5959 |           0.7362 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0445 |           2.5823 |           0.7365 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0467 |           2.5567 |           0.7380 |
[32m[20221208 14:19:25 @agent_ppo2.py:179][0m |          -0.0498 |           2.5333 |           0.7417 |
[32m[20221208 14:19:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:19:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.81
[32m[20221208 14:19:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.43
[32m[20221208 14:19:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.86
[32m[20221208 14:19:25 @agent_ppo2.py:137][0m Total time:       3.34 min
[32m[20221208 14:19:25 @agent_ppo2.py:139][0m 270336 total steps have happened
[32m[20221208 14:19:25 @agent_ppo2.py:115][0m #------------------------ Iteration 132 --------------------------#
[32m[20221208 14:19:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |           0.0114 |           3.3972 |           0.7324 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0056 |           3.1371 |           0.7357 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0243 |           3.0390 |           0.7378 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0314 |           2.9913 |           0.7418 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0398 |           2.9183 |           0.7427 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0429 |           2.9077 |           0.7459 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0482 |           2.8569 |           0.7477 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0493 |           2.8189 |           0.7480 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0530 |           2.8316 |           0.7512 |
[32m[20221208 14:19:26 @agent_ppo2.py:179][0m |          -0.0548 |           2.8027 |           0.7523 |
[32m[20221208 14:19:26 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.53
[32m[20221208 14:19:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.94
[32m[20221208 14:19:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.27
[32m[20221208 14:19:27 @agent_ppo2.py:137][0m Total time:       3.37 min
[32m[20221208 14:19:27 @agent_ppo2.py:139][0m 272384 total steps have happened
[32m[20221208 14:19:27 @agent_ppo2.py:115][0m #------------------------ Iteration 133 --------------------------#
[32m[20221208 14:19:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:27 @agent_ppo2.py:179][0m |           0.0072 |           3.2056 |           0.7413 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0163 |           3.0870 |           0.7421 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0257 |           2.9936 |           0.7408 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0328 |           2.9581 |           0.7431 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0397 |           2.9155 |           0.7457 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0441 |           2.9086 |           0.7480 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0459 |           2.9014 |           0.7491 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0469 |           2.8700 |           0.7507 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0478 |           2.8455 |           0.7537 |
[32m[20221208 14:19:28 @agent_ppo2.py:179][0m |          -0.0514 |           2.8108 |           0.7565 |
[32m[20221208 14:19:28 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:19:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.88
[32m[20221208 14:19:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.56
[32m[20221208 14:19:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.17
[32m[20221208 14:19:28 @agent_ppo2.py:137][0m Total time:       3.39 min
[32m[20221208 14:19:28 @agent_ppo2.py:139][0m 274432 total steps have happened
[32m[20221208 14:19:28 @agent_ppo2.py:115][0m #------------------------ Iteration 134 --------------------------#
[32m[20221208 14:19:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |           0.0227 |           2.5831 |           0.7593 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0074 |           2.3951 |           0.7584 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0268 |           2.3023 |           0.7582 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0338 |           2.2387 |           0.7593 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0370 |           2.2335 |           0.7585 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0411 |           2.1834 |           0.7592 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0440 |           2.1500 |           0.7629 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0501 |           2.1154 |           0.7636 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0500 |           2.1451 |           0.7646 |
[32m[20221208 14:19:29 @agent_ppo2.py:179][0m |          -0.0539 |           2.1257 |           0.7682 |
[32m[20221208 14:19:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.75
[32m[20221208 14:19:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.50
[32m[20221208 14:19:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.94
[32m[20221208 14:19:30 @agent_ppo2.py:137][0m Total time:       3.42 min
[32m[20221208 14:19:30 @agent_ppo2.py:139][0m 276480 total steps have happened
[32m[20221208 14:19:30 @agent_ppo2.py:115][0m #------------------------ Iteration 135 --------------------------#
[32m[20221208 14:19:30 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:30 @agent_ppo2.py:179][0m |           0.0092 |           2.7450 |           0.8024 |
[32m[20221208 14:19:30 @agent_ppo2.py:179][0m |          -0.0138 |           2.6748 |           0.7940 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0257 |           2.6411 |           0.8008 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0334 |           2.5976 |           0.8066 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0403 |           2.5740 |           0.8116 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0410 |           2.5748 |           0.8124 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0439 |           2.5255 |           0.8129 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0473 |           2.5382 |           0.8185 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0482 |           2.5272 |           0.8195 |
[32m[20221208 14:19:31 @agent_ppo2.py:179][0m |          -0.0509 |           2.4810 |           0.8252 |
[32m[20221208 14:19:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.71
[32m[20221208 14:19:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.66
[32m[20221208 14:19:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.66
[32m[20221208 14:19:31 @agent_ppo2.py:137][0m Total time:       3.44 min
[32m[20221208 14:19:31 @agent_ppo2.py:139][0m 278528 total steps have happened
[32m[20221208 14:19:31 @agent_ppo2.py:115][0m #------------------------ Iteration 136 --------------------------#
[32m[20221208 14:19:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |           0.0081 |           1.9659 |           0.7966 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0161 |           1.8809 |           0.7883 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0277 |           1.8361 |           0.7991 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0320 |           1.8271 |           0.7968 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0384 |           1.8217 |           0.7972 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0420 |           1.7815 |           0.8001 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0414 |           1.7534 |           0.7964 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0475 |           1.7682 |           0.8027 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0509 |           1.7570 |           0.8038 |
[32m[20221208 14:19:32 @agent_ppo2.py:179][0m |          -0.0496 |           1.7333 |           0.8018 |
[32m[20221208 14:19:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:19:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.63
[32m[20221208 14:19:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.06
[32m[20221208 14:19:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.11
[32m[20221208 14:19:33 @agent_ppo2.py:137][0m Total time:       3.47 min
[32m[20221208 14:19:33 @agent_ppo2.py:139][0m 280576 total steps have happened
[32m[20221208 14:19:33 @agent_ppo2.py:115][0m #------------------------ Iteration 137 --------------------------#
[32m[20221208 14:19:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:33 @agent_ppo2.py:179][0m |           0.0120 |           1.7680 |           0.7918 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0064 |           1.6640 |           0.7880 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0161 |           1.6078 |           0.7799 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0269 |           1.5781 |           0.7899 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0371 |           1.5744 |           0.7880 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0381 |           1.5568 |           0.7903 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0432 |           1.5308 |           0.7908 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0450 |           1.5064 |           0.7926 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0469 |           1.4830 |           0.7939 |
[32m[20221208 14:19:34 @agent_ppo2.py:179][0m |          -0.0501 |           1.4703 |           0.7987 |
[32m[20221208 14:19:34 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:19:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.19
[32m[20221208 14:19:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.87
[32m[20221208 14:19:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.77
[32m[20221208 14:19:34 @agent_ppo2.py:137][0m Total time:       3.49 min
[32m[20221208 14:19:34 @agent_ppo2.py:139][0m 282624 total steps have happened
[32m[20221208 14:19:34 @agent_ppo2.py:115][0m #------------------------ Iteration 138 --------------------------#
[32m[20221208 14:19:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |           0.0249 |           1.7809 |           0.7875 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0037 |           1.7024 |           0.7948 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0191 |           1.6748 |           0.8003 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0264 |           1.6560 |           0.8016 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0320 |           1.6186 |           0.8033 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0368 |           1.6042 |           0.8085 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0394 |           1.5984 |           0.8094 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0424 |           1.5815 |           0.8126 |
[32m[20221208 14:19:35 @agent_ppo2.py:179][0m |          -0.0450 |           1.5703 |           0.8163 |
[32m[20221208 14:19:36 @agent_ppo2.py:179][0m |          -0.0472 |           1.5641 |           0.8190 |
[32m[20221208 14:19:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.98
[32m[20221208 14:19:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.08
[32m[20221208 14:19:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.70
[32m[20221208 14:19:36 @agent_ppo2.py:137][0m Total time:       3.52 min
[32m[20221208 14:19:36 @agent_ppo2.py:139][0m 284672 total steps have happened
[32m[20221208 14:19:36 @agent_ppo2.py:115][0m #------------------------ Iteration 139 --------------------------#
[32m[20221208 14:19:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:36 @agent_ppo2.py:179][0m |           0.0130 |           2.4253 |           0.8068 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0117 |           2.2153 |           0.7998 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0242 |           2.1385 |           0.8069 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0282 |           2.0763 |           0.8047 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0375 |           2.0373 |           0.8120 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0402 |           2.0215 |           0.8135 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0459 |           2.0048 |           0.8139 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0468 |           1.9772 |           0.8145 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0494 |           1.9696 |           0.8151 |
[32m[20221208 14:19:37 @agent_ppo2.py:179][0m |          -0.0511 |           1.9467 |           0.8139 |
[32m[20221208 14:19:37 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.17
[32m[20221208 14:19:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.97
[32m[20221208 14:19:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.74
[32m[20221208 14:19:37 @agent_ppo2.py:137][0m Total time:       3.54 min
[32m[20221208 14:19:37 @agent_ppo2.py:139][0m 286720 total steps have happened
[32m[20221208 14:19:37 @agent_ppo2.py:115][0m #------------------------ Iteration 140 --------------------------#
[32m[20221208 14:19:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |           0.0188 |           3.0474 |           0.8864 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0132 |           2.7101 |           0.8871 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0231 |           2.5122 |           0.8943 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0322 |           2.4159 |           0.8934 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0385 |           2.3092 |           0.8924 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0449 |           2.2341 |           0.8975 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0470 |           2.1815 |           0.8976 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0499 |           2.1661 |           0.9029 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0527 |           2.1389 |           0.9055 |
[32m[20221208 14:19:38 @agent_ppo2.py:179][0m |          -0.0548 |           2.1215 |           0.9060 |
[32m[20221208 14:19:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.68
[32m[20221208 14:19:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.49
[32m[20221208 14:19:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.69
[32m[20221208 14:19:39 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 121.69
[32m[20221208 14:19:39 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 121.69
[32m[20221208 14:19:39 @agent_ppo2.py:137][0m Total time:       3.57 min
[32m[20221208 14:19:39 @agent_ppo2.py:139][0m 288768 total steps have happened
[32m[20221208 14:19:39 @agent_ppo2.py:115][0m #------------------------ Iteration 141 --------------------------#
[32m[20221208 14:19:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:39 @agent_ppo2.py:179][0m |           0.0123 |           3.1420 |           0.9104 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0119 |           2.9205 |           0.9102 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0285 |           2.8619 |           0.9112 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0320 |           2.8323 |           0.9121 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0385 |           2.7870 |           0.9110 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0410 |           2.7757 |           0.9164 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0432 |           2.8099 |           0.9155 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0457 |           2.7853 |           0.9174 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0490 |           2.7491 |           0.9201 |
[32m[20221208 14:19:40 @agent_ppo2.py:179][0m |          -0.0511 |           2.7267 |           0.9205 |
[32m[20221208 14:19:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.47
[32m[20221208 14:19:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.35
[32m[20221208 14:19:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.55
[32m[20221208 14:19:40 @agent_ppo2.py:137][0m Total time:       3.59 min
[32m[20221208 14:19:40 @agent_ppo2.py:139][0m 290816 total steps have happened
[32m[20221208 14:19:40 @agent_ppo2.py:115][0m #------------------------ Iteration 142 --------------------------#
[32m[20221208 14:19:41 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |           0.0119 |           0.9403 |           0.8709 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0162 |           0.8133 |           0.8636 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0235 |           0.7641 |           0.8655 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0341 |           0.7379 |           0.8659 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0403 |           0.7251 |           0.8712 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0465 |           0.7024 |           0.8710 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0454 |           0.6980 |           0.8674 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0520 |           0.7003 |           0.8750 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0532 |           0.6810 |           0.8770 |
[32m[20221208 14:19:41 @agent_ppo2.py:179][0m |          -0.0545 |           0.6647 |           0.8726 |
[32m[20221208 14:19:41 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.64
[32m[20221208 14:19:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.85
[32m[20221208 14:19:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.87
[32m[20221208 14:19:42 @agent_ppo2.py:137][0m Total time:       3.62 min
[32m[20221208 14:19:42 @agent_ppo2.py:139][0m 292864 total steps have happened
[32m[20221208 14:19:42 @agent_ppo2.py:115][0m #------------------------ Iteration 143 --------------------------#
[32m[20221208 14:19:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:42 @agent_ppo2.py:179][0m |           0.0108 |           3.1873 |           0.8948 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0151 |           3.1225 |           0.8890 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0280 |           3.0278 |           0.8912 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0337 |           2.9583 |           0.8950 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0393 |           2.9175 |           0.8954 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0447 |           2.8889 |           0.8994 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0468 |           2.8460 |           0.9016 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0492 |           2.8110 |           0.9001 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0509 |           2.7909 |           0.9040 |
[32m[20221208 14:19:43 @agent_ppo2.py:179][0m |          -0.0525 |           2.7883 |           0.9036 |
[32m[20221208 14:19:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.47
[32m[20221208 14:19:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.22
[32m[20221208 14:19:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.64
[32m[20221208 14:19:43 @agent_ppo2.py:137][0m Total time:       3.64 min
[32m[20221208 14:19:43 @agent_ppo2.py:139][0m 294912 total steps have happened
[32m[20221208 14:19:43 @agent_ppo2.py:115][0m #------------------------ Iteration 144 --------------------------#
[32m[20221208 14:19:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |           0.0072 |           2.8922 |           0.9114 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0134 |           2.7666 |           0.9150 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0234 |           2.7335 |           0.9161 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0345 |           2.6712 |           0.9224 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0363 |           2.6567 |           0.9276 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0418 |           2.6310 |           0.9303 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0457 |           2.6206 |           0.9334 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0480 |           2.6186 |           0.9349 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0515 |           2.6099 |           0.9349 |
[32m[20221208 14:19:44 @agent_ppo2.py:179][0m |          -0.0533 |           2.5844 |           0.9400 |
[32m[20221208 14:19:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:19:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.75
[32m[20221208 14:19:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.71
[32m[20221208 14:19:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.50
[32m[20221208 14:19:45 @agent_ppo2.py:137][0m Total time:       3.67 min
[32m[20221208 14:19:45 @agent_ppo2.py:139][0m 296960 total steps have happened
[32m[20221208 14:19:45 @agent_ppo2.py:115][0m #------------------------ Iteration 145 --------------------------#
[32m[20221208 14:19:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:45 @agent_ppo2.py:179][0m |           0.0237 |           2.0592 |           0.9241 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0076 |           1.7586 |           0.9351 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0185 |           1.6515 |           0.9394 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0243 |           1.5558 |           0.9443 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0329 |           1.5057 |           0.9520 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0382 |           1.4589 |           0.9510 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0426 |           1.4200 |           0.9542 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0489 |           1.3842 |           0.9553 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0502 |           1.3693 |           0.9564 |
[32m[20221208 14:19:46 @agent_ppo2.py:179][0m |          -0.0489 |           1.3589 |           0.9579 |
[32m[20221208 14:19:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:19:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.01
[32m[20221208 14:19:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.35
[32m[20221208 14:19:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.14
[32m[20221208 14:19:46 @agent_ppo2.py:137][0m Total time:       3.69 min
[32m[20221208 14:19:46 @agent_ppo2.py:139][0m 299008 total steps have happened
[32m[20221208 14:19:46 @agent_ppo2.py:115][0m #------------------------ Iteration 146 --------------------------#
[32m[20221208 14:19:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |           0.0190 |           1.4752 |           0.9719 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0084 |           1.2009 |           0.9691 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0262 |           1.0848 |           0.9760 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0319 |           1.0072 |           0.9788 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0398 |           0.9607 |           0.9833 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0461 |           0.9105 |           0.9872 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0496 |           0.8764 |           0.9892 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0498 |           0.8446 |           0.9898 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0498 |           0.8162 |           0.9926 |
[32m[20221208 14:19:47 @agent_ppo2.py:179][0m |          -0.0565 |           0.7893 |           0.9938 |
[32m[20221208 14:19:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.92
[32m[20221208 14:19:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.60
[32m[20221208 14:19:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.10
[32m[20221208 14:19:48 @agent_ppo2.py:137][0m Total time:       3.72 min
[32m[20221208 14:19:48 @agent_ppo2.py:139][0m 301056 total steps have happened
[32m[20221208 14:19:48 @agent_ppo2.py:115][0m #------------------------ Iteration 147 --------------------------#
[32m[20221208 14:19:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:48 @agent_ppo2.py:179][0m |           0.0105 |           2.1172 |           0.9971 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0159 |           1.9274 |           0.9879 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0237 |           1.7980 |           0.9902 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0259 |           1.7297 |           0.9916 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0331 |           1.6682 |           0.9918 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0393 |           1.6411 |           0.9915 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0429 |           1.5975 |           0.9929 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0467 |           1.5612 |           0.9956 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0475 |           1.5597 |           0.9942 |
[32m[20221208 14:19:49 @agent_ppo2.py:179][0m |          -0.0496 |           1.5084 |           0.9994 |
[32m[20221208 14:19:49 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.78
[32m[20221208 14:19:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.36
[32m[20221208 14:19:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.29
[32m[20221208 14:19:49 @agent_ppo2.py:137][0m Total time:       3.74 min
[32m[20221208 14:19:49 @agent_ppo2.py:139][0m 303104 total steps have happened
[32m[20221208 14:19:49 @agent_ppo2.py:115][0m #------------------------ Iteration 148 --------------------------#
[32m[20221208 14:19:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |           0.0296 |           4.1034 |           1.0142 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0032 |           3.7500 |           0.9935 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0266 |           3.6562 |           1.0062 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0404 |           3.6171 |           1.0125 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0423 |           3.5554 |           1.0161 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0512 |           3.5337 |           1.0207 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0547 |           3.5497 |           1.0205 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0581 |           3.4658 |           1.0225 |
[32m[20221208 14:19:50 @agent_ppo2.py:179][0m |          -0.0601 |           3.4243 |           1.0226 |
[32m[20221208 14:19:51 @agent_ppo2.py:179][0m |          -0.0643 |           3.4210 |           1.0256 |
[32m[20221208 14:19:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.98
[32m[20221208 14:19:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.89
[32m[20221208 14:19:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.31
[32m[20221208 14:19:51 @agent_ppo2.py:137][0m Total time:       3.77 min
[32m[20221208 14:19:51 @agent_ppo2.py:139][0m 305152 total steps have happened
[32m[20221208 14:19:51 @agent_ppo2.py:115][0m #------------------------ Iteration 149 --------------------------#
[32m[20221208 14:19:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:51 @agent_ppo2.py:179][0m |           0.0151 |           3.5525 |           1.0102 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0059 |           3.2978 |           1.0018 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0201 |           3.2775 |           1.0108 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0290 |           3.2315 |           1.0157 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0341 |           3.1838 |           1.0158 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0366 |           3.1897 |           1.0176 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0437 |           3.1585 |           1.0206 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0460 |           3.1581 |           1.0243 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0503 |           3.1327 |           1.0302 |
[32m[20221208 14:19:52 @agent_ppo2.py:179][0m |          -0.0507 |           3.1217 |           1.0321 |
[32m[20221208 14:19:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:19:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.57
[32m[20221208 14:19:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.00
[32m[20221208 14:19:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.83
[32m[20221208 14:19:52 @agent_ppo2.py:137][0m Total time:       3.79 min
[32m[20221208 14:19:52 @agent_ppo2.py:139][0m 307200 total steps have happened
[32m[20221208 14:19:52 @agent_ppo2.py:115][0m #------------------------ Iteration 150 --------------------------#
[32m[20221208 14:19:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |           0.0218 |           3.3097 |           1.0514 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0129 |           2.9965 |           1.0586 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0288 |           2.9239 |           1.0672 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0321 |           2.9173 |           1.0660 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0386 |           2.8730 |           1.0688 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0432 |           2.8544 |           1.0700 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0431 |           2.8265 |           1.0692 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0477 |           2.7900 |           1.0739 |
[32m[20221208 14:19:53 @agent_ppo2.py:179][0m |          -0.0489 |           2.7719 |           1.0684 |
[32m[20221208 14:19:54 @agent_ppo2.py:179][0m |          -0.0513 |           2.7735 |           1.0766 |
[32m[20221208 14:19:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.34
[32m[20221208 14:19:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.97
[32m[20221208 14:19:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.38
[32m[20221208 14:19:54 @agent_ppo2.py:137][0m Total time:       3.82 min
[32m[20221208 14:19:54 @agent_ppo2.py:139][0m 309248 total steps have happened
[32m[20221208 14:19:54 @agent_ppo2.py:115][0m #------------------------ Iteration 151 --------------------------#
[32m[20221208 14:19:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:54 @agent_ppo2.py:179][0m |           0.0107 |           2.6099 |           1.0758 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0165 |           2.2168 |           1.0701 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0313 |           2.0654 |           1.0763 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0362 |           1.9833 |           1.0783 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0425 |           1.9039 |           1.0800 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0459 |           1.8398 |           1.0820 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0500 |           1.8049 |           1.0869 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0508 |           1.7689 |           1.0823 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0541 |           1.7270 |           1.0885 |
[32m[20221208 14:19:55 @agent_ppo2.py:179][0m |          -0.0571 |           1.7008 |           1.0914 |
[32m[20221208 14:19:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.17
[32m[20221208 14:19:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.59
[32m[20221208 14:19:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.55
[32m[20221208 14:19:55 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 155.55
[32m[20221208 14:19:55 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 155.55
[32m[20221208 14:19:55 @agent_ppo2.py:137][0m Total time:       3.84 min
[32m[20221208 14:19:55 @agent_ppo2.py:139][0m 311296 total steps have happened
[32m[20221208 14:19:55 @agent_ppo2.py:115][0m #------------------------ Iteration 152 --------------------------#
[32m[20221208 14:19:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:19:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |           0.0149 |           2.5443 |           1.0632 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |           0.0097 |           2.2590 |           1.0559 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0065 |           2.1859 |           1.0483 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0245 |           2.1735 |           1.0568 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0231 |           2.1009 |           1.0544 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0328 |           2.0731 |           1.0616 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0406 |           2.0669 |           1.0673 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0430 |           2.0454 |           1.0690 |
[32m[20221208 14:19:56 @agent_ppo2.py:179][0m |          -0.0413 |           2.0306 |           1.0738 |
[32m[20221208 14:19:57 @agent_ppo2.py:179][0m |          -0.0278 |           2.0313 |           1.0582 |
[32m[20221208 14:19:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:19:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.08
[32m[20221208 14:19:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.12
[32m[20221208 14:19:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.64
[32m[20221208 14:19:57 @agent_ppo2.py:137][0m Total time:       3.87 min
[32m[20221208 14:19:57 @agent_ppo2.py:139][0m 313344 total steps have happened
[32m[20221208 14:19:57 @agent_ppo2.py:115][0m #------------------------ Iteration 153 --------------------------#
[32m[20221208 14:19:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |           0.0143 |           1.8069 |           1.0788 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0060 |           1.7389 |           1.0815 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0194 |           1.7139 |           1.0838 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0243 |           1.7030 |           1.0907 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0283 |           1.6867 |           1.0971 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0318 |           1.6645 |           1.0981 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0357 |           1.6659 |           1.1026 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0371 |           1.6428 |           1.1046 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0367 |           1.6513 |           1.1073 |
[32m[20221208 14:19:58 @agent_ppo2.py:179][0m |          -0.0396 |           1.6226 |           1.1121 |
[32m[20221208 14:19:58 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:19:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.02
[32m[20221208 14:19:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.38
[32m[20221208 14:19:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.93
[32m[20221208 14:19:58 @agent_ppo2.py:137][0m Total time:       3.89 min
[32m[20221208 14:19:58 @agent_ppo2.py:139][0m 315392 total steps have happened
[32m[20221208 14:19:58 @agent_ppo2.py:115][0m #------------------------ Iteration 154 --------------------------#
[32m[20221208 14:19:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:19:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |           0.0163 |           2.4656 |           1.1420 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0058 |           2.2639 |           1.1405 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0220 |           2.1842 |           1.1465 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0306 |           2.1130 |           1.1541 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0336 |           2.0642 |           1.1607 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0397 |           2.0186 |           1.1618 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0425 |           2.0130 |           1.1684 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0421 |           1.9649 |           1.1677 |
[32m[20221208 14:19:59 @agent_ppo2.py:179][0m |          -0.0469 |           1.9571 |           1.1725 |
[32m[20221208 14:20:00 @agent_ppo2.py:179][0m |          -0.0479 |           1.9363 |           1.1731 |
[32m[20221208 14:20:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.16
[32m[20221208 14:20:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.83
[32m[20221208 14:20:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.47
[32m[20221208 14:20:00 @agent_ppo2.py:137][0m Total time:       3.92 min
[32m[20221208 14:20:00 @agent_ppo2.py:139][0m 317440 total steps have happened
[32m[20221208 14:20:00 @agent_ppo2.py:115][0m #------------------------ Iteration 155 --------------------------#
[32m[20221208 14:20:00 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:20:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |           0.0145 |           2.5340 |           1.1390 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |           0.0002 |           2.2812 |           1.1331 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0162 |           2.1481 |           1.1425 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0276 |           2.0925 |           1.1471 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0357 |           2.0374 |           1.1455 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0410 |           1.9750 |           1.1496 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0446 |           1.9715 |           1.1478 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0447 |           1.9424 |           1.1510 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0479 |           1.9340 |           1.1517 |
[32m[20221208 14:20:01 @agent_ppo2.py:179][0m |          -0.0499 |           1.9042 |           1.1539 |
[32m[20221208 14:20:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:20:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.90
[32m[20221208 14:20:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.03
[32m[20221208 14:20:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.55
[32m[20221208 14:20:02 @agent_ppo2.py:137][0m Total time:       3.94 min
[32m[20221208 14:20:02 @agent_ppo2.py:139][0m 319488 total steps have happened
[32m[20221208 14:20:02 @agent_ppo2.py:115][0m #------------------------ Iteration 156 --------------------------#
[32m[20221208 14:20:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |           0.0122 |           4.2854 |           1.1779 |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |          -0.0135 |           4.0303 |           1.1679 |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |          -0.0281 |           3.9322 |           1.1773 |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |          -0.0299 |           3.8563 |           1.1763 |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |          -0.0387 |           3.8148 |           1.1783 |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |          -0.0440 |           3.7543 |           1.1866 |
[32m[20221208 14:20:02 @agent_ppo2.py:179][0m |          -0.0461 |           3.7207 |           1.1919 |
[32m[20221208 14:20:03 @agent_ppo2.py:179][0m |          -0.0471 |           3.6699 |           1.1885 |
[32m[20221208 14:20:03 @agent_ppo2.py:179][0m |          -0.0489 |           3.6242 |           1.1945 |
[32m[20221208 14:20:03 @agent_ppo2.py:179][0m |          -0.0506 |           3.5723 |           1.1990 |
[32m[20221208 14:20:03 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:20:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.73
[32m[20221208 14:20:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.62
[32m[20221208 14:20:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.07
[32m[20221208 14:20:03 @agent_ppo2.py:137][0m Total time:       3.97 min
[32m[20221208 14:20:03 @agent_ppo2.py:139][0m 321536 total steps have happened
[32m[20221208 14:20:03 @agent_ppo2.py:115][0m #------------------------ Iteration 157 --------------------------#
[32m[20221208 14:20:04 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:20:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |           0.0198 |           2.5502 |           1.1861 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0111 |           2.2407 |           1.1758 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0210 |           2.1527 |           1.1871 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0305 |           2.0908 |           1.1865 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0361 |           2.0543 |           1.1907 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0326 |           2.0289 |           1.1962 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0407 |           2.0095 |           1.1988 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0452 |           1.9891 |           1.2012 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0452 |           1.9785 |           1.2027 |
[32m[20221208 14:20:04 @agent_ppo2.py:179][0m |          -0.0481 |           1.9412 |           1.2048 |
[32m[20221208 14:20:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:20:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.42
[32m[20221208 14:20:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.66
[32m[20221208 14:20:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.33
[32m[20221208 14:20:05 @agent_ppo2.py:137][0m Total time:       4.00 min
[32m[20221208 14:20:05 @agent_ppo2.py:139][0m 323584 total steps have happened
[32m[20221208 14:20:05 @agent_ppo2.py:115][0m #------------------------ Iteration 158 --------------------------#
[32m[20221208 14:20:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:05 @agent_ppo2.py:179][0m |           0.0187 |           2.6253 |           1.2054 |
[32m[20221208 14:20:05 @agent_ppo2.py:179][0m |           0.0139 |           2.3347 |           1.1880 |
[32m[20221208 14:20:05 @agent_ppo2.py:179][0m |          -0.0216 |           2.2305 |           1.2102 |
[32m[20221208 14:20:05 @agent_ppo2.py:179][0m |          -0.0321 |           2.1636 |           1.2173 |
[32m[20221208 14:20:05 @agent_ppo2.py:179][0m |          -0.0375 |           2.1262 |           1.2222 |
[32m[20221208 14:20:06 @agent_ppo2.py:179][0m |          -0.0399 |           2.0729 |           1.2232 |
[32m[20221208 14:20:06 @agent_ppo2.py:179][0m |          -0.0457 |           2.0329 |           1.2227 |
[32m[20221208 14:20:06 @agent_ppo2.py:179][0m |          -0.0444 |           2.0115 |           1.2301 |
[32m[20221208 14:20:06 @agent_ppo2.py:179][0m |          -0.0470 |           1.9392 |           1.2314 |
[32m[20221208 14:20:06 @agent_ppo2.py:179][0m |          -0.0485 |           1.8996 |           1.2293 |
[32m[20221208 14:20:06 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:20:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.24
[32m[20221208 14:20:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.16
[32m[20221208 14:20:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.86
[32m[20221208 14:20:06 @agent_ppo2.py:137][0m Total time:       4.02 min
[32m[20221208 14:20:06 @agent_ppo2.py:139][0m 325632 total steps have happened
[32m[20221208 14:20:06 @agent_ppo2.py:115][0m #------------------------ Iteration 159 --------------------------#
[32m[20221208 14:20:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |           0.0070 |           3.2389 |           1.2713 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0023 |           2.9623 |           1.2653 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0133 |           2.8301 |           1.2468 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0331 |           2.7594 |           1.2798 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0415 |           2.6742 |           1.2822 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0440 |           2.6246 |           1.2914 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0466 |           2.5858 |           1.2902 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0511 |           2.5358 |           1.2941 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0540 |           2.4993 |           1.2936 |
[32m[20221208 14:20:07 @agent_ppo2.py:179][0m |          -0.0557 |           2.4652 |           1.3016 |
[32m[20221208 14:20:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.42
[32m[20221208 14:20:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.79
[32m[20221208 14:20:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.91
[32m[20221208 14:20:08 @agent_ppo2.py:137][0m Total time:       4.05 min
[32m[20221208 14:20:08 @agent_ppo2.py:139][0m 327680 total steps have happened
[32m[20221208 14:20:08 @agent_ppo2.py:115][0m #------------------------ Iteration 160 --------------------------#
[32m[20221208 14:20:08 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:08 @agent_ppo2.py:179][0m |           0.0209 |           3.0251 |           1.3111 |
[32m[20221208 14:20:08 @agent_ppo2.py:179][0m |          -0.0060 |           2.8050 |           1.3146 |
[32m[20221208 14:20:08 @agent_ppo2.py:179][0m |          -0.0272 |           2.7255 |           1.3290 |
[32m[20221208 14:20:08 @agent_ppo2.py:179][0m |          -0.0353 |           2.6509 |           1.3337 |
[32m[20221208 14:20:08 @agent_ppo2.py:179][0m |          -0.0402 |           2.5931 |           1.3331 |
[32m[20221208 14:20:09 @agent_ppo2.py:179][0m |          -0.0423 |           2.5154 |           1.3357 |
[32m[20221208 14:20:09 @agent_ppo2.py:179][0m |          -0.0491 |           2.4925 |           1.3398 |
[32m[20221208 14:20:09 @agent_ppo2.py:179][0m |          -0.0514 |           2.4519 |           1.3412 |
[32m[20221208 14:20:09 @agent_ppo2.py:179][0m |          -0.0528 |           2.4287 |           1.3444 |
[32m[20221208 14:20:09 @agent_ppo2.py:179][0m |          -0.0550 |           2.3972 |           1.3413 |
[32m[20221208 14:20:09 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.80
[32m[20221208 14:20:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.07
[32m[20221208 14:20:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.56
[32m[20221208 14:20:09 @agent_ppo2.py:137][0m Total time:       4.07 min
[32m[20221208 14:20:09 @agent_ppo2.py:139][0m 329728 total steps have happened
[32m[20221208 14:20:09 @agent_ppo2.py:115][0m #------------------------ Iteration 161 --------------------------#
[32m[20221208 14:20:10 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |           0.0894 |           1.3465 |           1.3383 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |           0.0691 |           0.9786 |           1.1947 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |           0.0122 |           0.9015 |           1.2234 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0102 |           0.8221 |           1.2674 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0188 |           0.8015 |           1.2928 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0255 |           0.7824 |           1.3083 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0296 |           0.7582 |           1.3101 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0348 |           0.7490 |           1.3239 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0385 |           0.7317 |           1.3237 |
[32m[20221208 14:20:10 @agent_ppo2.py:179][0m |          -0.0430 |           0.7247 |           1.3356 |
[32m[20221208 14:20:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.12
[32m[20221208 14:20:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.16
[32m[20221208 14:20:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.59
[32m[20221208 14:20:11 @agent_ppo2.py:137][0m Total time:       4.10 min
[32m[20221208 14:20:11 @agent_ppo2.py:139][0m 331776 total steps have happened
[32m[20221208 14:20:11 @agent_ppo2.py:115][0m #------------------------ Iteration 162 --------------------------#
[32m[20221208 14:20:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:11 @agent_ppo2.py:179][0m |           0.0096 |           3.0640 |           1.3481 |
[32m[20221208 14:20:11 @agent_ppo2.py:179][0m |          -0.0148 |           2.5506 |           1.3466 |
[32m[20221208 14:20:11 @agent_ppo2.py:179][0m |          -0.0193 |           2.3777 |           1.3530 |
[32m[20221208 14:20:11 @agent_ppo2.py:179][0m |          -0.0334 |           2.2161 |           1.3645 |
[32m[20221208 14:20:12 @agent_ppo2.py:179][0m |          -0.0352 |           2.1397 |           1.3653 |
[32m[20221208 14:20:12 @agent_ppo2.py:179][0m |          -0.0398 |           2.0499 |           1.3718 |
[32m[20221208 14:20:12 @agent_ppo2.py:179][0m |          -0.0421 |           1.9850 |           1.3713 |
[32m[20221208 14:20:12 @agent_ppo2.py:179][0m |          -0.0442 |           1.9308 |           1.3745 |
[32m[20221208 14:20:12 @agent_ppo2.py:179][0m |          -0.0439 |           1.9114 |           1.3753 |
[32m[20221208 14:20:12 @agent_ppo2.py:179][0m |          -0.0455 |           1.8604 |           1.3829 |
[32m[20221208 14:20:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.70
[32m[20221208 14:20:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.49
[32m[20221208 14:20:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.12
[32m[20221208 14:20:12 @agent_ppo2.py:137][0m Total time:       4.12 min
[32m[20221208 14:20:12 @agent_ppo2.py:139][0m 333824 total steps have happened
[32m[20221208 14:20:12 @agent_ppo2.py:115][0m #------------------------ Iteration 163 --------------------------#
[32m[20221208 14:20:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |           0.0184 |           5.9949 |           1.4385 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0127 |           5.5579 |           1.4350 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0289 |           5.3659 |           1.4535 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0354 |           5.2654 |           1.4520 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0444 |           5.2188 |           1.4630 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0456 |           5.1508 |           1.4594 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0518 |           5.1389 |           1.4689 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0524 |           5.0912 |           1.4682 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0576 |           5.0263 |           1.4775 |
[32m[20221208 14:20:13 @agent_ppo2.py:179][0m |          -0.0587 |           5.0340 |           1.4782 |
[32m[20221208 14:20:13 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.08
[32m[20221208 14:20:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.83
[32m[20221208 14:20:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.09
[32m[20221208 14:20:14 @agent_ppo2.py:137][0m Total time:       4.15 min
[32m[20221208 14:20:14 @agent_ppo2.py:139][0m 335872 total steps have happened
[32m[20221208 14:20:14 @agent_ppo2.py:115][0m #------------------------ Iteration 164 --------------------------#
[32m[20221208 14:20:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:14 @agent_ppo2.py:179][0m |           0.0288 |           4.1671 |           1.4882 |
[32m[20221208 14:20:14 @agent_ppo2.py:179][0m |           0.0112 |           3.8425 |           1.4362 |
[32m[20221208 14:20:14 @agent_ppo2.py:179][0m |          -0.0187 |           3.6998 |           1.4759 |
[32m[20221208 14:20:14 @agent_ppo2.py:179][0m |          -0.0287 |           3.6273 |           1.4788 |
[32m[20221208 14:20:15 @agent_ppo2.py:179][0m |          -0.0357 |           3.5642 |           1.4891 |
[32m[20221208 14:20:15 @agent_ppo2.py:179][0m |          -0.0390 |           3.5540 |           1.4996 |
[32m[20221208 14:20:15 @agent_ppo2.py:179][0m |          -0.0436 |           3.4957 |           1.5019 |
[32m[20221208 14:20:15 @agent_ppo2.py:179][0m |          -0.0436 |           3.4637 |           1.4987 |
[32m[20221208 14:20:15 @agent_ppo2.py:179][0m |          -0.0455 |           3.4035 |           1.5044 |
[32m[20221208 14:20:15 @agent_ppo2.py:179][0m |          -0.0519 |           3.4165 |           1.5152 |
[32m[20221208 14:20:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.56
[32m[20221208 14:20:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.63
[32m[20221208 14:20:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.13
[32m[20221208 14:20:15 @agent_ppo2.py:137][0m Total time:       4.17 min
[32m[20221208 14:20:15 @agent_ppo2.py:139][0m 337920 total steps have happened
[32m[20221208 14:20:15 @agent_ppo2.py:115][0m #------------------------ Iteration 165 --------------------------#
[32m[20221208 14:20:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |           0.0038 |           2.4133 |           1.5277 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0191 |           2.3011 |           1.5256 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0290 |           2.2498 |           1.5369 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0342 |           2.1957 |           1.5373 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0361 |           2.1488 |           1.5353 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0408 |           2.1129 |           1.5412 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0444 |           2.1026 |           1.5513 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0458 |           2.0747 |           1.5515 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0478 |           2.0398 |           1.5532 |
[32m[20221208 14:20:16 @agent_ppo2.py:179][0m |          -0.0504 |           2.0247 |           1.5613 |
[32m[20221208 14:20:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.98
[32m[20221208 14:20:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.15
[32m[20221208 14:20:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.67
[32m[20221208 14:20:17 @agent_ppo2.py:137][0m Total time:       4.20 min
[32m[20221208 14:20:17 @agent_ppo2.py:139][0m 339968 total steps have happened
[32m[20221208 14:20:17 @agent_ppo2.py:115][0m #------------------------ Iteration 166 --------------------------#
[32m[20221208 14:20:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:20:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:17 @agent_ppo2.py:179][0m |           0.0336 |           5.8955 |           1.5768 |
[32m[20221208 14:20:17 @agent_ppo2.py:179][0m |           0.0014 |           5.5986 |           1.5523 |
[32m[20221208 14:20:17 @agent_ppo2.py:179][0m |          -0.0281 |           5.4429 |           1.5685 |
[32m[20221208 14:20:17 @agent_ppo2.py:179][0m |          -0.0391 |           5.2998 |           1.5892 |
[32m[20221208 14:20:17 @agent_ppo2.py:179][0m |          -0.0451 |           5.2149 |           1.5900 |
[32m[20221208 14:20:18 @agent_ppo2.py:179][0m |          -0.0483 |           5.1323 |           1.5944 |
[32m[20221208 14:20:18 @agent_ppo2.py:179][0m |          -0.0536 |           5.0406 |           1.5970 |
[32m[20221208 14:20:18 @agent_ppo2.py:179][0m |          -0.0569 |           4.9923 |           1.6057 |
[32m[20221208 14:20:18 @agent_ppo2.py:179][0m |          -0.0573 |           4.9355 |           1.5975 |
[32m[20221208 14:20:18 @agent_ppo2.py:179][0m |          -0.0609 |           4.8872 |           1.6047 |
[32m[20221208 14:20:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.75
[32m[20221208 14:20:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.33
[32m[20221208 14:20:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.87
[32m[20221208 14:20:18 @agent_ppo2.py:137][0m Total time:       4.22 min
[32m[20221208 14:20:18 @agent_ppo2.py:139][0m 342016 total steps have happened
[32m[20221208 14:20:18 @agent_ppo2.py:115][0m #------------------------ Iteration 167 --------------------------#
[32m[20221208 14:20:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |           0.0110 |           2.7092 |           1.6376 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0205 |           2.4156 |           1.6317 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0248 |           2.3140 |           1.6269 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0373 |           2.2388 |           1.6446 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0432 |           2.2142 |           1.6458 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0505 |           2.1639 |           1.6595 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0486 |           2.1328 |           1.6568 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0562 |           2.1125 |           1.6629 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0603 |           2.0819 |           1.6676 |
[32m[20221208 14:20:19 @agent_ppo2.py:179][0m |          -0.0618 |           2.0781 |           1.6729 |
[32m[20221208 14:20:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.38
[32m[20221208 14:20:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.16
[32m[20221208 14:20:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.81
[32m[20221208 14:20:20 @agent_ppo2.py:137][0m Total time:       4.25 min
[32m[20221208 14:20:20 @agent_ppo2.py:139][0m 344064 total steps have happened
[32m[20221208 14:20:20 @agent_ppo2.py:115][0m #------------------------ Iteration 168 --------------------------#
[32m[20221208 14:20:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:20 @agent_ppo2.py:179][0m |           0.0141 |           3.9717 |           1.6499 |
[32m[20221208 14:20:20 @agent_ppo2.py:179][0m |          -0.0127 |           3.7365 |           1.6493 |
[32m[20221208 14:20:20 @agent_ppo2.py:179][0m |          -0.0303 |           3.5776 |           1.6519 |
[32m[20221208 14:20:20 @agent_ppo2.py:179][0m |          -0.0333 |           3.4929 |           1.6656 |
[32m[20221208 14:20:20 @agent_ppo2.py:179][0m |          -0.0390 |           3.3569 |           1.6679 |
[32m[20221208 14:20:21 @agent_ppo2.py:179][0m |          -0.0433 |           3.2840 |           1.6676 |
[32m[20221208 14:20:21 @agent_ppo2.py:179][0m |          -0.0481 |           3.2288 |           1.6797 |
[32m[20221208 14:20:21 @agent_ppo2.py:179][0m |          -0.0497 |           3.2180 |           1.6843 |
[32m[20221208 14:20:21 @agent_ppo2.py:179][0m |          -0.0524 |           3.0862 |           1.6895 |
[32m[20221208 14:20:21 @agent_ppo2.py:179][0m |          -0.0538 |           3.0493 |           1.6924 |
[32m[20221208 14:20:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.57
[32m[20221208 14:20:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.00
[32m[20221208 14:20:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.95
[32m[20221208 14:20:21 @agent_ppo2.py:137][0m Total time:       4.27 min
[32m[20221208 14:20:21 @agent_ppo2.py:139][0m 346112 total steps have happened
[32m[20221208 14:20:21 @agent_ppo2.py:115][0m #------------------------ Iteration 169 --------------------------#
[32m[20221208 14:20:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |           0.0174 |           2.1923 |           1.7130 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0055 |           1.9583 |           1.7030 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0234 |           1.8913 |           1.7219 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0347 |           1.8462 |           1.7325 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0384 |           1.8168 |           1.7314 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0400 |           1.7948 |           1.7415 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0442 |           1.7734 |           1.7427 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0471 |           1.7477 |           1.7452 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0482 |           1.7439 |           1.7499 |
[32m[20221208 14:20:22 @agent_ppo2.py:179][0m |          -0.0513 |           1.7182 |           1.7507 |
[32m[20221208 14:20:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.41
[32m[20221208 14:20:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.45
[32m[20221208 14:20:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.37
[32m[20221208 14:20:23 @agent_ppo2.py:137][0m Total time:       4.30 min
[32m[20221208 14:20:23 @agent_ppo2.py:139][0m 348160 total steps have happened
[32m[20221208 14:20:23 @agent_ppo2.py:115][0m #------------------------ Iteration 170 --------------------------#
[32m[20221208 14:20:23 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:23 @agent_ppo2.py:179][0m |           0.0291 |           2.3435 |           1.7535 |
[32m[20221208 14:20:23 @agent_ppo2.py:179][0m |          -0.0068 |           1.9904 |           1.7382 |
[32m[20221208 14:20:23 @agent_ppo2.py:179][0m |          -0.0237 |           1.8550 |           1.7474 |
[32m[20221208 14:20:23 @agent_ppo2.py:179][0m |          -0.0289 |           1.7728 |           1.7474 |
[32m[20221208 14:20:23 @agent_ppo2.py:179][0m |          -0.0333 |           1.7228 |           1.7423 |
[32m[20221208 14:20:24 @agent_ppo2.py:179][0m |          -0.0370 |           1.6973 |           1.7458 |
[32m[20221208 14:20:24 @agent_ppo2.py:179][0m |          -0.0372 |           1.6552 |           1.7402 |
[32m[20221208 14:20:24 @agent_ppo2.py:179][0m |          -0.0421 |           1.6269 |           1.7471 |
[32m[20221208 14:20:24 @agent_ppo2.py:179][0m |          -0.0431 |           1.6224 |           1.7477 |
[32m[20221208 14:20:24 @agent_ppo2.py:179][0m |          -0.0461 |           1.6113 |           1.7544 |
[32m[20221208 14:20:24 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.67
[32m[20221208 14:20:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.01
[32m[20221208 14:20:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.01
[32m[20221208 14:20:24 @agent_ppo2.py:137][0m Total time:       4.32 min
[32m[20221208 14:20:24 @agent_ppo2.py:139][0m 350208 total steps have happened
[32m[20221208 14:20:24 @agent_ppo2.py:115][0m #------------------------ Iteration 171 --------------------------#
[32m[20221208 14:20:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |           0.0134 |           2.6981 |           1.7132 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |           0.0012 |           2.4179 |           1.6958 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0189 |           2.3260 |           1.7154 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0294 |           2.2424 |           1.7290 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0375 |           2.1795 |           1.7356 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0393 |           2.1024 |           1.7405 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0402 |           2.0506 |           1.7485 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0420 |           2.0307 |           1.7412 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0442 |           1.9246 |           1.7579 |
[32m[20221208 14:20:25 @agent_ppo2.py:179][0m |          -0.0471 |           1.8891 |           1.7613 |
[32m[20221208 14:20:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.58
[32m[20221208 14:20:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.40
[32m[20221208 14:20:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.20
[32m[20221208 14:20:26 @agent_ppo2.py:137][0m Total time:       4.35 min
[32m[20221208 14:20:26 @agent_ppo2.py:139][0m 352256 total steps have happened
[32m[20221208 14:20:26 @agent_ppo2.py:115][0m #------------------------ Iteration 172 --------------------------#
[32m[20221208 14:20:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:26 @agent_ppo2.py:179][0m |           0.0289 |           3.8383 |           1.7439 |
[32m[20221208 14:20:26 @agent_ppo2.py:179][0m |          -0.0059 |           3.1043 |           1.7497 |
[32m[20221208 14:20:26 @agent_ppo2.py:179][0m |          -0.0221 |           2.8747 |           1.7542 |
[32m[20221208 14:20:26 @agent_ppo2.py:179][0m |          -0.0324 |           2.7363 |           1.7507 |
[32m[20221208 14:20:26 @agent_ppo2.py:179][0m |          -0.0371 |           2.6650 |           1.7614 |
[32m[20221208 14:20:27 @agent_ppo2.py:179][0m |          -0.0458 |           2.5704 |           1.7586 |
[32m[20221208 14:20:27 @agent_ppo2.py:179][0m |          -0.0547 |           2.5128 |           1.7730 |
[32m[20221208 14:20:27 @agent_ppo2.py:179][0m |          -0.0555 |           2.4417 |           1.7747 |
[32m[20221208 14:20:27 @agent_ppo2.py:179][0m |          -0.0603 |           2.4399 |           1.7785 |
[32m[20221208 14:20:27 @agent_ppo2.py:179][0m |          -0.0646 |           2.3796 |           1.7848 |
[32m[20221208 14:20:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.67
[32m[20221208 14:20:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.97
[32m[20221208 14:20:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.45
[32m[20221208 14:20:27 @agent_ppo2.py:137][0m Total time:       4.37 min
[32m[20221208 14:20:27 @agent_ppo2.py:139][0m 354304 total steps have happened
[32m[20221208 14:20:27 @agent_ppo2.py:115][0m #------------------------ Iteration 173 --------------------------#
[32m[20221208 14:20:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |           0.0194 |           4.3860 |           1.8548 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0076 |           4.1207 |           1.8689 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0311 |           4.0109 |           1.8847 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0382 |           3.9525 |           1.8802 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0419 |           3.9058 |           1.8879 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0454 |           3.8060 |           1.8840 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0516 |           3.8065 |           1.8892 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0525 |           3.7547 |           1.8911 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0540 |           3.7324 |           1.9040 |
[32m[20221208 14:20:28 @agent_ppo2.py:179][0m |          -0.0581 |           3.6938 |           1.9041 |
[32m[20221208 14:20:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.42
[32m[20221208 14:20:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.36
[32m[20221208 14:20:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.19
[32m[20221208 14:20:29 @agent_ppo2.py:137][0m Total time:       4.40 min
[32m[20221208 14:20:29 @agent_ppo2.py:139][0m 356352 total steps have happened
[32m[20221208 14:20:29 @agent_ppo2.py:115][0m #------------------------ Iteration 174 --------------------------#
[32m[20221208 14:20:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:29 @agent_ppo2.py:179][0m |           0.0248 |           3.6519 |           1.8516 |
[32m[20221208 14:20:29 @agent_ppo2.py:179][0m |          -0.0098 |           3.1088 |           1.8398 |
[32m[20221208 14:20:29 @agent_ppo2.py:179][0m |          -0.0275 |           2.9186 |           1.8411 |
[32m[20221208 14:20:29 @agent_ppo2.py:179][0m |          -0.0370 |           2.8145 |           1.8456 |
[32m[20221208 14:20:29 @agent_ppo2.py:179][0m |          -0.0459 |           2.6949 |           1.8650 |
[32m[20221208 14:20:30 @agent_ppo2.py:179][0m |          -0.0458 |           2.6169 |           1.8711 |
[32m[20221208 14:20:30 @agent_ppo2.py:179][0m |          -0.0504 |           2.5431 |           1.8680 |
[32m[20221208 14:20:30 @agent_ppo2.py:179][0m |          -0.0539 |           2.4966 |           1.8750 |
[32m[20221208 14:20:30 @agent_ppo2.py:179][0m |          -0.0557 |           2.4706 |           1.8771 |
[32m[20221208 14:20:30 @agent_ppo2.py:179][0m |          -0.0565 |           2.4380 |           1.8797 |
[32m[20221208 14:20:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.70
[32m[20221208 14:20:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.39
[32m[20221208 14:20:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.27
[32m[20221208 14:20:30 @agent_ppo2.py:137][0m Total time:       4.42 min
[32m[20221208 14:20:30 @agent_ppo2.py:139][0m 358400 total steps have happened
[32m[20221208 14:20:30 @agent_ppo2.py:115][0m #------------------------ Iteration 175 --------------------------#
[32m[20221208 14:20:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |           0.0285 |           5.0622 |           1.8791 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0036 |           4.6575 |           1.8916 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0245 |           4.5011 |           1.8934 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0354 |           4.4113 |           1.9114 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0387 |           4.3526 |           1.9126 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0435 |           4.2658 |           1.9176 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0469 |           4.2384 |           1.9288 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0489 |           4.2179 |           1.9324 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0523 |           4.1462 |           1.9380 |
[32m[20221208 14:20:31 @agent_ppo2.py:179][0m |          -0.0541 |           4.1311 |           1.9416 |
[32m[20221208 14:20:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.73
[32m[20221208 14:20:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.87
[32m[20221208 14:20:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.96
[32m[20221208 14:20:32 @agent_ppo2.py:137][0m Total time:       4.45 min
[32m[20221208 14:20:32 @agent_ppo2.py:139][0m 360448 total steps have happened
[32m[20221208 14:20:32 @agent_ppo2.py:115][0m #------------------------ Iteration 176 --------------------------#
[32m[20221208 14:20:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:20:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:32 @agent_ppo2.py:179][0m |           0.0212 |           4.7573 |           1.9869 |
[32m[20221208 14:20:32 @agent_ppo2.py:179][0m |          -0.0109 |           4.4994 |           1.9687 |
[32m[20221208 14:20:32 @agent_ppo2.py:179][0m |          -0.0213 |           4.3834 |           1.9684 |
[32m[20221208 14:20:32 @agent_ppo2.py:179][0m |          -0.0354 |           4.3096 |           1.9843 |
[32m[20221208 14:20:32 @agent_ppo2.py:179][0m |          -0.0415 |           4.2342 |           1.9874 |
[32m[20221208 14:20:33 @agent_ppo2.py:179][0m |          -0.0487 |           4.1741 |           2.0061 |
[32m[20221208 14:20:33 @agent_ppo2.py:179][0m |          -0.0513 |           4.1616 |           2.0046 |
[32m[20221208 14:20:33 @agent_ppo2.py:179][0m |          -0.0557 |           4.1139 |           2.0125 |
[32m[20221208 14:20:33 @agent_ppo2.py:179][0m |          -0.0574 |           4.0840 |           2.0144 |
[32m[20221208 14:20:33 @agent_ppo2.py:179][0m |          -0.0590 |           4.0721 |           2.0219 |
[32m[20221208 14:20:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.03
[32m[20221208 14:20:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.16
[32m[20221208 14:20:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.68
[32m[20221208 14:20:33 @agent_ppo2.py:137][0m Total time:       4.47 min
[32m[20221208 14:20:33 @agent_ppo2.py:139][0m 362496 total steps have happened
[32m[20221208 14:20:33 @agent_ppo2.py:115][0m #------------------------ Iteration 177 --------------------------#
[32m[20221208 14:20:34 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:20:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |           0.0163 |           4.2138 |           2.0375 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0026 |           4.0553 |           2.0169 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0177 |           3.9794 |           2.0381 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0297 |           3.8968 |           2.0352 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0391 |           3.8947 |           2.0480 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0416 |           3.8411 |           2.0450 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0442 |           3.7829 |           2.0508 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0490 |           3.7713 |           2.0622 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0505 |           3.7232 |           2.0607 |
[32m[20221208 14:20:34 @agent_ppo2.py:179][0m |          -0.0525 |           3.6916 |           2.0695 |
[32m[20221208 14:20:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.37
[32m[20221208 14:20:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.30
[32m[20221208 14:20:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.31
[32m[20221208 14:20:35 @agent_ppo2.py:137][0m Total time:       4.50 min
[32m[20221208 14:20:35 @agent_ppo2.py:139][0m 364544 total steps have happened
[32m[20221208 14:20:35 @agent_ppo2.py:115][0m #------------------------ Iteration 178 --------------------------#
[32m[20221208 14:20:35 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:20:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:35 @agent_ppo2.py:179][0m |           0.0178 |           0.3987 |           2.0747 |
[32m[20221208 14:20:35 @agent_ppo2.py:179][0m |          -0.0038 |           0.3318 |           2.0656 |
[32m[20221208 14:20:35 @agent_ppo2.py:179][0m |          -0.0102 |           0.3216 |           2.0550 |
[32m[20221208 14:20:35 @agent_ppo2.py:179][0m |          -0.0129 |           0.3183 |           2.0502 |
[32m[20221208 14:20:35 @agent_ppo2.py:179][0m |          -0.0147 |           0.3132 |           2.0501 |
[32m[20221208 14:20:36 @agent_ppo2.py:179][0m |          -0.0191 |           0.3128 |           2.0506 |
[32m[20221208 14:20:36 @agent_ppo2.py:179][0m |          -0.0164 |           0.3096 |           2.0553 |
[32m[20221208 14:20:36 @agent_ppo2.py:179][0m |          -0.0153 |           0.3086 |           2.0412 |
[32m[20221208 14:20:36 @agent_ppo2.py:179][0m |          -0.0172 |           0.3065 |           2.0581 |
[32m[20221208 14:20:36 @agent_ppo2.py:179][0m |          -0.0180 |           0.3048 |           2.0657 |
[32m[20221208 14:20:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.66
[32m[20221208 14:20:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.03
[32m[20221208 14:20:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.73
[32m[20221208 14:20:36 @agent_ppo2.py:137][0m Total time:       4.52 min
[32m[20221208 14:20:36 @agent_ppo2.py:139][0m 366592 total steps have happened
[32m[20221208 14:20:36 @agent_ppo2.py:115][0m #------------------------ Iteration 179 --------------------------#
[32m[20221208 14:20:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:20:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |           0.0161 |           5.8338 |           2.0384 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0133 |           5.4417 |           2.0188 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0335 |           5.3078 |           2.0249 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0374 |           5.0919 |           2.0280 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0454 |           5.0024 |           2.0387 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0481 |           4.9453 |           2.0367 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0522 |           4.8934 |           2.0437 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0555 |           4.8304 |           2.0486 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0558 |           4.7591 |           2.0483 |
[32m[20221208 14:20:37 @agent_ppo2.py:179][0m |          -0.0597 |           4.7388 |           2.0561 |
[32m[20221208 14:20:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.26
[32m[20221208 14:20:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.16
[32m[20221208 14:20:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.56
[32m[20221208 14:20:38 @agent_ppo2.py:137][0m Total time:       4.55 min
[32m[20221208 14:20:38 @agent_ppo2.py:139][0m 368640 total steps have happened
[32m[20221208 14:20:38 @agent_ppo2.py:115][0m #------------------------ Iteration 180 --------------------------#
[32m[20221208 14:20:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:38 @agent_ppo2.py:179][0m |           0.0235 |           2.4512 |           2.0847 |
[32m[20221208 14:20:38 @agent_ppo2.py:179][0m |          -0.0173 |           2.1354 |           2.0971 |
[32m[20221208 14:20:38 @agent_ppo2.py:179][0m |          -0.0265 |           2.0233 |           2.0949 |
[32m[20221208 14:20:38 @agent_ppo2.py:179][0m |          -0.0351 |           1.9208 |           2.0901 |
[32m[20221208 14:20:38 @agent_ppo2.py:179][0m |          -0.0412 |           1.8628 |           2.0895 |
[32m[20221208 14:20:39 @agent_ppo2.py:179][0m |          -0.0464 |           1.8121 |           2.1006 |
[32m[20221208 14:20:39 @agent_ppo2.py:179][0m |          -0.0479 |           1.7518 |           2.1005 |
[32m[20221208 14:20:39 @agent_ppo2.py:179][0m |          -0.0510 |           1.7147 |           2.1108 |
[32m[20221208 14:20:39 @agent_ppo2.py:179][0m |          -0.0539 |           1.6628 |           2.1073 |
[32m[20221208 14:20:39 @agent_ppo2.py:179][0m |          -0.0538 |           1.6369 |           2.1172 |
[32m[20221208 14:20:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.75
[32m[20221208 14:20:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.93
[32m[20221208 14:20:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.95
[32m[20221208 14:20:39 @agent_ppo2.py:137][0m Total time:       4.57 min
[32m[20221208 14:20:39 @agent_ppo2.py:139][0m 370688 total steps have happened
[32m[20221208 14:20:39 @agent_ppo2.py:115][0m #------------------------ Iteration 181 --------------------------#
[32m[20221208 14:20:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:20:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |           0.0288 |           4.7933 |           2.0750 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0097 |           4.2922 |           2.1003 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0314 |           4.1298 |           2.1002 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0395 |           4.0622 |           2.1076 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0447 |           4.0046 |           2.1091 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0518 |           3.9504 |           2.1189 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0580 |           3.8936 |           2.1283 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0601 |           3.8800 |           2.1369 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0608 |           3.8500 |           2.1342 |
[32m[20221208 14:20:40 @agent_ppo2.py:179][0m |          -0.0647 |           3.8150 |           2.1411 |
[32m[20221208 14:20:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.03
[32m[20221208 14:20:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.51
[32m[20221208 14:20:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.99
[32m[20221208 14:20:41 @agent_ppo2.py:137][0m Total time:       4.60 min
[32m[20221208 14:20:41 @agent_ppo2.py:139][0m 372736 total steps have happened
[32m[20221208 14:20:41 @agent_ppo2.py:115][0m #------------------------ Iteration 182 --------------------------#
[32m[20221208 14:20:41 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:41 @agent_ppo2.py:179][0m |           0.0071 |           0.3431 |           2.1298 |
[32m[20221208 14:20:41 @agent_ppo2.py:179][0m |          -0.0082 |           0.2746 |           2.1068 |
[32m[20221208 14:20:41 @agent_ppo2.py:179][0m |          -0.0163 |           0.2579 |           2.1077 |
[32m[20221208 14:20:41 @agent_ppo2.py:179][0m |          -0.0207 |           0.2511 |           2.1096 |
[32m[20221208 14:20:41 @agent_ppo2.py:179][0m |          -0.0202 |           0.2443 |           2.1131 |
[32m[20221208 14:20:42 @agent_ppo2.py:179][0m |          -0.0226 |           0.2402 |           2.1051 |
[32m[20221208 14:20:42 @agent_ppo2.py:179][0m |          -0.0238 |           0.2348 |           2.1076 |
[32m[20221208 14:20:42 @agent_ppo2.py:179][0m |          -0.0229 |           0.2360 |           2.1040 |
[32m[20221208 14:20:42 @agent_ppo2.py:179][0m |          -0.0284 |           0.2311 |           2.1088 |
[32m[20221208 14:20:42 @agent_ppo2.py:179][0m |          -0.0262 |           0.2307 |           2.1115 |
[32m[20221208 14:20:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.15
[32m[20221208 14:20:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.36
[32m[20221208 14:20:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.36
[32m[20221208 14:20:42 @agent_ppo2.py:137][0m Total time:       4.62 min
[32m[20221208 14:20:42 @agent_ppo2.py:139][0m 374784 total steps have happened
[32m[20221208 14:20:42 @agent_ppo2.py:115][0m #------------------------ Iteration 183 --------------------------#
[32m[20221208 14:20:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |           0.0122 |           2.8309 |           2.1169 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0206 |           2.5404 |           2.1017 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0331 |           2.4178 |           2.1122 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0428 |           2.3251 |           2.1231 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0489 |           2.2739 |           2.1290 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0525 |           2.2379 |           2.1255 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0553 |           2.2086 |           2.1351 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0597 |           2.1542 |           2.1314 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0607 |           2.1470 |           2.1428 |
[32m[20221208 14:20:43 @agent_ppo2.py:179][0m |          -0.0635 |           2.1132 |           2.1377 |
[32m[20221208 14:20:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.79
[32m[20221208 14:20:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.41
[32m[20221208 14:20:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.62
[32m[20221208 14:20:44 @agent_ppo2.py:137][0m Total time:       4.65 min
[32m[20221208 14:20:44 @agent_ppo2.py:139][0m 376832 total steps have happened
[32m[20221208 14:20:44 @agent_ppo2.py:115][0m #------------------------ Iteration 184 --------------------------#
[32m[20221208 14:20:44 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:20:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:44 @agent_ppo2.py:179][0m |           0.0044 |           0.3596 |           2.2008 |
[32m[20221208 14:20:44 @agent_ppo2.py:179][0m |          -0.0078 |           0.2731 |           2.1886 |
[32m[20221208 14:20:44 @agent_ppo2.py:179][0m |          -0.0142 |           0.2541 |           2.1769 |
[32m[20221208 14:20:44 @agent_ppo2.py:179][0m |          -0.0180 |           0.2439 |           2.1895 |
[32m[20221208 14:20:44 @agent_ppo2.py:179][0m |          -0.0201 |           0.2370 |           2.1905 |
[32m[20221208 14:20:45 @agent_ppo2.py:179][0m |          -0.0233 |           0.2339 |           2.1876 |
[32m[20221208 14:20:45 @agent_ppo2.py:179][0m |          -0.0179 |           0.2326 |           2.1697 |
[32m[20221208 14:20:45 @agent_ppo2.py:179][0m |          -0.0240 |           0.2276 |           2.1865 |
[32m[20221208 14:20:45 @agent_ppo2.py:179][0m |          -0.0238 |           0.2267 |           2.1907 |
[32m[20221208 14:20:45 @agent_ppo2.py:179][0m |          -0.0211 |           0.2217 |           2.1754 |
[32m[20221208 14:20:45 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:20:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.26
[32m[20221208 14:20:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.70
[32m[20221208 14:20:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.41
[32m[20221208 14:20:45 @agent_ppo2.py:137][0m Total time:       4.67 min
[32m[20221208 14:20:45 @agent_ppo2.py:139][0m 378880 total steps have happened
[32m[20221208 14:20:45 @agent_ppo2.py:115][0m #------------------------ Iteration 185 --------------------------#
[32m[20221208 14:20:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |           0.0317 |           3.6631 |           2.1403 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0072 |           3.2733 |           2.1308 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0269 |           3.1577 |           2.1491 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0360 |           3.0284 |           2.1588 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0432 |           2.9555 |           2.1679 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0461 |           2.8700 |           2.1765 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0484 |           2.8145 |           2.1770 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0517 |           2.7731 |           2.1817 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0561 |           2.7014 |           2.1880 |
[32m[20221208 14:20:46 @agent_ppo2.py:179][0m |          -0.0567 |           2.6142 |           2.1930 |
[32m[20221208 14:20:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:20:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.48
[32m[20221208 14:20:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.47
[32m[20221208 14:20:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.55
[32m[20221208 14:20:47 @agent_ppo2.py:137][0m Total time:       4.70 min
[32m[20221208 14:20:47 @agent_ppo2.py:139][0m 380928 total steps have happened
[32m[20221208 14:20:47 @agent_ppo2.py:115][0m #------------------------ Iteration 186 --------------------------#
[32m[20221208 14:20:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:47 @agent_ppo2.py:179][0m |           0.0270 |           3.3545 |           2.2162 |
[32m[20221208 14:20:47 @agent_ppo2.py:179][0m |          -0.0041 |           2.8913 |           2.1972 |
[32m[20221208 14:20:47 @agent_ppo2.py:179][0m |          -0.0209 |           2.7411 |           2.2284 |
[32m[20221208 14:20:47 @agent_ppo2.py:179][0m |          -0.0298 |           2.6084 |           2.2246 |
[32m[20221208 14:20:47 @agent_ppo2.py:179][0m |          -0.0399 |           2.5519 |           2.2384 |
[32m[20221208 14:20:48 @agent_ppo2.py:179][0m |          -0.0424 |           2.4903 |           2.2331 |
[32m[20221208 14:20:48 @agent_ppo2.py:179][0m |          -0.0479 |           2.4465 |           2.2446 |
[32m[20221208 14:20:48 @agent_ppo2.py:179][0m |          -0.0545 |           2.3968 |           2.2559 |
[32m[20221208 14:20:48 @agent_ppo2.py:179][0m |          -0.0564 |           2.3713 |           2.2585 |
[32m[20221208 14:20:48 @agent_ppo2.py:179][0m |          -0.0575 |           2.3282 |           2.2603 |
[32m[20221208 14:20:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.69
[32m[20221208 14:20:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.10
[32m[20221208 14:20:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.03
[32m[20221208 14:20:48 @agent_ppo2.py:137][0m Total time:       4.72 min
[32m[20221208 14:20:48 @agent_ppo2.py:139][0m 382976 total steps have happened
[32m[20221208 14:20:48 @agent_ppo2.py:115][0m #------------------------ Iteration 187 --------------------------#
[32m[20221208 14:20:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |           0.0213 |           4.0112 |           2.1766 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0147 |           3.0815 |           2.1537 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0316 |           2.6832 |           2.1671 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0427 |           2.4976 |           2.1685 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0484 |           2.3839 |           2.1771 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0537 |           2.3202 |           2.1729 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0571 |           2.2809 |           2.1773 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0595 |           2.2273 |           2.1821 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0611 |           2.1838 |           2.1733 |
[32m[20221208 14:20:49 @agent_ppo2.py:179][0m |          -0.0633 |           2.1149 |           2.1847 |
[32m[20221208 14:20:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.42
[32m[20221208 14:20:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.64
[32m[20221208 14:20:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.20
[32m[20221208 14:20:50 @agent_ppo2.py:137][0m Total time:       4.75 min
[32m[20221208 14:20:50 @agent_ppo2.py:139][0m 385024 total steps have happened
[32m[20221208 14:20:50 @agent_ppo2.py:115][0m #------------------------ Iteration 188 --------------------------#
[32m[20221208 14:20:50 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:20:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:50 @agent_ppo2.py:179][0m |           0.0369 |           1.0276 |           2.1740 |
[32m[20221208 14:20:50 @agent_ppo2.py:179][0m |           0.0049 |           0.6344 |           2.1589 |
[32m[20221208 14:20:50 @agent_ppo2.py:179][0m |          -0.0228 |           0.5519 |           2.1818 |
[32m[20221208 14:20:50 @agent_ppo2.py:179][0m |          -0.0355 |           0.5139 |           2.1843 |
[32m[20221208 14:20:50 @agent_ppo2.py:179][0m |          -0.0388 |           0.4939 |           2.1955 |
[32m[20221208 14:20:51 @agent_ppo2.py:179][0m |          -0.0414 |           0.4735 |           2.1956 |
[32m[20221208 14:20:51 @agent_ppo2.py:179][0m |          -0.0458 |           0.4623 |           2.2103 |
[32m[20221208 14:20:51 @agent_ppo2.py:179][0m |          -0.0461 |           0.4508 |           2.2084 |
[32m[20221208 14:20:51 @agent_ppo2.py:179][0m |          -0.0488 |           0.4425 |           2.2140 |
[32m[20221208 14:20:51 @agent_ppo2.py:179][0m |          -0.0484 |           0.4361 |           2.2191 |
[32m[20221208 14:20:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.77
[32m[20221208 14:20:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.23
[32m[20221208 14:20:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.02
[32m[20221208 14:20:51 @agent_ppo2.py:137][0m Total time:       4.77 min
[32m[20221208 14:20:51 @agent_ppo2.py:139][0m 387072 total steps have happened
[32m[20221208 14:20:51 @agent_ppo2.py:115][0m #------------------------ Iteration 189 --------------------------#
[32m[20221208 14:20:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |           0.0069 |           2.1490 |           2.2368 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0201 |           1.4710 |           2.2154 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0297 |           1.2841 |           2.2344 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0433 |           1.2018 |           2.2532 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0513 |           1.1496 |           2.2586 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0538 |           1.1041 |           2.2506 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0581 |           1.0841 |           2.2564 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0591 |           1.0449 |           2.2530 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0595 |           1.0273 |           2.2619 |
[32m[20221208 14:20:52 @agent_ppo2.py:179][0m |          -0.0626 |           1.0246 |           2.2695 |
[32m[20221208 14:20:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.04
[32m[20221208 14:20:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.08
[32m[20221208 14:20:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.92
[32m[20221208 14:20:53 @agent_ppo2.py:137][0m Total time:       4.80 min
[32m[20221208 14:20:53 @agent_ppo2.py:139][0m 389120 total steps have happened
[32m[20221208 14:20:53 @agent_ppo2.py:115][0m #------------------------ Iteration 190 --------------------------#
[32m[20221208 14:20:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:53 @agent_ppo2.py:179][0m |           0.0273 |           1.3078 |           2.3103 |
[32m[20221208 14:20:53 @agent_ppo2.py:179][0m |          -0.0159 |           0.9963 |           2.2615 |
[32m[20221208 14:20:53 @agent_ppo2.py:179][0m |          -0.0341 |           0.9133 |           2.2866 |
[32m[20221208 14:20:53 @agent_ppo2.py:179][0m |          -0.0424 |           0.8720 |           2.2981 |
[32m[20221208 14:20:53 @agent_ppo2.py:179][0m |          -0.0468 |           0.8477 |           2.3019 |
[32m[20221208 14:20:54 @agent_ppo2.py:179][0m |          -0.0499 |           0.8284 |           2.3070 |
[32m[20221208 14:20:54 @agent_ppo2.py:179][0m |          -0.0534 |           0.8112 |           2.3095 |
[32m[20221208 14:20:54 @agent_ppo2.py:179][0m |          -0.0565 |           0.7884 |           2.3186 |
[32m[20221208 14:20:54 @agent_ppo2.py:179][0m |          -0.0580 |           0.7803 |           2.3210 |
[32m[20221208 14:20:54 @agent_ppo2.py:179][0m |          -0.0603 |           0.7702 |           2.3255 |
[32m[20221208 14:20:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:20:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.94
[32m[20221208 14:20:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.72
[32m[20221208 14:20:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.11
[32m[20221208 14:20:54 @agent_ppo2.py:137][0m Total time:       4.82 min
[32m[20221208 14:20:54 @agent_ppo2.py:139][0m 391168 total steps have happened
[32m[20221208 14:20:54 @agent_ppo2.py:115][0m #------------------------ Iteration 191 --------------------------#
[32m[20221208 14:20:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |           0.0149 |           4.5860 |           2.3550 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0029 |           4.0280 |           2.3158 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0266 |           3.9130 |           2.3502 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0382 |           3.7957 |           2.3436 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0430 |           3.6789 |           2.3607 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0474 |           3.6591 |           2.3577 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0536 |           3.5610 |           2.3540 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0569 |           3.5577 |           2.3713 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0623 |           3.4466 |           2.3804 |
[32m[20221208 14:20:55 @agent_ppo2.py:179][0m |          -0.0638 |           3.4169 |           2.3782 |
[32m[20221208 14:20:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:20:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.73
[32m[20221208 14:20:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.14
[32m[20221208 14:20:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.24
[32m[20221208 14:20:56 @agent_ppo2.py:137][0m Total time:       4.85 min
[32m[20221208 14:20:56 @agent_ppo2.py:139][0m 393216 total steps have happened
[32m[20221208 14:20:56 @agent_ppo2.py:115][0m #------------------------ Iteration 192 --------------------------#
[32m[20221208 14:20:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:56 @agent_ppo2.py:179][0m |           0.0267 |           4.8533 |           2.3118 |
[32m[20221208 14:20:56 @agent_ppo2.py:179][0m |           0.0305 |           4.2958 |           2.2376 |
[32m[20221208 14:20:56 @agent_ppo2.py:179][0m |           0.0010 |           4.1389 |           2.2743 |
[32m[20221208 14:20:56 @agent_ppo2.py:179][0m |          -0.0261 |           4.0340 |           2.3247 |
[32m[20221208 14:20:56 @agent_ppo2.py:179][0m |          -0.0337 |           3.9520 |           2.3446 |
[32m[20221208 14:20:57 @agent_ppo2.py:179][0m |          -0.0398 |           3.9022 |           2.3428 |
[32m[20221208 14:20:57 @agent_ppo2.py:179][0m |          -0.0415 |           3.8698 |           2.3481 |
[32m[20221208 14:20:57 @agent_ppo2.py:179][0m |          -0.0487 |           3.8497 |           2.3547 |
[32m[20221208 14:20:57 @agent_ppo2.py:179][0m |          -0.0474 |           3.7970 |           2.3616 |
[32m[20221208 14:20:57 @agent_ppo2.py:179][0m |          -0.0501 |           3.7664 |           2.3655 |
[32m[20221208 14:20:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.33
[32m[20221208 14:20:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.55
[32m[20221208 14:20:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.04
[32m[20221208 14:20:57 @agent_ppo2.py:137][0m Total time:       4.87 min
[32m[20221208 14:20:57 @agent_ppo2.py:139][0m 395264 total steps have happened
[32m[20221208 14:20:57 @agent_ppo2.py:115][0m #------------------------ Iteration 193 --------------------------#
[32m[20221208 14:20:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |           0.0118 |           4.5681 |           2.4511 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0143 |           4.1402 |           2.4411 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0308 |           3.9702 |           2.4696 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0379 |           3.8542 |           2.4886 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0433 |           3.7785 |           2.4908 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0486 |           3.7097 |           2.5051 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0491 |           3.6631 |           2.5130 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0521 |           3.6279 |           2.5180 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0556 |           3.5857 |           2.5211 |
[32m[20221208 14:20:58 @agent_ppo2.py:179][0m |          -0.0565 |           3.5660 |           2.5297 |
[32m[20221208 14:20:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:20:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.84
[32m[20221208 14:20:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.87
[32m[20221208 14:20:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.68
[32m[20221208 14:20:59 @agent_ppo2.py:137][0m Total time:       4.90 min
[32m[20221208 14:20:59 @agent_ppo2.py:139][0m 397312 total steps have happened
[32m[20221208 14:20:59 @agent_ppo2.py:115][0m #------------------------ Iteration 194 --------------------------#
[32m[20221208 14:20:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:20:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:20:59 @agent_ppo2.py:179][0m |           0.0310 |           2.9048 |           2.4843 |
[32m[20221208 14:20:59 @agent_ppo2.py:179][0m |          -0.0052 |           2.7382 |           2.4714 |
[32m[20221208 14:20:59 @agent_ppo2.py:179][0m |          -0.0294 |           2.6591 |           2.5114 |
[32m[20221208 14:20:59 @agent_ppo2.py:179][0m |          -0.0386 |           2.5960 |           2.5144 |
[32m[20221208 14:20:59 @agent_ppo2.py:179][0m |          -0.0426 |           2.5406 |           2.5272 |
[32m[20221208 14:21:00 @agent_ppo2.py:179][0m |          -0.0456 |           2.5185 |           2.5214 |
[32m[20221208 14:21:00 @agent_ppo2.py:179][0m |          -0.0487 |           2.4842 |           2.5318 |
[32m[20221208 14:21:00 @agent_ppo2.py:179][0m |          -0.0534 |           2.4617 |           2.5348 |
[32m[20221208 14:21:00 @agent_ppo2.py:179][0m |          -0.0537 |           2.4507 |           2.5360 |
[32m[20221208 14:21:00 @agent_ppo2.py:179][0m |          -0.0563 |           2.4147 |           2.5413 |
[32m[20221208 14:21:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.68
[32m[20221208 14:21:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.89
[32m[20221208 14:21:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.02
[32m[20221208 14:21:00 @agent_ppo2.py:137][0m Total time:       4.92 min
[32m[20221208 14:21:00 @agent_ppo2.py:139][0m 399360 total steps have happened
[32m[20221208 14:21:00 @agent_ppo2.py:115][0m #------------------------ Iteration 195 --------------------------#
[32m[20221208 14:21:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |           0.0254 |           5.8034 |           2.4424 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0065 |           5.3365 |           2.4171 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0204 |           5.2183 |           2.4336 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0386 |           5.1506 |           2.4721 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0421 |           5.1226 |           2.4672 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0478 |           5.0648 |           2.4778 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0495 |           5.0354 |           2.4815 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0523 |           5.0174 |           2.4827 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0572 |           4.9968 |           2.4863 |
[32m[20221208 14:21:01 @agent_ppo2.py:179][0m |          -0.0574 |           4.9911 |           2.4852 |
[32m[20221208 14:21:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.54
[32m[20221208 14:21:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.00
[32m[20221208 14:21:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.40
[32m[20221208 14:21:02 @agent_ppo2.py:137][0m Total time:       4.95 min
[32m[20221208 14:21:02 @agent_ppo2.py:139][0m 401408 total steps have happened
[32m[20221208 14:21:02 @agent_ppo2.py:115][0m #------------------------ Iteration 196 --------------------------#
[32m[20221208 14:21:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:02 @agent_ppo2.py:179][0m |           0.0258 |           4.3780 |           2.5021 |
[32m[20221208 14:21:02 @agent_ppo2.py:179][0m |           0.0227 |           3.9742 |           2.4567 |
[32m[20221208 14:21:02 @agent_ppo2.py:179][0m |          -0.0054 |           3.8486 |           2.4630 |
[32m[20221208 14:21:02 @agent_ppo2.py:179][0m |          -0.0299 |           3.7597 |           2.5061 |
[32m[20221208 14:21:02 @agent_ppo2.py:179][0m |          -0.0399 |           3.7218 |           2.5122 |
[32m[20221208 14:21:03 @agent_ppo2.py:179][0m |          -0.0445 |           3.6735 |           2.5370 |
[32m[20221208 14:21:03 @agent_ppo2.py:179][0m |          -0.0455 |           3.6362 |           2.5460 |
[32m[20221208 14:21:03 @agent_ppo2.py:179][0m |          -0.0511 |           3.6287 |           2.5449 |
[32m[20221208 14:21:03 @agent_ppo2.py:179][0m |          -0.0543 |           3.5951 |           2.5638 |
[32m[20221208 14:21:03 @agent_ppo2.py:179][0m |          -0.0563 |           3.5613 |           2.5598 |
[32m[20221208 14:21:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.19
[32m[20221208 14:21:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.95
[32m[20221208 14:21:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.62
[32m[20221208 14:21:03 @agent_ppo2.py:137][0m Total time:       4.97 min
[32m[20221208 14:21:03 @agent_ppo2.py:139][0m 403456 total steps have happened
[32m[20221208 14:21:03 @agent_ppo2.py:115][0m #------------------------ Iteration 197 --------------------------#
[32m[20221208 14:21:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |           0.0308 |           2.6569 |           2.5949 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0092 |           2.3040 |           2.5990 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0278 |           2.2392 |           2.6104 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0304 |           2.2098 |           2.6323 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0398 |           2.1672 |           2.6350 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0378 |           2.1512 |           2.6315 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0442 |           2.1119 |           2.6415 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0435 |           2.0931 |           2.6468 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0485 |           2.0894 |           2.6579 |
[32m[20221208 14:21:04 @agent_ppo2.py:179][0m |          -0.0492 |           2.0932 |           2.6626 |
[32m[20221208 14:21:04 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.82
[32m[20221208 14:21:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.95
[32m[20221208 14:21:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.90
[32m[20221208 14:21:05 @agent_ppo2.py:137][0m Total time:       5.00 min
[32m[20221208 14:21:05 @agent_ppo2.py:139][0m 405504 total steps have happened
[32m[20221208 14:21:05 @agent_ppo2.py:115][0m #------------------------ Iteration 198 --------------------------#
[32m[20221208 14:21:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:05 @agent_ppo2.py:179][0m |           0.0341 |           4.6970 |           2.6318 |
[32m[20221208 14:21:05 @agent_ppo2.py:179][0m |           0.0076 |           4.3479 |           2.6515 |
[32m[20221208 14:21:05 @agent_ppo2.py:179][0m |          -0.0226 |           4.2377 |           2.6769 |
[32m[20221208 14:21:05 @agent_ppo2.py:179][0m |          -0.0340 |           4.1238 |           2.7021 |
[32m[20221208 14:21:05 @agent_ppo2.py:179][0m |          -0.0375 |           4.0465 |           2.7103 |
[32m[20221208 14:21:05 @agent_ppo2.py:179][0m |          -0.0436 |           3.9838 |           2.7193 |
[32m[20221208 14:21:06 @agent_ppo2.py:179][0m |          -0.0434 |           3.9353 |           2.7206 |
[32m[20221208 14:21:06 @agent_ppo2.py:179][0m |          -0.0474 |           3.9025 |           2.7221 |
[32m[20221208 14:21:06 @agent_ppo2.py:179][0m |          -0.0509 |           3.8412 |           2.7316 |
[32m[20221208 14:21:06 @agent_ppo2.py:179][0m |          -0.0528 |           3.7355 |           2.7332 |
[32m[20221208 14:21:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.25
[32m[20221208 14:21:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.59
[32m[20221208 14:21:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.26
[32m[20221208 14:21:06 @agent_ppo2.py:137][0m Total time:       5.02 min
[32m[20221208 14:21:06 @agent_ppo2.py:139][0m 407552 total steps have happened
[32m[20221208 14:21:06 @agent_ppo2.py:115][0m #------------------------ Iteration 199 --------------------------#
[32m[20221208 14:21:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |           0.0177 |           0.3782 |           2.7112 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |           0.0055 |           0.3044 |           2.6575 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |           0.0001 |           0.2995 |           2.6597 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0081 |           0.3048 |           2.6683 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0101 |           0.2904 |           2.6956 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0128 |           0.2871 |           2.6343 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0172 |           0.2852 |           2.6917 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0202 |           0.2845 |           2.7073 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0076 |           0.2844 |           2.6836 |
[32m[20221208 14:21:07 @agent_ppo2.py:179][0m |          -0.0228 |           0.2854 |           2.7079 |
[32m[20221208 14:21:07 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:21:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.84
[32m[20221208 14:21:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.96
[32m[20221208 14:21:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.25
[32m[20221208 14:21:08 @agent_ppo2.py:137][0m Total time:       5.05 min
[32m[20221208 14:21:08 @agent_ppo2.py:139][0m 409600 total steps have happened
[32m[20221208 14:21:08 @agent_ppo2.py:115][0m #------------------------ Iteration 200 --------------------------#
[32m[20221208 14:21:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:08 @agent_ppo2.py:179][0m |           0.0274 |           3.3632 |           2.6367 |
[32m[20221208 14:21:08 @agent_ppo2.py:179][0m |           0.0399 |           3.0231 |           2.5359 |
[32m[20221208 14:21:08 @agent_ppo2.py:179][0m |           0.0033 |           2.8430 |           2.5770 |
[32m[20221208 14:21:08 @agent_ppo2.py:179][0m |          -0.0252 |           2.7505 |           2.6375 |
[32m[20221208 14:21:08 @agent_ppo2.py:179][0m |          -0.0399 |           2.6576 |           2.6529 |
[32m[20221208 14:21:08 @agent_ppo2.py:179][0m |          -0.0435 |           2.6590 |           2.6659 |
[32m[20221208 14:21:09 @agent_ppo2.py:179][0m |          -0.0463 |           2.6086 |           2.6655 |
[32m[20221208 14:21:09 @agent_ppo2.py:179][0m |          -0.0523 |           2.5242 |           2.6742 |
[32m[20221208 14:21:09 @agent_ppo2.py:179][0m |          -0.0564 |           2.4788 |           2.6862 |
[32m[20221208 14:21:09 @agent_ppo2.py:179][0m |          -0.0595 |           2.4223 |           2.6982 |
[32m[20221208 14:21:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.94
[32m[20221208 14:21:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.80
[32m[20221208 14:21:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.35
[32m[20221208 14:21:09 @agent_ppo2.py:137][0m Total time:       5.07 min
[32m[20221208 14:21:09 @agent_ppo2.py:139][0m 411648 total steps have happened
[32m[20221208 14:21:09 @agent_ppo2.py:115][0m #------------------------ Iteration 201 --------------------------#
[32m[20221208 14:21:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |           0.0251 |           4.6858 |           2.6959 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0085 |           4.1224 |           2.6904 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0292 |           3.9995 |           2.7017 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0377 |           3.9523 |           2.7125 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0401 |           3.8679 |           2.7224 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0412 |           3.8099 |           2.7316 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0453 |           3.7774 |           2.7318 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0492 |           3.7618 |           2.7516 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0503 |           3.7080 |           2.7529 |
[32m[20221208 14:21:10 @agent_ppo2.py:179][0m |          -0.0503 |           3.6669 |           2.7620 |
[32m[20221208 14:21:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.63
[32m[20221208 14:21:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.98
[32m[20221208 14:21:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.21
[32m[20221208 14:21:11 @agent_ppo2.py:137][0m Total time:       5.10 min
[32m[20221208 14:21:11 @agent_ppo2.py:139][0m 413696 total steps have happened
[32m[20221208 14:21:11 @agent_ppo2.py:115][0m #------------------------ Iteration 202 --------------------------#
[32m[20221208 14:21:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:21:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:11 @agent_ppo2.py:179][0m |           0.0318 |           6.1041 |           2.8198 |
[32m[20221208 14:21:11 @agent_ppo2.py:179][0m |           0.0146 |           5.7872 |           2.7382 |
[32m[20221208 14:21:11 @agent_ppo2.py:179][0m |          -0.0235 |           5.7006 |           2.8161 |
[32m[20221208 14:21:11 @agent_ppo2.py:179][0m |          -0.0305 |           5.6629 |           2.8316 |
[32m[20221208 14:21:11 @agent_ppo2.py:179][0m |          -0.0409 |           5.5838 |           2.8346 |
[32m[20221208 14:21:11 @agent_ppo2.py:179][0m |          -0.0446 |           5.5636 |           2.8569 |
[32m[20221208 14:21:12 @agent_ppo2.py:179][0m |          -0.0498 |           5.5777 |           2.8559 |
[32m[20221208 14:21:12 @agent_ppo2.py:179][0m |          -0.0522 |           5.4774 |           2.8654 |
[32m[20221208 14:21:12 @agent_ppo2.py:179][0m |          -0.0525 |           5.4876 |           2.8632 |
[32m[20221208 14:21:12 @agent_ppo2.py:179][0m |          -0.0533 |           5.4666 |           2.8656 |
[32m[20221208 14:21:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.57
[32m[20221208 14:21:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.60
[32m[20221208 14:21:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.37
[32m[20221208 14:21:12 @agent_ppo2.py:137][0m Total time:       5.12 min
[32m[20221208 14:21:12 @agent_ppo2.py:139][0m 415744 total steps have happened
[32m[20221208 14:21:12 @agent_ppo2.py:115][0m #------------------------ Iteration 203 --------------------------#
[32m[20221208 14:21:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |           0.0338 |           7.1302 |           2.7344 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |           0.0036 |           6.6741 |           2.4286 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0171 |           6.5251 |           2.4403 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0317 |           6.5048 |           2.4581 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0385 |           6.3896 |           2.4578 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0436 |           6.3521 |           2.4791 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0441 |           6.3172 |           2.4645 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0495 |           6.2413 |           2.4797 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0522 |           6.1969 |           2.4868 |
[32m[20221208 14:21:13 @agent_ppo2.py:179][0m |          -0.0551 |           6.1620 |           2.4977 |
[32m[20221208 14:21:13 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:21:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.85
[32m[20221208 14:21:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.62
[32m[20221208 14:21:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.57
[32m[20221208 14:21:14 @agent_ppo2.py:137][0m Total time:       5.15 min
[32m[20221208 14:21:14 @agent_ppo2.py:139][0m 417792 total steps have happened
[32m[20221208 14:21:14 @agent_ppo2.py:115][0m #------------------------ Iteration 204 --------------------------#
[32m[20221208 14:21:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:14 @agent_ppo2.py:179][0m |           0.0192 |           4.6011 |           2.9501 |
[32m[20221208 14:21:14 @agent_ppo2.py:179][0m |          -0.0057 |           4.0827 |           2.9378 |
[32m[20221208 14:21:14 @agent_ppo2.py:179][0m |          -0.0278 |           3.8698 |           2.9684 |
[32m[20221208 14:21:14 @agent_ppo2.py:179][0m |          -0.0347 |           3.7190 |           2.9596 |
[32m[20221208 14:21:14 @agent_ppo2.py:179][0m |          -0.0433 |           3.5992 |           2.9898 |
[32m[20221208 14:21:15 @agent_ppo2.py:179][0m |          -0.0467 |           3.5202 |           2.9951 |
[32m[20221208 14:21:15 @agent_ppo2.py:179][0m |          -0.0480 |           3.4503 |           3.0241 |
[32m[20221208 14:21:15 @agent_ppo2.py:179][0m |          -0.0499 |           3.3725 |           3.0121 |
[32m[20221208 14:21:15 @agent_ppo2.py:179][0m |          -0.0533 |           3.2886 |           3.0347 |
[32m[20221208 14:21:15 @agent_ppo2.py:179][0m |          -0.0551 |           3.2427 |           3.0268 |
[32m[20221208 14:21:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.02
[32m[20221208 14:21:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.45
[32m[20221208 14:21:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.49
[32m[20221208 14:21:15 @agent_ppo2.py:137][0m Total time:       5.17 min
[32m[20221208 14:21:15 @agent_ppo2.py:139][0m 419840 total steps have happened
[32m[20221208 14:21:15 @agent_ppo2.py:115][0m #------------------------ Iteration 205 --------------------------#
[32m[20221208 14:21:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |           0.0165 |           5.1131 |           2.9929 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0152 |           4.6431 |           2.9842 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0273 |           4.4735 |           2.9932 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0418 |           4.3503 |           3.0129 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0472 |           4.3312 |           3.0384 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0528 |           4.2804 |           3.0432 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0524 |           4.1860 |           3.0495 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0551 |           4.1481 |           3.0502 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0599 |           4.0586 |           3.0525 |
[32m[20221208 14:21:16 @agent_ppo2.py:179][0m |          -0.0618 |           4.0265 |           3.0770 |
[32m[20221208 14:21:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.35
[32m[20221208 14:21:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.67
[32m[20221208 14:21:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.18
[32m[20221208 14:21:17 @agent_ppo2.py:137][0m Total time:       5.20 min
[32m[20221208 14:21:17 @agent_ppo2.py:139][0m 421888 total steps have happened
[32m[20221208 14:21:17 @agent_ppo2.py:115][0m #------------------------ Iteration 206 --------------------------#
[32m[20221208 14:21:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:17 @agent_ppo2.py:179][0m |           0.0099 |           4.6344 |           3.1409 |
[32m[20221208 14:21:17 @agent_ppo2.py:179][0m |          -0.0071 |           4.3682 |           3.0787 |
[32m[20221208 14:21:17 @agent_ppo2.py:179][0m |          -0.0273 |           4.2780 |           3.1018 |
[32m[20221208 14:21:17 @agent_ppo2.py:179][0m |          -0.0361 |           4.1991 |           3.1204 |
[32m[20221208 14:21:17 @agent_ppo2.py:179][0m |          -0.0424 |           4.1480 |           3.1414 |
[32m[20221208 14:21:17 @agent_ppo2.py:179][0m |          -0.0459 |           4.1210 |           3.1558 |
[32m[20221208 14:21:18 @agent_ppo2.py:179][0m |          -0.0486 |           4.0899 |           3.1583 |
[32m[20221208 14:21:18 @agent_ppo2.py:179][0m |          -0.0516 |           4.0229 |           3.1742 |
[32m[20221208 14:21:18 @agent_ppo2.py:179][0m |          -0.0545 |           4.0174 |           3.1761 |
[32m[20221208 14:21:18 @agent_ppo2.py:179][0m |          -0.0576 |           3.9773 |           3.1982 |
[32m[20221208 14:21:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.04
[32m[20221208 14:21:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.34
[32m[20221208 14:21:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.83
[32m[20221208 14:21:18 @agent_ppo2.py:137][0m Total time:       5.22 min
[32m[20221208 14:21:18 @agent_ppo2.py:139][0m 423936 total steps have happened
[32m[20221208 14:21:18 @agent_ppo2.py:115][0m #------------------------ Iteration 207 --------------------------#
[32m[20221208 14:21:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |           0.0167 |           2.5136 |           3.1709 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |           0.0051 |           2.2394 |           3.1604 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0195 |           2.1405 |           3.2282 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0256 |           2.0752 |           3.2609 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0314 |           2.0266 |           3.2792 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0354 |           1.9825 |           3.3030 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0382 |           1.9488 |           3.3234 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0374 |           1.9670 |           3.3499 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0348 |           1.8998 |           3.3212 |
[32m[20221208 14:21:19 @agent_ppo2.py:179][0m |          -0.0391 |           1.8874 |           3.3520 |
[32m[20221208 14:21:19 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:21:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.13
[32m[20221208 14:21:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.44
[32m[20221208 14:21:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.77
[32m[20221208 14:21:20 @agent_ppo2.py:137][0m Total time:       5.24 min
[32m[20221208 14:21:20 @agent_ppo2.py:139][0m 425984 total steps have happened
[32m[20221208 14:21:20 @agent_ppo2.py:115][0m #------------------------ Iteration 208 --------------------------#
[32m[20221208 14:21:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |           0.0223 |           5.0329 |           3.3003 |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |           0.0470 |           4.5226 |           3.0245 |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |          -0.0169 |           4.3449 |           3.1778 |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |          -0.0266 |           4.2743 |           3.2471 |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |          -0.0349 |           4.2082 |           3.2851 |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |          -0.0395 |           4.1511 |           3.3066 |
[32m[20221208 14:21:20 @agent_ppo2.py:179][0m |          -0.0428 |           4.1062 |           3.3216 |
[32m[20221208 14:21:21 @agent_ppo2.py:179][0m |          -0.0466 |           4.0610 |           3.3284 |
[32m[20221208 14:21:21 @agent_ppo2.py:179][0m |          -0.0482 |           4.0392 |           3.3445 |
[32m[20221208 14:21:21 @agent_ppo2.py:179][0m |          -0.0497 |           4.0065 |           3.3420 |
[32m[20221208 14:21:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.58
[32m[20221208 14:21:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.15
[32m[20221208 14:21:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.05
[32m[20221208 14:21:21 @agent_ppo2.py:137][0m Total time:       5.27 min
[32m[20221208 14:21:21 @agent_ppo2.py:139][0m 428032 total steps have happened
[32m[20221208 14:21:21 @agent_ppo2.py:115][0m #------------------------ Iteration 209 --------------------------#
[32m[20221208 14:21:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |           0.0504 |           2.6397 |           3.2703 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |           0.0225 |           2.1163 |           3.2314 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0156 |           1.9669 |           3.2715 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0294 |           1.8386 |           3.2933 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0361 |           1.7889 |           3.3146 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0410 |           1.7552 |           3.3215 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0477 |           1.6901 |           3.3467 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0501 |           1.6458 |           3.3546 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0515 |           1.5886 |           3.3666 |
[32m[20221208 14:21:22 @agent_ppo2.py:179][0m |          -0.0542 |           1.5617 |           3.3811 |
[32m[20221208 14:21:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.74
[32m[20221208 14:21:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.96
[32m[20221208 14:21:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 166.06
[32m[20221208 14:21:23 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 166.06
[32m[20221208 14:21:23 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 166.06
[32m[20221208 14:21:23 @agent_ppo2.py:137][0m Total time:       5.29 min
[32m[20221208 14:21:23 @agent_ppo2.py:139][0m 430080 total steps have happened
[32m[20221208 14:21:23 @agent_ppo2.py:115][0m #------------------------ Iteration 210 --------------------------#
[32m[20221208 14:21:23 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:21:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |           0.0143 |           3.6220 |           3.3983 |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |          -0.0020 |           3.1983 |           3.3094 |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |          -0.0238 |           3.0391 |           3.4115 |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |          -0.0317 |           2.9536 |           3.4310 |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |          -0.0417 |           2.8901 |           3.4483 |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |          -0.0455 |           2.8383 |           3.4495 |
[32m[20221208 14:21:23 @agent_ppo2.py:179][0m |          -0.0476 |           2.7944 |           3.4637 |
[32m[20221208 14:21:24 @agent_ppo2.py:179][0m |          -0.0519 |           2.7545 |           3.4821 |
[32m[20221208 14:21:24 @agent_ppo2.py:179][0m |          -0.0524 |           2.7152 |           3.4807 |
[32m[20221208 14:21:24 @agent_ppo2.py:179][0m |          -0.0554 |           2.7108 |           3.4905 |
[32m[20221208 14:21:24 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.98
[32m[20221208 14:21:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.60
[32m[20221208 14:21:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.23
[32m[20221208 14:21:24 @agent_ppo2.py:137][0m Total time:       5.32 min
[32m[20221208 14:21:24 @agent_ppo2.py:139][0m 432128 total steps have happened
[32m[20221208 14:21:24 @agent_ppo2.py:115][0m #------------------------ Iteration 211 --------------------------#
[32m[20221208 14:21:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |           0.0518 |           2.2799 |           3.3629 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |           0.0070 |           1.5650 |           3.2736 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0161 |           1.2623 |           3.3140 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0354 |           1.1205 |           3.3646 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0403 |           1.0275 |           3.3757 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0434 |           0.9722 |           3.3772 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0468 |           0.9376 |           3.3974 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0501 |           0.9063 |           3.3842 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0506 |           0.8903 |           3.4051 |
[32m[20221208 14:21:25 @agent_ppo2.py:179][0m |          -0.0539 |           0.8628 |           3.4156 |
[32m[20221208 14:21:25 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.57
[32m[20221208 14:21:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.76
[32m[20221208 14:21:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.16
[32m[20221208 14:21:26 @agent_ppo2.py:137][0m Total time:       5.34 min
[32m[20221208 14:21:26 @agent_ppo2.py:139][0m 434176 total steps have happened
[32m[20221208 14:21:26 @agent_ppo2.py:115][0m #------------------------ Iteration 212 --------------------------#
[32m[20221208 14:21:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |           0.0160 |           3.4963 |           3.3457 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0080 |           2.9627 |           3.3093 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0297 |           2.7245 |           3.3261 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0413 |           2.6084 |           3.3211 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0496 |           2.4511 |           3.3262 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0521 |           2.3959 |           3.3390 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0557 |           2.3582 |           3.3406 |
[32m[20221208 14:21:26 @agent_ppo2.py:179][0m |          -0.0592 |           2.2535 |           3.3621 |
[32m[20221208 14:21:27 @agent_ppo2.py:179][0m |          -0.0604 |           2.1557 |           3.3701 |
[32m[20221208 14:21:27 @agent_ppo2.py:179][0m |          -0.0628 |           2.1046 |           3.3711 |
[32m[20221208 14:21:27 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:21:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.00
[32m[20221208 14:21:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.43
[32m[20221208 14:21:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.59
[32m[20221208 14:21:27 @agent_ppo2.py:137][0m Total time:       5.37 min
[32m[20221208 14:21:27 @agent_ppo2.py:139][0m 436224 total steps have happened
[32m[20221208 14:21:27 @agent_ppo2.py:115][0m #------------------------ Iteration 213 --------------------------#
[32m[20221208 14:21:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |           0.0391 |           3.3153 |           3.4386 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |           0.0202 |           2.9847 |           3.3780 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0088 |           2.8709 |           3.4179 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0211 |           2.7816 |           3.4410 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0298 |           2.7409 |           3.4644 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0347 |           2.7022 |           3.4747 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0396 |           2.6738 |           3.4819 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0414 |           2.6347 |           3.4963 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0423 |           2.6022 |           3.5032 |
[32m[20221208 14:21:28 @agent_ppo2.py:179][0m |          -0.0434 |           2.5706 |           3.5042 |
[32m[20221208 14:21:28 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:21:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.30
[32m[20221208 14:21:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.47
[32m[20221208 14:21:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.80
[32m[20221208 14:21:28 @agent_ppo2.py:137][0m Total time:       5.39 min
[32m[20221208 14:21:28 @agent_ppo2.py:139][0m 438272 total steps have happened
[32m[20221208 14:21:28 @agent_ppo2.py:115][0m #------------------------ Iteration 214 --------------------------#
[32m[20221208 14:21:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |           0.0143 |           6.3907 |           3.4179 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0074 |           6.0184 |           3.4018 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0267 |           5.8664 |           3.4212 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0272 |           5.7240 |           3.4240 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0425 |           5.6254 |           3.4540 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0467 |           5.5617 |           3.4690 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0482 |           5.4484 |           3.4631 |
[32m[20221208 14:21:29 @agent_ppo2.py:179][0m |          -0.0542 |           5.3590 |           3.4756 |
[32m[20221208 14:21:30 @agent_ppo2.py:179][0m |          -0.0568 |           5.2821 |           3.5019 |
[32m[20221208 14:21:30 @agent_ppo2.py:179][0m |          -0.0592 |           5.2235 |           3.5057 |
[32m[20221208 14:21:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.41
[32m[20221208 14:21:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.44
[32m[20221208 14:21:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.24
[32m[20221208 14:21:30 @agent_ppo2.py:137][0m Total time:       5.42 min
[32m[20221208 14:21:30 @agent_ppo2.py:139][0m 440320 total steps have happened
[32m[20221208 14:21:30 @agent_ppo2.py:115][0m #------------------------ Iteration 215 --------------------------#
[32m[20221208 14:21:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |           0.0345 |           2.4874 |           3.7009 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |           0.0061 |           1.6763 |           3.6262 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0141 |           1.4723 |           3.6625 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0313 |           1.3794 |           3.6765 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0437 |           1.3304 |           3.7098 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0486 |           1.2790 |           3.7137 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0526 |           1.2236 |           3.7248 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0555 |           1.1967 |           3.7298 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0593 |           1.1622 |           3.7619 |
[32m[20221208 14:21:31 @agent_ppo2.py:179][0m |          -0.0597 |           1.1405 |           3.7492 |
[32m[20221208 14:21:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.30
[32m[20221208 14:21:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.57
[32m[20221208 14:21:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.84
[32m[20221208 14:21:31 @agent_ppo2.py:137][0m Total time:       5.44 min
[32m[20221208 14:21:31 @agent_ppo2.py:139][0m 442368 total steps have happened
[32m[20221208 14:21:31 @agent_ppo2.py:115][0m #------------------------ Iteration 216 --------------------------#
[32m[20221208 14:21:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |           0.0252 |           5.9500 |           3.5817 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0026 |           4.9399 |           3.5324 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0172 |           4.7359 |           3.5782 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0331 |           4.6795 |           3.6302 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0392 |           4.5547 |           3.6468 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0396 |           4.4936 |           3.6463 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0456 |           4.4793 |           3.6657 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0467 |           4.4032 |           3.6746 |
[32m[20221208 14:21:32 @agent_ppo2.py:179][0m |          -0.0453 |           4.3675 |           3.6904 |
[32m[20221208 14:21:33 @agent_ppo2.py:179][0m |          -0.0482 |           4.3378 |           3.6791 |
[32m[20221208 14:21:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.33
[32m[20221208 14:21:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.95
[32m[20221208 14:21:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 153.55
[32m[20221208 14:21:33 @agent_ppo2.py:137][0m Total time:       5.47 min
[32m[20221208 14:21:33 @agent_ppo2.py:139][0m 444416 total steps have happened
[32m[20221208 14:21:33 @agent_ppo2.py:115][0m #------------------------ Iteration 217 --------------------------#
[32m[20221208 14:21:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:33 @agent_ppo2.py:179][0m |           0.0252 |           5.1176 |           3.6212 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |           0.0003 |           4.8498 |           3.5764 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0239 |           4.7426 |           3.6782 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0309 |           4.6527 |           3.7072 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0363 |           4.5502 |           3.7119 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0397 |           4.4951 |           3.7326 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0376 |           4.4584 |           3.7204 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0385 |           4.4322 |           3.7180 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0428 |           4.3835 |           3.7422 |
[32m[20221208 14:21:34 @agent_ppo2.py:179][0m |          -0.0451 |           4.3254 |           3.7554 |
[32m[20221208 14:21:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.71
[32m[20221208 14:21:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.06
[32m[20221208 14:21:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.22
[32m[20221208 14:21:34 @agent_ppo2.py:137][0m Total time:       5.49 min
[32m[20221208 14:21:34 @agent_ppo2.py:139][0m 446464 total steps have happened
[32m[20221208 14:21:34 @agent_ppo2.py:115][0m #------------------------ Iteration 218 --------------------------#
[32m[20221208 14:21:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |           0.0396 |           1.2293 |           3.8112 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |           0.0104 |           0.9805 |           3.7519 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0260 |           0.8729 |           3.8045 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0365 |           0.8177 |           3.8236 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0418 |           0.7801 |           3.8251 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0475 |           0.7502 |           3.8434 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0501 |           0.7271 |           3.8555 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0543 |           0.7029 |           3.8698 |
[32m[20221208 14:21:35 @agent_ppo2.py:179][0m |          -0.0550 |           0.6862 |           3.8771 |
[32m[20221208 14:21:36 @agent_ppo2.py:179][0m |          -0.0597 |           0.6733 |           3.9082 |
[32m[20221208 14:21:36 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:21:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.80
[32m[20221208 14:21:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.05
[32m[20221208 14:21:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.52
[32m[20221208 14:21:36 @agent_ppo2.py:137][0m Total time:       5.52 min
[32m[20221208 14:21:36 @agent_ppo2.py:139][0m 448512 total steps have happened
[32m[20221208 14:21:36 @agent_ppo2.py:115][0m #------------------------ Iteration 219 --------------------------#
[32m[20221208 14:21:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:36 @agent_ppo2.py:179][0m |           0.0247 |           3.9081 |           3.9053 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |           0.0079 |           3.3049 |           3.8428 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0213 |           3.1127 |           3.9204 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0373 |           2.9690 |           3.9498 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0457 |           2.8595 |           3.9614 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0496 |           2.7723 |           3.9806 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0507 |           2.7389 |           3.9855 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0544 |           2.6315 |           3.9958 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0575 |           2.5937 |           4.0144 |
[32m[20221208 14:21:37 @agent_ppo2.py:179][0m |          -0.0588 |           2.5445 |           4.0206 |
[32m[20221208 14:21:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.49
[32m[20221208 14:21:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.73
[32m[20221208 14:21:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.50
[32m[20221208 14:21:37 @agent_ppo2.py:137][0m Total time:       5.54 min
[32m[20221208 14:21:37 @agent_ppo2.py:139][0m 450560 total steps have happened
[32m[20221208 14:21:37 @agent_ppo2.py:115][0m #------------------------ Iteration 220 --------------------------#
[32m[20221208 14:21:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |           0.0097 |           5.6905 |           3.9866 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0161 |           5.0812 |           3.9927 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0313 |           4.9164 |           4.0179 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0392 |           4.8045 |           4.0627 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0458 |           4.6528 |           4.0859 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0477 |           4.5811 |           4.1095 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0493 |           4.4502 |           4.1130 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0523 |           4.3618 |           4.1232 |
[32m[20221208 14:21:38 @agent_ppo2.py:179][0m |          -0.0538 |           4.2796 |           4.1387 |
[32m[20221208 14:21:39 @agent_ppo2.py:179][0m |          -0.0538 |           4.2512 |           4.1460 |
[32m[20221208 14:21:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.19
[32m[20221208 14:21:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.39
[32m[20221208 14:21:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.50
[32m[20221208 14:21:39 @agent_ppo2.py:137][0m Total time:       5.57 min
[32m[20221208 14:21:39 @agent_ppo2.py:139][0m 452608 total steps have happened
[32m[20221208 14:21:39 @agent_ppo2.py:115][0m #------------------------ Iteration 221 --------------------------#
[32m[20221208 14:21:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:39 @agent_ppo2.py:179][0m |           0.0335 |           4.4042 |           4.1939 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0022 |           3.7069 |           4.1552 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0188 |           3.4891 |           4.1752 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0297 |           3.2972 |           4.2468 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0430 |           3.2086 |           4.2551 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0496 |           3.1693 |           4.2600 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0528 |           3.0746 |           4.2949 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0571 |           3.0178 |           4.2947 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0571 |           2.9375 |           4.2905 |
[32m[20221208 14:21:40 @agent_ppo2.py:179][0m |          -0.0596 |           2.8857 |           4.3042 |
[32m[20221208 14:21:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.96
[32m[20221208 14:21:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.28
[32m[20221208 14:21:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.77
[32m[20221208 14:21:40 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 169.77
[32m[20221208 14:21:40 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 169.77
[32m[20221208 14:21:40 @agent_ppo2.py:137][0m Total time:       5.59 min
[32m[20221208 14:21:40 @agent_ppo2.py:139][0m 454656 total steps have happened
[32m[20221208 14:21:40 @agent_ppo2.py:115][0m #------------------------ Iteration 222 --------------------------#
[32m[20221208 14:21:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |           0.0399 |           3.5208 |           4.1807 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |           0.0479 |           2.9047 |           4.0501 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0050 |           2.7306 |           4.2013 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0206 |           2.6476 |           4.3074 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0292 |           2.5914 |           4.3557 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0362 |           2.5741 |           4.3716 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0414 |           2.4977 |           4.4040 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0396 |           2.4600 |           4.4086 |
[32m[20221208 14:21:41 @agent_ppo2.py:179][0m |          -0.0453 |           2.4008 |           4.4278 |
[32m[20221208 14:21:42 @agent_ppo2.py:179][0m |          -0.0465 |           2.3853 |           4.4318 |
[32m[20221208 14:21:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.24
[32m[20221208 14:21:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.63
[32m[20221208 14:21:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.23
[32m[20221208 14:21:42 @agent_ppo2.py:137][0m Total time:       5.62 min
[32m[20221208 14:21:42 @agent_ppo2.py:139][0m 456704 total steps have happened
[32m[20221208 14:21:42 @agent_ppo2.py:115][0m #------------------------ Iteration 223 --------------------------#
[32m[20221208 14:21:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:42 @agent_ppo2.py:179][0m |           0.0247 |           5.0065 |           4.3293 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |           0.0229 |           4.2344 |           4.2555 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0176 |           3.9646 |           4.3250 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0196 |           3.8167 |           4.3426 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0369 |           3.7581 |           4.3611 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0423 |           3.6406 |           4.4024 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0493 |           3.5462 |           4.4215 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0515 |           3.5314 |           4.4386 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0536 |           3.4759 |           4.4553 |
[32m[20221208 14:21:43 @agent_ppo2.py:179][0m |          -0.0564 |           3.4584 |           4.4773 |
[32m[20221208 14:21:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.58
[32m[20221208 14:21:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.08
[32m[20221208 14:21:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.19
[32m[20221208 14:21:43 @agent_ppo2.py:137][0m Total time:       5.64 min
[32m[20221208 14:21:43 @agent_ppo2.py:139][0m 458752 total steps have happened
[32m[20221208 14:21:43 @agent_ppo2.py:115][0m #------------------------ Iteration 224 --------------------------#
[32m[20221208 14:21:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |           0.0253 |           4.4774 |           4.3758 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |           0.0005 |           3.9468 |           4.3482 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0213 |           3.7337 |           4.4413 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0348 |           3.5653 |           4.4608 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0414 |           3.4418 |           4.4878 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0468 |           3.3351 |           4.4923 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0456 |           3.2606 |           4.5016 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0478 |           3.2014 |           4.4973 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0534 |           3.1421 |           4.4971 |
[32m[20221208 14:21:44 @agent_ppo2.py:179][0m |          -0.0553 |           3.0776 |           4.5294 |
[32m[20221208 14:21:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.06
[32m[20221208 14:21:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.03
[32m[20221208 14:21:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 153.02
[32m[20221208 14:21:45 @agent_ppo2.py:137][0m Total time:       5.67 min
[32m[20221208 14:21:45 @agent_ppo2.py:139][0m 460800 total steps have happened
[32m[20221208 14:21:45 @agent_ppo2.py:115][0m #------------------------ Iteration 225 --------------------------#
[32m[20221208 14:21:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:45 @agent_ppo2.py:179][0m |           0.0078 |           1.2309 |           4.4447 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0152 |           0.8758 |           4.4252 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0245 |           0.7436 |           4.4237 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0333 |           0.6663 |           4.4435 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0370 |           0.6108 |           4.4683 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0368 |           0.5685 |           4.4756 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0386 |           0.5374 |           4.4902 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0401 |           0.5095 |           4.4753 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0422 |           0.4912 |           4.4789 |
[32m[20221208 14:21:46 @agent_ppo2.py:179][0m |          -0.0433 |           0.4731 |           4.4795 |
[32m[20221208 14:21:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.30
[32m[20221208 14:21:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.32
[32m[20221208 14:21:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.86
[32m[20221208 14:21:46 @agent_ppo2.py:137][0m Total time:       5.69 min
[32m[20221208 14:21:46 @agent_ppo2.py:139][0m 462848 total steps have happened
[32m[20221208 14:21:46 @agent_ppo2.py:115][0m #------------------------ Iteration 226 --------------------------#
[32m[20221208 14:21:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |           0.0274 |           9.5150 |           4.5873 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |           0.0227 |           8.4778 |           4.3785 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0048 |           8.2520 |           4.4384 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0336 |           8.1708 |           4.5388 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0442 |           7.9951 |           4.5747 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0521 |           7.9596 |           4.6036 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0488 |           7.9257 |           4.5985 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0584 |           7.8658 |           4.6216 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0595 |           7.8454 |           4.6115 |
[32m[20221208 14:21:47 @agent_ppo2.py:179][0m |          -0.0611 |           7.7754 |           4.6290 |
[32m[20221208 14:21:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.02
[32m[20221208 14:21:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.98
[32m[20221208 14:21:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.54
[32m[20221208 14:21:48 @agent_ppo2.py:137][0m Total time:       5.72 min
[32m[20221208 14:21:48 @agent_ppo2.py:139][0m 464896 total steps have happened
[32m[20221208 14:21:48 @agent_ppo2.py:115][0m #------------------------ Iteration 227 --------------------------#
[32m[20221208 14:21:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:48 @agent_ppo2.py:179][0m |           0.0448 |           3.9471 |           4.4198 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |           0.0095 |           3.3593 |           4.2500 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0209 |           3.1543 |           4.3785 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0307 |           3.0477 |           4.4291 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0368 |           2.9473 |           4.4427 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0353 |           2.8926 |           4.4104 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0451 |           2.8732 |           4.4580 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0502 |           2.7875 |           4.5007 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0489 |           2.7449 |           4.4985 |
[32m[20221208 14:21:49 @agent_ppo2.py:179][0m |          -0.0546 |           2.7163 |           4.5157 |
[32m[20221208 14:21:49 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.55
[32m[20221208 14:21:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.54
[32m[20221208 14:21:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.77
[32m[20221208 14:21:49 @agent_ppo2.py:137][0m Total time:       5.74 min
[32m[20221208 14:21:49 @agent_ppo2.py:139][0m 466944 total steps have happened
[32m[20221208 14:21:49 @agent_ppo2.py:115][0m #------------------------ Iteration 228 --------------------------#
[32m[20221208 14:21:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |           0.0275 |           7.3433 |           4.7212 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |           0.0316 |           6.9687 |           4.5031 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |           0.0084 |           6.7949 |           4.5327 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0204 |           6.5919 |           4.6691 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0368 |           6.4428 |           4.7577 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0436 |           6.3552 |           4.7959 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0448 |           6.2560 |           4.8194 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0480 |           6.1993 |           4.8430 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0521 |           6.0842 |           4.8592 |
[32m[20221208 14:21:50 @agent_ppo2.py:179][0m |          -0.0547 |           6.0429 |           4.8731 |
[32m[20221208 14:21:50 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.46
[32m[20221208 14:21:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.24
[32m[20221208 14:21:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.35
[32m[20221208 14:21:51 @agent_ppo2.py:137][0m Total time:       5.77 min
[32m[20221208 14:21:51 @agent_ppo2.py:139][0m 468992 total steps have happened
[32m[20221208 14:21:51 @agent_ppo2.py:115][0m #------------------------ Iteration 229 --------------------------#
[32m[20221208 14:21:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:51 @agent_ppo2.py:179][0m |           0.0201 |           5.8574 |           4.7888 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |           0.0001 |           5.7409 |           4.7227 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0004 |           5.6445 |           4.7291 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0191 |           5.5955 |           4.7491 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0331 |           5.5760 |           4.8098 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0383 |           5.5305 |           4.8505 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0424 |           5.5361 |           4.8627 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0444 |           5.5106 |           4.8703 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0465 |           5.5006 |           4.8893 |
[32m[20221208 14:21:52 @agent_ppo2.py:179][0m |          -0.0436 |           5.5484 |           4.9030 |
[32m[20221208 14:21:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.16
[32m[20221208 14:21:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.86
[32m[20221208 14:21:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.91
[32m[20221208 14:21:52 @agent_ppo2.py:137][0m Total time:       5.79 min
[32m[20221208 14:21:52 @agent_ppo2.py:139][0m 471040 total steps have happened
[32m[20221208 14:21:52 @agent_ppo2.py:115][0m #------------------------ Iteration 230 --------------------------#
[32m[20221208 14:21:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:21:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |           0.0343 |           7.2270 |           4.9623 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0116 |           6.7660 |           4.9852 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0246 |           6.6144 |           5.0066 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0331 |           6.4216 |           5.0314 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0418 |           6.2870 |           5.0596 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0487 |           6.1811 |           5.0816 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0508 |           6.1872 |           5.0747 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0518 |           6.1008 |           5.0878 |
[32m[20221208 14:21:53 @agent_ppo2.py:179][0m |          -0.0546 |           6.0173 |           5.0895 |
[32m[20221208 14:21:54 @agent_ppo2.py:179][0m |          -0.0584 |           5.9728 |           5.1368 |
[32m[20221208 14:21:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.48
[32m[20221208 14:21:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.70
[32m[20221208 14:21:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.24
[32m[20221208 14:21:54 @agent_ppo2.py:137][0m Total time:       5.82 min
[32m[20221208 14:21:54 @agent_ppo2.py:139][0m 473088 total steps have happened
[32m[20221208 14:21:54 @agent_ppo2.py:115][0m #------------------------ Iteration 231 --------------------------#
[32m[20221208 14:21:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:54 @agent_ppo2.py:179][0m |           0.0419 |           5.7752 |           4.9078 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |           0.0040 |           5.2417 |           4.8019 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0223 |           5.1366 |           4.8982 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0313 |           5.0184 |           4.9450 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0407 |           4.9830 |           4.9836 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0463 |           4.9045 |           5.0244 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0491 |           4.8888 |           5.0504 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0524 |           4.8294 |           5.0607 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0481 |           4.8315 |           5.0662 |
[32m[20221208 14:21:55 @agent_ppo2.py:179][0m |          -0.0521 |           4.8215 |           5.0642 |
[32m[20221208 14:21:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.33
[32m[20221208 14:21:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.60
[32m[20221208 14:21:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.49
[32m[20221208 14:21:55 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 179.49
[32m[20221208 14:21:55 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 179.49
[32m[20221208 14:21:55 @agent_ppo2.py:137][0m Total time:       5.84 min
[32m[20221208 14:21:55 @agent_ppo2.py:139][0m 475136 total steps have happened
[32m[20221208 14:21:55 @agent_ppo2.py:115][0m #------------------------ Iteration 232 --------------------------#
[32m[20221208 14:21:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |           0.0442 |           5.4991 |           5.0964 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |           0.0149 |           5.3273 |           4.8703 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0159 |           5.2129 |           5.0083 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0295 |           5.1614 |           5.0876 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0357 |           5.0904 |           5.0958 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0419 |           5.0703 |           5.1241 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0417 |           5.0566 |           5.1376 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0447 |           5.0431 |           5.1375 |
[32m[20221208 14:21:56 @agent_ppo2.py:179][0m |          -0.0483 |           5.0278 |           5.1700 |
[32m[20221208 14:21:57 @agent_ppo2.py:179][0m |          -0.0520 |           5.0241 |           5.1989 |
[32m[20221208 14:21:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:21:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.45
[32m[20221208 14:21:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.05
[32m[20221208 14:21:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.74
[32m[20221208 14:21:57 @agent_ppo2.py:137][0m Total time:       5.87 min
[32m[20221208 14:21:57 @agent_ppo2.py:139][0m 477184 total steps have happened
[32m[20221208 14:21:57 @agent_ppo2.py:115][0m #------------------------ Iteration 233 --------------------------#
[32m[20221208 14:21:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:57 @agent_ppo2.py:179][0m |           0.0275 |           6.0342 |           5.0252 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0063 |           5.4711 |           5.0216 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0188 |           5.3282 |           5.0563 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0331 |           5.2244 |           5.1002 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0391 |           5.2038 |           5.1073 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0437 |           5.1265 |           5.1425 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0422 |           5.1108 |           5.1255 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0475 |           5.0442 |           5.1393 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0489 |           5.0671 |           5.1673 |
[32m[20221208 14:21:58 @agent_ppo2.py:179][0m |          -0.0497 |           5.0191 |           5.1819 |
[32m[20221208 14:21:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:21:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.91
[32m[20221208 14:21:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.45
[32m[20221208 14:21:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.45
[32m[20221208 14:21:58 @agent_ppo2.py:137][0m Total time:       5.89 min
[32m[20221208 14:21:58 @agent_ppo2.py:139][0m 479232 total steps have happened
[32m[20221208 14:21:58 @agent_ppo2.py:115][0m #------------------------ Iteration 234 --------------------------#
[32m[20221208 14:21:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:21:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |           0.0320 |           4.2181 |           5.0608 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0026 |           3.6956 |           5.0156 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0215 |           3.5279 |           5.0822 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0336 |           3.4125 |           5.0867 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0388 |           3.3375 |           5.1168 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0420 |           3.2808 |           5.1234 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0460 |           3.2358 |           5.1515 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0464 |           3.1988 |           5.1565 |
[32m[20221208 14:21:59 @agent_ppo2.py:179][0m |          -0.0476 |           3.1748 |           5.1664 |
[32m[20221208 14:22:00 @agent_ppo2.py:179][0m |          -0.0511 |           3.1466 |           5.1964 |
[32m[20221208 14:22:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.09
[32m[20221208 14:22:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.78
[32m[20221208 14:22:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 176.09
[32m[20221208 14:22:00 @agent_ppo2.py:137][0m Total time:       5.92 min
[32m[20221208 14:22:00 @agent_ppo2.py:139][0m 481280 total steps have happened
[32m[20221208 14:22:00 @agent_ppo2.py:115][0m #------------------------ Iteration 235 --------------------------#
[32m[20221208 14:22:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:00 @agent_ppo2.py:179][0m |           0.0296 |           5.9870 |           5.1210 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |           0.0092 |           5.7309 |           5.0471 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0142 |           5.5732 |           5.0974 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0297 |           5.5311 |           5.1403 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0291 |           5.4800 |           5.1415 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0374 |           5.3809 |           5.1602 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0398 |           5.3850 |           5.1919 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0452 |           5.3569 |           5.2317 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0485 |           5.2535 |           5.2252 |
[32m[20221208 14:22:01 @agent_ppo2.py:179][0m |          -0.0503 |           5.2352 |           5.2742 |
[32m[20221208 14:22:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.39
[32m[20221208 14:22:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.17
[32m[20221208 14:22:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.00
[32m[20221208 14:22:01 @agent_ppo2.py:137][0m Total time:       5.94 min
[32m[20221208 14:22:01 @agent_ppo2.py:139][0m 483328 total steps have happened
[32m[20221208 14:22:01 @agent_ppo2.py:115][0m #------------------------ Iteration 236 --------------------------#
[32m[20221208 14:22:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |           0.0407 |           5.6507 |           5.1891 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |           0.0008 |           5.3077 |           5.1994 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0217 |           5.1626 |           5.2857 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0313 |           5.1048 |           5.2973 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0385 |           5.0470 |           5.3211 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0405 |           4.9944 |           5.3204 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0441 |           4.9876 |           5.3379 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0485 |           4.9379 |           5.3461 |
[32m[20221208 14:22:02 @agent_ppo2.py:179][0m |          -0.0517 |           4.8727 |           5.3833 |
[32m[20221208 14:22:03 @agent_ppo2.py:179][0m |          -0.0516 |           4.8986 |           5.3702 |
[32m[20221208 14:22:03 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:22:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.84
[32m[20221208 14:22:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.30
[32m[20221208 14:22:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.16
[32m[20221208 14:22:03 @agent_ppo2.py:137][0m Total time:       5.97 min
[32m[20221208 14:22:03 @agent_ppo2.py:139][0m 485376 total steps have happened
[32m[20221208 14:22:03 @agent_ppo2.py:115][0m #------------------------ Iteration 237 --------------------------#
[32m[20221208 14:22:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |           0.0348 |           7.6458 |           5.1978 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |           0.0153 |           7.3340 |           4.9722 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0173 |           7.1144 |           5.1876 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0305 |           7.0569 |           5.2292 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0392 |           6.9866 |           5.2420 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0380 |           6.9828 |           5.2575 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0444 |           6.9406 |           5.3010 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0470 |           6.8239 |           5.2875 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0500 |           6.8383 |           5.3036 |
[32m[20221208 14:22:04 @agent_ppo2.py:179][0m |          -0.0521 |           6.7778 |           5.3205 |
[32m[20221208 14:22:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:22:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.20
[32m[20221208 14:22:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.40
[32m[20221208 14:22:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.01
[32m[20221208 14:22:04 @agent_ppo2.py:137][0m Total time:       5.99 min
[32m[20221208 14:22:04 @agent_ppo2.py:139][0m 487424 total steps have happened
[32m[20221208 14:22:04 @agent_ppo2.py:115][0m #------------------------ Iteration 238 --------------------------#
[32m[20221208 14:22:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:22:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |           0.0413 |           4.0949 |           5.0953 |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |           0.0095 |           3.7526 |           5.0315 |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |          -0.0096 |           3.5765 |           5.0720 |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |          -0.0266 |           3.3824 |           5.1873 |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |          -0.0285 |           3.2848 |           5.2041 |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |          -0.0391 |           3.1264 |           5.2350 |
[32m[20221208 14:22:05 @agent_ppo2.py:179][0m |          -0.0420 |           2.9637 |           5.2407 |
[32m[20221208 14:22:06 @agent_ppo2.py:179][0m |          -0.0430 |           2.8295 |           5.2577 |
[32m[20221208 14:22:06 @agent_ppo2.py:179][0m |          -0.0470 |           2.7074 |           5.2948 |
[32m[20221208 14:22:06 @agent_ppo2.py:179][0m |          -0.0507 |           2.5875 |           5.3265 |
[32m[20221208 14:22:06 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:22:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.84
[32m[20221208 14:22:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.94
[32m[20221208 14:22:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.98
[32m[20221208 14:22:06 @agent_ppo2.py:137][0m Total time:       6.02 min
[32m[20221208 14:22:06 @agent_ppo2.py:139][0m 489472 total steps have happened
[32m[20221208 14:22:06 @agent_ppo2.py:115][0m #------------------------ Iteration 239 --------------------------#
[32m[20221208 14:22:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |           0.0217 |           5.7564 |           5.3734 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0110 |           5.2069 |           5.3115 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0217 |           5.0353 |           5.3687 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0356 |           4.9696 |           5.4226 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0421 |           4.8986 |           5.4663 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0460 |           4.8489 |           5.4689 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0502 |           4.7927 |           5.5080 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0515 |           4.7748 |           5.5255 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0491 |           4.7516 |           5.5348 |
[32m[20221208 14:22:07 @agent_ppo2.py:179][0m |          -0.0509 |           4.7356 |           5.5381 |
[32m[20221208 14:22:07 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:22:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.91
[32m[20221208 14:22:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.97
[32m[20221208 14:22:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 185.82
[32m[20221208 14:22:08 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 185.82
[32m[20221208 14:22:08 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 185.82
[32m[20221208 14:22:08 @agent_ppo2.py:137][0m Total time:       6.04 min
[32m[20221208 14:22:08 @agent_ppo2.py:139][0m 491520 total steps have happened
[32m[20221208 14:22:08 @agent_ppo2.py:115][0m #------------------------ Iteration 240 --------------------------#
[32m[20221208 14:22:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:08 @agent_ppo2.py:179][0m |           0.0235 |           3.3393 |           5.5350 |
[32m[20221208 14:22:08 @agent_ppo2.py:179][0m |           0.0026 |           3.1372 |           5.1553 |
[32m[20221208 14:22:08 @agent_ppo2.py:179][0m |          -0.0191 |           3.0609 |           5.5224 |
[32m[20221208 14:22:08 @agent_ppo2.py:179][0m |          -0.0271 |           3.0220 |           5.6256 |
[32m[20221208 14:22:08 @agent_ppo2.py:179][0m |          -0.0335 |           2.9904 |           5.5165 |
[32m[20221208 14:22:08 @agent_ppo2.py:179][0m |          -0.0386 |           2.9692 |           5.6733 |
[32m[20221208 14:22:09 @agent_ppo2.py:179][0m |          -0.0413 |           2.9412 |           5.7095 |
[32m[20221208 14:22:09 @agent_ppo2.py:179][0m |          -0.0422 |           2.9366 |           5.7303 |
[32m[20221208 14:22:09 @agent_ppo2.py:179][0m |          -0.0433 |           2.9327 |           5.6652 |
[32m[20221208 14:22:09 @agent_ppo2.py:179][0m |          -0.0466 |           2.8955 |           5.7193 |
[32m[20221208 14:22:09 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:22:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.70
[32m[20221208 14:22:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.47
[32m[20221208 14:22:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.88
[32m[20221208 14:22:09 @agent_ppo2.py:137][0m Total time:       6.07 min
[32m[20221208 14:22:09 @agent_ppo2.py:139][0m 493568 total steps have happened
[32m[20221208 14:22:09 @agent_ppo2.py:115][0m #------------------------ Iteration 241 --------------------------#
[32m[20221208 14:22:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |           0.0410 |           5.9624 |           5.8105 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |           0.0118 |           5.1495 |           5.6799 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0149 |           4.9396 |           5.7330 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0341 |           4.7694 |           5.8002 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0430 |           4.6868 |           5.8289 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0493 |           4.5810 |           5.8641 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0520 |           4.5274 |           5.8859 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0539 |           4.4620 |           5.8906 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0587 |           4.3808 |           5.9155 |
[32m[20221208 14:22:10 @agent_ppo2.py:179][0m |          -0.0595 |           4.3490 |           5.9224 |
[32m[20221208 14:22:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.31
[32m[20221208 14:22:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.79
[32m[20221208 14:22:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.71
[32m[20221208 14:22:11 @agent_ppo2.py:137][0m Total time:       6.09 min
[32m[20221208 14:22:11 @agent_ppo2.py:139][0m 495616 total steps have happened
[32m[20221208 14:22:11 @agent_ppo2.py:115][0m #------------------------ Iteration 242 --------------------------#
[32m[20221208 14:22:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:11 @agent_ppo2.py:179][0m |           0.0342 |          10.8623 |           5.8030 |
[32m[20221208 14:22:11 @agent_ppo2.py:179][0m |          -0.0133 |           9.0826 |           5.6912 |
[32m[20221208 14:22:11 @agent_ppo2.py:179][0m |          -0.0333 |           8.4956 |           5.7719 |
[32m[20221208 14:22:11 @agent_ppo2.py:179][0m |          -0.0452 |           8.0371 |           5.8178 |
[32m[20221208 14:22:11 @agent_ppo2.py:179][0m |          -0.0469 |           7.7499 |           5.8188 |
[32m[20221208 14:22:11 @agent_ppo2.py:179][0m |          -0.0533 |           7.4527 |           5.8506 |
[32m[20221208 14:22:12 @agent_ppo2.py:179][0m |          -0.0542 |           7.3320 |           5.8611 |
[32m[20221208 14:22:12 @agent_ppo2.py:179][0m |          -0.0565 |           7.1300 |           5.8670 |
[32m[20221208 14:22:12 @agent_ppo2.py:179][0m |          -0.0586 |           7.0498 |           5.8902 |
[32m[20221208 14:22:12 @agent_ppo2.py:179][0m |          -0.0618 |           6.9007 |           5.8988 |
[32m[20221208 14:22:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:22:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.92
[32m[20221208 14:22:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.72
[32m[20221208 14:22:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.23
[32m[20221208 14:22:12 @agent_ppo2.py:137][0m Total time:       6.12 min
[32m[20221208 14:22:12 @agent_ppo2.py:139][0m 497664 total steps have happened
[32m[20221208 14:22:12 @agent_ppo2.py:115][0m #------------------------ Iteration 243 --------------------------#
[32m[20221208 14:22:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |           0.0372 |           6.6588 |           5.9054 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |           0.0042 |           5.9902 |           5.8566 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0213 |           5.7231 |           5.9345 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0269 |           5.5077 |           6.0046 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0378 |           5.3680 |           6.0500 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0312 |           5.2399 |           6.0518 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0415 |           5.0902 |           6.0808 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0421 |           5.0005 |           6.0886 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0510 |           4.9204 |           6.1407 |
[32m[20221208 14:22:13 @agent_ppo2.py:179][0m |          -0.0500 |           4.8040 |           6.1291 |
[32m[20221208 14:22:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.48
[32m[20221208 14:22:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.06
[32m[20221208 14:22:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 177.03
[32m[20221208 14:22:14 @agent_ppo2.py:137][0m Total time:       6.15 min
[32m[20221208 14:22:14 @agent_ppo2.py:139][0m 499712 total steps have happened
[32m[20221208 14:22:14 @agent_ppo2.py:115][0m #------------------------ Iteration 244 --------------------------#
[32m[20221208 14:22:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:14 @agent_ppo2.py:179][0m |           0.0196 |           8.8575 |           5.8043 |
[32m[20221208 14:22:14 @agent_ppo2.py:179][0m |           0.0017 |           8.2930 |           5.6470 |
[32m[20221208 14:22:14 @agent_ppo2.py:179][0m |          -0.0236 |           8.1345 |           5.7276 |
[32m[20221208 14:22:14 @agent_ppo2.py:179][0m |          -0.0363 |           8.0270 |           5.8064 |
[32m[20221208 14:22:14 @agent_ppo2.py:179][0m |          -0.0422 |           8.0313 |           5.7980 |
[32m[20221208 14:22:14 @agent_ppo2.py:179][0m |          -0.0476 |           7.9101 |           5.8471 |
[32m[20221208 14:22:15 @agent_ppo2.py:179][0m |          -0.0509 |           7.8790 |           5.8715 |
[32m[20221208 14:22:15 @agent_ppo2.py:179][0m |          -0.0537 |           7.8170 |           5.8910 |
[32m[20221208 14:22:15 @agent_ppo2.py:179][0m |          -0.0552 |           7.8375 |           5.9149 |
[32m[20221208 14:22:15 @agent_ppo2.py:179][0m |          -0.0553 |           7.7855 |           5.8955 |
[32m[20221208 14:22:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.83
[32m[20221208 14:22:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.94
[32m[20221208 14:22:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.89
[32m[20221208 14:22:15 @agent_ppo2.py:137][0m Total time:       6.17 min
[32m[20221208 14:22:15 @agent_ppo2.py:139][0m 501760 total steps have happened
[32m[20221208 14:22:15 @agent_ppo2.py:115][0m #------------------------ Iteration 245 --------------------------#
[32m[20221208 14:22:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |           0.0365 |           6.4028 |           5.9362 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |           0.0138 |           6.0253 |           5.9243 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0148 |           5.9210 |           5.9974 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0270 |           5.8852 |           6.0895 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0336 |           5.8119 |           6.0924 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0363 |           5.7666 |           6.1085 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0414 |           5.7120 |           6.1146 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0423 |           5.7153 |           6.1325 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0451 |           5.6835 |           6.1847 |
[32m[20221208 14:22:16 @agent_ppo2.py:179][0m |          -0.0478 |           5.6955 |           6.1731 |
[32m[20221208 14:22:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.22
[32m[20221208 14:22:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.12
[32m[20221208 14:22:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.97
[32m[20221208 14:22:17 @agent_ppo2.py:137][0m Total time:       6.20 min
[32m[20221208 14:22:17 @agent_ppo2.py:139][0m 503808 total steps have happened
[32m[20221208 14:22:17 @agent_ppo2.py:115][0m #------------------------ Iteration 246 --------------------------#
[32m[20221208 14:22:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |           0.0973 |           3.6622 |           6.0838 |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |           0.0312 |           3.4357 |           5.9179 |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |          -0.0070 |           3.3691 |           6.1835 |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |          -0.0052 |           3.3349 |           6.0831 |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |          -0.0143 |           3.3201 |           5.9561 |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |           0.0057 |           3.3002 |           5.9300 |
[32m[20221208 14:22:17 @agent_ppo2.py:179][0m |          -0.0261 |           3.2854 |           6.2776 |
[32m[20221208 14:22:18 @agent_ppo2.py:179][0m |          -0.0203 |           3.2648 |           6.1273 |
[32m[20221208 14:22:18 @agent_ppo2.py:179][0m |          -0.0335 |           3.2666 |           6.2613 |
[32m[20221208 14:22:18 @agent_ppo2.py:179][0m |          -0.0329 |           3.2514 |           6.3127 |
[32m[20221208 14:22:18 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:22:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.88
[32m[20221208 14:22:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.72
[32m[20221208 14:22:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.75
[32m[20221208 14:22:18 @agent_ppo2.py:137][0m Total time:       6.22 min
[32m[20221208 14:22:18 @agent_ppo2.py:139][0m 505856 total steps have happened
[32m[20221208 14:22:18 @agent_ppo2.py:115][0m #------------------------ Iteration 247 --------------------------#
[32m[20221208 14:22:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |           0.0425 |           6.1151 |           6.1057 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |           0.0173 |           5.5230 |           5.9252 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0132 |           5.2843 |           6.1286 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0238 |           5.0716 |           6.1993 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0297 |           4.9106 |           6.2526 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0379 |           4.7561 |           6.2824 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0408 |           4.7070 |           6.3022 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0444 |           4.5789 |           6.3295 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0480 |           4.4525 |           6.3559 |
[32m[20221208 14:22:19 @agent_ppo2.py:179][0m |          -0.0486 |           4.3739 |           6.3663 |
[32m[20221208 14:22:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.13
[32m[20221208 14:22:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.62
[32m[20221208 14:22:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.85
[32m[20221208 14:22:20 @agent_ppo2.py:137][0m Total time:       6.24 min
[32m[20221208 14:22:20 @agent_ppo2.py:139][0m 507904 total steps have happened
[32m[20221208 14:22:20 @agent_ppo2.py:115][0m #------------------------ Iteration 248 --------------------------#
[32m[20221208 14:22:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |           0.0247 |           9.2094 |           6.0739 |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |          -0.0066 |           8.4383 |           5.9510 |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |          -0.0264 |           8.1110 |           6.0792 |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |          -0.0396 |           8.1126 |           6.1536 |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |          -0.0442 |           7.9994 |           6.1550 |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |          -0.0481 |           8.0232 |           6.1698 |
[32m[20221208 14:22:20 @agent_ppo2.py:179][0m |          -0.0510 |           7.9667 |           6.2029 |
[32m[20221208 14:22:21 @agent_ppo2.py:179][0m |          -0.0558 |           7.8742 |           6.2161 |
[32m[20221208 14:22:21 @agent_ppo2.py:179][0m |          -0.0570 |           7.8110 |           6.2226 |
[32m[20221208 14:22:21 @agent_ppo2.py:179][0m |          -0.0580 |           7.7565 |           6.2315 |
[32m[20221208 14:22:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.24
[32m[20221208 14:22:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.27
[32m[20221208 14:22:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.67
[32m[20221208 14:22:21 @agent_ppo2.py:137][0m Total time:       6.27 min
[32m[20221208 14:22:21 @agent_ppo2.py:139][0m 509952 total steps have happened
[32m[20221208 14:22:21 @agent_ppo2.py:115][0m #------------------------ Iteration 249 --------------------------#
[32m[20221208 14:22:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |           0.0461 |           4.5679 |           6.2468 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |           0.0085 |           4.2231 |           6.2830 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |           0.0023 |           4.0314 |           6.2752 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0108 |           3.8029 |           6.2805 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0249 |           3.6801 |           6.4012 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0241 |           3.5495 |           6.3649 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0299 |           3.4656 |           6.3975 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0386 |           3.4227 |           6.4086 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0384 |           3.3594 |           6.4727 |
[32m[20221208 14:22:22 @agent_ppo2.py:179][0m |          -0.0376 |           3.3055 |           6.5219 |
[32m[20221208 14:22:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:22:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.95
[32m[20221208 14:22:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.00
[32m[20221208 14:22:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.41
[32m[20221208 14:22:23 @agent_ppo2.py:137][0m Total time:       6.29 min
[32m[20221208 14:22:23 @agent_ppo2.py:139][0m 512000 total steps have happened
[32m[20221208 14:22:23 @agent_ppo2.py:115][0m #------------------------ Iteration 250 --------------------------#
[32m[20221208 14:22:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |           0.0144 |           8.1410 |           6.5515 |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |          -0.0165 |           7.1521 |           6.4834 |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |          -0.0319 |           6.8268 |           6.5408 |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |          -0.0365 |           6.6034 |           6.5447 |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |          -0.0452 |           6.4248 |           6.5765 |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |          -0.0468 |           6.3327 |           6.5825 |
[32m[20221208 14:22:23 @agent_ppo2.py:179][0m |          -0.0512 |           6.2266 |           6.5908 |
[32m[20221208 14:22:24 @agent_ppo2.py:179][0m |          -0.0538 |           6.1146 |           6.6149 |
[32m[20221208 14:22:24 @agent_ppo2.py:179][0m |          -0.0573 |           6.0392 |           6.6197 |
[32m[20221208 14:22:24 @agent_ppo2.py:179][0m |          -0.0595 |           6.0001 |           6.6414 |
[32m[20221208 14:22:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:22:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.47
[32m[20221208 14:22:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.06
[32m[20221208 14:22:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.84
[32m[20221208 14:22:24 @agent_ppo2.py:137][0m Total time:       6.32 min
[32m[20221208 14:22:24 @agent_ppo2.py:139][0m 514048 total steps have happened
[32m[20221208 14:22:24 @agent_ppo2.py:115][0m #------------------------ Iteration 251 --------------------------#
[32m[20221208 14:22:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |           0.0289 |           6.9411 |           6.3604 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |           0.0118 |           6.2289 |           6.2279 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0203 |           5.8554 |           6.4538 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0305 |           5.7213 |           6.4791 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0347 |           5.4950 |           6.5197 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0400 |           5.4043 |           6.5873 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0459 |           5.3375 |           6.6068 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0465 |           5.2762 |           6.6256 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0447 |           5.1907 |           6.6544 |
[32m[20221208 14:22:25 @agent_ppo2.py:179][0m |          -0.0511 |           5.1549 |           6.6912 |
[32m[20221208 14:22:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.41
[32m[20221208 14:22:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.95
[32m[20221208 14:22:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.56
[32m[20221208 14:22:26 @agent_ppo2.py:137][0m Total time:       6.34 min
[32m[20221208 14:22:26 @agent_ppo2.py:139][0m 516096 total steps have happened
[32m[20221208 14:22:26 @agent_ppo2.py:115][0m #------------------------ Iteration 252 --------------------------#
[32m[20221208 14:22:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |           0.0367 |           8.3808 |           6.5834 |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |           0.0025 |           7.9918 |           6.2808 |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |          -0.0257 |           7.8114 |           6.5998 |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |          -0.0342 |           7.7435 |           6.6618 |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |          -0.0377 |           7.7127 |           6.6511 |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |          -0.0441 |           7.6769 |           6.7093 |
[32m[20221208 14:22:26 @agent_ppo2.py:179][0m |          -0.0484 |           7.5984 |           6.7076 |
[32m[20221208 14:22:27 @agent_ppo2.py:179][0m |          -0.0510 |           7.6146 |           6.7688 |
[32m[20221208 14:22:27 @agent_ppo2.py:179][0m |          -0.0487 |           7.5339 |           6.7631 |
[32m[20221208 14:22:27 @agent_ppo2.py:179][0m |          -0.0552 |           7.5125 |           6.7939 |
[32m[20221208 14:22:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.92
[32m[20221208 14:22:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.05
[32m[20221208 14:22:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.16
[32m[20221208 14:22:27 @agent_ppo2.py:137][0m Total time:       6.37 min
[32m[20221208 14:22:27 @agent_ppo2.py:139][0m 518144 total steps have happened
[32m[20221208 14:22:27 @agent_ppo2.py:115][0m #------------------------ Iteration 253 --------------------------#
[32m[20221208 14:22:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |           0.0332 |           8.3901 |           6.7812 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |           0.0074 |           8.1354 |           6.7133 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0133 |           8.0056 |           6.8257 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0283 |           7.9864 |           6.8619 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0335 |           7.9299 |           6.9043 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0380 |           7.9123 |           6.9241 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0424 |           7.8547 |           6.9477 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0448 |           7.8326 |           6.9674 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0459 |           7.8073 |           6.9853 |
[32m[20221208 14:22:28 @agent_ppo2.py:179][0m |          -0.0483 |           7.8390 |           6.9840 |
[32m[20221208 14:22:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.76
[32m[20221208 14:22:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.64
[32m[20221208 14:22:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.36
[32m[20221208 14:22:29 @agent_ppo2.py:137][0m Total time:       6.39 min
[32m[20221208 14:22:29 @agent_ppo2.py:139][0m 520192 total steps have happened
[32m[20221208 14:22:29 @agent_ppo2.py:115][0m #------------------------ Iteration 254 --------------------------#
[32m[20221208 14:22:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |           0.0462 |           8.9117 |           6.8067 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |           0.0218 |           8.5327 |           6.5434 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |          -0.0101 |           8.4196 |           6.7665 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |          -0.0269 |           8.4446 |           6.9464 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |          -0.0352 |           8.3094 |           7.0131 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |          -0.0390 |           8.3023 |           7.0595 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |          -0.0431 |           8.3004 |           7.0722 |
[32m[20221208 14:22:29 @agent_ppo2.py:179][0m |          -0.0463 |           8.2433 |           7.1147 |
[32m[20221208 14:22:30 @agent_ppo2.py:179][0m |          -0.0464 |           8.2428 |           7.1334 |
[32m[20221208 14:22:30 @agent_ppo2.py:179][0m |          -0.0514 |           8.1726 |           7.1528 |
[32m[20221208 14:22:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.30
[32m[20221208 14:22:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.42
[32m[20221208 14:22:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.64
[32m[20221208 14:22:30 @agent_ppo2.py:137][0m Total time:       6.42 min
[32m[20221208 14:22:30 @agent_ppo2.py:139][0m 522240 total steps have happened
[32m[20221208 14:22:30 @agent_ppo2.py:115][0m #------------------------ Iteration 255 --------------------------#
[32m[20221208 14:22:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |           0.0369 |           6.9186 |           7.1178 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |           0.0275 |           6.3076 |           6.7642 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |           0.0064 |           6.1226 |           6.6601 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0178 |           6.0708 |           6.8451 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0334 |           5.9298 |           7.0306 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0387 |           5.8887 |           7.1102 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0420 |           5.8035 |           7.1624 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0460 |           5.7502 |           7.1982 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0462 |           5.7501 |           7.1545 |
[32m[20221208 14:22:31 @agent_ppo2.py:179][0m |          -0.0464 |           5.6784 |           7.1954 |
[32m[20221208 14:22:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.72
[32m[20221208 14:22:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.65
[32m[20221208 14:22:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.48
[32m[20221208 14:22:32 @agent_ppo2.py:137][0m Total time:       6.44 min
[32m[20221208 14:22:32 @agent_ppo2.py:139][0m 524288 total steps have happened
[32m[20221208 14:22:32 @agent_ppo2.py:115][0m #------------------------ Iteration 256 --------------------------#
[32m[20221208 14:22:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |           0.0305 |           8.3203 |           7.1649 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0101 |           7.9638 |           7.0897 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0278 |           7.8514 |           7.2055 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0356 |           7.8238 |           7.2542 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0420 |           7.7385 |           7.2567 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0459 |           7.6784 |           7.3096 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0493 |           7.6187 |           7.3005 |
[32m[20221208 14:22:32 @agent_ppo2.py:179][0m |          -0.0509 |           7.5895 |           7.3300 |
[32m[20221208 14:22:33 @agent_ppo2.py:179][0m |          -0.0547 |           7.5520 |           7.3553 |
[32m[20221208 14:22:33 @agent_ppo2.py:179][0m |          -0.0545 |           7.5271 |           7.3492 |
[32m[20221208 14:22:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.83
[32m[20221208 14:22:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.46
[32m[20221208 14:22:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.73
[32m[20221208 14:22:33 @agent_ppo2.py:137][0m Total time:       6.47 min
[32m[20221208 14:22:33 @agent_ppo2.py:139][0m 526336 total steps have happened
[32m[20221208 14:22:33 @agent_ppo2.py:115][0m #------------------------ Iteration 257 --------------------------#
[32m[20221208 14:22:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |           0.0429 |           6.3642 |           6.9543 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |           0.0168 |           6.0233 |           6.7821 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0176 |           5.8787 |           7.0032 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0261 |           5.8318 |           7.0416 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0357 |           5.6716 |           7.0719 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0411 |           5.5816 |           7.0903 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0429 |           5.4920 |           7.1390 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0478 |           5.4103 |           7.1630 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0426 |           5.3628 |           7.1776 |
[32m[20221208 14:22:34 @agent_ppo2.py:179][0m |          -0.0494 |           5.2301 |           7.2094 |
[32m[20221208 14:22:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.03
[32m[20221208 14:22:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.75
[32m[20221208 14:22:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.31
[32m[20221208 14:22:35 @agent_ppo2.py:137][0m Total time:       6.49 min
[32m[20221208 14:22:35 @agent_ppo2.py:139][0m 528384 total steps have happened
[32m[20221208 14:22:35 @agent_ppo2.py:115][0m #------------------------ Iteration 258 --------------------------#
[32m[20221208 14:22:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |           0.0351 |           4.9679 |           7.2109 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |           0.0085 |           4.5179 |           7.0279 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |          -0.0217 |           4.3605 |           7.2002 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |          -0.0328 |           4.2557 |           7.2585 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |          -0.0379 |           4.1715 |           7.2722 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |          -0.0416 |           4.0755 |           7.3234 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |          -0.0506 |           4.0022 |           7.3266 |
[32m[20221208 14:22:35 @agent_ppo2.py:179][0m |          -0.0480 |           4.0208 |           7.3450 |
[32m[20221208 14:22:36 @agent_ppo2.py:179][0m |          -0.0541 |           3.9282 |           7.3718 |
[32m[20221208 14:22:36 @agent_ppo2.py:179][0m |          -0.0559 |           3.8846 |           7.3979 |
[32m[20221208 14:22:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.09
[32m[20221208 14:22:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.63
[32m[20221208 14:22:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.54
[32m[20221208 14:22:36 @agent_ppo2.py:137][0m Total time:       6.52 min
[32m[20221208 14:22:36 @agent_ppo2.py:139][0m 530432 total steps have happened
[32m[20221208 14:22:36 @agent_ppo2.py:115][0m #------------------------ Iteration 259 --------------------------#
[32m[20221208 14:22:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |           0.0203 |           4.3319 |           7.2259 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0049 |           4.0275 |           7.1910 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0114 |           3.8298 |           7.1302 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0219 |           3.7513 |           7.2829 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0272 |           3.6427 |           7.1605 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0337 |           3.6021 |           7.2553 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0368 |           3.5735 |           7.3612 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0363 |           3.5156 |           7.3079 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0391 |           3.4355 |           7.3859 |
[32m[20221208 14:22:37 @agent_ppo2.py:179][0m |          -0.0400 |           3.4377 |           7.3318 |
[32m[20221208 14:22:37 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:22:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.40
[32m[20221208 14:22:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.24
[32m[20221208 14:22:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.68
[32m[20221208 14:22:38 @agent_ppo2.py:137][0m Total time:       6.54 min
[32m[20221208 14:22:38 @agent_ppo2.py:139][0m 532480 total steps have happened
[32m[20221208 14:22:38 @agent_ppo2.py:115][0m #------------------------ Iteration 260 --------------------------#
[32m[20221208 14:22:38 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:22:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |           0.0370 |           6.4909 |           7.2623 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |           0.0449 |           5.9735 |           7.2918 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |          -0.0035 |           5.7946 |           7.4131 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |          -0.0238 |           5.6924 |           7.5299 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |          -0.0326 |           5.5976 |           7.5887 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |          -0.0375 |           5.4900 |           7.6457 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |          -0.0419 |           5.4325 |           7.6495 |
[32m[20221208 14:22:38 @agent_ppo2.py:179][0m |          -0.0451 |           5.3162 |           7.6855 |
[32m[20221208 14:22:39 @agent_ppo2.py:179][0m |          -0.0481 |           5.2946 |           7.7166 |
[32m[20221208 14:22:39 @agent_ppo2.py:179][0m |          -0.0491 |           5.2362 |           7.7418 |
[32m[20221208 14:22:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.75
[32m[20221208 14:22:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.50
[32m[20221208 14:22:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.85
[32m[20221208 14:22:39 @agent_ppo2.py:137][0m Total time:       6.57 min
[32m[20221208 14:22:39 @agent_ppo2.py:139][0m 534528 total steps have happened
[32m[20221208 14:22:39 @agent_ppo2.py:115][0m #------------------------ Iteration 261 --------------------------#
[32m[20221208 14:22:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |           0.0315 |           5.8958 |           7.3324 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0016 |           4.9645 |           7.3163 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0173 |           4.5833 |           7.5211 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0270 |           4.4714 |           7.5770 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0318 |           4.2750 |           7.5576 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0398 |           4.1233 |           7.6422 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0439 |           4.0249 |           7.6620 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0452 |           3.9326 |           7.6725 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0472 |           3.8623 |           7.7125 |
[32m[20221208 14:22:40 @agent_ppo2.py:179][0m |          -0.0487 |           3.8191 |           7.7115 |
[32m[20221208 14:22:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.10
[32m[20221208 14:22:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.75
[32m[20221208 14:22:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.35
[32m[20221208 14:22:41 @agent_ppo2.py:137][0m Total time:       6.59 min
[32m[20221208 14:22:41 @agent_ppo2.py:139][0m 536576 total steps have happened
[32m[20221208 14:22:41 @agent_ppo2.py:115][0m #------------------------ Iteration 262 --------------------------#
[32m[20221208 14:22:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |           0.0246 |           9.0788 |           7.4172 |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |          -0.0060 |           8.5795 |           7.3935 |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |          -0.0255 |           8.4030 |           7.4518 |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |          -0.0343 |           8.3202 |           7.4846 |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |          -0.0366 |           8.1702 |           7.5182 |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |          -0.0424 |           8.0392 |           7.5316 |
[32m[20221208 14:22:41 @agent_ppo2.py:179][0m |          -0.0438 |           7.9477 |           7.5521 |
[32m[20221208 14:22:42 @agent_ppo2.py:179][0m |          -0.0472 |           7.8906 |           7.5792 |
[32m[20221208 14:22:42 @agent_ppo2.py:179][0m |          -0.0458 |           7.8161 |           7.5789 |
[32m[20221208 14:22:42 @agent_ppo2.py:179][0m |          -0.0509 |           7.7075 |           7.5717 |
[32m[20221208 14:22:42 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:22:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.04
[32m[20221208 14:22:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.23
[32m[20221208 14:22:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.76
[32m[20221208 14:22:42 @agent_ppo2.py:137][0m Total time:       6.62 min
[32m[20221208 14:22:42 @agent_ppo2.py:139][0m 538624 total steps have happened
[32m[20221208 14:22:42 @agent_ppo2.py:115][0m #------------------------ Iteration 263 --------------------------#
[32m[20221208 14:22:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |           0.0483 |           9.1469 |           7.5958 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |           0.0023 |           8.7286 |           7.6107 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0178 |           8.5073 |           7.6570 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0336 |           8.4172 |           7.7645 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0386 |           8.1646 |           7.7818 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0426 |           8.0595 |           7.8122 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0461 |           8.0106 |           7.8304 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0492 |           7.8946 |           7.8681 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0532 |           7.7658 |           7.8969 |
[32m[20221208 14:22:43 @agent_ppo2.py:179][0m |          -0.0564 |           7.6788 |           7.9193 |
[32m[20221208 14:22:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.55
[32m[20221208 14:22:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.90
[32m[20221208 14:22:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.98
[32m[20221208 14:22:44 @agent_ppo2.py:137][0m Total time:       6.64 min
[32m[20221208 14:22:44 @agent_ppo2.py:139][0m 540672 total steps have happened
[32m[20221208 14:22:44 @agent_ppo2.py:115][0m #------------------------ Iteration 264 --------------------------#
[32m[20221208 14:22:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |           0.0322 |           9.0242 |           7.6789 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |           0.0215 |           8.0411 |           7.3713 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |          -0.0176 |           7.7306 |           7.5830 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |          -0.0338 |           7.5252 |           7.6545 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |          -0.0437 |           7.3862 |           7.7270 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |          -0.0511 |           7.2566 |           7.7809 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |          -0.0542 |           7.1205 |           7.8277 |
[32m[20221208 14:22:44 @agent_ppo2.py:179][0m |          -0.0566 |           7.0029 |           7.8512 |
[32m[20221208 14:22:45 @agent_ppo2.py:179][0m |          -0.0616 |           6.8571 |           7.8457 |
[32m[20221208 14:22:45 @agent_ppo2.py:179][0m |          -0.0649 |           6.7843 |           7.8832 |
[32m[20221208 14:22:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.42
[32m[20221208 14:22:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.59
[32m[20221208 14:22:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 159.33
[32m[20221208 14:22:45 @agent_ppo2.py:137][0m Total time:       6.67 min
[32m[20221208 14:22:45 @agent_ppo2.py:139][0m 542720 total steps have happened
[32m[20221208 14:22:45 @agent_ppo2.py:115][0m #------------------------ Iteration 265 --------------------------#
[32m[20221208 14:22:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |           0.0281 |           5.2900 |           7.7496 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |           0.0218 |           4.7515 |           7.3855 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0154 |           4.5059 |           7.6448 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0293 |           4.3505 |           7.7809 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0396 |           4.2556 |           7.8668 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0464 |           4.2047 |           7.8725 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0490 |           4.1670 |           7.9410 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0484 |           4.1285 |           7.9077 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0499 |           4.0665 |           7.9594 |
[32m[20221208 14:22:46 @agent_ppo2.py:179][0m |          -0.0580 |           4.0672 |           7.9636 |
[32m[20221208 14:22:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.49
[32m[20221208 14:22:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.14
[32m[20221208 14:22:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.47
[32m[20221208 14:22:47 @agent_ppo2.py:137][0m Total time:       6.69 min
[32m[20221208 14:22:47 @agent_ppo2.py:139][0m 544768 total steps have happened
[32m[20221208 14:22:47 @agent_ppo2.py:115][0m #------------------------ Iteration 266 --------------------------#
[32m[20221208 14:22:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |           0.0195 |           4.6057 |           7.8529 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |           0.0106 |           4.0873 |           7.4555 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |           0.0040 |           3.8544 |           7.4471 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |          -0.0299 |           3.7043 |           7.8168 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |          -0.0378 |           3.5887 |           7.9525 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |          -0.0452 |           3.5316 |           7.9121 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |          -0.0497 |           3.4098 |           7.9492 |
[32m[20221208 14:22:47 @agent_ppo2.py:179][0m |          -0.0522 |           3.3721 |           7.9821 |
[32m[20221208 14:22:48 @agent_ppo2.py:179][0m |          -0.0557 |           3.2837 |           7.9905 |
[32m[20221208 14:22:48 @agent_ppo2.py:179][0m |          -0.0541 |           3.2478 |           8.0116 |
[32m[20221208 14:22:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.28
[32m[20221208 14:22:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.53
[32m[20221208 14:22:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.61
[32m[20221208 14:22:48 @agent_ppo2.py:137][0m Total time:       6.72 min
[32m[20221208 14:22:48 @agent_ppo2.py:139][0m 546816 total steps have happened
[32m[20221208 14:22:48 @agent_ppo2.py:115][0m #------------------------ Iteration 267 --------------------------#
[32m[20221208 14:22:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |           0.0329 |           9.6542 |           7.9039 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |           0.0130 |           9.2026 |           7.5557 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0054 |           9.1103 |           7.6822 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0285 |           8.9857 |           7.9218 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0400 |           8.9176 |           7.9841 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0458 |           8.8438 |           8.0381 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0515 |           8.7849 |           8.0647 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0523 |           8.7438 |           8.0934 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0533 |           8.6735 |           8.0883 |
[32m[20221208 14:22:49 @agent_ppo2.py:179][0m |          -0.0565 |           8.6443 |           8.1199 |
[32m[20221208 14:22:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.52
[32m[20221208 14:22:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.88
[32m[20221208 14:22:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.02
[32m[20221208 14:22:50 @agent_ppo2.py:137][0m Total time:       6.74 min
[32m[20221208 14:22:50 @agent_ppo2.py:139][0m 548864 total steps have happened
[32m[20221208 14:22:50 @agent_ppo2.py:115][0m #------------------------ Iteration 268 --------------------------#
[32m[20221208 14:22:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |           0.0428 |           2.5947 |           8.2816 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |           0.0345 |           1.9582 |           8.1894 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |          -0.0030 |           1.8670 |           8.0100 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |          -0.0151 |           1.7812 |           8.1448 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |          -0.0324 |           1.7652 |           8.1964 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |          -0.0346 |           1.6979 |           8.1633 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |          -0.0398 |           1.6559 |           8.2319 |
[32m[20221208 14:22:50 @agent_ppo2.py:179][0m |          -0.0454 |           1.6205 |           8.2689 |
[32m[20221208 14:22:51 @agent_ppo2.py:179][0m |          -0.0454 |           1.6049 |           8.2160 |
[32m[20221208 14:22:51 @agent_ppo2.py:179][0m |          -0.0494 |           1.5903 |           8.2513 |
[32m[20221208 14:22:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:22:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.21
[32m[20221208 14:22:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.97
[32m[20221208 14:22:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.75
[32m[20221208 14:22:51 @agent_ppo2.py:137][0m Total time:       6.77 min
[32m[20221208 14:22:51 @agent_ppo2.py:139][0m 550912 total steps have happened
[32m[20221208 14:22:51 @agent_ppo2.py:115][0m #------------------------ Iteration 269 --------------------------#
[32m[20221208 14:22:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |           0.0279 |           8.4891 |           8.2774 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0027 |           7.9111 |           8.1976 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0267 |           7.7160 |           8.2864 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0368 |           7.5935 |           8.3202 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0411 |           7.4810 |           8.3607 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0459 |           7.3490 |           8.3673 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0455 |           7.3094 |           8.3335 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0506 |           7.1891 |           8.3660 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0553 |           7.0475 |           8.4144 |
[32m[20221208 14:22:52 @agent_ppo2.py:179][0m |          -0.0592 |           6.9415 |           8.4148 |
[32m[20221208 14:22:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:22:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 175.72
[32m[20221208 14:22:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.36
[32m[20221208 14:22:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.16
[32m[20221208 14:22:53 @agent_ppo2.py:137][0m Total time:       6.79 min
[32m[20221208 14:22:53 @agent_ppo2.py:139][0m 552960 total steps have happened
[32m[20221208 14:22:53 @agent_ppo2.py:115][0m #------------------------ Iteration 270 --------------------------#
[32m[20221208 14:22:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |           0.0284 |           8.5470 |           8.1890 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0113 |           8.0493 |           8.2641 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0272 |           7.7538 |           8.3246 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0412 |           7.6535 |           8.3656 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0402 |           7.4264 |           8.3839 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0444 |           7.3796 |           8.3929 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0517 |           7.2362 |           8.4149 |
[32m[20221208 14:22:53 @agent_ppo2.py:179][0m |          -0.0554 |           7.1179 |           8.4437 |
[32m[20221208 14:22:54 @agent_ppo2.py:179][0m |          -0.0556 |           7.0589 |           8.4493 |
[32m[20221208 14:22:54 @agent_ppo2.py:179][0m |          -0.0582 |           6.9788 |           8.4794 |
[32m[20221208 14:22:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.41
[32m[20221208 14:22:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.76
[32m[20221208 14:22:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.34
[32m[20221208 14:22:54 @agent_ppo2.py:137][0m Total time:       6.82 min
[32m[20221208 14:22:54 @agent_ppo2.py:139][0m 555008 total steps have happened
[32m[20221208 14:22:54 @agent_ppo2.py:115][0m #------------------------ Iteration 271 --------------------------#
[32m[20221208 14:22:54 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:22:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |           0.0390 |           6.4018 |           8.1649 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |           0.0306 |           5.8800 |           7.9474 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0035 |           5.7678 |           7.9494 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0238 |           5.6703 |           8.1567 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0347 |           5.5701 |           8.2491 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0400 |           5.5647 |           8.3003 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0449 |           5.4973 |           8.3286 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0452 |           5.4147 |           8.3386 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0469 |           5.4032 |           8.3566 |
[32m[20221208 14:22:55 @agent_ppo2.py:179][0m |          -0.0484 |           5.4191 |           8.3760 |
[32m[20221208 14:22:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:22:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.41
[32m[20221208 14:22:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.28
[32m[20221208 14:22:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.70
[32m[20221208 14:22:56 @agent_ppo2.py:137][0m Total time:       6.84 min
[32m[20221208 14:22:56 @agent_ppo2.py:139][0m 557056 total steps have happened
[32m[20221208 14:22:56 @agent_ppo2.py:115][0m #------------------------ Iteration 272 --------------------------#
[32m[20221208 14:22:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:22:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |           0.0482 |           9.0558 |           8.5638 |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |           0.0297 |           8.3065 |           8.1963 |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |          -0.0099 |           8.0834 |           8.5792 |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |          -0.0247 |           7.9726 |           8.6060 |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |          -0.0407 |           7.9017 |           8.7168 |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |          -0.0477 |           7.8257 |           8.7528 |
[32m[20221208 14:22:56 @agent_ppo2.py:179][0m |          -0.0473 |           7.7733 |           8.7807 |
[32m[20221208 14:22:57 @agent_ppo2.py:179][0m |          -0.0544 |           7.7039 |           8.7836 |
[32m[20221208 14:22:57 @agent_ppo2.py:179][0m |          -0.0551 |           7.6467 |           8.7979 |
[32m[20221208 14:22:57 @agent_ppo2.py:179][0m |          -0.0605 |           7.5942 |           8.8344 |
[32m[20221208 14:22:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:22:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.81
[32m[20221208 14:22:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.29
[32m[20221208 14:22:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 164.98
[32m[20221208 14:22:57 @agent_ppo2.py:137][0m Total time:       6.87 min
[32m[20221208 14:22:57 @agent_ppo2.py:139][0m 559104 total steps have happened
[32m[20221208 14:22:57 @agent_ppo2.py:115][0m #------------------------ Iteration 273 --------------------------#
[32m[20221208 14:22:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |           0.0305 |           3.9218 |           8.5213 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0045 |           3.6287 |           8.5801 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0119 |           3.5147 |           8.3840 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0273 |           3.4460 |           8.5283 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0315 |           3.3729 |           8.5858 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0379 |           3.3254 |           8.7262 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0410 |           3.2687 |           8.7080 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0440 |           3.2395 |           8.7878 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0459 |           3.1819 |           8.7839 |
[32m[20221208 14:22:58 @agent_ppo2.py:179][0m |          -0.0476 |           3.1601 |           8.7941 |
[32m[20221208 14:22:58 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:22:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.69
[32m[20221208 14:22:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.54
[32m[20221208 14:22:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.84
[32m[20221208 14:22:59 @agent_ppo2.py:137][0m Total time:       6.89 min
[32m[20221208 14:22:59 @agent_ppo2.py:139][0m 561152 total steps have happened
[32m[20221208 14:22:59 @agent_ppo2.py:115][0m #------------------------ Iteration 274 --------------------------#
[32m[20221208 14:22:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:22:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |           0.0375 |           8.5953 |           8.3728 |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |           0.0218 |           8.2199 |           7.9028 |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |          -0.0132 |           8.0626 |           8.2643 |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |          -0.0249 |           7.9872 |           8.3254 |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |          -0.0381 |           7.8916 |           8.5018 |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |          -0.0405 |           7.7998 |           8.5092 |
[32m[20221208 14:22:59 @agent_ppo2.py:179][0m |          -0.0471 |           7.8550 |           8.5597 |
[32m[20221208 14:23:00 @agent_ppo2.py:179][0m |          -0.0501 |           7.7461 |           8.5670 |
[32m[20221208 14:23:00 @agent_ppo2.py:179][0m |          -0.0530 |           7.6630 |           8.5903 |
[32m[20221208 14:23:00 @agent_ppo2.py:179][0m |          -0.0531 |           7.6677 |           8.6231 |
[32m[20221208 14:23:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.14
[32m[20221208 14:23:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.21
[32m[20221208 14:23:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.23
[32m[20221208 14:23:00 @agent_ppo2.py:137][0m Total time:       6.92 min
[32m[20221208 14:23:00 @agent_ppo2.py:139][0m 563200 total steps have happened
[32m[20221208 14:23:00 @agent_ppo2.py:115][0m #------------------------ Iteration 275 --------------------------#
[32m[20221208 14:23:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |           0.0251 |           6.2547 |           8.6856 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0115 |           5.7805 |           8.6440 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0289 |           5.5529 |           8.8030 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0388 |           5.4151 |           8.8156 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0464 |           5.2969 |           8.8509 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0444 |           5.1986 |           8.8661 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0509 |           5.0882 |           8.8827 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0525 |           5.0112 |           8.8992 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0560 |           4.9341 |           8.9247 |
[32m[20221208 14:23:01 @agent_ppo2.py:179][0m |          -0.0555 |           4.8239 |           8.9346 |
[32m[20221208 14:23:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.98
[32m[20221208 14:23:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.47
[32m[20221208 14:23:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.28
[32m[20221208 14:23:02 @agent_ppo2.py:137][0m Total time:       6.94 min
[32m[20221208 14:23:02 @agent_ppo2.py:139][0m 565248 total steps have happened
[32m[20221208 14:23:02 @agent_ppo2.py:115][0m #------------------------ Iteration 276 --------------------------#
[32m[20221208 14:23:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |           0.0195 |           3.9432 |           8.5939 |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |          -0.0002 |           3.6246 |           8.5637 |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |          -0.0127 |           3.5460 |           8.5895 |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |          -0.0229 |           3.4895 |           8.5725 |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |          -0.0358 |           3.4080 |           8.6922 |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |          -0.0390 |           3.3996 |           8.7596 |
[32m[20221208 14:23:02 @agent_ppo2.py:179][0m |          -0.0425 |           3.3607 |           8.7343 |
[32m[20221208 14:23:03 @agent_ppo2.py:179][0m |          -0.0427 |           3.3430 |           8.7439 |
[32m[20221208 14:23:03 @agent_ppo2.py:179][0m |          -0.0424 |           3.3451 |           8.7842 |
[32m[20221208 14:23:03 @agent_ppo2.py:179][0m |          -0.0445 |           3.2775 |           8.7840 |
[32m[20221208 14:23:03 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:23:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.87
[32m[20221208 14:23:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.42
[32m[20221208 14:23:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 189.46
[32m[20221208 14:23:03 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 189.46
[32m[20221208 14:23:03 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 189.46
[32m[20221208 14:23:03 @agent_ppo2.py:137][0m Total time:       6.97 min
[32m[20221208 14:23:03 @agent_ppo2.py:139][0m 567296 total steps have happened
[32m[20221208 14:23:03 @agent_ppo2.py:115][0m #------------------------ Iteration 277 --------------------------#
[32m[20221208 14:23:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |           0.0562 |           6.3894 |           8.4471 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |           0.0229 |           5.8265 |           8.3770 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0070 |           5.5236 |           8.7020 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0249 |           5.3340 |           8.6987 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0334 |           5.1974 |           8.8129 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0406 |           5.0509 |           8.8480 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0435 |           4.9409 |           8.8714 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0480 |           4.8122 |           8.8806 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0441 |           4.7200 |           8.8928 |
[32m[20221208 14:23:04 @agent_ppo2.py:179][0m |          -0.0497 |           4.6710 |           8.8959 |
[32m[20221208 14:23:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.10
[32m[20221208 14:23:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.68
[32m[20221208 14:23:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.92
[32m[20221208 14:23:05 @agent_ppo2.py:137][0m Total time:       6.99 min
[32m[20221208 14:23:05 @agent_ppo2.py:139][0m 569344 total steps have happened
[32m[20221208 14:23:05 @agent_ppo2.py:115][0m #------------------------ Iteration 278 --------------------------#
[32m[20221208 14:23:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |           0.0300 |           9.0277 |           8.7798 |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |          -0.0016 |           8.0242 |           8.5898 |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |          -0.0278 |           7.6689 |           8.8117 |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |          -0.0368 |           7.4454 |           8.8547 |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |          -0.0441 |           7.3208 |           8.8798 |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |          -0.0479 |           7.1649 |           8.9304 |
[32m[20221208 14:23:05 @agent_ppo2.py:179][0m |          -0.0484 |           6.9967 |           8.9156 |
[32m[20221208 14:23:06 @agent_ppo2.py:179][0m |          -0.0522 |           6.9045 |           8.9184 |
[32m[20221208 14:23:06 @agent_ppo2.py:179][0m |          -0.0550 |           6.8717 |           8.9402 |
[32m[20221208 14:23:06 @agent_ppo2.py:179][0m |          -0.0549 |           6.7799 |           8.9685 |
[32m[20221208 14:23:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.44
[32m[20221208 14:23:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.29
[32m[20221208 14:23:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.03
[32m[20221208 14:23:06 @agent_ppo2.py:137][0m Total time:       7.02 min
[32m[20221208 14:23:06 @agent_ppo2.py:139][0m 571392 total steps have happened
[32m[20221208 14:23:06 @agent_ppo2.py:115][0m #------------------------ Iteration 279 --------------------------#
[32m[20221208 14:23:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |           0.0409 |           7.6502 |           8.8754 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |           0.0226 |           6.9223 |           8.7203 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0158 |           6.6220 |           8.8632 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0303 |           6.4634 |           8.9661 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0426 |           6.2841 |           9.0349 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0491 |           6.1597 |           9.0524 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0520 |           6.1017 |           9.0978 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0566 |           5.9561 |           9.1065 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0602 |           5.8449 |           9.1417 |
[32m[20221208 14:23:07 @agent_ppo2.py:179][0m |          -0.0613 |           5.7761 |           9.1851 |
[32m[20221208 14:23:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:23:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.86
[32m[20221208 14:23:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.68
[32m[20221208 14:23:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.98
[32m[20221208 14:23:08 @agent_ppo2.py:137][0m Total time:       7.04 min
[32m[20221208 14:23:08 @agent_ppo2.py:139][0m 573440 total steps have happened
[32m[20221208 14:23:08 @agent_ppo2.py:115][0m #------------------------ Iteration 280 --------------------------#
[32m[20221208 14:23:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |           0.0169 |           8.3746 |           8.8308 |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |           0.0390 |           7.7480 |           8.5071 |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |           0.0088 |           7.5073 |           8.2128 |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |          -0.0222 |           7.3120 |           8.5999 |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |          -0.0354 |           7.1745 |           8.7849 |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |          -0.0430 |           7.0821 |           8.8623 |
[32m[20221208 14:23:08 @agent_ppo2.py:179][0m |          -0.0441 |           6.9706 |           8.8678 |
[32m[20221208 14:23:09 @agent_ppo2.py:179][0m |          -0.0515 |           6.9026 |           8.9423 |
[32m[20221208 14:23:09 @agent_ppo2.py:179][0m |          -0.0540 |           6.8301 |           8.9610 |
[32m[20221208 14:23:09 @agent_ppo2.py:179][0m |          -0.0575 |           6.7727 |           8.9948 |
[32m[20221208 14:23:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.23
[32m[20221208 14:23:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.69
[32m[20221208 14:23:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.49
[32m[20221208 14:23:09 @agent_ppo2.py:137][0m Total time:       7.07 min
[32m[20221208 14:23:09 @agent_ppo2.py:139][0m 575488 total steps have happened
[32m[20221208 14:23:09 @agent_ppo2.py:115][0m #------------------------ Iteration 281 --------------------------#
[32m[20221208 14:23:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |           0.0367 |           6.5280 |           8.9796 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |           0.0043 |           5.9656 |           8.8775 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0229 |           5.8705 |           9.0546 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0327 |           5.7384 |           9.1670 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0423 |           5.6614 |           9.1834 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0448 |           5.6454 |           9.2722 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0483 |           5.6127 |           9.2695 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0510 |           5.5630 |           9.2532 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0487 |           5.5231 |           9.2905 |
[32m[20221208 14:23:10 @agent_ppo2.py:179][0m |          -0.0519 |           5.5024 |           9.2515 |
[32m[20221208 14:23:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.46
[32m[20221208 14:23:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.49
[32m[20221208 14:23:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.14
[32m[20221208 14:23:11 @agent_ppo2.py:137][0m Total time:       7.09 min
[32m[20221208 14:23:11 @agent_ppo2.py:139][0m 577536 total steps have happened
[32m[20221208 14:23:11 @agent_ppo2.py:115][0m #------------------------ Iteration 282 --------------------------#
[32m[20221208 14:23:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |           0.0272 |           8.7182 |           9.0151 |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |           0.0045 |           8.1456 |           8.8251 |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |          -0.0188 |           7.8076 |           8.9341 |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |          -0.0330 |           7.5494 |           9.0644 |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |          -0.0397 |           7.3835 |           9.0985 |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |          -0.0447 |           7.2812 |           9.1669 |
[32m[20221208 14:23:11 @agent_ppo2.py:179][0m |          -0.0481 |           7.1562 |           9.1625 |
[32m[20221208 14:23:12 @agent_ppo2.py:179][0m |          -0.0511 |           7.0134 |           9.1911 |
[32m[20221208 14:23:12 @agent_ppo2.py:179][0m |          -0.0536 |           6.9604 |           9.2017 |
[32m[20221208 14:23:12 @agent_ppo2.py:179][0m |          -0.0551 |           6.8616 |           9.2295 |
[32m[20221208 14:23:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:23:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.55
[32m[20221208 14:23:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.77
[32m[20221208 14:23:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.04
[32m[20221208 14:23:12 @agent_ppo2.py:137][0m Total time:       7.12 min
[32m[20221208 14:23:12 @agent_ppo2.py:139][0m 579584 total steps have happened
[32m[20221208 14:23:12 @agent_ppo2.py:115][0m #------------------------ Iteration 283 --------------------------#
[32m[20221208 14:23:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |           0.0543 |           6.5862 |           8.9552 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |           0.0168 |           6.0750 |           8.8543 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0127 |           5.8449 |           9.1371 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0271 |           5.7116 |           9.2396 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0390 |           5.6474 |           9.2879 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0437 |           5.5620 |           9.3578 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0481 |           5.5031 |           9.4074 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0518 |           5.5010 |           9.4241 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0503 |           5.4160 |           9.4431 |
[32m[20221208 14:23:13 @agent_ppo2.py:179][0m |          -0.0535 |           5.4152 |           9.4604 |
[32m[20221208 14:23:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.24
[32m[20221208 14:23:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.27
[32m[20221208 14:23:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.78
[32m[20221208 14:23:14 @agent_ppo2.py:137][0m Total time:       7.14 min
[32m[20221208 14:23:14 @agent_ppo2.py:139][0m 581632 total steps have happened
[32m[20221208 14:23:14 @agent_ppo2.py:115][0m #------------------------ Iteration 284 --------------------------#
[32m[20221208 14:23:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |           0.0331 |           9.1580 |           9.4803 |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |           0.0104 |           8.0644 |           9.2709 |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |          -0.0077 |           7.4612 |           9.4527 |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |           0.0185 |           6.9869 |           8.9190 |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |          -0.0177 |           6.8087 |           9.3172 |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |          -0.0356 |           6.5012 |           9.5199 |
[32m[20221208 14:23:14 @agent_ppo2.py:179][0m |          -0.0431 |           6.3329 |           9.6077 |
[32m[20221208 14:23:15 @agent_ppo2.py:179][0m |          -0.0464 |           6.1695 |           9.6652 |
[32m[20221208 14:23:15 @agent_ppo2.py:179][0m |          -0.0525 |           5.9861 |           9.6948 |
[32m[20221208 14:23:15 @agent_ppo2.py:179][0m |          -0.0551 |           5.9414 |           9.7307 |
[32m[20221208 14:23:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.56
[32m[20221208 14:23:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.12
[32m[20221208 14:23:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.81
[32m[20221208 14:23:15 @agent_ppo2.py:137][0m Total time:       7.17 min
[32m[20221208 14:23:15 @agent_ppo2.py:139][0m 583680 total steps have happened
[32m[20221208 14:23:15 @agent_ppo2.py:115][0m #------------------------ Iteration 285 --------------------------#
[32m[20221208 14:23:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:23:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |           0.0299 |           8.9721 |           9.6280 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |           0.0131 |           7.9948 |           9.4355 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |           0.0121 |           7.7090 |           9.0658 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0248 |           7.4240 |           9.4787 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0378 |           7.2543 |           9.5889 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0462 |           7.1506 |           9.6740 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0532 |           7.0449 |           9.7345 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0520 |           6.9631 |           9.7299 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0548 |           6.8349 |           9.7437 |
[32m[20221208 14:23:16 @agent_ppo2.py:179][0m |          -0.0592 |           6.7325 |           9.7755 |
[32m[20221208 14:23:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.89
[32m[20221208 14:23:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.55
[32m[20221208 14:23:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.37
[32m[20221208 14:23:17 @agent_ppo2.py:137][0m Total time:       7.19 min
[32m[20221208 14:23:17 @agent_ppo2.py:139][0m 585728 total steps have happened
[32m[20221208 14:23:17 @agent_ppo2.py:115][0m #------------------------ Iteration 286 --------------------------#
[32m[20221208 14:23:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |           0.0593 |           6.8688 |           9.4413 |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |           0.0159 |           5.9401 |           9.2444 |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |           0.0001 |           5.6735 |           9.4250 |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |          -0.0213 |           5.5090 |           9.5954 |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |          -0.0348 |           5.3914 |           9.6658 |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |          -0.0380 |           5.3118 |           9.6128 |
[32m[20221208 14:23:17 @agent_ppo2.py:179][0m |          -0.0484 |           5.2435 |           9.6814 |
[32m[20221208 14:23:18 @agent_ppo2.py:179][0m |          -0.0529 |           5.1598 |           9.7511 |
[32m[20221208 14:23:18 @agent_ppo2.py:179][0m |          -0.0558 |           5.1004 |           9.7878 |
[32m[20221208 14:23:18 @agent_ppo2.py:179][0m |          -0.0539 |           5.0890 |           9.8090 |
[32m[20221208 14:23:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.35
[32m[20221208 14:23:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.22
[32m[20221208 14:23:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.36
[32m[20221208 14:23:18 @agent_ppo2.py:137][0m Total time:       7.22 min
[32m[20221208 14:23:18 @agent_ppo2.py:139][0m 587776 total steps have happened
[32m[20221208 14:23:18 @agent_ppo2.py:115][0m #------------------------ Iteration 287 --------------------------#
[32m[20221208 14:23:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |           0.0411 |           7.0135 |           9.6598 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |           0.0316 |           6.4495 |           9.2872 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0060 |           6.3245 |           9.4894 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0249 |           6.1367 |           9.6727 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0349 |           6.0888 |           9.8075 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0414 |           5.9888 |           9.8510 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0490 |           5.9356 |           9.9089 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0532 |           5.8726 |           9.8941 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0542 |           5.8140 |           9.9342 |
[32m[20221208 14:23:19 @agent_ppo2.py:179][0m |          -0.0594 |           5.7911 |           9.9611 |
[32m[20221208 14:23:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.90
[32m[20221208 14:23:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.57
[32m[20221208 14:23:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.13
[32m[20221208 14:23:20 @agent_ppo2.py:137][0m Total time:       7.24 min
[32m[20221208 14:23:20 @agent_ppo2.py:139][0m 589824 total steps have happened
[32m[20221208 14:23:20 @agent_ppo2.py:115][0m #------------------------ Iteration 288 --------------------------#
[32m[20221208 14:23:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |           0.0270 |           8.3191 |           9.9975 |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |          -0.0068 |           7.8942 |           9.8551 |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |          -0.0216 |           7.6790 |          10.0460 |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |          -0.0307 |           7.5560 |          10.0576 |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |          -0.0390 |           7.4472 |          10.1832 |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |          -0.0434 |           7.4018 |          10.1872 |
[32m[20221208 14:23:20 @agent_ppo2.py:179][0m |          -0.0471 |           7.3029 |          10.2602 |
[32m[20221208 14:23:21 @agent_ppo2.py:179][0m |          -0.0489 |           7.2455 |          10.1987 |
[32m[20221208 14:23:21 @agent_ppo2.py:179][0m |          -0.0549 |           7.1552 |          10.2680 |
[32m[20221208 14:23:21 @agent_ppo2.py:179][0m |          -0.0545 |           7.1161 |          10.2967 |
[32m[20221208 14:23:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.29
[32m[20221208 14:23:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.78
[32m[20221208 14:23:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.74
[32m[20221208 14:23:21 @agent_ppo2.py:137][0m Total time:       7.27 min
[32m[20221208 14:23:21 @agent_ppo2.py:139][0m 591872 total steps have happened
[32m[20221208 14:23:21 @agent_ppo2.py:115][0m #------------------------ Iteration 289 --------------------------#
[32m[20221208 14:23:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |           0.0766 |           6.7182 |           9.4992 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |           0.0125 |           6.1537 |           7.1185 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0134 |           6.0371 |           6.9706 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0330 |           5.9177 |           7.2350 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0481 |           5.8602 |           7.3656 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0550 |           5.8428 |           7.4754 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0585 |           5.7856 |           7.5275 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0651 |           5.7428 |           7.5910 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0693 |           5.7137 |           7.6286 |
[32m[20221208 14:23:22 @agent_ppo2.py:179][0m |          -0.0705 |           5.7634 |           7.6613 |
[32m[20221208 14:23:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.79
[32m[20221208 14:23:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.27
[32m[20221208 14:23:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.46
[32m[20221208 14:23:23 @agent_ppo2.py:137][0m Total time:       7.29 min
[32m[20221208 14:23:23 @agent_ppo2.py:139][0m 593920 total steps have happened
[32m[20221208 14:23:23 @agent_ppo2.py:115][0m #------------------------ Iteration 290 --------------------------#
[32m[20221208 14:23:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |           0.0534 |           8.9878 |          10.1734 |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |           0.0219 |           7.9284 |          10.1184 |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |          -0.0067 |           7.4862 |          10.1771 |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |          -0.0312 |           7.1963 |          10.4247 |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |          -0.0356 |           7.0257 |          10.4582 |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |          -0.0399 |           6.8921 |          10.4916 |
[32m[20221208 14:23:23 @agent_ppo2.py:179][0m |          -0.0517 |           6.7402 |          10.6133 |
[32m[20221208 14:23:24 @agent_ppo2.py:179][0m |          -0.0527 |           6.5880 |          10.5990 |
[32m[20221208 14:23:24 @agent_ppo2.py:179][0m |          -0.0564 |           6.4673 |          10.6266 |
[32m[20221208 14:23:24 @agent_ppo2.py:179][0m |          -0.0587 |           6.4031 |          10.6603 |
[32m[20221208 14:23:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.99
[32m[20221208 14:23:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.01
[32m[20221208 14:23:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.05
[32m[20221208 14:23:24 @agent_ppo2.py:137][0m Total time:       7.32 min
[32m[20221208 14:23:24 @agent_ppo2.py:139][0m 595968 total steps have happened
[32m[20221208 14:23:24 @agent_ppo2.py:115][0m #------------------------ Iteration 291 --------------------------#
[32m[20221208 14:23:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |           0.0202 |           9.7445 |          10.6727 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0038 |           8.8977 |          10.4486 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0261 |           8.7014 |          10.6500 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0365 |           8.5984 |          10.6890 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0465 |           8.4591 |          10.7680 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0519 |           8.3528 |          10.7926 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0527 |           8.3594 |          10.7971 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0563 |           8.2658 |          10.7968 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0577 |           8.2228 |          10.8169 |
[32m[20221208 14:23:25 @agent_ppo2.py:179][0m |          -0.0592 |           8.1611 |          10.8366 |
[32m[20221208 14:23:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 179.80
[32m[20221208 14:23:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.91
[32m[20221208 14:23:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.71
[32m[20221208 14:23:26 @agent_ppo2.py:137][0m Total time:       7.34 min
[32m[20221208 14:23:26 @agent_ppo2.py:139][0m 598016 total steps have happened
[32m[20221208 14:23:26 @agent_ppo2.py:115][0m #------------------------ Iteration 292 --------------------------#
[32m[20221208 14:23:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |           0.0221 |           7.1937 |          10.6599 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |           0.0215 |           6.4752 |          10.4799 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |          -0.0087 |           6.2498 |          10.6244 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |          -0.0238 |           6.0963 |          10.7963 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |          -0.0346 |           5.9748 |          10.8020 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |          -0.0469 |           5.9237 |          10.9095 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |          -0.0492 |           5.8476 |          10.9703 |
[32m[20221208 14:23:26 @agent_ppo2.py:179][0m |          -0.0457 |           5.8206 |          10.9447 |
[32m[20221208 14:23:27 @agent_ppo2.py:179][0m |          -0.0509 |           5.7328 |          10.9839 |
[32m[20221208 14:23:27 @agent_ppo2.py:179][0m |          -0.0551 |           5.7227 |          10.9935 |
[32m[20221208 14:23:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.38
[32m[20221208 14:23:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.38
[32m[20221208 14:23:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.40
[32m[20221208 14:23:27 @agent_ppo2.py:137][0m Total time:       7.37 min
[32m[20221208 14:23:27 @agent_ppo2.py:139][0m 600064 total steps have happened
[32m[20221208 14:23:27 @agent_ppo2.py:115][0m #------------------------ Iteration 293 --------------------------#
[32m[20221208 14:23:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:23:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |           0.0345 |           6.2756 |          10.9563 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |           0.0025 |           5.8269 |          10.8848 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0208 |           5.5835 |          11.0472 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0266 |           5.4662 |          11.1192 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0243 |           5.4057 |          11.0796 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0388 |           5.2655 |          11.1637 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0447 |           5.1743 |          11.1845 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0490 |           5.1024 |          11.2166 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0502 |           5.0178 |          11.2275 |
[32m[20221208 14:23:28 @agent_ppo2.py:179][0m |          -0.0489 |           4.9359 |          11.2244 |
[32m[20221208 14:23:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.73
[32m[20221208 14:23:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.13
[32m[20221208 14:23:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.74
[32m[20221208 14:23:29 @agent_ppo2.py:137][0m Total time:       7.39 min
[32m[20221208 14:23:29 @agent_ppo2.py:139][0m 602112 total steps have happened
[32m[20221208 14:23:29 @agent_ppo2.py:115][0m #------------------------ Iteration 294 --------------------------#
[32m[20221208 14:23:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |           0.0654 |           9.3020 |          10.7869 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |           0.0104 |           8.8621 |          10.9190 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |          -0.0100 |           8.6270 |          11.1144 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |          -0.0261 |           8.4921 |          11.2299 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |          -0.0344 |           8.3795 |          11.2945 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |          -0.0324 |           8.2718 |          11.2716 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |          -0.0378 |           8.3108 |          11.3057 |
[32m[20221208 14:23:29 @agent_ppo2.py:179][0m |          -0.0424 |           8.2112 |          11.3593 |
[32m[20221208 14:23:30 @agent_ppo2.py:179][0m |          -0.0450 |           8.1212 |          11.3429 |
[32m[20221208 14:23:30 @agent_ppo2.py:179][0m |          -0.0494 |           8.0931 |          11.3630 |
[32m[20221208 14:23:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.57
[32m[20221208 14:23:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.11
[32m[20221208 14:23:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.18
[32m[20221208 14:23:30 @agent_ppo2.py:137][0m Total time:       7.42 min
[32m[20221208 14:23:30 @agent_ppo2.py:139][0m 604160 total steps have happened
[32m[20221208 14:23:30 @agent_ppo2.py:115][0m #------------------------ Iteration 295 --------------------------#
[32m[20221208 14:23:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |           0.0489 |           9.0846 |          10.8590 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |           0.0180 |           8.7772 |          10.5129 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0184 |           8.6096 |          10.9730 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0310 |           8.4482 |          11.1335 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0414 |           8.4175 |          11.2571 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0404 |           8.2769 |          11.2204 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0452 |           8.2347 |          11.2905 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0471 |           8.1792 |          11.3145 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0492 |           8.0878 |          11.3586 |
[32m[20221208 14:23:31 @agent_ppo2.py:179][0m |          -0.0538 |           8.1044 |          11.3900 |
[32m[20221208 14:23:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:23:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.94
[32m[20221208 14:23:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.04
[32m[20221208 14:23:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.95
[32m[20221208 14:23:32 @agent_ppo2.py:137][0m Total time:       7.44 min
[32m[20221208 14:23:32 @agent_ppo2.py:139][0m 606208 total steps have happened
[32m[20221208 14:23:32 @agent_ppo2.py:115][0m #------------------------ Iteration 296 --------------------------#
[32m[20221208 14:23:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |           0.0446 |           6.5959 |          11.0557 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |           0.0208 |           6.1696 |          10.8902 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |          -0.0216 |           5.9362 |          11.2344 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |          -0.0328 |           5.7487 |          11.3069 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |          -0.0389 |           5.6072 |          11.3808 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |          -0.0420 |           5.4867 |          11.3835 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |          -0.0490 |           5.3411 |          11.4484 |
[32m[20221208 14:23:32 @agent_ppo2.py:179][0m |          -0.0475 |           5.2433 |          11.4491 |
[32m[20221208 14:23:33 @agent_ppo2.py:179][0m |          -0.0514 |           5.1617 |          11.4259 |
[32m[20221208 14:23:33 @agent_ppo2.py:179][0m |          -0.0536 |           5.0969 |          11.4855 |
[32m[20221208 14:23:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.59
[32m[20221208 14:23:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.61
[32m[20221208 14:23:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.35
[32m[20221208 14:23:33 @agent_ppo2.py:137][0m Total time:       7.47 min
[32m[20221208 14:23:33 @agent_ppo2.py:139][0m 608256 total steps have happened
[32m[20221208 14:23:33 @agent_ppo2.py:115][0m #------------------------ Iteration 297 --------------------------#
[32m[20221208 14:23:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |           0.0329 |           6.8696 |          11.3388 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |           0.0033 |           6.3116 |          11.2809 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0262 |           6.1768 |          11.5429 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0328 |           6.0338 |          11.4780 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0382 |           6.0167 |          11.5388 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0456 |           5.8960 |          11.5902 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0482 |           5.9022 |          11.6124 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0500 |           5.8381 |          11.6009 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0523 |           5.8236 |          11.6384 |
[32m[20221208 14:23:34 @agent_ppo2.py:179][0m |          -0.0533 |           5.7565 |          11.6218 |
[32m[20221208 14:23:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.50
[32m[20221208 14:23:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.58
[32m[20221208 14:23:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.03
[32m[20221208 14:23:35 @agent_ppo2.py:137][0m Total time:       7.49 min
[32m[20221208 14:23:35 @agent_ppo2.py:139][0m 610304 total steps have happened
[32m[20221208 14:23:35 @agent_ppo2.py:115][0m #------------------------ Iteration 298 --------------------------#
[32m[20221208 14:23:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |           0.0237 |           3.9856 |          11.4559 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |           0.0463 |           3.5460 |          10.6200 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |          -0.0145 |           3.4347 |          11.0231 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |          -0.0321 |           3.3272 |          11.2234 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |          -0.0391 |           3.2575 |          11.3373 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |          -0.0447 |           3.1926 |          11.3307 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |          -0.0498 |           3.1229 |          11.4079 |
[32m[20221208 14:23:35 @agent_ppo2.py:179][0m |          -0.0518 |           3.0334 |          11.4200 |
[32m[20221208 14:23:36 @agent_ppo2.py:179][0m |          -0.0566 |           2.9534 |          11.4784 |
[32m[20221208 14:23:36 @agent_ppo2.py:179][0m |          -0.0568 |           2.8963 |          11.4871 |
[32m[20221208 14:23:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.33
[32m[20221208 14:23:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.53
[32m[20221208 14:23:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.08
[32m[20221208 14:23:36 @agent_ppo2.py:137][0m Total time:       7.52 min
[32m[20221208 14:23:36 @agent_ppo2.py:139][0m 612352 total steps have happened
[32m[20221208 14:23:36 @agent_ppo2.py:115][0m #------------------------ Iteration 299 --------------------------#
[32m[20221208 14:23:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |           0.0281 |           6.6962 |          11.8386 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |           0.0602 |           6.3999 |          11.0639 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |           0.0179 |           6.2068 |          11.3433 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0059 |           6.0928 |          11.6603 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0183 |           6.0218 |          11.8063 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0308 |           5.9614 |          11.9427 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0355 |           5.9791 |          11.9583 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0428 |           5.8510 |          12.0169 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0481 |           5.8127 |          12.0771 |
[32m[20221208 14:23:37 @agent_ppo2.py:179][0m |          -0.0503 |           5.7789 |          12.0419 |
[32m[20221208 14:23:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.47
[32m[20221208 14:23:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.55
[32m[20221208 14:23:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.76
[32m[20221208 14:23:38 @agent_ppo2.py:137][0m Total time:       7.54 min
[32m[20221208 14:23:38 @agent_ppo2.py:139][0m 614400 total steps have happened
[32m[20221208 14:23:38 @agent_ppo2.py:115][0m #------------------------ Iteration 300 --------------------------#
[32m[20221208 14:23:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |           0.0546 |           7.0697 |          11.4658 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |           0.0403 |           6.5950 |          10.5536 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |          -0.0034 |           6.4244 |          11.3733 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |          -0.0249 |           6.2968 |          11.6012 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |          -0.0361 |           6.1949 |          11.7071 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |          -0.0391 |           6.1304 |          11.7351 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |          -0.0439 |           6.0658 |          11.7015 |
[32m[20221208 14:23:38 @agent_ppo2.py:179][0m |          -0.0498 |           6.0189 |          11.7760 |
[32m[20221208 14:23:39 @agent_ppo2.py:179][0m |          -0.0518 |           5.9979 |          11.7793 |
[32m[20221208 14:23:39 @agent_ppo2.py:179][0m |          -0.0570 |           5.9515 |          11.8380 |
[32m[20221208 14:23:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.02
[32m[20221208 14:23:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.69
[32m[20221208 14:23:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.96
[32m[20221208 14:23:39 @agent_ppo2.py:137][0m Total time:       7.57 min
[32m[20221208 14:23:39 @agent_ppo2.py:139][0m 616448 total steps have happened
[32m[20221208 14:23:39 @agent_ppo2.py:115][0m #------------------------ Iteration 301 --------------------------#
[32m[20221208 14:23:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |           0.0151 |           9.3477 |          11.7750 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0063 |           8.8521 |          11.6120 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0266 |           8.6905 |          11.8460 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0329 |           8.6084 |          11.8039 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0421 |           8.5400 |          11.9211 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0472 |           8.4804 |          11.9735 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0461 |           8.4491 |          11.8939 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0526 |           8.4696 |          11.9636 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0553 |           8.3665 |          12.0050 |
[32m[20221208 14:23:40 @agent_ppo2.py:179][0m |          -0.0566 |           8.3582 |          11.9944 |
[32m[20221208 14:23:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:23:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.86
[32m[20221208 14:23:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.22
[32m[20221208 14:23:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 171.14
[32m[20221208 14:23:41 @agent_ppo2.py:137][0m Total time:       7.59 min
[32m[20221208 14:23:41 @agent_ppo2.py:139][0m 618496 total steps have happened
[32m[20221208 14:23:41 @agent_ppo2.py:115][0m #------------------------ Iteration 302 --------------------------#
[32m[20221208 14:23:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |           0.0501 |           6.5388 |          11.2765 |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |           0.0205 |           6.3556 |          11.2808 |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |          -0.0010 |           6.2394 |          11.4915 |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |          -0.0241 |           6.1968 |          11.6754 |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |          -0.0366 |           6.0934 |          11.8193 |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |          -0.0428 |           6.0578 |          11.8979 |
[32m[20221208 14:23:41 @agent_ppo2.py:179][0m |          -0.0453 |           6.0669 |          11.8991 |
[32m[20221208 14:23:42 @agent_ppo2.py:179][0m |          -0.0488 |           5.9993 |          11.9362 |
[32m[20221208 14:23:42 @agent_ppo2.py:179][0m |          -0.0523 |           5.9839 |          11.9593 |
[32m[20221208 14:23:42 @agent_ppo2.py:179][0m |          -0.0558 |           5.9099 |          11.9585 |
[32m[20221208 14:23:42 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:23:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.57
[32m[20221208 14:23:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.19
[32m[20221208 14:23:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.15
[32m[20221208 14:23:42 @agent_ppo2.py:137][0m Total time:       7.62 min
[32m[20221208 14:23:42 @agent_ppo2.py:139][0m 620544 total steps have happened
[32m[20221208 14:23:42 @agent_ppo2.py:115][0m #------------------------ Iteration 303 --------------------------#
[32m[20221208 14:23:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |           0.0420 |           6.5151 |          11.4499 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |           0.0200 |           6.0242 |          11.0670 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0137 |           5.7989 |          11.6432 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0314 |           5.6034 |          11.7367 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0404 |           5.5182 |          11.8139 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0448 |           5.4201 |          11.8810 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0499 |           5.2660 |          11.9102 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0525 |           5.2174 |          11.9387 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0551 |           5.1588 |          11.9770 |
[32m[20221208 14:23:43 @agent_ppo2.py:179][0m |          -0.0574 |           4.9894 |          11.9948 |
[32m[20221208 14:23:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.49
[32m[20221208 14:23:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.75
[32m[20221208 14:23:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.70
[32m[20221208 14:23:44 @agent_ppo2.py:137][0m Total time:       7.64 min
[32m[20221208 14:23:44 @agent_ppo2.py:139][0m 622592 total steps have happened
[32m[20221208 14:23:44 @agent_ppo2.py:115][0m #------------------------ Iteration 304 --------------------------#
[32m[20221208 14:23:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |           0.0699 |           7.9682 |          11.6237 |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |           0.0566 |           6.8258 |          10.1295 |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |           0.0296 |           6.3410 |          10.1941 |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |          -0.0085 |           6.0790 |          10.8660 |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |          -0.0223 |           5.8712 |          11.1024 |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |          -0.0228 |           5.7584 |          11.1016 |
[32m[20221208 14:23:44 @agent_ppo2.py:179][0m |          -0.0312 |           5.7545 |          11.1572 |
[32m[20221208 14:23:45 @agent_ppo2.py:179][0m |          -0.0397 |           5.5465 |          11.2757 |
[32m[20221208 14:23:45 @agent_ppo2.py:179][0m |          -0.0473 |           5.5345 |          11.3028 |
[32m[20221208 14:23:45 @agent_ppo2.py:179][0m |          -0.0436 |           5.4422 |          11.2557 |
[32m[20221208 14:23:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.75
[32m[20221208 14:23:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.15
[32m[20221208 14:23:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.64
[32m[20221208 14:23:45 @agent_ppo2.py:137][0m Total time:       7.67 min
[32m[20221208 14:23:45 @agent_ppo2.py:139][0m 624640 total steps have happened
[32m[20221208 14:23:45 @agent_ppo2.py:115][0m #------------------------ Iteration 305 --------------------------#
[32m[20221208 14:23:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |           0.0660 |           8.3519 |          11.7073 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |           0.0990 |           7.3248 |          10.3570 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |           0.0022 |           7.0355 |          10.4605 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0194 |           6.7948 |          10.7501 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0346 |           6.6779 |          10.9661 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0443 |           6.5470 |          11.1617 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0510 |           6.4874 |          11.1954 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0546 |           6.3796 |          11.2819 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0577 |           6.3268 |          11.2774 |
[32m[20221208 14:23:46 @agent_ppo2.py:179][0m |          -0.0603 |           6.2300 |          11.3273 |
[32m[20221208 14:23:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:23:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.29
[32m[20221208 14:23:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.92
[32m[20221208 14:23:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.10
[32m[20221208 14:23:47 @agent_ppo2.py:137][0m Total time:       7.69 min
[32m[20221208 14:23:47 @agent_ppo2.py:139][0m 626688 total steps have happened
[32m[20221208 14:23:47 @agent_ppo2.py:115][0m #------------------------ Iteration 306 --------------------------#
[32m[20221208 14:23:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |           0.0497 |           9.9540 |          11.9928 |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |           0.0610 |           9.2546 |          11.1021 |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |           0.0176 |           9.0884 |          11.3285 |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |          -0.0026 |           8.9823 |          11.5708 |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |          -0.0212 |           8.8521 |          11.9233 |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |          -0.0287 |           8.7348 |          12.0664 |
[32m[20221208 14:23:47 @agent_ppo2.py:179][0m |          -0.0355 |           8.6837 |          12.1944 |
[32m[20221208 14:23:48 @agent_ppo2.py:179][0m |          -0.0438 |           8.6275 |          12.2870 |
[32m[20221208 14:23:48 @agent_ppo2.py:179][0m |          -0.0473 |           8.4654 |          12.2910 |
[32m[20221208 14:23:48 @agent_ppo2.py:179][0m |          -0.0493 |           8.4187 |          12.3204 |
[32m[20221208 14:23:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:23:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.87
[32m[20221208 14:23:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.06
[32m[20221208 14:23:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.80
[32m[20221208 14:23:48 @agent_ppo2.py:137][0m Total time:       7.72 min
[32m[20221208 14:23:48 @agent_ppo2.py:139][0m 628736 total steps have happened
[32m[20221208 14:23:48 @agent_ppo2.py:115][0m #------------------------ Iteration 307 --------------------------#
[32m[20221208 14:23:49 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:23:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |           0.0481 |          10.0502 |          12.0006 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |           0.0385 |           9.3247 |          11.5259 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0101 |           9.0612 |          12.0351 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0301 |           8.9691 |          12.1753 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0373 |           8.9269 |          12.2302 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0436 |           8.8485 |          12.3055 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0457 |           8.7843 |          12.2472 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0477 |           8.7200 |          12.2260 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0515 |           8.6469 |          12.2874 |
[32m[20221208 14:23:49 @agent_ppo2.py:179][0m |          -0.0557 |           8.6270 |          12.3093 |
[32m[20221208 14:23:49 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 14:23:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.96
[32m[20221208 14:23:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.68
[32m[20221208 14:23:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 177.81
[32m[20221208 14:23:50 @agent_ppo2.py:137][0m Total time:       7.75 min
[32m[20221208 14:23:50 @agent_ppo2.py:139][0m 630784 total steps have happened
[32m[20221208 14:23:50 @agent_ppo2.py:115][0m #------------------------ Iteration 308 --------------------------#
[32m[20221208 14:23:50 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:23:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:50 @agent_ppo2.py:179][0m |           0.0576 |           8.0174 |          11.7670 |
[32m[20221208 14:23:50 @agent_ppo2.py:179][0m |           0.0319 |           7.6088 |          11.9224 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |           0.0151 |           7.4581 |          11.6594 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0129 |           7.3151 |          10.9833 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0443 |           7.2641 |           9.3061 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0535 |           7.1740 |           9.2082 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0580 |           7.0882 |           9.2225 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0617 |           7.0662 |           9.2343 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0642 |           7.0427 |           9.2423 |
[32m[20221208 14:23:51 @agent_ppo2.py:179][0m |          -0.0669 |           6.9261 |           9.2391 |
[32m[20221208 14:23:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:23:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.82
[32m[20221208 14:23:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.86
[32m[20221208 14:23:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.20
[32m[20221208 14:23:51 @agent_ppo2.py:137][0m Total time:       7.77 min
[32m[20221208 14:23:51 @agent_ppo2.py:139][0m 632832 total steps have happened
[32m[20221208 14:23:51 @agent_ppo2.py:115][0m #------------------------ Iteration 309 --------------------------#
[32m[20221208 14:23:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |           0.0240 |           7.7920 |          12.5945 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0078 |           6.9179 |          12.5917 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0231 |           6.4693 |          12.5351 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0379 |           6.1858 |          12.7051 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0426 |           5.9293 |          12.7031 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0441 |           5.7186 |          12.6575 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0452 |           5.5624 |          12.6881 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0523 |           5.3934 |          12.7063 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0557 |           5.2559 |          12.7079 |
[32m[20221208 14:23:52 @agent_ppo2.py:179][0m |          -0.0579 |           5.0736 |          12.6503 |
[32m[20221208 14:23:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.58
[32m[20221208 14:23:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.49
[32m[20221208 14:23:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.05
[32m[20221208 14:23:53 @agent_ppo2.py:137][0m Total time:       7.80 min
[32m[20221208 14:23:53 @agent_ppo2.py:139][0m 634880 total steps have happened
[32m[20221208 14:23:53 @agent_ppo2.py:115][0m #------------------------ Iteration 310 --------------------------#
[32m[20221208 14:23:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:53 @agent_ppo2.py:179][0m |           0.0403 |           6.4130 |          12.4062 |
[32m[20221208 14:23:53 @agent_ppo2.py:179][0m |           0.0207 |           5.3865 |          11.8900 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |           0.0030 |           4.9933 |          12.1883 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0240 |           4.7456 |          12.2967 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0377 |           4.5738 |          12.4353 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0433 |           4.4522 |          12.4753 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0472 |           4.3729 |          12.4735 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0545 |           4.2863 |          12.5512 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0558 |           4.2089 |          12.5425 |
[32m[20221208 14:23:54 @agent_ppo2.py:179][0m |          -0.0579 |           4.1507 |          12.5497 |
[32m[20221208 14:23:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.10
[32m[20221208 14:23:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.78
[32m[20221208 14:23:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.05
[32m[20221208 14:23:54 @agent_ppo2.py:137][0m Total time:       7.82 min
[32m[20221208 14:23:54 @agent_ppo2.py:139][0m 636928 total steps have happened
[32m[20221208 14:23:54 @agent_ppo2.py:115][0m #------------------------ Iteration 311 --------------------------#
[32m[20221208 14:23:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:23:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |           0.0630 |           7.2424 |          12.5266 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |           0.0166 |           6.2585 |          12.3867 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0143 |           5.8970 |          12.7732 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0369 |           5.6254 |          12.8158 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0400 |           5.5179 |          12.7632 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0421 |           5.3550 |          12.7210 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0519 |           5.2400 |          12.7393 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0617 |           5.1647 |          12.7362 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0673 |           5.0911 |          12.8591 |
[32m[20221208 14:23:55 @agent_ppo2.py:179][0m |          -0.0699 |           5.0481 |          12.8677 |
[32m[20221208 14:23:55 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:23:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.26
[32m[20221208 14:23:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.57
[32m[20221208 14:23:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.04
[32m[20221208 14:23:56 @agent_ppo2.py:137][0m Total time:       7.85 min
[32m[20221208 14:23:56 @agent_ppo2.py:139][0m 638976 total steps have happened
[32m[20221208 14:23:56 @agent_ppo2.py:115][0m #------------------------ Iteration 312 --------------------------#
[32m[20221208 14:23:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:56 @agent_ppo2.py:179][0m |           0.0386 |           7.6662 |          12.5471 |
[32m[20221208 14:23:56 @agent_ppo2.py:179][0m |           0.0006 |           7.3641 |          12.4931 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0085 |           7.2354 |          12.5955 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0267 |           7.1234 |          12.6287 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0329 |           7.0744 |          12.6284 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0380 |           7.0379 |          12.7003 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0425 |           6.9750 |          12.6970 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0462 |           6.9766 |          12.6923 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0468 |           6.9299 |          12.7231 |
[32m[20221208 14:23:57 @agent_ppo2.py:179][0m |          -0.0540 |           6.8804 |          12.7032 |
[32m[20221208 14:23:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.46
[32m[20221208 14:23:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.61
[32m[20221208 14:23:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.22
[32m[20221208 14:23:57 @agent_ppo2.py:137][0m Total time:       7.87 min
[32m[20221208 14:23:57 @agent_ppo2.py:139][0m 641024 total steps have happened
[32m[20221208 14:23:57 @agent_ppo2.py:115][0m #------------------------ Iteration 313 --------------------------#
[32m[20221208 14:23:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |           0.0304 |           4.6131 |          12.6237 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0016 |           3.9443 |          12.3972 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0250 |           3.8560 |          12.6841 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0302 |           3.7158 |          12.5447 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0319 |           3.6057 |          12.5250 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0309 |           3.5631 |          12.3981 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0375 |           3.4902 |          12.3967 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0383 |           3.4487 |          12.2413 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0439 |           3.3729 |          12.2267 |
[32m[20221208 14:23:58 @agent_ppo2.py:179][0m |          -0.0471 |           3.3377 |          12.3243 |
[32m[20221208 14:23:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:23:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.22
[32m[20221208 14:23:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.39
[32m[20221208 14:23:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.33
[32m[20221208 14:23:59 @agent_ppo2.py:137][0m Total time:       7.90 min
[32m[20221208 14:23:59 @agent_ppo2.py:139][0m 643072 total steps have happened
[32m[20221208 14:23:59 @agent_ppo2.py:115][0m #------------------------ Iteration 314 --------------------------#
[32m[20221208 14:23:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:23:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:23:59 @agent_ppo2.py:179][0m |           0.0362 |           2.1525 |          12.7224 |
[32m[20221208 14:23:59 @agent_ppo2.py:179][0m |           0.0015 |           1.6161 |          12.5746 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |          -0.0029 |           1.4907 |          12.6492 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |           0.0229 |           1.4103 |          12.2581 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |           0.0031 |           1.3459 |          12.4794 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |          -0.0179 |           1.3035 |          12.6474 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |          -0.0225 |           1.2730 |          12.6557 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |          -0.0183 |           1.2566 |          12.6562 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |          -0.0155 |           1.2341 |          12.6747 |
[32m[20221208 14:24:00 @agent_ppo2.py:179][0m |          -0.0261 |           1.2205 |          12.7497 |
[32m[20221208 14:24:00 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:24:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.95
[32m[20221208 14:24:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 28.89
[32m[20221208 14:24:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.38
[32m[20221208 14:24:00 @agent_ppo2.py:137][0m Total time:       7.92 min
[32m[20221208 14:24:00 @agent_ppo2.py:139][0m 645120 total steps have happened
[32m[20221208 14:24:00 @agent_ppo2.py:115][0m #------------------------ Iteration 315 --------------------------#
[32m[20221208 14:24:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |           0.0498 |           8.4462 |          12.7885 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |           0.0299 |           7.5560 |          12.3388 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0197 |           7.2793 |          12.7743 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0428 |           7.0853 |          12.9818 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0436 |           6.8984 |          12.9792 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0552 |           6.7467 |          13.0439 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0573 |           6.6318 |          12.9926 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0615 |           6.4866 |          12.9784 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0677 |           6.4374 |          12.9709 |
[32m[20221208 14:24:01 @agent_ppo2.py:179][0m |          -0.0693 |           6.3099 |          13.0520 |
[32m[20221208 14:24:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.94
[32m[20221208 14:24:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.06
[32m[20221208 14:24:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 177.73
[32m[20221208 14:24:02 @agent_ppo2.py:137][0m Total time:       7.95 min
[32m[20221208 14:24:02 @agent_ppo2.py:139][0m 647168 total steps have happened
[32m[20221208 14:24:02 @agent_ppo2.py:115][0m #------------------------ Iteration 316 --------------------------#
[32m[20221208 14:24:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:02 @agent_ppo2.py:179][0m |           0.0379 |           7.0769 |          12.9725 |
[32m[20221208 14:24:02 @agent_ppo2.py:179][0m |           0.0063 |           6.4991 |          12.8070 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0169 |           6.2739 |          12.9110 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0374 |           6.1836 |          13.0474 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0468 |           6.0936 |          13.0889 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0545 |           5.9663 |          13.1276 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0576 |           5.9481 |          13.1166 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0593 |           5.8875 |          13.0773 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0609 |           5.8297 |          13.0728 |
[32m[20221208 14:24:03 @agent_ppo2.py:179][0m |          -0.0652 |           5.8283 |          13.1182 |
[32m[20221208 14:24:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.84
[32m[20221208 14:24:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.96
[32m[20221208 14:24:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.32
[32m[20221208 14:24:03 @agent_ppo2.py:137][0m Total time:       7.97 min
[32m[20221208 14:24:03 @agent_ppo2.py:139][0m 649216 total steps have happened
[32m[20221208 14:24:03 @agent_ppo2.py:115][0m #------------------------ Iteration 317 --------------------------#
[32m[20221208 14:24:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |           0.0436 |           3.8745 |          12.8233 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |           0.0247 |           3.5188 |          12.4300 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0063 |           3.3385 |          12.6656 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0342 |           3.1862 |          13.0479 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0360 |           3.0876 |          13.0111 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0436 |           3.0152 |          12.9937 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0451 |           2.8751 |          12.9455 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0494 |           2.8127 |          12.9855 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0563 |           2.7314 |          13.0182 |
[32m[20221208 14:24:04 @agent_ppo2.py:179][0m |          -0.0511 |           2.6666 |          12.9181 |
[32m[20221208 14:24:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:24:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.16
[32m[20221208 14:24:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.24
[32m[20221208 14:24:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.18
[32m[20221208 14:24:05 @agent_ppo2.py:137][0m Total time:       8.00 min
[32m[20221208 14:24:05 @agent_ppo2.py:139][0m 651264 total steps have happened
[32m[20221208 14:24:05 @agent_ppo2.py:115][0m #------------------------ Iteration 318 --------------------------#
[32m[20221208 14:24:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:24:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:05 @agent_ppo2.py:179][0m |           0.0448 |           9.1488 |          12.9755 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0019 |           8.4499 |          13.0083 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0265 |           8.0852 |          13.1470 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0342 |           7.9061 |          13.1450 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0465 |           7.8307 |          13.2337 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0491 |           7.5973 |          13.2094 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0552 |           7.4848 |          13.1686 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0606 |           7.3936 |          13.1964 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0644 |           7.2898 |          13.2012 |
[32m[20221208 14:24:06 @agent_ppo2.py:179][0m |          -0.0669 |           7.1979 |          13.2062 |
[32m[20221208 14:24:06 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:24:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.77
[32m[20221208 14:24:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.61
[32m[20221208 14:24:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.23
[32m[20221208 14:24:06 @agent_ppo2.py:137][0m Total time:       8.03 min
[32m[20221208 14:24:06 @agent_ppo2.py:139][0m 653312 total steps have happened
[32m[20221208 14:24:06 @agent_ppo2.py:115][0m #------------------------ Iteration 319 --------------------------#
[32m[20221208 14:24:07 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:24:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |           0.0346 |           6.5491 |          13.0764 |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |           0.0018 |           5.9814 |          13.0662 |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |          -0.0184 |           5.8050 |          13.2575 |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |          -0.0307 |           5.6142 |          13.2906 |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |          -0.0385 |           5.5137 |          13.3490 |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |          -0.0470 |           5.3811 |          13.3384 |
[32m[20221208 14:24:07 @agent_ppo2.py:179][0m |          -0.0505 |           5.2906 |          13.3382 |
[32m[20221208 14:24:08 @agent_ppo2.py:179][0m |          -0.0550 |           5.2282 |          13.2647 |
[32m[20221208 14:24:08 @agent_ppo2.py:179][0m |          -0.0567 |           5.1265 |          13.3490 |
[32m[20221208 14:24:08 @agent_ppo2.py:179][0m |          -0.0603 |           5.0633 |          13.3169 |
[32m[20221208 14:24:08 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:24:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.43
[32m[20221208 14:24:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.29
[32m[20221208 14:24:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.80
[32m[20221208 14:24:08 @agent_ppo2.py:137][0m Total time:       8.05 min
[32m[20221208 14:24:08 @agent_ppo2.py:139][0m 655360 total steps have happened
[32m[20221208 14:24:08 @agent_ppo2.py:115][0m #------------------------ Iteration 320 --------------------------#
[32m[20221208 14:24:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |           0.0501 |           8.4132 |          12.8241 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |           0.0499 |           7.4446 |          12.1636 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0074 |           7.1207 |          12.4939 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0298 |           7.0161 |          12.7720 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0402 |           6.8577 |          12.8673 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0481 |           6.7267 |          12.8525 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0521 |           6.6118 |          12.8898 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0538 |           6.6105 |          12.8772 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0598 |           6.5374 |          12.8900 |
[32m[20221208 14:24:09 @agent_ppo2.py:179][0m |          -0.0635 |           6.4282 |          12.8777 |
[32m[20221208 14:24:09 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:24:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.18
[32m[20221208 14:24:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.19
[32m[20221208 14:24:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 176.34
[32m[20221208 14:24:10 @agent_ppo2.py:137][0m Total time:       8.08 min
[32m[20221208 14:24:10 @agent_ppo2.py:139][0m 657408 total steps have happened
[32m[20221208 14:24:10 @agent_ppo2.py:115][0m #------------------------ Iteration 321 --------------------------#
[32m[20221208 14:24:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:10 @agent_ppo2.py:179][0m |           0.0608 |           9.5605 |          12.8683 |
[32m[20221208 14:24:10 @agent_ppo2.py:179][0m |           0.0263 |           8.9196 |          12.4773 |
[32m[20221208 14:24:10 @agent_ppo2.py:179][0m |           0.0276 |           8.6638 |          12.0184 |
[32m[20221208 14:24:10 @agent_ppo2.py:179][0m |          -0.0156 |           8.5725 |          12.6188 |
[32m[20221208 14:24:10 @agent_ppo2.py:179][0m |          -0.0328 |           8.4071 |          12.8443 |
[32m[20221208 14:24:10 @agent_ppo2.py:179][0m |          -0.0433 |           8.3463 |          12.8582 |
[32m[20221208 14:24:11 @agent_ppo2.py:179][0m |          -0.0520 |           8.2676 |          12.9155 |
[32m[20221208 14:24:11 @agent_ppo2.py:179][0m |          -0.0548 |           8.1490 |          12.9115 |
[32m[20221208 14:24:11 @agent_ppo2.py:179][0m |          -0.0595 |           8.1253 |          12.8742 |
[32m[20221208 14:24:11 @agent_ppo2.py:179][0m |          -0.0572 |           7.9937 |          12.8008 |
[32m[20221208 14:24:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.33
[32m[20221208 14:24:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.02
[32m[20221208 14:24:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.13
[32m[20221208 14:24:11 @agent_ppo2.py:137][0m Total time:       8.10 min
[32m[20221208 14:24:11 @agent_ppo2.py:139][0m 659456 total steps have happened
[32m[20221208 14:24:11 @agent_ppo2.py:115][0m #------------------------ Iteration 322 --------------------------#
[32m[20221208 14:24:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |           0.1112 |           6.3942 |          12.4866 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |           0.0436 |           5.6835 |          12.0556 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0007 |           5.6263 |          12.1864 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0200 |           5.3475 |          12.4805 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0261 |           5.2865 |          12.5462 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0347 |           5.1513 |          12.5931 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0378 |           5.0953 |          12.5674 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0416 |           4.9721 |          12.6324 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0500 |           4.9317 |          12.6080 |
[32m[20221208 14:24:12 @agent_ppo2.py:179][0m |          -0.0489 |           4.8744 |          12.6111 |
[32m[20221208 14:24:12 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:24:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.85
[32m[20221208 14:24:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.46
[32m[20221208 14:24:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.69
[32m[20221208 14:24:13 @agent_ppo2.py:137][0m Total time:       8.13 min
[32m[20221208 14:24:13 @agent_ppo2.py:139][0m 661504 total steps have happened
[32m[20221208 14:24:13 @agent_ppo2.py:115][0m #------------------------ Iteration 323 --------------------------#
[32m[20221208 14:24:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:13 @agent_ppo2.py:179][0m |           0.0684 |           8.9693 |          12.5296 |
[32m[20221208 14:24:13 @agent_ppo2.py:179][0m |           0.0661 |           8.3641 |          11.6240 |
[32m[20221208 14:24:13 @agent_ppo2.py:179][0m |           0.0036 |           8.1085 |          12.5606 |
[32m[20221208 14:24:13 @agent_ppo2.py:179][0m |          -0.0269 |           7.8535 |          12.7824 |
[32m[20221208 14:24:13 @agent_ppo2.py:179][0m |          -0.0381 |           7.6697 |          12.9022 |
[32m[20221208 14:24:14 @agent_ppo2.py:179][0m |          -0.0420 |           7.5141 |          12.8627 |
[32m[20221208 14:24:14 @agent_ppo2.py:179][0m |          -0.0479 |           7.3602 |          12.8983 |
[32m[20221208 14:24:14 @agent_ppo2.py:179][0m |          -0.0500 |           7.2626 |          12.9081 |
[32m[20221208 14:24:14 @agent_ppo2.py:179][0m |          -0.0534 |           7.1435 |          12.9159 |
[32m[20221208 14:24:14 @agent_ppo2.py:179][0m |          -0.0584 |           7.0206 |          12.8919 |
[32m[20221208 14:24:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.17
[32m[20221208 14:24:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.05
[32m[20221208 14:24:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.46
[32m[20221208 14:24:14 @agent_ppo2.py:137][0m Total time:       8.15 min
[32m[20221208 14:24:14 @agent_ppo2.py:139][0m 663552 total steps have happened
[32m[20221208 14:24:14 @agent_ppo2.py:115][0m #------------------------ Iteration 324 --------------------------#
[32m[20221208 14:24:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |           0.0498 |           8.4660 |          12.4285 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |           0.0182 |           7.5414 |          12.1227 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0259 |           7.2459 |          12.3271 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0423 |           7.0786 |          12.5098 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0513 |           6.9162 |          12.4997 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0591 |           6.7793 |          12.5555 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0620 |           6.6888 |          12.5149 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0656 |           6.5888 |          12.5080 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0651 |           6.4970 |          12.5156 |
[32m[20221208 14:24:15 @agent_ppo2.py:179][0m |          -0.0713 |           6.4428 |          12.4869 |
[32m[20221208 14:24:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.57
[32m[20221208 14:24:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.40
[32m[20221208 14:24:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.89
[32m[20221208 14:24:16 @agent_ppo2.py:137][0m Total time:       8.18 min
[32m[20221208 14:24:16 @agent_ppo2.py:139][0m 665600 total steps have happened
[32m[20221208 14:24:16 @agent_ppo2.py:115][0m #------------------------ Iteration 325 --------------------------#
[32m[20221208 14:24:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:16 @agent_ppo2.py:179][0m |           0.0352 |           7.1077 |          12.8212 |
[32m[20221208 14:24:16 @agent_ppo2.py:179][0m |           0.0043 |           6.2739 |          12.8254 |
[32m[20221208 14:24:16 @agent_ppo2.py:179][0m |          -0.0256 |           5.9191 |          12.8908 |
[32m[20221208 14:24:16 @agent_ppo2.py:179][0m |          -0.0373 |           5.7210 |          13.0324 |
[32m[20221208 14:24:16 @agent_ppo2.py:179][0m |          -0.0494 |           5.5264 |          12.9925 |
[32m[20221208 14:24:16 @agent_ppo2.py:179][0m |          -0.0494 |           5.3914 |          13.0357 |
[32m[20221208 14:24:17 @agent_ppo2.py:179][0m |          -0.0524 |           5.3088 |          12.9516 |
[32m[20221208 14:24:17 @agent_ppo2.py:179][0m |          -0.0571 |           5.2299 |          12.9339 |
[32m[20221208 14:24:17 @agent_ppo2.py:179][0m |          -0.0581 |           5.1304 |          12.9098 |
[32m[20221208 14:24:17 @agent_ppo2.py:179][0m |          -0.0609 |           5.0791 |          12.9145 |
[32m[20221208 14:24:17 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:24:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.03
[32m[20221208 14:24:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.66
[32m[20221208 14:24:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 187.69
[32m[20221208 14:24:17 @agent_ppo2.py:137][0m Total time:       8.20 min
[32m[20221208 14:24:17 @agent_ppo2.py:139][0m 667648 total steps have happened
[32m[20221208 14:24:17 @agent_ppo2.py:115][0m #------------------------ Iteration 326 --------------------------#
[32m[20221208 14:24:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |           0.0590 |          10.1116 |          12.7979 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |           0.0222 |           9.6647 |          12.4270 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |           0.0004 |           9.3337 |          12.8552 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0225 |           9.1768 |          13.0180 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0381 |           8.9944 |          13.0605 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0464 |           8.9076 |          13.0760 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0500 |           8.8155 |          13.0490 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0564 |           8.7650 |          13.0934 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0587 |           8.6839 |          13.0528 |
[32m[20221208 14:24:18 @agent_ppo2.py:179][0m |          -0.0621 |           8.6041 |          12.9418 |
[32m[20221208 14:24:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.90
[32m[20221208 14:24:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.57
[32m[20221208 14:24:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.24
[32m[20221208 14:24:19 @agent_ppo2.py:137][0m Total time:       8.23 min
[32m[20221208 14:24:19 @agent_ppo2.py:139][0m 669696 total steps have happened
[32m[20221208 14:24:19 @agent_ppo2.py:115][0m #------------------------ Iteration 327 --------------------------#
[32m[20221208 14:24:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:19 @agent_ppo2.py:179][0m |           0.0423 |           9.7081 |          12.8269 |
[32m[20221208 14:24:19 @agent_ppo2.py:179][0m |           0.0171 |           9.0558 |          12.8452 |
[32m[20221208 14:24:19 @agent_ppo2.py:179][0m |          -0.0075 |           8.8527 |          12.9496 |
[32m[20221208 14:24:19 @agent_ppo2.py:179][0m |          -0.0261 |           8.6256 |          12.9636 |
[32m[20221208 14:24:19 @agent_ppo2.py:179][0m |          -0.0405 |           8.4630 |          13.1275 |
[32m[20221208 14:24:20 @agent_ppo2.py:179][0m |          -0.0445 |           8.4594 |          13.0503 |
[32m[20221208 14:24:20 @agent_ppo2.py:179][0m |          -0.0522 |           8.2766 |          13.0467 |
[32m[20221208 14:24:20 @agent_ppo2.py:179][0m |          -0.0575 |           8.2257 |          13.0501 |
[32m[20221208 14:24:20 @agent_ppo2.py:179][0m |          -0.0612 |           8.1560 |          13.0386 |
[32m[20221208 14:24:20 @agent_ppo2.py:179][0m |          -0.0613 |           8.0849 |          13.0260 |
[32m[20221208 14:24:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 161.28
[32m[20221208 14:24:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.51
[32m[20221208 14:24:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.25
[32m[20221208 14:24:20 @agent_ppo2.py:137][0m Total time:       8.25 min
[32m[20221208 14:24:20 @agent_ppo2.py:139][0m 671744 total steps have happened
[32m[20221208 14:24:20 @agent_ppo2.py:115][0m #------------------------ Iteration 328 --------------------------#
[32m[20221208 14:24:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |           0.0241 |           9.3790 |          12.5028 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |           0.0120 |           8.9133 |          12.3121 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0156 |           8.6083 |          12.3851 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0323 |           8.4468 |          12.4963 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0375 |           8.3360 |          12.3988 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0417 |           8.1716 |          12.4215 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0508 |           8.1111 |          12.2966 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0511 |           8.0248 |          12.2085 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0530 |           7.9635 |          12.2118 |
[32m[20221208 14:24:21 @agent_ppo2.py:179][0m |          -0.0554 |           7.8161 |          12.1057 |
[32m[20221208 14:24:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 175.65
[32m[20221208 14:24:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.17
[32m[20221208 14:24:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.17
[32m[20221208 14:24:22 @agent_ppo2.py:137][0m Total time:       8.28 min
[32m[20221208 14:24:22 @agent_ppo2.py:139][0m 673792 total steps have happened
[32m[20221208 14:24:22 @agent_ppo2.py:115][0m #------------------------ Iteration 329 --------------------------#
[32m[20221208 14:24:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:22 @agent_ppo2.py:179][0m |           0.0393 |           8.2143 |          12.4322 |
[32m[20221208 14:24:22 @agent_ppo2.py:179][0m |           0.0251 |           7.3005 |          11.8864 |
[32m[20221208 14:24:22 @agent_ppo2.py:179][0m |          -0.0053 |           6.9266 |          11.9261 |
[32m[20221208 14:24:22 @agent_ppo2.py:179][0m |          -0.0247 |           6.6845 |          12.2890 |
[32m[20221208 14:24:22 @agent_ppo2.py:179][0m |          -0.0356 |           6.5390 |          12.3872 |
[32m[20221208 14:24:23 @agent_ppo2.py:179][0m |          -0.0433 |           6.4532 |          12.4546 |
[32m[20221208 14:24:23 @agent_ppo2.py:179][0m |          -0.0426 |           6.3045 |          12.3166 |
[32m[20221208 14:24:23 @agent_ppo2.py:179][0m |          -0.0436 |           6.1913 |          12.4024 |
[32m[20221208 14:24:23 @agent_ppo2.py:179][0m |          -0.0466 |           6.0956 |          12.2644 |
[32m[20221208 14:24:23 @agent_ppo2.py:179][0m |          -0.0500 |           6.0612 |          12.3311 |
[32m[20221208 14:24:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.46
[32m[20221208 14:24:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.19
[32m[20221208 14:24:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.19
[32m[20221208 14:24:23 @agent_ppo2.py:137][0m Total time:       8.30 min
[32m[20221208 14:24:23 @agent_ppo2.py:139][0m 675840 total steps have happened
[32m[20221208 14:24:23 @agent_ppo2.py:115][0m #------------------------ Iteration 330 --------------------------#
[32m[20221208 14:24:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |           0.0682 |           9.2073 |          11.8385 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |           0.0524 |           8.6111 |          11.0355 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |           0.0255 |           8.2002 |          11.6302 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0039 |           7.9737 |          11.8183 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0166 |           7.6667 |          12.0507 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0339 |           7.4175 |          12.0365 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0412 |           7.2966 |          12.0283 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0451 |           7.0016 |          12.0527 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0525 |           6.7910 |          12.0788 |
[32m[20221208 14:24:24 @agent_ppo2.py:179][0m |          -0.0513 |           6.6606 |          11.9667 |
[32m[20221208 14:24:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 181.84
[32m[20221208 14:24:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.47
[32m[20221208 14:24:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 170.49
[32m[20221208 14:24:25 @agent_ppo2.py:137][0m Total time:       8.33 min
[32m[20221208 14:24:25 @agent_ppo2.py:139][0m 677888 total steps have happened
[32m[20221208 14:24:25 @agent_ppo2.py:115][0m #------------------------ Iteration 331 --------------------------#
[32m[20221208 14:24:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:25 @agent_ppo2.py:179][0m |           0.0505 |           6.8948 |          11.9934 |
[32m[20221208 14:24:25 @agent_ppo2.py:179][0m |           0.0367 |           6.5143 |          11.6173 |
[32m[20221208 14:24:25 @agent_ppo2.py:179][0m |          -0.0067 |           6.3607 |          11.9512 |
[32m[20221208 14:24:25 @agent_ppo2.py:179][0m |          -0.0239 |           6.2288 |          12.1296 |
[32m[20221208 14:24:25 @agent_ppo2.py:179][0m |          -0.0366 |           6.1582 |          12.2004 |
[32m[20221208 14:24:25 @agent_ppo2.py:179][0m |          -0.0333 |           6.1087 |          12.2400 |
[32m[20221208 14:24:26 @agent_ppo2.py:179][0m |          -0.0435 |           6.0885 |          12.2070 |
[32m[20221208 14:24:26 @agent_ppo2.py:179][0m |          -0.0509 |           5.9501 |          12.2480 |
[32m[20221208 14:24:26 @agent_ppo2.py:179][0m |          -0.0544 |           5.8932 |          12.2173 |
[32m[20221208 14:24:26 @agent_ppo2.py:179][0m |          -0.0509 |           5.8485 |          11.9155 |
[32m[20221208 14:24:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.16
[32m[20221208 14:24:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.51
[32m[20221208 14:24:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.40
[32m[20221208 14:24:26 @agent_ppo2.py:137][0m Total time:       8.35 min
[32m[20221208 14:24:26 @agent_ppo2.py:139][0m 679936 total steps have happened
[32m[20221208 14:24:26 @agent_ppo2.py:115][0m #------------------------ Iteration 332 --------------------------#
[32m[20221208 14:24:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |           0.0858 |          10.3287 |          11.7125 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |           0.0586 |           9.0042 |          11.0381 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |           0.0157 |           8.5790 |          11.0751 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0165 |           8.2609 |          11.5280 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0290 |           7.9837 |          11.7297 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0385 |           7.8008 |          11.7928 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0468 |           7.7195 |          11.7997 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0491 |           7.5588 |          11.8144 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0523 |           7.4653 |          11.7351 |
[32m[20221208 14:24:27 @agent_ppo2.py:179][0m |          -0.0562 |           7.3785 |          11.6684 |
[32m[20221208 14:24:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.49
[32m[20221208 14:24:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.80
[32m[20221208 14:24:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.48
[32m[20221208 14:24:28 @agent_ppo2.py:137][0m Total time:       8.38 min
[32m[20221208 14:24:28 @agent_ppo2.py:139][0m 681984 total steps have happened
[32m[20221208 14:24:28 @agent_ppo2.py:115][0m #------------------------ Iteration 333 --------------------------#
[32m[20221208 14:24:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:28 @agent_ppo2.py:179][0m |           0.0281 |           4.8653 |          11.7482 |
[32m[20221208 14:24:28 @agent_ppo2.py:179][0m |           0.0071 |           3.8508 |          11.5870 |
[32m[20221208 14:24:28 @agent_ppo2.py:179][0m |          -0.0210 |           3.5883 |          11.7600 |
[32m[20221208 14:24:28 @agent_ppo2.py:179][0m |          -0.0304 |           3.3851 |          11.6673 |
[32m[20221208 14:24:28 @agent_ppo2.py:179][0m |          -0.0426 |           3.2291 |          11.5873 |
[32m[20221208 14:24:28 @agent_ppo2.py:179][0m |          -0.0435 |           3.1314 |          11.7093 |
[32m[20221208 14:24:29 @agent_ppo2.py:179][0m |          -0.0459 |           3.0314 |          11.6864 |
[32m[20221208 14:24:29 @agent_ppo2.py:179][0m |          -0.0486 |           2.9476 |          11.5028 |
[32m[20221208 14:24:29 @agent_ppo2.py:179][0m |          -0.0501 |           2.8871 |          11.4583 |
[32m[20221208 14:24:29 @agent_ppo2.py:179][0m |          -0.0544 |           2.8479 |          11.3999 |
[32m[20221208 14:24:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:24:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.95
[32m[20221208 14:24:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.77
[32m[20221208 14:24:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.22
[32m[20221208 14:24:29 @agent_ppo2.py:137][0m Total time:       8.40 min
[32m[20221208 14:24:29 @agent_ppo2.py:139][0m 684032 total steps have happened
[32m[20221208 14:24:29 @agent_ppo2.py:115][0m #------------------------ Iteration 334 --------------------------#
[32m[20221208 14:24:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |           0.0336 |           6.7937 |          12.0067 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0006 |           6.3520 |          11.9341 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0297 |           6.0129 |          12.1519 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0427 |           5.8383 |          12.0676 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0458 |           5.6735 |          11.9235 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0468 |           5.5379 |          11.9160 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0553 |           5.4262 |          11.8410 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0605 |           5.2966 |          11.6887 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0592 |           5.2294 |          11.6330 |
[32m[20221208 14:24:30 @agent_ppo2.py:179][0m |          -0.0614 |           5.1724 |          11.5722 |
[32m[20221208 14:24:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.74
[32m[20221208 14:24:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.31
[32m[20221208 14:24:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.67
[32m[20221208 14:24:31 @agent_ppo2.py:137][0m Total time:       8.43 min
[32m[20221208 14:24:31 @agent_ppo2.py:139][0m 686080 total steps have happened
[32m[20221208 14:24:31 @agent_ppo2.py:115][0m #------------------------ Iteration 335 --------------------------#
[32m[20221208 14:24:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:31 @agent_ppo2.py:179][0m |           0.0594 |           9.7731 |          11.3026 |
[32m[20221208 14:24:31 @agent_ppo2.py:179][0m |           0.0508 |           9.0553 |          10.9729 |
[32m[20221208 14:24:31 @agent_ppo2.py:179][0m |           0.0190 |           8.7432 |          11.0352 |
[32m[20221208 14:24:31 @agent_ppo2.py:179][0m |          -0.0203 |           8.4835 |          11.3412 |
[32m[20221208 14:24:31 @agent_ppo2.py:179][0m |          -0.0324 |           8.1829 |          11.4810 |
[32m[20221208 14:24:31 @agent_ppo2.py:179][0m |          -0.0423 |           8.0185 |          11.4838 |
[32m[20221208 14:24:32 @agent_ppo2.py:179][0m |          -0.0463 |           7.7610 |          11.4694 |
[32m[20221208 14:24:32 @agent_ppo2.py:179][0m |          -0.0529 |           7.6094 |          11.4939 |
[32m[20221208 14:24:32 @agent_ppo2.py:179][0m |          -0.0534 |           7.4694 |          11.3723 |
[32m[20221208 14:24:32 @agent_ppo2.py:179][0m |          -0.0600 |           7.3835 |          11.1854 |
[32m[20221208 14:24:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.22
[32m[20221208 14:24:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.13
[32m[20221208 14:24:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.29
[32m[20221208 14:24:32 @agent_ppo2.py:137][0m Total time:       8.45 min
[32m[20221208 14:24:32 @agent_ppo2.py:139][0m 688128 total steps have happened
[32m[20221208 14:24:32 @agent_ppo2.py:115][0m #------------------------ Iteration 336 --------------------------#
[32m[20221208 14:24:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |           0.0527 |          10.6994 |          11.1524 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |           0.0868 |           9.9115 |          10.8521 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |           0.0139 |           9.5736 |          10.8182 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0139 |           9.4427 |          11.2876 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0362 |           9.2483 |          11.3605 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0440 |           9.1422 |          11.2347 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0472 |           9.0541 |          11.3340 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0513 |           9.0232 |          11.2941 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0577 |           8.8877 |          11.1922 |
[32m[20221208 14:24:33 @agent_ppo2.py:179][0m |          -0.0582 |           8.8100 |          11.2702 |
[32m[20221208 14:24:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 190.13
[32m[20221208 14:24:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 205.32
[32m[20221208 14:24:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.06
[32m[20221208 14:24:34 @agent_ppo2.py:137][0m Total time:       8.48 min
[32m[20221208 14:24:34 @agent_ppo2.py:139][0m 690176 total steps have happened
[32m[20221208 14:24:34 @agent_ppo2.py:115][0m #------------------------ Iteration 337 --------------------------#
[32m[20221208 14:24:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:34 @agent_ppo2.py:179][0m |           0.0541 |          11.3741 |          11.2930 |
[32m[20221208 14:24:34 @agent_ppo2.py:179][0m |           0.0213 |          10.6732 |          10.9702 |
[32m[20221208 14:24:34 @agent_ppo2.py:179][0m |          -0.0073 |          10.4633 |          10.9662 |
[32m[20221208 14:24:34 @agent_ppo2.py:179][0m |          -0.0206 |          10.3009 |          11.0153 |
[32m[20221208 14:24:34 @agent_ppo2.py:179][0m |          -0.0321 |          10.1260 |          11.0031 |
[32m[20221208 14:24:34 @agent_ppo2.py:179][0m |          -0.0426 |           9.9418 |          10.9055 |
[32m[20221208 14:24:35 @agent_ppo2.py:179][0m |          -0.0495 |           9.9073 |          10.8609 |
[32m[20221208 14:24:35 @agent_ppo2.py:179][0m |          -0.0520 |           9.8718 |          10.8294 |
[32m[20221208 14:24:35 @agent_ppo2.py:179][0m |          -0.0565 |           9.7477 |          10.7932 |
[32m[20221208 14:24:35 @agent_ppo2.py:179][0m |          -0.0594 |           9.7506 |          10.6882 |
[32m[20221208 14:24:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.83
[32m[20221208 14:24:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.69
[32m[20221208 14:24:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.21
[32m[20221208 14:24:35 @agent_ppo2.py:137][0m Total time:       8.50 min
[32m[20221208 14:24:35 @agent_ppo2.py:139][0m 692224 total steps have happened
[32m[20221208 14:24:35 @agent_ppo2.py:115][0m #------------------------ Iteration 338 --------------------------#
[32m[20221208 14:24:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |           0.0580 |           9.8144 |          11.1516 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |           0.0233 |           9.1686 |          10.8091 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0070 |           8.8634 |          10.9321 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0285 |           8.6074 |          10.9400 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0401 |           8.2966 |          10.9710 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0459 |           8.0877 |          10.7903 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0495 |           7.9291 |          10.7029 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0530 |           7.6876 |          10.6582 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0570 |           7.4810 |          10.5450 |
[32m[20221208 14:24:36 @agent_ppo2.py:179][0m |          -0.0586 |           7.3134 |          10.4875 |
[32m[20221208 14:24:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.44
[32m[20221208 14:24:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.68
[32m[20221208 14:24:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.85
[32m[20221208 14:24:37 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 190.85
[32m[20221208 14:24:37 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 190.85
[32m[20221208 14:24:37 @agent_ppo2.py:137][0m Total time:       8.53 min
[32m[20221208 14:24:37 @agent_ppo2.py:139][0m 694272 total steps have happened
[32m[20221208 14:24:37 @agent_ppo2.py:115][0m #------------------------ Iteration 339 --------------------------#
[32m[20221208 14:24:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:37 @agent_ppo2.py:179][0m |           0.0422 |           8.1082 |          10.8694 |
[32m[20221208 14:24:37 @agent_ppo2.py:179][0m |           0.0147 |           7.4724 |          11.2282 |
[32m[20221208 14:24:37 @agent_ppo2.py:179][0m |          -0.0092 |           7.2392 |          10.8367 |
[32m[20221208 14:24:37 @agent_ppo2.py:179][0m |          -0.0321 |           7.1287 |          10.8898 |
[32m[20221208 14:24:37 @agent_ppo2.py:179][0m |          -0.0361 |           7.0135 |          10.4651 |
[32m[20221208 14:24:37 @agent_ppo2.py:179][0m |          -0.0409 |           6.9151 |          10.4939 |
[32m[20221208 14:24:38 @agent_ppo2.py:179][0m |          -0.0426 |           6.8523 |          10.3915 |
[32m[20221208 14:24:38 @agent_ppo2.py:179][0m |          -0.0467 |           6.7577 |          10.3134 |
[32m[20221208 14:24:38 @agent_ppo2.py:179][0m |          -0.0540 |           6.7443 |          10.1725 |
[32m[20221208 14:24:38 @agent_ppo2.py:179][0m |          -0.0544 |           6.6866 |          10.1316 |
[32m[20221208 14:24:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.14
[32m[20221208 14:24:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.30
[32m[20221208 14:24:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.78
[32m[20221208 14:24:38 @agent_ppo2.py:137][0m Total time:       8.55 min
[32m[20221208 14:24:38 @agent_ppo2.py:139][0m 696320 total steps have happened
[32m[20221208 14:24:38 @agent_ppo2.py:115][0m #------------------------ Iteration 340 --------------------------#
[32m[20221208 14:24:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |           0.0582 |           8.3362 |          10.0200 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |           0.0509 |           7.5327 |          10.1945 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0029 |           7.1382 |          10.1877 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0208 |           6.8668 |          10.1751 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0369 |           6.6903 |          10.1915 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0396 |           6.5553 |           9.8992 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0418 |           6.4328 |           9.9523 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0475 |           6.3110 |           9.9341 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0551 |           6.1741 |           9.7029 |
[32m[20221208 14:24:39 @agent_ppo2.py:179][0m |          -0.0557 |           6.1316 |           9.7381 |
[32m[20221208 14:24:39 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.83
[32m[20221208 14:24:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.23
[32m[20221208 14:24:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.21
[32m[20221208 14:24:40 @agent_ppo2.py:137][0m Total time:       8.58 min
[32m[20221208 14:24:40 @agent_ppo2.py:139][0m 698368 total steps have happened
[32m[20221208 14:24:40 @agent_ppo2.py:115][0m #------------------------ Iteration 341 --------------------------#
[32m[20221208 14:24:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:40 @agent_ppo2.py:179][0m |           0.0594 |           9.8825 |           9.9337 |
[32m[20221208 14:24:40 @agent_ppo2.py:179][0m |           0.0494 |           9.3817 |           9.6048 |
[32m[20221208 14:24:40 @agent_ppo2.py:179][0m |           0.0044 |           9.1998 |           9.2032 |
[32m[20221208 14:24:40 @agent_ppo2.py:179][0m |          -0.0201 |           9.0348 |           9.3149 |
[32m[20221208 14:24:40 @agent_ppo2.py:179][0m |          -0.0287 |           8.9898 |           9.4082 |
[32m[20221208 14:24:40 @agent_ppo2.py:179][0m |          -0.0352 |           8.8706 |           9.2904 |
[32m[20221208 14:24:41 @agent_ppo2.py:179][0m |          -0.0344 |           8.7644 |           9.0911 |
[32m[20221208 14:24:41 @agent_ppo2.py:179][0m |          -0.0479 |           8.7278 |           8.8564 |
[32m[20221208 14:24:41 @agent_ppo2.py:179][0m |          -0.0503 |           8.6708 |           8.6640 |
[32m[20221208 14:24:41 @agent_ppo2.py:179][0m |          -0.0503 |           8.6292 |           8.7667 |
[32m[20221208 14:24:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.89
[32m[20221208 14:24:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.80
[32m[20221208 14:24:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.85
[32m[20221208 14:24:41 @agent_ppo2.py:137][0m Total time:       8.60 min
[32m[20221208 14:24:41 @agent_ppo2.py:139][0m 700416 total steps have happened
[32m[20221208 14:24:41 @agent_ppo2.py:115][0m #------------------------ Iteration 342 --------------------------#
[32m[20221208 14:24:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |           0.0346 |           7.5632 |          10.0466 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |           0.0027 |           7.1000 |          10.0185 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0217 |           6.8435 |           9.9974 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0346 |           6.7153 |           9.9053 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0374 |           6.6087 |           9.8491 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0407 |           6.5609 |           9.8296 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0479 |           6.4462 |           9.7022 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0484 |           6.3390 |           9.4932 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0484 |           6.2886 |           9.4532 |
[32m[20221208 14:24:42 @agent_ppo2.py:179][0m |          -0.0503 |           6.1969 |           9.1607 |
[32m[20221208 14:24:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.75
[32m[20221208 14:24:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.86
[32m[20221208 14:24:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.11
[32m[20221208 14:24:43 @agent_ppo2.py:137][0m Total time:       8.63 min
[32m[20221208 14:24:43 @agent_ppo2.py:139][0m 702464 total steps have happened
[32m[20221208 14:24:43 @agent_ppo2.py:115][0m #------------------------ Iteration 343 --------------------------#
[32m[20221208 14:24:43 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:24:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:43 @agent_ppo2.py:179][0m |           0.0385 |           9.8585 |           9.6245 |
[32m[20221208 14:24:43 @agent_ppo2.py:179][0m |           0.0224 |           9.2036 |           9.5504 |
[32m[20221208 14:24:43 @agent_ppo2.py:179][0m |          -0.0111 |           8.7695 |           9.7314 |
[32m[20221208 14:24:43 @agent_ppo2.py:179][0m |          -0.0277 |           8.4140 |           9.8363 |
[32m[20221208 14:24:43 @agent_ppo2.py:179][0m |          -0.0403 |           8.2082 |           9.7859 |
[32m[20221208 14:24:44 @agent_ppo2.py:179][0m |          -0.0435 |           7.9887 |           9.9497 |
[32m[20221208 14:24:44 @agent_ppo2.py:179][0m |          -0.0487 |           7.8873 |           9.7645 |
[32m[20221208 14:24:44 @agent_ppo2.py:179][0m |          -0.0508 |           7.6686 |           9.7444 |
[32m[20221208 14:24:44 @agent_ppo2.py:179][0m |          -0.0543 |           7.5527 |           9.4926 |
[32m[20221208 14:24:44 @agent_ppo2.py:179][0m |          -0.0550 |           7.4464 |           9.5190 |
[32m[20221208 14:24:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 201.93
[32m[20221208 14:24:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 215.66
[32m[20221208 14:24:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.24
[32m[20221208 14:24:44 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 200.24
[32m[20221208 14:24:44 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 200.24
[32m[20221208 14:24:44 @agent_ppo2.py:137][0m Total time:       8.65 min
[32m[20221208 14:24:44 @agent_ppo2.py:139][0m 704512 total steps have happened
[32m[20221208 14:24:44 @agent_ppo2.py:115][0m #------------------------ Iteration 344 --------------------------#
[32m[20221208 14:24:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |           0.0608 |           5.0249 |          10.0283 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |           0.0151 |           4.3300 |          10.1018 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0018 |           4.1533 |          10.1844 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0154 |           4.0984 |          10.1202 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0326 |           4.0016 |          10.0459 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0387 |           3.9560 |           9.7639 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0402 |           3.9136 |          10.0040 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0480 |           3.8658 |           9.5953 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0497 |           3.8396 |           9.6293 |
[32m[20221208 14:24:45 @agent_ppo2.py:179][0m |          -0.0470 |           3.8129 |           9.1095 |
[32m[20221208 14:24:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.51
[32m[20221208 14:24:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.80
[32m[20221208 14:24:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.61
[32m[20221208 14:24:46 @agent_ppo2.py:137][0m Total time:       8.68 min
[32m[20221208 14:24:46 @agent_ppo2.py:139][0m 706560 total steps have happened
[32m[20221208 14:24:46 @agent_ppo2.py:115][0m #------------------------ Iteration 345 --------------------------#
[32m[20221208 14:24:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:46 @agent_ppo2.py:179][0m |           0.0444 |           4.7006 |           9.4219 |
[32m[20221208 14:24:46 @agent_ppo2.py:179][0m |           0.0241 |           4.2405 |           9.3046 |
[32m[20221208 14:24:46 @agent_ppo2.py:179][0m |          -0.0077 |           4.0813 |           9.4300 |
[32m[20221208 14:24:46 @agent_ppo2.py:179][0m |          -0.0247 |           3.9570 |           9.4106 |
[32m[20221208 14:24:46 @agent_ppo2.py:179][0m |          -0.0332 |           3.8490 |           9.1735 |
[32m[20221208 14:24:46 @agent_ppo2.py:179][0m |          -0.0407 |           3.7250 |           9.1877 |
[32m[20221208 14:24:47 @agent_ppo2.py:179][0m |          -0.0406 |           3.6709 |           9.0901 |
[32m[20221208 14:24:47 @agent_ppo2.py:179][0m |          -0.0428 |           3.6224 |           9.0335 |
[32m[20221208 14:24:47 @agent_ppo2.py:179][0m |          -0.0412 |           3.5405 |           9.2811 |
[32m[20221208 14:24:47 @agent_ppo2.py:179][0m |          -0.0480 |           3.4743 |           8.7292 |
[32m[20221208 14:24:47 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:24:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.03
[32m[20221208 14:24:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.98
[32m[20221208 14:24:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.72
[32m[20221208 14:24:47 @agent_ppo2.py:137][0m Total time:       8.70 min
[32m[20221208 14:24:47 @agent_ppo2.py:139][0m 708608 total steps have happened
[32m[20221208 14:24:47 @agent_ppo2.py:115][0m #------------------------ Iteration 346 --------------------------#
[32m[20221208 14:24:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |           0.0513 |           4.3013 |           9.1951 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |           0.0164 |           3.8587 |           8.9535 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0118 |           3.7318 |           8.8848 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0212 |           3.6064 |           8.7739 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0223 |           3.4651 |           8.5602 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0336 |           3.3626 |           8.0956 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0349 |           3.3084 |           8.0650 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0380 |           3.1934 |           7.8123 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0428 |           3.1009 |           7.7204 |
[32m[20221208 14:24:48 @agent_ppo2.py:179][0m |          -0.0406 |           3.0768 |           7.4158 |
[32m[20221208 14:24:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.03
[32m[20221208 14:24:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.39
[32m[20221208 14:24:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.58
[32m[20221208 14:24:49 @agent_ppo2.py:137][0m Total time:       8.73 min
[32m[20221208 14:24:49 @agent_ppo2.py:139][0m 710656 total steps have happened
[32m[20221208 14:24:49 @agent_ppo2.py:115][0m #------------------------ Iteration 347 --------------------------#
[32m[20221208 14:24:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:49 @agent_ppo2.py:179][0m |           0.0838 |           4.9206 |           7.7292 |
[32m[20221208 14:24:49 @agent_ppo2.py:179][0m |           0.0494 |           4.2707 |           7.9982 |
[32m[20221208 14:24:49 @agent_ppo2.py:179][0m |           0.0183 |           4.0621 |           7.9197 |
[32m[20221208 14:24:49 @agent_ppo2.py:179][0m |          -0.0116 |           3.9221 |           7.1562 |
[32m[20221208 14:24:49 @agent_ppo2.py:179][0m |          -0.0281 |           3.8539 |           7.3255 |
[32m[20221208 14:24:49 @agent_ppo2.py:179][0m |          -0.0333 |           3.7437 |           7.0076 |
[32m[20221208 14:24:50 @agent_ppo2.py:179][0m |          -0.0366 |           3.6824 |           7.1055 |
[32m[20221208 14:24:50 @agent_ppo2.py:179][0m |          -0.0430 |           3.5808 |           6.8433 |
[32m[20221208 14:24:50 @agent_ppo2.py:179][0m |          -0.0483 |           3.5396 |           6.7083 |
[32m[20221208 14:24:50 @agent_ppo2.py:179][0m |          -0.0493 |           3.4622 |           6.5527 |
[32m[20221208 14:24:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.82
[32m[20221208 14:24:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.37
[32m[20221208 14:24:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.29
[32m[20221208 14:24:50 @agent_ppo2.py:137][0m Total time:       8.75 min
[32m[20221208 14:24:50 @agent_ppo2.py:139][0m 712704 total steps have happened
[32m[20221208 14:24:50 @agent_ppo2.py:115][0m #------------------------ Iteration 348 --------------------------#
[32m[20221208 14:24:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |           0.0862 |           8.5034 |           6.1125 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |           0.0523 |           7.6215 |           6.6942 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0028 |           7.2812 |           6.2613 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0202 |           7.0473 |           6.1024 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0266 |           6.9646 |           6.3312 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0327 |           6.8102 |           6.0636 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0377 |           6.7512 |           5.7348 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0437 |           6.6801 |           5.6864 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0473 |           6.6153 |           5.5507 |
[32m[20221208 14:24:51 @agent_ppo2.py:179][0m |          -0.0527 |           6.5680 |           5.5366 |
[32m[20221208 14:24:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.24
[32m[20221208 14:24:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.74
[32m[20221208 14:24:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 186.65
[32m[20221208 14:24:52 @agent_ppo2.py:137][0m Total time:       8.78 min
[32m[20221208 14:24:52 @agent_ppo2.py:139][0m 714752 total steps have happened
[32m[20221208 14:24:52 @agent_ppo2.py:115][0m #------------------------ Iteration 349 --------------------------#
[32m[20221208 14:24:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:52 @agent_ppo2.py:179][0m |           0.0339 |           8.9996 |           6.2980 |
[32m[20221208 14:24:52 @agent_ppo2.py:179][0m |           0.0001 |           8.1285 |           6.3888 |
[32m[20221208 14:24:52 @agent_ppo2.py:179][0m |          -0.0205 |           7.8994 |           6.2279 |
[32m[20221208 14:24:52 @agent_ppo2.py:179][0m |          -0.0345 |           7.6467 |           6.1173 |
[32m[20221208 14:24:52 @agent_ppo2.py:179][0m |          -0.0411 |           7.5179 |           5.9235 |
[32m[20221208 14:24:52 @agent_ppo2.py:179][0m |          -0.0481 |           7.4260 |           5.5061 |
[32m[20221208 14:24:53 @agent_ppo2.py:179][0m |          -0.0563 |           7.3380 |           5.2618 |
[32m[20221208 14:24:53 @agent_ppo2.py:179][0m |          -0.0562 |           7.2693 |           5.0258 |
[32m[20221208 14:24:53 @agent_ppo2.py:179][0m |          -0.0604 |           7.1995 |           4.8160 |
[32m[20221208 14:24:53 @agent_ppo2.py:179][0m |          -0.0608 |           7.1390 |           4.4987 |
[32m[20221208 14:24:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.50
[32m[20221208 14:24:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.79
[32m[20221208 14:24:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.19
[32m[20221208 14:24:53 @agent_ppo2.py:137][0m Total time:       8.80 min
[32m[20221208 14:24:53 @agent_ppo2.py:139][0m 716800 total steps have happened
[32m[20221208 14:24:53 @agent_ppo2.py:115][0m #------------------------ Iteration 350 --------------------------#
[32m[20221208 14:24:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:24:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |           0.0657 |           3.3042 |           6.2477 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |           0.0417 |           2.2451 |           7.2680 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |           0.0108 |           1.8128 |           7.0702 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0140 |           1.5604 |           6.8894 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0240 |           1.4050 |           6.8265 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0315 |           1.3038 |           6.5944 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0393 |           1.2350 |           6.4878 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0413 |           1.1907 |           6.2555 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0444 |           1.1516 |           5.9021 |
[32m[20221208 14:24:54 @agent_ppo2.py:179][0m |          -0.0479 |           1.1352 |           5.6188 |
[32m[20221208 14:24:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.11
[32m[20221208 14:24:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.39
[32m[20221208 14:24:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.76
[32m[20221208 14:24:55 @agent_ppo2.py:137][0m Total time:       8.83 min
[32m[20221208 14:24:55 @agent_ppo2.py:139][0m 718848 total steps have happened
[32m[20221208 14:24:55 @agent_ppo2.py:115][0m #------------------------ Iteration 351 --------------------------#
[32m[20221208 14:24:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:55 @agent_ppo2.py:179][0m |           0.0588 |          12.1827 |           3.8716 |
[32m[20221208 14:24:55 @agent_ppo2.py:179][0m |           0.0313 |          11.2087 |           4.5726 |
[32m[20221208 14:24:55 @agent_ppo2.py:179][0m |           0.0053 |          10.7903 |           4.4101 |
[32m[20221208 14:24:55 @agent_ppo2.py:179][0m |          -0.0254 |          10.5120 |           3.6921 |
[32m[20221208 14:24:55 @agent_ppo2.py:179][0m |          -0.0383 |          10.3260 |           3.2704 |
[32m[20221208 14:24:55 @agent_ppo2.py:179][0m |          -0.0483 |          10.1390 |           2.8529 |
[32m[20221208 14:24:56 @agent_ppo2.py:179][0m |          -0.0515 |          10.0311 |           2.4211 |
[32m[20221208 14:24:56 @agent_ppo2.py:179][0m |          -0.0558 |           9.8956 |           2.5136 |
[32m[20221208 14:24:56 @agent_ppo2.py:179][0m |          -0.0605 |           9.8233 |           2.1562 |
[32m[20221208 14:24:56 @agent_ppo2.py:179][0m |          -0.0618 |           9.6881 |           2.2172 |
[32m[20221208 14:24:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:24:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.98
[32m[20221208 14:24:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.08
[32m[20221208 14:24:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.81
[32m[20221208 14:24:56 @agent_ppo2.py:137][0m Total time:       8.85 min
[32m[20221208 14:24:56 @agent_ppo2.py:139][0m 720896 total steps have happened
[32m[20221208 14:24:56 @agent_ppo2.py:115][0m #------------------------ Iteration 352 --------------------------#
[32m[20221208 14:24:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |           0.0653 |           8.3049 |           5.1270 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |           0.0449 |           7.8274 |           4.9903 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |           0.0022 |           7.6170 |           4.8604 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0204 |           7.4202 |           4.7569 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0323 |           7.3252 |           4.4590 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0399 |           7.2179 |           4.1204 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0461 |           7.1380 |           3.9338 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0500 |           7.0949 |           4.0141 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0505 |           6.9827 |           3.5877 |
[32m[20221208 14:24:57 @agent_ppo2.py:179][0m |          -0.0471 |           6.8811 |           3.5856 |
[32m[20221208 14:24:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:24:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.00
[32m[20221208 14:24:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.22
[32m[20221208 14:24:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.16
[32m[20221208 14:24:58 @agent_ppo2.py:137][0m Total time:       8.88 min
[32m[20221208 14:24:58 @agent_ppo2.py:139][0m 722944 total steps have happened
[32m[20221208 14:24:58 @agent_ppo2.py:115][0m #------------------------ Iteration 353 --------------------------#
[32m[20221208 14:24:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:24:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:24:58 @agent_ppo2.py:179][0m |           0.0482 |           8.9450 |           5.0791 |
[32m[20221208 14:24:58 @agent_ppo2.py:179][0m |           0.0333 |           7.9218 |           4.5405 |
[32m[20221208 14:24:58 @agent_ppo2.py:179][0m |          -0.0054 |           7.5145 |           4.8608 |
[32m[20221208 14:24:58 @agent_ppo2.py:179][0m |          -0.0218 |           7.1913 |           4.6811 |
[32m[20221208 14:24:58 @agent_ppo2.py:179][0m |          -0.0163 |           7.0011 |           5.0345 |
[32m[20221208 14:24:58 @agent_ppo2.py:179][0m |          -0.0244 |           6.8705 |           4.4380 |
[32m[20221208 14:24:59 @agent_ppo2.py:179][0m |          -0.0408 |           6.6881 |           4.1766 |
[32m[20221208 14:24:59 @agent_ppo2.py:179][0m |          -0.0476 |           6.5813 |           4.2842 |
[32m[20221208 14:24:59 @agent_ppo2.py:179][0m |          -0.0503 |           6.4436 |           4.0529 |
[32m[20221208 14:24:59 @agent_ppo2.py:179][0m |          -0.0552 |           6.3736 |           3.7305 |
[32m[20221208 14:24:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:24:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.98
[32m[20221208 14:24:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 206.14
[32m[20221208 14:24:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.12
[32m[20221208 14:24:59 @agent_ppo2.py:137][0m Total time:       8.90 min
[32m[20221208 14:24:59 @agent_ppo2.py:139][0m 724992 total steps have happened
[32m[20221208 14:24:59 @agent_ppo2.py:115][0m #------------------------ Iteration 354 --------------------------#
[32m[20221208 14:25:00 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:25:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |           0.1529 |           7.6997 |           4.1688 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |           0.0586 |           7.0232 |           5.3351 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |           0.0107 |           6.6494 |           4.4677 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0119 |           6.3869 |           4.0311 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0280 |           6.1609 |           3.5880 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0325 |           5.9680 |           3.7516 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0385 |           5.7909 |           3.5024 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0450 |           5.6093 |           3.3880 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0495 |           5.4849 |           3.1051 |
[32m[20221208 14:25:00 @agent_ppo2.py:179][0m |          -0.0542 |           5.3638 |           3.1041 |
[32m[20221208 14:25:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.10
[32m[20221208 14:25:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.18
[32m[20221208 14:25:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 204.95
[32m[20221208 14:25:01 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 204.95
[32m[20221208 14:25:01 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 204.95
[32m[20221208 14:25:01 @agent_ppo2.py:137][0m Total time:       8.93 min
[32m[20221208 14:25:01 @agent_ppo2.py:139][0m 727040 total steps have happened
[32m[20221208 14:25:01 @agent_ppo2.py:115][0m #------------------------ Iteration 355 --------------------------#
[32m[20221208 14:25:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:01 @agent_ppo2.py:179][0m |           0.0661 |          10.9981 |           4.2480 |
[32m[20221208 14:25:01 @agent_ppo2.py:179][0m |           0.0550 |          10.4351 |           5.4413 |
[32m[20221208 14:25:01 @agent_ppo2.py:179][0m |           0.0125 |          10.3238 |           3.9629 |
[32m[20221208 14:25:01 @agent_ppo2.py:179][0m |          -0.0155 |          10.0259 |           3.3638 |
[32m[20221208 14:25:01 @agent_ppo2.py:179][0m |          -0.0277 |           9.8873 |           3.2063 |
[32m[20221208 14:25:02 @agent_ppo2.py:179][0m |          -0.0359 |           9.7910 |           2.8386 |
[32m[20221208 14:25:02 @agent_ppo2.py:179][0m |          -0.0432 |           9.7108 |           2.4331 |
[32m[20221208 14:25:02 @agent_ppo2.py:179][0m |          -0.0382 |           9.6474 |           2.4912 |
[32m[20221208 14:25:02 @agent_ppo2.py:179][0m |          -0.0429 |           9.5630 |           2.2561 |
[32m[20221208 14:25:02 @agent_ppo2.py:179][0m |          -0.0478 |           9.4359 |           1.8617 |
[32m[20221208 14:25:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:25:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.87
[32m[20221208 14:25:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 200.76
[32m[20221208 14:25:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.86
[32m[20221208 14:25:02 @agent_ppo2.py:137][0m Total time:       8.95 min
[32m[20221208 14:25:02 @agent_ppo2.py:139][0m 729088 total steps have happened
[32m[20221208 14:25:02 @agent_ppo2.py:115][0m #------------------------ Iteration 356 --------------------------#
[32m[20221208 14:25:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |           0.0445 |          11.4102 |           3.6494 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |           0.0100 |          10.7123 |           4.0696 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0141 |          10.3380 |           3.9434 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0216 |          10.1285 |           3.5499 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0395 |           9.9179 |           3.2395 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0411 |           9.7282 |           2.9253 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0489 |           9.5452 |           2.9946 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0494 |           9.3988 |           2.8664 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0502 |           9.3226 |           2.7006 |
[32m[20221208 14:25:03 @agent_ppo2.py:179][0m |          -0.0537 |           9.2583 |           2.3550 |
[32m[20221208 14:25:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.55
[32m[20221208 14:25:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.57
[32m[20221208 14:25:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 195.53
[32m[20221208 14:25:04 @agent_ppo2.py:137][0m Total time:       8.98 min
[32m[20221208 14:25:04 @agent_ppo2.py:139][0m 731136 total steps have happened
[32m[20221208 14:25:04 @agent_ppo2.py:115][0m #------------------------ Iteration 357 --------------------------#
[32m[20221208 14:25:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:04 @agent_ppo2.py:179][0m |           0.0359 |           7.1795 |           1.9231 |
[32m[20221208 14:25:04 @agent_ppo2.py:179][0m |           0.0510 |           6.5987 |           2.5780 |
[32m[20221208 14:25:04 @agent_ppo2.py:179][0m |          -0.0079 |           6.4345 |           2.0112 |
[32m[20221208 14:25:04 @agent_ppo2.py:179][0m |          -0.0265 |           6.2641 |           1.0403 |
[32m[20221208 14:25:04 @agent_ppo2.py:179][0m |          -0.0361 |           6.1267 |           0.9645 |
[32m[20221208 14:25:05 @agent_ppo2.py:179][0m |          -0.0445 |           6.0418 |           0.3428 |
[32m[20221208 14:25:05 @agent_ppo2.py:179][0m |          -0.0490 |           5.9392 |          -0.0737 |
[32m[20221208 14:25:05 @agent_ppo2.py:179][0m |          -0.0485 |           5.8645 |          -0.0761 |
[32m[20221208 14:25:05 @agent_ppo2.py:179][0m |          -0.0517 |           5.8073 |          -0.3056 |
[32m[20221208 14:25:05 @agent_ppo2.py:179][0m |          -0.0539 |           5.7356 |          -0.4150 |
[32m[20221208 14:25:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.03
[32m[20221208 14:25:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 200.26
[32m[20221208 14:25:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.11
[32m[20221208 14:25:05 @agent_ppo2.py:137][0m Total time:       9.00 min
[32m[20221208 14:25:05 @agent_ppo2.py:139][0m 733184 total steps have happened
[32m[20221208 14:25:05 @agent_ppo2.py:115][0m #------------------------ Iteration 358 --------------------------#
[32m[20221208 14:25:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |           0.1060 |          10.5939 |           1.5946 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |           0.0962 |          10.1899 |           4.1108 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |           0.0582 |           9.9496 |           3.6127 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |           0.0194 |           9.8770 |           3.0126 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |           0.0020 |           9.7134 |           2.5212 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |          -0.0108 |           9.7083 |           1.9431 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |          -0.0190 |           9.5444 |           1.2933 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |          -0.0284 |           9.4680 |           0.7768 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |          -0.0366 |           9.3769 |           0.4329 |
[32m[20221208 14:25:06 @agent_ppo2.py:179][0m |          -0.0406 |           9.3324 |           0.1352 |
[32m[20221208 14:25:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:25:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 208.89
[32m[20221208 14:25:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 211.34
[32m[20221208 14:25:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.14
[32m[20221208 14:25:07 @agent_ppo2.py:137][0m Total time:       9.03 min
[32m[20221208 14:25:07 @agent_ppo2.py:139][0m 735232 total steps have happened
[32m[20221208 14:25:07 @agent_ppo2.py:115][0m #------------------------ Iteration 359 --------------------------#
[32m[20221208 14:25:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:07 @agent_ppo2.py:179][0m |           0.0599 |           5.8379 |           0.7611 |
[32m[20221208 14:25:07 @agent_ppo2.py:179][0m |           0.0289 |           5.2636 |           1.4503 |
[32m[20221208 14:25:07 @agent_ppo2.py:179][0m |          -0.0055 |           5.0621 |           0.7569 |
[32m[20221208 14:25:07 @agent_ppo2.py:179][0m |          -0.0267 |           4.9094 |           0.4839 |
[32m[20221208 14:25:07 @agent_ppo2.py:179][0m |          -0.0355 |           4.8606 |           0.3759 |
[32m[20221208 14:25:08 @agent_ppo2.py:179][0m |          -0.0431 |           4.7545 |           0.2936 |
[32m[20221208 14:25:08 @agent_ppo2.py:179][0m |          -0.0488 |           4.7075 |          -0.2197 |
[32m[20221208 14:25:08 @agent_ppo2.py:179][0m |          -0.0509 |           4.7966 |          -0.3424 |
[32m[20221208 14:25:08 @agent_ppo2.py:179][0m |          -0.0528 |           4.6137 |          -0.9609 |
[32m[20221208 14:25:08 @agent_ppo2.py:179][0m |          -0.0605 |           4.5314 |          -1.1097 |
[32m[20221208 14:25:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.70
[32m[20221208 14:25:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.97
[32m[20221208 14:25:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 197.18
[32m[20221208 14:25:08 @agent_ppo2.py:137][0m Total time:       9.05 min
[32m[20221208 14:25:08 @agent_ppo2.py:139][0m 737280 total steps have happened
[32m[20221208 14:25:08 @agent_ppo2.py:115][0m #------------------------ Iteration 360 --------------------------#
[32m[20221208 14:25:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |           0.0387 |           7.0511 |           0.9049 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |           0.0081 |           6.0369 |           1.2395 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0119 |           5.5661 |           0.9956 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0299 |           5.2479 |           0.4467 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0383 |           5.0359 |           0.0012 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0437 |           4.8395 |          -0.2938 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0430 |           4.6807 |          -0.4533 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0486 |           4.5799 |          -0.7969 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0494 |           4.5066 |          -1.0681 |
[32m[20221208 14:25:09 @agent_ppo2.py:179][0m |          -0.0546 |           4.4220 |          -1.6132 |
[32m[20221208 14:25:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.14
[32m[20221208 14:25:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 213.74
[32m[20221208 14:25:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.48
[32m[20221208 14:25:10 @agent_ppo2.py:137][0m Total time:       9.08 min
[32m[20221208 14:25:10 @agent_ppo2.py:139][0m 739328 total steps have happened
[32m[20221208 14:25:10 @agent_ppo2.py:115][0m #------------------------ Iteration 361 --------------------------#
[32m[20221208 14:25:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:10 @agent_ppo2.py:179][0m |           0.0540 |           1.3576 |           0.9034 |
[32m[20221208 14:25:10 @agent_ppo2.py:179][0m |           0.0015 |           0.8899 |           1.0139 |
[32m[20221208 14:25:10 @agent_ppo2.py:179][0m |          -0.0066 |           0.8429 |           1.0471 |
[32m[20221208 14:25:10 @agent_ppo2.py:179][0m |          -0.0082 |           0.8139 |           1.4409 |
[32m[20221208 14:25:10 @agent_ppo2.py:179][0m |          -0.0020 |           0.7948 |           1.1569 |
[32m[20221208 14:25:11 @agent_ppo2.py:179][0m |          -0.0127 |           0.7857 |           1.0724 |
[32m[20221208 14:25:11 @agent_ppo2.py:179][0m |           0.0004 |           0.7771 |           1.0056 |
[32m[20221208 14:25:11 @agent_ppo2.py:179][0m |          -0.0094 |           0.7697 |           1.0171 |
[32m[20221208 14:25:11 @agent_ppo2.py:179][0m |          -0.0155 |           0.7696 |           0.8143 |
[32m[20221208 14:25:11 @agent_ppo2.py:179][0m |          -0.0119 |           0.7637 |           1.1048 |
[32m[20221208 14:25:11 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:25:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.42
[32m[20221208 14:25:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.62
[32m[20221208 14:25:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 183.02
[32m[20221208 14:25:11 @agent_ppo2.py:137][0m Total time:       9.10 min
[32m[20221208 14:25:11 @agent_ppo2.py:139][0m 741376 total steps have happened
[32m[20221208 14:25:11 @agent_ppo2.py:115][0m #------------------------ Iteration 362 --------------------------#
[32m[20221208 14:25:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |           0.0604 |           8.1692 |           1.1473 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |           0.0452 |           7.3726 |           3.0719 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |           0.0038 |           7.1098 |           2.0493 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0222 |           6.9640 |           1.7131 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0257 |           6.9761 |           1.2150 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0421 |           6.8137 |           1.0544 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0465 |           6.7409 |           0.6419 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0452 |           6.7153 |           0.5787 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0520 |           6.6529 |           0.4072 |
[32m[20221208 14:25:12 @agent_ppo2.py:179][0m |          -0.0525 |           6.5597 |           0.0951 |
[32m[20221208 14:25:12 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.48
[32m[20221208 14:25:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.63
[32m[20221208 14:25:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.88
[32m[20221208 14:25:13 @agent_ppo2.py:137][0m Total time:       9.13 min
[32m[20221208 14:25:13 @agent_ppo2.py:139][0m 743424 total steps have happened
[32m[20221208 14:25:13 @agent_ppo2.py:115][0m #------------------------ Iteration 363 --------------------------#
[32m[20221208 14:25:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:13 @agent_ppo2.py:179][0m |           0.0973 |           8.2310 |           2.6249 |
[32m[20221208 14:25:13 @agent_ppo2.py:179][0m |           0.0438 |           7.6465 |           2.9677 |
[32m[20221208 14:25:13 @agent_ppo2.py:179][0m |          -0.0013 |           7.3537 |           1.6548 |
[32m[20221208 14:25:13 @agent_ppo2.py:179][0m |          -0.0158 |           7.1591 |           1.8016 |
[32m[20221208 14:25:13 @agent_ppo2.py:179][0m |          -0.0284 |           7.0106 |           1.2409 |
[32m[20221208 14:25:14 @agent_ppo2.py:179][0m |          -0.0370 |           6.8865 |           1.0083 |
[32m[20221208 14:25:14 @agent_ppo2.py:179][0m |          -0.0427 |           6.7502 |           0.5925 |
[32m[20221208 14:25:14 @agent_ppo2.py:179][0m |          -0.0410 |           6.6262 |           0.1824 |
[32m[20221208 14:25:14 @agent_ppo2.py:179][0m |          -0.0427 |           6.5656 |          -0.0088 |
[32m[20221208 14:25:14 @agent_ppo2.py:179][0m |          -0.0482 |           6.5447 |          -0.1803 |
[32m[20221208 14:25:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:25:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.15
[32m[20221208 14:25:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.59
[32m[20221208 14:25:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.44
[32m[20221208 14:25:14 @agent_ppo2.py:137][0m Total time:       9.15 min
[32m[20221208 14:25:14 @agent_ppo2.py:139][0m 745472 total steps have happened
[32m[20221208 14:25:14 @agent_ppo2.py:115][0m #------------------------ Iteration 364 --------------------------#
[32m[20221208 14:25:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |           0.0669 |           7.0344 |           2.3325 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |           0.0350 |           6.4521 |           1.6290 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0015 |           6.0979 |           1.6479 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0164 |           5.8298 |           0.9851 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0254 |           5.6256 |           0.5436 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0324 |           5.4921 |           0.6161 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0383 |           5.2287 |           0.1805 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0379 |           5.0347 |           0.0292 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0435 |           4.8120 |          -0.0182 |
[32m[20221208 14:25:15 @agent_ppo2.py:179][0m |          -0.0478 |           4.7195 |          -0.4324 |
[32m[20221208 14:25:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.18
[32m[20221208 14:25:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.69
[32m[20221208 14:25:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 195.44
[32m[20221208 14:25:16 @agent_ppo2.py:137][0m Total time:       9.18 min
[32m[20221208 14:25:16 @agent_ppo2.py:139][0m 747520 total steps have happened
[32m[20221208 14:25:16 @agent_ppo2.py:115][0m #------------------------ Iteration 365 --------------------------#
[32m[20221208 14:25:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:16 @agent_ppo2.py:179][0m |           0.0735 |          11.9026 |           1.7078 |
[32m[20221208 14:25:16 @agent_ppo2.py:179][0m |           0.0338 |          10.7437 |           3.2598 |
[32m[20221208 14:25:16 @agent_ppo2.py:179][0m |          -0.0069 |          10.2268 |           2.1669 |
[32m[20221208 14:25:16 @agent_ppo2.py:179][0m |          -0.0309 |           9.8868 |           1.7678 |
[32m[20221208 14:25:16 @agent_ppo2.py:179][0m |          -0.0382 |           9.5444 |           1.2807 |
[32m[20221208 14:25:17 @agent_ppo2.py:179][0m |          -0.0462 |           9.3389 |           1.4087 |
[32m[20221208 14:25:17 @agent_ppo2.py:179][0m |          -0.0502 |           9.1763 |           1.3774 |
[32m[20221208 14:25:17 @agent_ppo2.py:179][0m |          -0.0544 |           8.9083 |           0.7207 |
[32m[20221208 14:25:17 @agent_ppo2.py:179][0m |          -0.0600 |           8.7229 |           0.5215 |
[32m[20221208 14:25:17 @agent_ppo2.py:179][0m |          -0.0597 |           8.6071 |          -0.0047 |
[32m[20221208 14:25:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:25:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 182.24
[32m[20221208 14:25:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.40
[32m[20221208 14:25:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.36
[32m[20221208 14:25:17 @agent_ppo2.py:137][0m Total time:       9.20 min
[32m[20221208 14:25:17 @agent_ppo2.py:139][0m 749568 total steps have happened
[32m[20221208 14:25:17 @agent_ppo2.py:115][0m #------------------------ Iteration 366 --------------------------#
[32m[20221208 14:25:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |           0.0861 |          13.1728 |           0.9289 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |           0.0440 |          12.2489 |           1.8615 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0011 |          11.8855 |           1.3767 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0197 |          11.7077 |           0.7172 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0312 |          11.5349 |          -0.0635 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0317 |          11.4342 |           0.0590 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0306 |          11.2994 |           0.1508 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0466 |          11.2467 |          -0.3446 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0547 |          11.0927 |          -0.3058 |
[32m[20221208 14:25:18 @agent_ppo2.py:179][0m |          -0.0576 |          11.0809 |          -0.9061 |
[32m[20221208 14:25:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.56
[32m[20221208 14:25:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.35
[32m[20221208 14:25:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.65
[32m[20221208 14:25:19 @agent_ppo2.py:137][0m Total time:       9.23 min
[32m[20221208 14:25:19 @agent_ppo2.py:139][0m 751616 total steps have happened
[32m[20221208 14:25:19 @agent_ppo2.py:115][0m #------------------------ Iteration 367 --------------------------#
[32m[20221208 14:25:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:19 @agent_ppo2.py:179][0m |           0.0365 |           4.5641 |           0.7844 |
[32m[20221208 14:25:19 @agent_ppo2.py:179][0m |           0.0171 |           4.1527 |           1.0758 |
[32m[20221208 14:25:19 @agent_ppo2.py:179][0m |          -0.0079 |           4.0442 |           1.1720 |
[32m[20221208 14:25:19 @agent_ppo2.py:179][0m |          -0.0198 |           3.9744 |           0.9531 |
[32m[20221208 14:25:19 @agent_ppo2.py:179][0m |          -0.0338 |           3.9324 |           0.9685 |
[32m[20221208 14:25:20 @agent_ppo2.py:179][0m |          -0.0377 |           3.8427 |           0.4498 |
[32m[20221208 14:25:20 @agent_ppo2.py:179][0m |          -0.0411 |           3.8070 |           0.4732 |
[32m[20221208 14:25:20 @agent_ppo2.py:179][0m |          -0.0420 |           3.7391 |           0.4680 |
[32m[20221208 14:25:20 @agent_ppo2.py:179][0m |          -0.0468 |           3.6993 |           0.2326 |
[32m[20221208 14:25:20 @agent_ppo2.py:179][0m |          -0.0476 |           3.6561 |          -0.1808 |
[32m[20221208 14:25:20 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.12
[32m[20221208 14:25:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.16
[32m[20221208 14:25:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.43
[32m[20221208 14:25:20 @agent_ppo2.py:137][0m Total time:       9.25 min
[32m[20221208 14:25:20 @agent_ppo2.py:139][0m 753664 total steps have happened
[32m[20221208 14:25:20 @agent_ppo2.py:115][0m #------------------------ Iteration 368 --------------------------#
[32m[20221208 14:25:21 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:25:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |           0.0452 |           4.9163 |          -0.1251 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |           0.0124 |           4.4570 |          -0.3056 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0123 |           4.3007 |          -0.5155 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0256 |           4.1545 |          -0.1319 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0291 |           4.0334 |          -0.2052 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0314 |           3.9513 |          -0.2355 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0424 |           3.8960 |          -0.4939 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0474 |           3.8363 |          -0.8161 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0450 |           3.7996 |          -0.5535 |
[32m[20221208 14:25:21 @agent_ppo2.py:179][0m |          -0.0484 |           3.7714 |          -0.7767 |
[32m[20221208 14:25:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.55
[32m[20221208 14:25:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.80
[32m[20221208 14:25:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.92
[32m[20221208 14:25:22 @agent_ppo2.py:137][0m Total time:       9.28 min
[32m[20221208 14:25:22 @agent_ppo2.py:139][0m 755712 total steps have happened
[32m[20221208 14:25:22 @agent_ppo2.py:115][0m #------------------------ Iteration 369 --------------------------#
[32m[20221208 14:25:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:22 @agent_ppo2.py:179][0m |           0.0300 |           6.9429 |           0.4997 |
[32m[20221208 14:25:22 @agent_ppo2.py:179][0m |           0.0099 |           5.8229 |           0.8982 |
[32m[20221208 14:25:22 @agent_ppo2.py:179][0m |           0.0013 |           5.4014 |           1.2277 |
[32m[20221208 14:25:22 @agent_ppo2.py:179][0m |          -0.0229 |           5.1036 |           0.5009 |
[32m[20221208 14:25:22 @agent_ppo2.py:179][0m |          -0.0315 |           4.9220 |           0.2259 |
[32m[20221208 14:25:22 @agent_ppo2.py:179][0m |          -0.0394 |           4.7588 |           0.1701 |
[32m[20221208 14:25:23 @agent_ppo2.py:179][0m |          -0.0420 |           4.6141 |          -0.1374 |
[32m[20221208 14:25:23 @agent_ppo2.py:179][0m |          -0.0490 |           4.4616 |          -0.2196 |
[32m[20221208 14:25:23 @agent_ppo2.py:179][0m |          -0.0500 |           4.3648 |          -0.8918 |
[32m[20221208 14:25:23 @agent_ppo2.py:179][0m |          -0.0518 |           4.2988 |          -1.0039 |
[32m[20221208 14:25:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.85
[32m[20221208 14:25:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.38
[32m[20221208 14:25:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.72
[32m[20221208 14:25:23 @agent_ppo2.py:137][0m Total time:       9.30 min
[32m[20221208 14:25:23 @agent_ppo2.py:139][0m 757760 total steps have happened
[32m[20221208 14:25:23 @agent_ppo2.py:115][0m #------------------------ Iteration 370 --------------------------#
[32m[20221208 14:25:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |           0.1082 |           8.5221 |           0.4288 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |           0.1112 |           6.9971 |           0.9105 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |           0.0463 |           6.5607 |           1.2698 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |           0.0056 |           6.1661 |           0.5154 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |          -0.0167 |           5.9251 |           0.0919 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |          -0.0312 |           5.7454 |          -0.4059 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |          -0.0346 |           5.5845 |          -0.1059 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |          -0.0462 |           5.4712 |          -0.6250 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |          -0.0539 |           5.3282 |          -0.7730 |
[32m[20221208 14:25:24 @agent_ppo2.py:179][0m |          -0.0529 |           5.2227 |          -1.1987 |
[32m[20221208 14:25:24 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.51
[32m[20221208 14:25:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.28
[32m[20221208 14:25:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.55
[32m[20221208 14:25:25 @agent_ppo2.py:137][0m Total time:       9.33 min
[32m[20221208 14:25:25 @agent_ppo2.py:139][0m 759808 total steps have happened
[32m[20221208 14:25:25 @agent_ppo2.py:115][0m #------------------------ Iteration 371 --------------------------#
[32m[20221208 14:25:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:25 @agent_ppo2.py:179][0m |           0.0710 |           7.7736 |           1.3257 |
[32m[20221208 14:25:25 @agent_ppo2.py:179][0m |           0.0767 |           7.4682 |           3.2777 |
[32m[20221208 14:25:25 @agent_ppo2.py:179][0m |           0.0277 |           7.3509 |           3.0783 |
[32m[20221208 14:25:25 @agent_ppo2.py:179][0m |           0.0095 |           7.2001 |           1.9666 |
[32m[20221208 14:25:25 @agent_ppo2.py:179][0m |          -0.0156 |           7.0676 |           1.0131 |
[32m[20221208 14:25:25 @agent_ppo2.py:179][0m |          -0.0229 |           7.0037 |           1.2611 |
[32m[20221208 14:25:26 @agent_ppo2.py:179][0m |          -0.0316 |           6.8963 |           1.1188 |
[32m[20221208 14:25:26 @agent_ppo2.py:179][0m |          -0.0386 |           6.8534 |           0.5949 |
[32m[20221208 14:25:26 @agent_ppo2.py:179][0m |          -0.0444 |           6.7658 |           0.3052 |
[32m[20221208 14:25:26 @agent_ppo2.py:179][0m |          -0.0477 |           6.6822 |           0.0341 |
[32m[20221208 14:25:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.09
[32m[20221208 14:25:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.02
[32m[20221208 14:25:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.24
[32m[20221208 14:25:26 @agent_ppo2.py:137][0m Total time:       9.35 min
[32m[20221208 14:25:26 @agent_ppo2.py:139][0m 761856 total steps have happened
[32m[20221208 14:25:26 @agent_ppo2.py:115][0m #------------------------ Iteration 372 --------------------------#
[32m[20221208 14:25:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |           0.0326 |           9.1317 |           2.0305 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0021 |           8.2348 |           0.5312 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0202 |           7.9001 |           0.2737 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0310 |           7.6957 |          -0.1143 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0357 |           7.5158 |          -0.3542 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0461 |           7.4433 |          -0.8813 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0507 |           7.3221 |          -0.7802 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0510 |           7.2712 |          -1.4175 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0531 |           7.1893 |          -1.7879 |
[32m[20221208 14:25:27 @agent_ppo2.py:179][0m |          -0.0569 |           7.1576 |          -2.0170 |
[32m[20221208 14:25:27 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.50
[32m[20221208 14:25:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.82
[32m[20221208 14:25:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.32
[32m[20221208 14:25:28 @agent_ppo2.py:137][0m Total time:       9.38 min
[32m[20221208 14:25:28 @agent_ppo2.py:139][0m 763904 total steps have happened
[32m[20221208 14:25:28 @agent_ppo2.py:115][0m #------------------------ Iteration 373 --------------------------#
[32m[20221208 14:25:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |           0.0510 |           5.3122 |          -1.1118 |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |           0.0163 |           4.8544 |           0.1744 |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |          -0.0095 |           4.5550 |          -0.4873 |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |          -0.0270 |           4.3325 |          -0.7923 |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |          -0.0359 |           4.1948 |          -1.0179 |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |          -0.0390 |           4.1048 |          -1.5733 |
[32m[20221208 14:25:28 @agent_ppo2.py:179][0m |          -0.0441 |           4.0241 |          -1.5772 |
[32m[20221208 14:25:29 @agent_ppo2.py:179][0m |          -0.0478 |           3.9311 |          -2.0842 |
[32m[20221208 14:25:29 @agent_ppo2.py:179][0m |          -0.0450 |           3.9006 |          -2.5031 |
[32m[20221208 14:25:29 @agent_ppo2.py:179][0m |          -0.0514 |           3.8144 |          -2.7249 |
[32m[20221208 14:25:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.76
[32m[20221208 14:25:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.52
[32m[20221208 14:25:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.41
[32m[20221208 14:25:29 @agent_ppo2.py:137][0m Total time:       9.40 min
[32m[20221208 14:25:29 @agent_ppo2.py:139][0m 765952 total steps have happened
[32m[20221208 14:25:29 @agent_ppo2.py:115][0m #------------------------ Iteration 374 --------------------------#
[32m[20221208 14:25:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |           0.0834 |          10.8588 |          -4.0384 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |           0.0823 |           9.9934 |          -0.4101 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |           0.0163 |           9.5459 |          -1.9476 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0064 |           9.2009 |          -3.2491 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0192 |           9.0279 |          -3.1885 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0327 |           8.8563 |          -3.9225 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0409 |           8.6655 |          -4.3053 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0442 |           8.5509 |          -5.3439 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0470 |           8.4388 |          -5.2612 |
[32m[20221208 14:25:30 @agent_ppo2.py:179][0m |          -0.0540 |           8.3090 |          -5.6142 |
[32m[20221208 14:25:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.47
[32m[20221208 14:25:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.02
[32m[20221208 14:25:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.97
[32m[20221208 14:25:31 @agent_ppo2.py:137][0m Total time:       9.43 min
[32m[20221208 14:25:31 @agent_ppo2.py:139][0m 768000 total steps have happened
[32m[20221208 14:25:31 @agent_ppo2.py:115][0m #------------------------ Iteration 375 --------------------------#
[32m[20221208 14:25:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |           0.0884 |           8.1302 |          -3.5590 |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |           0.0598 |           7.3570 |          -0.4916 |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |           0.0165 |           7.0988 |          -1.3644 |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |          -0.0178 |           6.8758 |          -3.0052 |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |          -0.0324 |           6.6869 |          -3.1974 |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |          -0.0399 |           6.5595 |          -3.5160 |
[32m[20221208 14:25:31 @agent_ppo2.py:179][0m |          -0.0423 |           6.5578 |          -3.5585 |
[32m[20221208 14:25:32 @agent_ppo2.py:179][0m |          -0.0506 |           6.4226 |          -4.0571 |
[32m[20221208 14:25:32 @agent_ppo2.py:179][0m |          -0.0562 |           6.3051 |          -4.7245 |
[32m[20221208 14:25:32 @agent_ppo2.py:179][0m |          -0.0576 |           6.2372 |          -5.4667 |
[32m[20221208 14:25:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.06
[32m[20221208 14:25:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.14
[32m[20221208 14:25:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.02
[32m[20221208 14:25:32 @agent_ppo2.py:137][0m Total time:       9.45 min
[32m[20221208 14:25:32 @agent_ppo2.py:139][0m 770048 total steps have happened
[32m[20221208 14:25:32 @agent_ppo2.py:115][0m #------------------------ Iteration 376 --------------------------#
[32m[20221208 14:25:33 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:25:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |           0.0374 |           4.2152 |          -3.0225 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |           0.0137 |           3.7754 |          -1.9976 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0009 |           3.6279 |          -2.9609 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0059 |           3.5333 |          -2.8965 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0255 |           3.4505 |          -3.4996 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0351 |           3.3992 |          -4.3256 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0399 |           3.3520 |          -4.2050 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0424 |           3.3047 |          -5.0203 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0434 |           3.2699 |          -4.8648 |
[32m[20221208 14:25:33 @agent_ppo2.py:179][0m |          -0.0415 |           3.2229 |          -4.1885 |
[32m[20221208 14:25:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.52
[32m[20221208 14:25:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.58
[32m[20221208 14:25:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.86
[32m[20221208 14:25:34 @agent_ppo2.py:137][0m Total time:       9.48 min
[32m[20221208 14:25:34 @agent_ppo2.py:139][0m 772096 total steps have happened
[32m[20221208 14:25:34 @agent_ppo2.py:115][0m #------------------------ Iteration 377 --------------------------#
[32m[20221208 14:25:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |           0.0455 |          10.5658 |          -3.3231 |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |           0.0798 |           9.4370 |          -1.3875 |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |           0.0424 |           9.1004 |           0.0029 |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |           0.0010 |           9.0704 |          -1.6063 |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |          -0.0233 |           8.9312 |          -2.2595 |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |          -0.0360 |           8.8736 |          -2.5052 |
[32m[20221208 14:25:34 @agent_ppo2.py:179][0m |          -0.0438 |           8.7478 |          -3.0863 |
[32m[20221208 14:25:35 @agent_ppo2.py:179][0m |          -0.0440 |           8.7079 |          -3.7602 |
[32m[20221208 14:25:35 @agent_ppo2.py:179][0m |          -0.0526 |           8.6460 |          -3.7047 |
[32m[20221208 14:25:35 @agent_ppo2.py:179][0m |          -0.0544 |           8.5809 |          -4.2420 |
[32m[20221208 14:25:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:25:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.36
[32m[20221208 14:25:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 199.70
[32m[20221208 14:25:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.51
[32m[20221208 14:25:35 @agent_ppo2.py:137][0m Total time:       9.50 min
[32m[20221208 14:25:35 @agent_ppo2.py:139][0m 774144 total steps have happened
[32m[20221208 14:25:35 @agent_ppo2.py:115][0m #------------------------ Iteration 378 --------------------------#
[32m[20221208 14:25:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |           0.0733 |           8.1175 |          -3.4061 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |           0.0329 |           6.7304 |          -2.9007 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |           0.0009 |           6.3101 |          -3.7176 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0257 |           5.9938 |          -4.3515 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0397 |           5.7992 |          -4.7451 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0455 |           5.6218 |          -5.1132 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0474 |           5.4644 |          -5.3933 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0553 |           5.3174 |          -5.9648 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0584 |           5.2152 |          -6.4790 |
[32m[20221208 14:25:36 @agent_ppo2.py:179][0m |          -0.0607 |           5.1042 |          -6.9754 |
[32m[20221208 14:25:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.39
[32m[20221208 14:25:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.42
[32m[20221208 14:25:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 207.46
[32m[20221208 14:25:37 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 207.46
[32m[20221208 14:25:37 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 207.46
[32m[20221208 14:25:37 @agent_ppo2.py:137][0m Total time:       9.53 min
[32m[20221208 14:25:37 @agent_ppo2.py:139][0m 776192 total steps have happened
[32m[20221208 14:25:37 @agent_ppo2.py:115][0m #------------------------ Iteration 379 --------------------------#
[32m[20221208 14:25:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |           0.0491 |           7.7840 |          -4.8968 |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |           0.0084 |           6.9999 |          -4.1240 |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |          -0.0175 |           6.6731 |          -4.4339 |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |          -0.0278 |           6.4769 |          -5.0263 |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |          -0.0369 |           6.3356 |          -6.0264 |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |          -0.0425 |           6.2584 |          -6.6287 |
[32m[20221208 14:25:37 @agent_ppo2.py:179][0m |          -0.0431 |           6.0708 |          -6.5525 |
[32m[20221208 14:25:38 @agent_ppo2.py:179][0m |          -0.0441 |           5.9607 |          -7.2023 |
[32m[20221208 14:25:38 @agent_ppo2.py:179][0m |          -0.0500 |           5.8760 |          -7.8526 |
[32m[20221208 14:25:38 @agent_ppo2.py:179][0m |          -0.0526 |           5.7868 |          -7.5255 |
[32m[20221208 14:25:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 148.77
[32m[20221208 14:25:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 224.20
[32m[20221208 14:25:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 173.20
[32m[20221208 14:25:38 @agent_ppo2.py:137][0m Total time:       9.55 min
[32m[20221208 14:25:38 @agent_ppo2.py:139][0m 778240 total steps have happened
[32m[20221208 14:25:38 @agent_ppo2.py:115][0m #------------------------ Iteration 380 --------------------------#
[32m[20221208 14:25:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |           0.0666 |           8.9379 |          -8.2831 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |           0.0475 |           8.2762 |          -4.7083 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |           0.0224 |           8.0853 |          -5.1232 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0075 |           7.9025 |          -6.8987 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0238 |           7.8182 |          -8.0603 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0325 |           7.7213 |          -8.9535 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0402 |           7.6397 |          -9.4264 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0429 |           7.5818 |         -10.2723 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0494 |           7.5244 |         -10.6241 |
[32m[20221208 14:25:39 @agent_ppo2.py:179][0m |          -0.0526 |           7.4543 |         -10.5568 |
[32m[20221208 14:25:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.38
[32m[20221208 14:25:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 202.52
[32m[20221208 14:25:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 172.91
[32m[20221208 14:25:40 @agent_ppo2.py:137][0m Total time:       9.58 min
[32m[20221208 14:25:40 @agent_ppo2.py:139][0m 780288 total steps have happened
[32m[20221208 14:25:40 @agent_ppo2.py:115][0m #------------------------ Iteration 381 --------------------------#
[32m[20221208 14:25:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |           0.0352 |           4.9644 |          -8.5999 |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |           0.0323 |           4.5939 |          -8.0524 |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |           0.0019 |           4.4701 |         -10.0407 |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |           0.0006 |           4.3324 |          -8.9114 |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |          -0.0183 |           4.2725 |          -9.1303 |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |          -0.0297 |           4.2021 |         -10.1620 |
[32m[20221208 14:25:40 @agent_ppo2.py:179][0m |          -0.0400 |           4.1443 |         -10.0318 |
[32m[20221208 14:25:41 @agent_ppo2.py:179][0m |          -0.0393 |           4.0785 |         -10.6744 |
[32m[20221208 14:25:41 @agent_ppo2.py:179][0m |          -0.0271 |           4.0230 |          -9.5016 |
[32m[20221208 14:25:41 @agent_ppo2.py:179][0m |          -0.0366 |           3.9737 |          -8.9054 |
[32m[20221208 14:25:41 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.35
[32m[20221208 14:25:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.90
[32m[20221208 14:25:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.23
[32m[20221208 14:25:41 @agent_ppo2.py:137][0m Total time:       9.60 min
[32m[20221208 14:25:41 @agent_ppo2.py:139][0m 782336 total steps have happened
[32m[20221208 14:25:41 @agent_ppo2.py:115][0m #------------------------ Iteration 382 --------------------------#
[32m[20221208 14:25:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |           0.0696 |          10.8897 |          -9.8078 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |           0.0657 |           9.8338 |          -7.7309 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |           0.0313 |           9.3088 |          -7.2080 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |           0.0262 |           9.0350 |          -7.1640 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |          -0.0179 |           8.8316 |          -9.5405 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |          -0.0333 |           8.6338 |         -10.7686 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |          -0.0450 |           8.4354 |         -10.7156 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |          -0.0530 |           8.2828 |         -11.2333 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |          -0.0556 |           8.1322 |         -11.4437 |
[32m[20221208 14:25:42 @agent_ppo2.py:179][0m |          -0.0587 |           8.0463 |         -11.0368 |
[32m[20221208 14:25:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 194.83
[32m[20221208 14:25:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.54
[32m[20221208 14:25:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.58
[32m[20221208 14:25:43 @agent_ppo2.py:137][0m Total time:       9.63 min
[32m[20221208 14:25:43 @agent_ppo2.py:139][0m 784384 total steps have happened
[32m[20221208 14:25:43 @agent_ppo2.py:115][0m #------------------------ Iteration 383 --------------------------#
[32m[20221208 14:25:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |           0.0547 |          12.5109 |          -9.7833 |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |           0.0390 |          11.7682 |          -8.1382 |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |           0.0040 |          11.4905 |          -9.6276 |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |          -0.0212 |          11.3284 |          -9.7253 |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |          -0.0345 |          11.2017 |         -10.5008 |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |          -0.0422 |          11.1059 |         -10.5289 |
[32m[20221208 14:25:43 @agent_ppo2.py:179][0m |          -0.0495 |          11.0033 |         -11.0173 |
[32m[20221208 14:25:44 @agent_ppo2.py:179][0m |          -0.0515 |          10.9066 |         -11.3259 |
[32m[20221208 14:25:44 @agent_ppo2.py:179][0m |          -0.0479 |          10.8874 |         -10.9820 |
[32m[20221208 14:25:44 @agent_ppo2.py:179][0m |          -0.0554 |          10.7411 |         -12.2862 |
[32m[20221208 14:25:44 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:25:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.69
[32m[20221208 14:25:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 209.67
[32m[20221208 14:25:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.64
[32m[20221208 14:25:44 @agent_ppo2.py:137][0m Total time:       9.65 min
[32m[20221208 14:25:44 @agent_ppo2.py:139][0m 786432 total steps have happened
[32m[20221208 14:25:44 @agent_ppo2.py:115][0m #------------------------ Iteration 384 --------------------------#
[32m[20221208 14:25:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |           0.0340 |           4.6984 |         -11.3432 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |           0.0302 |           4.3394 |          -9.3844 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |           0.0062 |           4.1897 |         -11.0722 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0136 |           4.0843 |         -13.7460 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0081 |           4.0234 |         -13.0562 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0240 |           3.9286 |         -13.5881 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0328 |           3.8744 |         -14.3718 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0351 |           3.8375 |         -15.3811 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0426 |           3.8004 |         -15.3596 |
[32m[20221208 14:25:45 @agent_ppo2.py:179][0m |          -0.0429 |           3.7347 |         -15.9311 |
[32m[20221208 14:25:45 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.09
[32m[20221208 14:25:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.28
[32m[20221208 14:25:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.43
[32m[20221208 14:25:46 @agent_ppo2.py:137][0m Total time:       9.68 min
[32m[20221208 14:25:46 @agent_ppo2.py:139][0m 788480 total steps have happened
[32m[20221208 14:25:46 @agent_ppo2.py:115][0m #------------------------ Iteration 385 --------------------------#
[32m[20221208 14:25:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |           0.0814 |          10.2862 |          -7.4811 |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |           0.0481 |           9.0518 |          -3.4404 |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |          -0.0074 |           8.7298 |          -6.2325 |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |          -0.0274 |           8.5108 |          -7.2342 |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |          -0.0486 |           8.3216 |          -8.2281 |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |          -0.0566 |           8.1094 |          -8.2936 |
[32m[20221208 14:25:46 @agent_ppo2.py:179][0m |          -0.0612 |           7.9702 |          -9.0687 |
[32m[20221208 14:25:47 @agent_ppo2.py:179][0m |          -0.0650 |           7.8974 |          -9.8615 |
[32m[20221208 14:25:47 @agent_ppo2.py:179][0m |          -0.0743 |           7.7515 |         -10.2721 |
[32m[20221208 14:25:47 @agent_ppo2.py:179][0m |          -0.0739 |           7.6023 |         -10.9741 |
[32m[20221208 14:25:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.25
[32m[20221208 14:25:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.40
[32m[20221208 14:25:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.11
[32m[20221208 14:25:47 @agent_ppo2.py:137][0m Total time:       9.70 min
[32m[20221208 14:25:47 @agent_ppo2.py:139][0m 790528 total steps have happened
[32m[20221208 14:25:47 @agent_ppo2.py:115][0m #------------------------ Iteration 386 --------------------------#
[32m[20221208 14:25:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |           0.0587 |           8.9760 |         -13.3421 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |           0.0354 |           7.5399 |         -10.8223 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |           0.0023 |           7.0063 |         -12.4343 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0214 |           6.5819 |         -13.3043 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0324 |           6.3794 |         -14.2957 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0393 |           6.1029 |         -14.5703 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0442 |           5.9257 |         -15.5566 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0472 |           5.7762 |         -15.4012 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0529 |           5.6549 |         -16.5125 |
[32m[20221208 14:25:48 @agent_ppo2.py:179][0m |          -0.0551 |           5.6159 |         -16.5023 |
[32m[20221208 14:25:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:25:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.24
[32m[20221208 14:25:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.01
[32m[20221208 14:25:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 187.94
[32m[20221208 14:25:49 @agent_ppo2.py:137][0m Total time:       9.73 min
[32m[20221208 14:25:49 @agent_ppo2.py:139][0m 792576 total steps have happened
[32m[20221208 14:25:49 @agent_ppo2.py:115][0m #------------------------ Iteration 387 --------------------------#
[32m[20221208 14:25:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |           0.0597 |           9.0278 |         -13.8385 |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |           0.0197 |           8.0146 |         -13.1654 |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |          -0.0077 |           7.6864 |         -12.9123 |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |          -0.0263 |           7.4801 |         -14.0765 |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |          -0.0325 |           7.3315 |         -15.4080 |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |          -0.0372 |           7.2256 |         -15.6729 |
[32m[20221208 14:25:49 @agent_ppo2.py:179][0m |          -0.0331 |           7.1269 |         -15.9673 |
[32m[20221208 14:25:50 @agent_ppo2.py:179][0m |          -0.0435 |           7.0646 |         -16.1077 |
[32m[20221208 14:25:50 @agent_ppo2.py:179][0m |          -0.0513 |           6.9897 |         -17.4647 |
[32m[20221208 14:25:50 @agent_ppo2.py:179][0m |          -0.0548 |           6.9149 |         -18.3331 |
[32m[20221208 14:25:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:25:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.46
[32m[20221208 14:25:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.73
[32m[20221208 14:25:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.70
[32m[20221208 14:25:50 @agent_ppo2.py:137][0m Total time:       9.75 min
[32m[20221208 14:25:50 @agent_ppo2.py:139][0m 794624 total steps have happened
[32m[20221208 14:25:50 @agent_ppo2.py:115][0m #------------------------ Iteration 388 --------------------------#
[32m[20221208 14:25:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |           0.0574 |           9.8760 |         -17.2819 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |           0.0783 |           8.8074 |          -8.0661 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |           0.0215 |           8.3306 |         -11.3575 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0080 |           8.0625 |         -12.8954 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0229 |           7.8965 |         -13.7787 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0362 |           7.6752 |         -14.2726 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0424 |           7.5542 |         -14.7214 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0468 |           7.4593 |         -15.4809 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0509 |           7.3537 |         -16.0853 |
[32m[20221208 14:25:51 @agent_ppo2.py:179][0m |          -0.0563 |           7.3057 |         -16.5733 |
[32m[20221208 14:25:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 192.08
[32m[20221208 14:25:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 216.34
[32m[20221208 14:25:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.14
[32m[20221208 14:25:52 @agent_ppo2.py:137][0m Total time:       9.78 min
[32m[20221208 14:25:52 @agent_ppo2.py:139][0m 796672 total steps have happened
[32m[20221208 14:25:52 @agent_ppo2.py:115][0m #------------------------ Iteration 389 --------------------------#
[32m[20221208 14:25:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |           0.0549 |           6.8180 |         -14.0258 |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |           0.0346 |           6.0162 |         -11.6192 |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |           0.0083 |           5.7533 |         -14.0110 |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |          -0.0185 |           5.5781 |         -14.7104 |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |          -0.0275 |           5.4694 |         -15.5899 |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |          -0.0341 |           5.3263 |         -15.8299 |
[32m[20221208 14:25:52 @agent_ppo2.py:179][0m |          -0.0400 |           5.2668 |         -17.4938 |
[32m[20221208 14:25:53 @agent_ppo2.py:179][0m |          -0.0440 |           5.1629 |         -17.8574 |
[32m[20221208 14:25:53 @agent_ppo2.py:179][0m |          -0.0492 |           5.0800 |         -18.1785 |
[32m[20221208 14:25:53 @agent_ppo2.py:179][0m |          -0.0486 |           5.0419 |         -18.0729 |
[32m[20221208 14:25:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.60
[32m[20221208 14:25:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 214.51
[32m[20221208 14:25:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.47
[32m[20221208 14:25:53 @agent_ppo2.py:137][0m Total time:       9.80 min
[32m[20221208 14:25:53 @agent_ppo2.py:139][0m 798720 total steps have happened
[32m[20221208 14:25:53 @agent_ppo2.py:115][0m #------------------------ Iteration 390 --------------------------#
[32m[20221208 14:25:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |           0.0914 |           5.4011 |         -16.8377 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |           0.0617 |           4.5419 |         -12.2510 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |           0.0181 |           4.3965 |         -13.2761 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |           0.0122 |           4.2945 |         -14.6024 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |          -0.0074 |           4.2481 |         -15.8541 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |          -0.0179 |           4.2339 |         -16.2328 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |          -0.0256 |           4.2233 |         -17.4763 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |          -0.0310 |           4.1318 |         -17.8472 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |          -0.0344 |           4.1004 |         -18.5104 |
[32m[20221208 14:25:54 @agent_ppo2.py:179][0m |          -0.0413 |           4.0668 |         -19.1602 |
[32m[20221208 14:25:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.85
[32m[20221208 14:25:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.48
[32m[20221208 14:25:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 209.75
[32m[20221208 14:25:55 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 209.75
[32m[20221208 14:25:55 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 209.75
[32m[20221208 14:25:55 @agent_ppo2.py:137][0m Total time:       9.83 min
[32m[20221208 14:25:55 @agent_ppo2.py:139][0m 800768 total steps have happened
[32m[20221208 14:25:55 @agent_ppo2.py:115][0m #------------------------ Iteration 391 --------------------------#
[32m[20221208 14:25:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:25:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |           0.0886 |          12.5901 |         -12.4200 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |           0.0550 |          11.2370 |          -8.4352 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |          -0.0040 |          10.5381 |         -11.7283 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |          -0.0238 |          10.1402 |         -12.9996 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |          -0.0370 |           9.7862 |         -13.8444 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |          -0.0444 |           9.5680 |         -14.5674 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |          -0.0479 |           9.3012 |         -15.3478 |
[32m[20221208 14:25:55 @agent_ppo2.py:179][0m |          -0.0547 |           8.9699 |         -16.3234 |
[32m[20221208 14:25:56 @agent_ppo2.py:179][0m |          -0.0558 |           8.7594 |         -16.4774 |
[32m[20221208 14:25:56 @agent_ppo2.py:179][0m |          -0.0568 |           8.5245 |         -16.5172 |
[32m[20221208 14:25:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:25:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.68
[32m[20221208 14:25:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.71
[32m[20221208 14:25:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.92
[32m[20221208 14:25:56 @agent_ppo2.py:137][0m Total time:       9.85 min
[32m[20221208 14:25:56 @agent_ppo2.py:139][0m 802816 total steps have happened
[32m[20221208 14:25:56 @agent_ppo2.py:115][0m #------------------------ Iteration 392 --------------------------#
[32m[20221208 14:25:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |           0.0523 |           6.7288 |         -17.5650 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |           0.0383 |           5.9479 |         -16.3662 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |           0.0050 |           5.7019 |         -15.3674 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0038 |           5.4979 |         -15.0608 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0190 |           5.3812 |         -16.2547 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0350 |           5.3237 |         -16.5700 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0365 |           5.2182 |         -14.9107 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0429 |           5.1656 |         -16.6921 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0434 |           5.0768 |         -17.2783 |
[32m[20221208 14:25:57 @agent_ppo2.py:179][0m |          -0.0480 |           5.0744 |         -18.4397 |
[32m[20221208 14:25:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:25:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.24
[32m[20221208 14:25:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.98
[32m[20221208 14:25:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.00
[32m[20221208 14:25:58 @agent_ppo2.py:137][0m Total time:       9.88 min
[32m[20221208 14:25:58 @agent_ppo2.py:139][0m 804864 total steps have happened
[32m[20221208 14:25:58 @agent_ppo2.py:115][0m #------------------------ Iteration 393 --------------------------#
[32m[20221208 14:25:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |           0.0628 |          11.8779 |         -10.5922 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |           0.0488 |          11.0503 |         -10.3344 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |           0.0091 |          10.8571 |         -11.0783 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |          -0.0094 |          10.6235 |         -11.6427 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |          -0.0338 |          10.4822 |         -11.9318 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |          -0.0422 |          10.3540 |         -12.5706 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |          -0.0503 |          10.3094 |         -12.7165 |
[32m[20221208 14:25:58 @agent_ppo2.py:179][0m |          -0.0561 |          10.2841 |         -13.4330 |
[32m[20221208 14:25:59 @agent_ppo2.py:179][0m |          -0.0609 |          10.1914 |         -13.8084 |
[32m[20221208 14:25:59 @agent_ppo2.py:179][0m |          -0.0635 |          10.1383 |         -14.3406 |
[32m[20221208 14:25:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:25:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.31
[32m[20221208 14:25:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.21
[32m[20221208 14:25:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.17
[32m[20221208 14:25:59 @agent_ppo2.py:137][0m Total time:       9.90 min
[32m[20221208 14:25:59 @agent_ppo2.py:139][0m 806912 total steps have happened
[32m[20221208 14:25:59 @agent_ppo2.py:115][0m #------------------------ Iteration 394 --------------------------#
[32m[20221208 14:25:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:25:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |           0.0707 |           5.8458 |          -6.7810 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |           0.0275 |           5.0076 |          -2.5804 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0102 |           4.7498 |          -3.3055 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0330 |           4.5519 |          -4.7933 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0451 |           4.4095 |          -5.7512 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0523 |           4.2347 |          -6.7735 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0602 |           4.1271 |          -7.0989 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0599 |           3.9720 |          -7.9550 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0641 |           3.9048 |          -7.9743 |
[32m[20221208 14:26:00 @agent_ppo2.py:179][0m |          -0.0684 |           3.7985 |          -8.3376 |
[32m[20221208 14:26:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:26:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.49
[32m[20221208 14:26:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.33
[32m[20221208 14:26:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.18
[32m[20221208 14:26:00 @agent_ppo2.py:137][0m Total time:       9.93 min
[32m[20221208 14:26:00 @agent_ppo2.py:139][0m 808960 total steps have happened
[32m[20221208 14:26:00 @agent_ppo2.py:115][0m #------------------------ Iteration 395 --------------------------#
[32m[20221208 14:26:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |           0.0465 |           9.6247 |         -19.5651 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |           0.0216 |           8.9724 |         -18.6704 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |           0.0073 |           8.6481 |         -17.9736 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |          -0.0244 |           8.4052 |         -20.6246 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |          -0.0318 |           8.2610 |         -20.6181 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |          -0.0398 |           8.0748 |         -21.4059 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |          -0.0431 |           7.9686 |         -21.3904 |
[32m[20221208 14:26:01 @agent_ppo2.py:179][0m |          -0.0430 |           7.8702 |         -22.6117 |
[32m[20221208 14:26:02 @agent_ppo2.py:179][0m |          -0.0507 |           7.7416 |         -22.7421 |
[32m[20221208 14:26:02 @agent_ppo2.py:179][0m |          -0.0568 |           7.6646 |         -23.1786 |
[32m[20221208 14:26:02 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:26:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.24
[32m[20221208 14:26:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.85
[32m[20221208 14:26:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.78
[32m[20221208 14:26:02 @agent_ppo2.py:137][0m Total time:       9.95 min
[32m[20221208 14:26:02 @agent_ppo2.py:139][0m 811008 total steps have happened
[32m[20221208 14:26:02 @agent_ppo2.py:115][0m #------------------------ Iteration 396 --------------------------#
[32m[20221208 14:26:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |           0.1892 |          12.2792 |         -13.3332 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |           0.0661 |          11.2446 |          -8.5030 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |           0.0053 |          11.0669 |         -11.7168 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0278 |          10.8909 |         -13.8119 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0439 |          10.8015 |         -15.1077 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0525 |          10.6628 |         -15.2124 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0569 |          10.5787 |         -16.2182 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0641 |          10.4338 |         -16.8178 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0682 |          10.3865 |         -17.4022 |
[32m[20221208 14:26:03 @agent_ppo2.py:179][0m |          -0.0655 |          10.2924 |         -17.3559 |
[32m[20221208 14:26:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.79
[32m[20221208 14:26:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.24
[32m[20221208 14:26:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.80
[32m[20221208 14:26:04 @agent_ppo2.py:137][0m Total time:       9.98 min
[32m[20221208 14:26:04 @agent_ppo2.py:139][0m 813056 total steps have happened
[32m[20221208 14:26:04 @agent_ppo2.py:115][0m #------------------------ Iteration 397 --------------------------#
[32m[20221208 14:26:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |           0.0411 |           5.7531 |         -13.8164 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |           0.0270 |           4.6265 |          -5.9139 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |          -0.0063 |           4.3233 |          -8.2876 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |          -0.0246 |           4.1227 |          -8.4734 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |          -0.0302 |           3.9752 |          -8.2277 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |          -0.0403 |           3.8423 |          -8.3879 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |          -0.0411 |           3.7342 |          -8.9787 |
[32m[20221208 14:26:04 @agent_ppo2.py:179][0m |          -0.0473 |           3.6543 |          -9.5483 |
[32m[20221208 14:26:05 @agent_ppo2.py:179][0m |          -0.0499 |           3.5693 |          -9.6157 |
[32m[20221208 14:26:05 @agent_ppo2.py:179][0m |          -0.0536 |           3.4985 |         -10.5933 |
[32m[20221208 14:26:05 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:26:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.85
[32m[20221208 14:26:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.88
[32m[20221208 14:26:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.93
[32m[20221208 14:26:05 @agent_ppo2.py:137][0m Total time:      10.00 min
[32m[20221208 14:26:05 @agent_ppo2.py:139][0m 815104 total steps have happened
[32m[20221208 14:26:05 @agent_ppo2.py:115][0m #------------------------ Iteration 398 --------------------------#
[32m[20221208 14:26:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |           0.0497 |           8.6219 |         -21.2169 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |           0.0312 |           8.0001 |         -19.2352 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |           0.0046 |           7.7922 |         -20.7892 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0262 |           7.5310 |         -22.6644 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0358 |           7.3542 |         -22.9062 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0451 |           7.3162 |         -24.3538 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0487 |           7.1807 |         -24.0627 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0533 |           7.0668 |         -24.5608 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0562 |           7.0681 |         -25.4654 |
[32m[20221208 14:26:06 @agent_ppo2.py:179][0m |          -0.0568 |           6.9386 |         -25.4929 |
[32m[20221208 14:26:06 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:26:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.59
[32m[20221208 14:26:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.27
[32m[20221208 14:26:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 203.45
[32m[20221208 14:26:07 @agent_ppo2.py:137][0m Total time:      10.03 min
[32m[20221208 14:26:07 @agent_ppo2.py:139][0m 817152 total steps have happened
[32m[20221208 14:26:07 @agent_ppo2.py:115][0m #------------------------ Iteration 399 --------------------------#
[32m[20221208 14:26:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |           0.1020 |           5.8558 |         -15.5938 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |           0.0799 |           5.3508 |          -6.8518 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |           0.0269 |           5.1707 |          -5.4067 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |          -0.0064 |           5.0130 |          -7.4733 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |          -0.0220 |           4.8561 |          -8.7892 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |          -0.0347 |           4.7620 |          -9.6183 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |          -0.0413 |           4.7191 |         -10.0377 |
[32m[20221208 14:26:07 @agent_ppo2.py:179][0m |          -0.0472 |           4.6685 |         -10.6237 |
[32m[20221208 14:26:08 @agent_ppo2.py:179][0m |          -0.0502 |           4.6128 |         -11.5557 |
[32m[20221208 14:26:08 @agent_ppo2.py:179][0m |          -0.0507 |           4.5159 |         -11.9262 |
[32m[20221208 14:26:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.82
[32m[20221208 14:26:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.92
[32m[20221208 14:26:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.44
[32m[20221208 14:26:08 @agent_ppo2.py:137][0m Total time:      10.05 min
[32m[20221208 14:26:08 @agent_ppo2.py:139][0m 819200 total steps have happened
[32m[20221208 14:26:08 @agent_ppo2.py:115][0m #------------------------ Iteration 400 --------------------------#
[32m[20221208 14:26:09 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:26:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |           0.0550 |           7.9843 |         -21.8890 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |           0.0398 |           7.4523 |         -16.2803 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |           0.0002 |           7.2286 |         -15.1043 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0204 |           7.0637 |         -17.0418 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0344 |           6.8844 |         -16.5225 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0420 |           6.7819 |         -17.6466 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0488 |           6.6866 |         -18.2311 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0533 |           6.6502 |         -19.2877 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0584 |           6.4921 |         -20.1691 |
[32m[20221208 14:26:09 @agent_ppo2.py:179][0m |          -0.0579 |           6.4267 |         -20.9143 |
[32m[20221208 14:26:09 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:26:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.49
[32m[20221208 14:26:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.88
[32m[20221208 14:26:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.75
[32m[20221208 14:26:10 @agent_ppo2.py:137][0m Total time:      10.08 min
[32m[20221208 14:26:10 @agent_ppo2.py:139][0m 821248 total steps have happened
[32m[20221208 14:26:10 @agent_ppo2.py:115][0m #------------------------ Iteration 401 --------------------------#
[32m[20221208 14:26:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:10 @agent_ppo2.py:179][0m |           0.0698 |           7.6238 |         -28.3174 |
[32m[20221208 14:26:10 @agent_ppo2.py:179][0m |           0.1175 |           7.0300 |         -19.2139 |
[32m[20221208 14:26:10 @agent_ppo2.py:179][0m |           0.0462 |           6.8031 |         -21.8154 |
[32m[20221208 14:26:10 @agent_ppo2.py:179][0m |           0.0305 |           6.6523 |         -23.3001 |
[32m[20221208 14:26:10 @agent_ppo2.py:179][0m |           0.0118 |           6.5144 |         -24.0537 |
[32m[20221208 14:26:11 @agent_ppo2.py:179][0m |          -0.0159 |           6.3700 |         -26.5188 |
[32m[20221208 14:26:11 @agent_ppo2.py:179][0m |          -0.0266 |           6.2284 |         -28.9804 |
[32m[20221208 14:26:11 @agent_ppo2.py:179][0m |          -0.0346 |           6.0848 |         -28.6889 |
[32m[20221208 14:26:11 @agent_ppo2.py:179][0m |          -0.0370 |           5.9619 |         -29.4205 |
[32m[20221208 14:26:11 @agent_ppo2.py:179][0m |          -0.0439 |           5.8360 |         -30.8269 |
[32m[20221208 14:26:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:26:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.00
[32m[20221208 14:26:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.63
[32m[20221208 14:26:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.97
[32m[20221208 14:26:11 @agent_ppo2.py:137][0m Total time:      10.11 min
[32m[20221208 14:26:11 @agent_ppo2.py:139][0m 823296 total steps have happened
[32m[20221208 14:26:11 @agent_ppo2.py:115][0m #------------------------ Iteration 402 --------------------------#
[32m[20221208 14:26:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |           0.0317 |           4.9021 |         -31.0982 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |           0.0231 |           4.4775 |         -29.4103 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0035 |           4.2602 |         -25.9755 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0229 |           4.0851 |         -26.9502 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0337 |           3.9156 |         -31.8829 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0369 |           3.7846 |         -31.1026 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0429 |           3.6770 |         -33.5837 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0450 |           3.6421 |         -34.0821 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0450 |           3.4899 |         -34.2572 |
[32m[20221208 14:26:12 @agent_ppo2.py:179][0m |          -0.0518 |           3.4066 |         -32.8458 |
[32m[20221208 14:26:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:26:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.91
[32m[20221208 14:26:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.28
[32m[20221208 14:26:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 199.78
[32m[20221208 14:26:13 @agent_ppo2.py:137][0m Total time:      10.13 min
[32m[20221208 14:26:13 @agent_ppo2.py:139][0m 825344 total steps have happened
[32m[20221208 14:26:13 @agent_ppo2.py:115][0m #------------------------ Iteration 403 --------------------------#
[32m[20221208 14:26:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:13 @agent_ppo2.py:179][0m |           0.0913 |          10.5520 |         -28.2371 |
[32m[20221208 14:26:13 @agent_ppo2.py:179][0m |           0.1381 |           9.7591 |          -8.9077 |
[32m[20221208 14:26:13 @agent_ppo2.py:179][0m |           0.0795 |           9.3716 |         -15.5098 |
[32m[20221208 14:26:13 @agent_ppo2.py:179][0m |           0.0291 |           9.0839 |         -18.5888 |
[32m[20221208 14:26:13 @agent_ppo2.py:179][0m |           0.0017 |           8.9543 |         -24.3696 |
[32m[20221208 14:26:14 @agent_ppo2.py:179][0m |          -0.0155 |           8.7547 |         -25.2231 |
[32m[20221208 14:26:14 @agent_ppo2.py:179][0m |          -0.0278 |           8.6327 |         -28.2118 |
[32m[20221208 14:26:14 @agent_ppo2.py:179][0m |          -0.0358 |           8.5140 |         -28.2335 |
[32m[20221208 14:26:14 @agent_ppo2.py:179][0m |          -0.0445 |           8.4758 |         -29.3236 |
[32m[20221208 14:26:14 @agent_ppo2.py:179][0m |          -0.0466 |           8.3663 |         -31.0371 |
[32m[20221208 14:26:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.98
[32m[20221208 14:26:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.73
[32m[20221208 14:26:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.16
[32m[20221208 14:26:14 @agent_ppo2.py:137][0m Total time:      10.16 min
[32m[20221208 14:26:14 @agent_ppo2.py:139][0m 827392 total steps have happened
[32m[20221208 14:26:14 @agent_ppo2.py:115][0m #------------------------ Iteration 404 --------------------------#
[32m[20221208 14:26:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |           0.0635 |          10.5642 |         -24.8484 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |           0.0503 |           9.6674 |         -22.2820 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |           0.0030 |           9.1406 |         -24.3826 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0230 |           8.7106 |         -25.9474 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0320 |           8.4616 |         -27.7709 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0413 |           8.1699 |         -28.3623 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0505 |           7.8925 |         -29.2934 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0550 |           7.7085 |         -30.3158 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0580 |           7.5345 |         -31.2152 |
[32m[20221208 14:26:15 @agent_ppo2.py:179][0m |          -0.0551 |           7.3938 |         -31.2072 |
[32m[20221208 14:26:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:26:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.55
[32m[20221208 14:26:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.28
[32m[20221208 14:26:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.30
[32m[20221208 14:26:16 @agent_ppo2.py:137][0m Total time:      10.18 min
[32m[20221208 14:26:16 @agent_ppo2.py:139][0m 829440 total steps have happened
[32m[20221208 14:26:16 @agent_ppo2.py:115][0m #------------------------ Iteration 405 --------------------------#
[32m[20221208 14:26:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:16 @agent_ppo2.py:179][0m |           0.0727 |           8.2493 |         -24.1818 |
[32m[20221208 14:26:16 @agent_ppo2.py:179][0m |           0.0683 |           7.5097 |         -17.5285 |
[32m[20221208 14:26:16 @agent_ppo2.py:179][0m |           0.0305 |           7.2433 |         -22.9591 |
[32m[20221208 14:26:16 @agent_ppo2.py:179][0m |          -0.0061 |           6.9953 |         -26.2964 |
[32m[20221208 14:26:16 @agent_ppo2.py:179][0m |          -0.0230 |           6.8228 |         -27.7241 |
[32m[20221208 14:26:17 @agent_ppo2.py:179][0m |          -0.0315 |           6.7148 |         -28.5273 |
[32m[20221208 14:26:17 @agent_ppo2.py:179][0m |          -0.0344 |           6.5971 |         -28.7390 |
[32m[20221208 14:26:17 @agent_ppo2.py:179][0m |          -0.0401 |           6.5244 |         -30.0943 |
[32m[20221208 14:26:17 @agent_ppo2.py:179][0m |          -0.0417 |           6.5389 |         -30.4026 |
[32m[20221208 14:26:17 @agent_ppo2.py:179][0m |          -0.0469 |           6.4155 |         -32.2784 |
[32m[20221208 14:26:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.30
[32m[20221208 14:26:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 211.25
[32m[20221208 14:26:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.66
[32m[20221208 14:26:17 @agent_ppo2.py:137][0m Total time:      10.20 min
[32m[20221208 14:26:17 @agent_ppo2.py:139][0m 831488 total steps have happened
[32m[20221208 14:26:17 @agent_ppo2.py:115][0m #------------------------ Iteration 406 --------------------------#
[32m[20221208 14:26:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |           0.0479 |          11.5751 |         -27.7369 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |           0.0303 |          11.0986 |         -22.6419 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |           0.0070 |          10.9910 |         -26.1153 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0124 |          10.7619 |         -26.4973 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0315 |          10.6278 |         -27.7297 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0312 |          10.5608 |         -28.5948 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0442 |          10.4399 |         -29.0558 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0486 |          10.3039 |         -29.3821 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0525 |          10.2613 |         -30.4834 |
[32m[20221208 14:26:18 @agent_ppo2.py:179][0m |          -0.0576 |          10.1735 |         -30.4210 |
[32m[20221208 14:26:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 195.35
[32m[20221208 14:26:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.38
[32m[20221208 14:26:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.55
[32m[20221208 14:26:19 @agent_ppo2.py:137][0m Total time:      10.23 min
[32m[20221208 14:26:19 @agent_ppo2.py:139][0m 833536 total steps have happened
[32m[20221208 14:26:19 @agent_ppo2.py:115][0m #------------------------ Iteration 407 --------------------------#
[32m[20221208 14:26:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:19 @agent_ppo2.py:179][0m |           0.0787 |          11.6503 |         -24.8505 |
[32m[20221208 14:26:19 @agent_ppo2.py:179][0m |           0.0688 |          11.2549 |         -16.5504 |
[32m[20221208 14:26:19 @agent_ppo2.py:179][0m |           0.0202 |          11.0501 |         -21.5206 |
[32m[20221208 14:26:19 @agent_ppo2.py:179][0m |          -0.0057 |          10.9424 |         -23.4253 |
[32m[20221208 14:26:19 @agent_ppo2.py:179][0m |          -0.0222 |          10.8678 |         -24.2196 |
[32m[20221208 14:26:20 @agent_ppo2.py:179][0m |          -0.0327 |          10.7731 |         -25.3930 |
[32m[20221208 14:26:20 @agent_ppo2.py:179][0m |          -0.0384 |          10.7064 |         -26.9082 |
[32m[20221208 14:26:20 @agent_ppo2.py:179][0m |          -0.0414 |          10.6978 |         -27.1904 |
[32m[20221208 14:26:20 @agent_ppo2.py:179][0m |          -0.0445 |          10.6137 |         -28.4142 |
[32m[20221208 14:26:20 @agent_ppo2.py:179][0m |          -0.0448 |          10.5977 |         -28.3416 |
[32m[20221208 14:26:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 187.69
[32m[20221208 14:26:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.52
[32m[20221208 14:26:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.01
[32m[20221208 14:26:20 @agent_ppo2.py:137][0m Total time:      10.25 min
[32m[20221208 14:26:20 @agent_ppo2.py:139][0m 835584 total steps have happened
[32m[20221208 14:26:20 @agent_ppo2.py:115][0m #------------------------ Iteration 408 --------------------------#
[32m[20221208 14:26:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |           0.0491 |           5.4095 |         -22.2455 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |           0.0310 |           4.5402 |         -18.8834 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |           0.0314 |           4.3305 |         -10.2692 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |           0.0245 |           4.2209 |          -6.4736 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |          -0.0048 |           4.1210 |          -7.3787 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |          -0.0203 |           4.0611 |          -8.3345 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |          -0.0280 |           4.0028 |          -9.2659 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |          -0.0317 |           3.9541 |          -9.8708 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |          -0.0332 |           3.9212 |         -10.9879 |
[32m[20221208 14:26:21 @agent_ppo2.py:179][0m |          -0.0392 |           3.8305 |         -11.7764 |
[32m[20221208 14:26:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:26:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.20
[32m[20221208 14:26:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.18
[32m[20221208 14:26:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 196.50
[32m[20221208 14:26:22 @agent_ppo2.py:137][0m Total time:      10.28 min
[32m[20221208 14:26:22 @agent_ppo2.py:139][0m 837632 total steps have happened
[32m[20221208 14:26:22 @agent_ppo2.py:115][0m #------------------------ Iteration 409 --------------------------#
[32m[20221208 14:26:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:22 @agent_ppo2.py:179][0m |           0.0801 |          11.1608 |         -25.8315 |
[32m[20221208 14:26:22 @agent_ppo2.py:179][0m |           0.0805 |          10.3601 |         -16.8565 |
[32m[20221208 14:26:22 @agent_ppo2.py:179][0m |           0.0111 |           9.8704 |         -20.6203 |
[32m[20221208 14:26:22 @agent_ppo2.py:179][0m |          -0.0222 |           9.6230 |         -23.4320 |
[32m[20221208 14:26:22 @agent_ppo2.py:179][0m |          -0.0318 |           9.4440 |         -24.4667 |
[32m[20221208 14:26:23 @agent_ppo2.py:179][0m |          -0.0436 |           9.3045 |         -25.4294 |
[32m[20221208 14:26:23 @agent_ppo2.py:179][0m |          -0.0504 |           9.1538 |         -26.4727 |
[32m[20221208 14:26:23 @agent_ppo2.py:179][0m |          -0.0532 |           8.9909 |         -27.3767 |
[32m[20221208 14:26:23 @agent_ppo2.py:179][0m |          -0.0548 |           8.9140 |         -28.2721 |
[32m[20221208 14:26:23 @agent_ppo2.py:179][0m |          -0.0563 |           8.7629 |         -26.9966 |
[32m[20221208 14:26:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.10
[32m[20221208 14:26:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.49
[32m[20221208 14:26:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.91
[32m[20221208 14:26:23 @agent_ppo2.py:137][0m Total time:      10.30 min
[32m[20221208 14:26:23 @agent_ppo2.py:139][0m 839680 total steps have happened
[32m[20221208 14:26:23 @agent_ppo2.py:115][0m #------------------------ Iteration 410 --------------------------#
[32m[20221208 14:26:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |           0.1464 |           8.4022 |         -26.1426 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |           0.0889 |           7.7203 |         -22.2490 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |           0.0478 |           7.3874 |         -22.7386 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |           0.0142 |           7.1489 |         -24.4752 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |          -0.0031 |           7.0646 |         -26.4204 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |          -0.0158 |           6.9013 |         -28.0022 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |          -0.0231 |           6.6911 |         -28.6549 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |          -0.0294 |           6.6732 |         -30.3456 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |          -0.0386 |           6.5160 |         -31.4291 |
[32m[20221208 14:26:24 @agent_ppo2.py:179][0m |          -0.0447 |           6.4673 |         -32.4792 |
[32m[20221208 14:26:24 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:26:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.60
[32m[20221208 14:26:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.26
[32m[20221208 14:26:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.05
[32m[20221208 14:26:25 @agent_ppo2.py:137][0m Total time:      10.33 min
[32m[20221208 14:26:25 @agent_ppo2.py:139][0m 841728 total steps have happened
[32m[20221208 14:26:25 @agent_ppo2.py:115][0m #------------------------ Iteration 411 --------------------------#
[32m[20221208 14:26:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:25 @agent_ppo2.py:179][0m |           0.0788 |          12.5813 |         -26.2373 |
[32m[20221208 14:26:25 @agent_ppo2.py:179][0m |           0.0537 |          11.5278 |         -20.9293 |
[32m[20221208 14:26:25 @agent_ppo2.py:179][0m |          -0.0061 |          11.1927 |         -25.6648 |
[32m[20221208 14:26:25 @agent_ppo2.py:179][0m |          -0.0231 |          10.9341 |         -28.2043 |
[32m[20221208 14:26:25 @agent_ppo2.py:179][0m |          -0.0366 |          10.7490 |         -29.0775 |
[32m[20221208 14:26:26 @agent_ppo2.py:179][0m |          -0.0441 |          10.5500 |         -29.8077 |
[32m[20221208 14:26:26 @agent_ppo2.py:179][0m |          -0.0509 |          10.3952 |         -31.1808 |
[32m[20221208 14:26:26 @agent_ppo2.py:179][0m |          -0.0540 |          10.2794 |         -32.7363 |
[32m[20221208 14:26:26 @agent_ppo2.py:179][0m |          -0.0585 |          10.1005 |         -32.5501 |
[32m[20221208 14:26:26 @agent_ppo2.py:179][0m |          -0.0628 |          10.0797 |         -33.4225 |
[32m[20221208 14:26:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.51
[32m[20221208 14:26:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 203.54
[32m[20221208 14:26:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.90
[32m[20221208 14:26:26 @agent_ppo2.py:137][0m Total time:      10.35 min
[32m[20221208 14:26:26 @agent_ppo2.py:139][0m 843776 total steps have happened
[32m[20221208 14:26:26 @agent_ppo2.py:115][0m #------------------------ Iteration 412 --------------------------#
[32m[20221208 14:26:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |           0.0444 |           5.4268 |         -32.6318 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |           0.0687 |           4.9294 |         -21.1566 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |           0.1268 |           4.8190 |         -18.5680 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |           0.0136 |           4.7456 |         -15.5490 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |          -0.0144 |           4.6431 |         -19.3930 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |          -0.0319 |           4.5716 |         -20.8667 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |          -0.0388 |           4.5133 |         -21.3803 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |          -0.0470 |           4.4853 |         -22.3915 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |          -0.0516 |           4.4302 |         -24.1637 |
[32m[20221208 14:26:27 @agent_ppo2.py:179][0m |          -0.0591 |           4.4058 |         -24.6045 |
[32m[20221208 14:26:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.90
[32m[20221208 14:26:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.03
[32m[20221208 14:26:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.23
[32m[20221208 14:26:28 @agent_ppo2.py:137][0m Total time:      10.38 min
[32m[20221208 14:26:28 @agent_ppo2.py:139][0m 845824 total steps have happened
[32m[20221208 14:26:28 @agent_ppo2.py:115][0m #------------------------ Iteration 413 --------------------------#
[32m[20221208 14:26:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:28 @agent_ppo2.py:179][0m |           0.0902 |           7.1281 |         -30.2183 |
[32m[20221208 14:26:28 @agent_ppo2.py:179][0m |           0.0371 |           6.6743 |         -19.7563 |
[32m[20221208 14:26:28 @agent_ppo2.py:179][0m |          -0.0075 |           6.3327 |         -21.5973 |
[32m[20221208 14:26:28 @agent_ppo2.py:179][0m |          -0.0086 |           6.1901 |         -23.1354 |
[32m[20221208 14:26:28 @agent_ppo2.py:179][0m |          -0.0320 |           6.0033 |         -24.2423 |
[32m[20221208 14:26:29 @agent_ppo2.py:179][0m |          -0.0476 |           5.8281 |         -26.1479 |
[32m[20221208 14:26:29 @agent_ppo2.py:179][0m |          -0.0518 |           5.7887 |         -26.4156 |
[32m[20221208 14:26:29 @agent_ppo2.py:179][0m |          -0.0597 |           5.6386 |         -28.2026 |
[32m[20221208 14:26:29 @agent_ppo2.py:179][0m |          -0.0611 |           5.6051 |         -28.5059 |
[32m[20221208 14:26:29 @agent_ppo2.py:179][0m |          -0.0582 |           5.6050 |         -28.5705 |
[32m[20221208 14:26:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.41
[32m[20221208 14:26:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.13
[32m[20221208 14:26:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.98
[32m[20221208 14:26:29 @agent_ppo2.py:137][0m Total time:      10.40 min
[32m[20221208 14:26:29 @agent_ppo2.py:139][0m 847872 total steps have happened
[32m[20221208 14:26:29 @agent_ppo2.py:115][0m #------------------------ Iteration 414 --------------------------#
[32m[20221208 14:26:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |           0.0669 |           5.0593 |         -37.3671 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |           0.0980 |           4.5525 |         -24.0672 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |           0.0446 |           4.4241 |         -14.7785 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |           0.0060 |           4.2813 |         -18.5818 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |          -0.0092 |           4.2521 |         -21.2779 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |          -0.0237 |           4.1469 |         -23.4779 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |          -0.0281 |           4.1093 |         -25.1586 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |          -0.0355 |           4.0561 |         -25.1613 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |          -0.0370 |           4.0079 |         -25.1103 |
[32m[20221208 14:26:30 @agent_ppo2.py:179][0m |          -0.0437 |           3.9930 |         -25.3638 |
[32m[20221208 14:26:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:26:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.26
[32m[20221208 14:26:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.20
[32m[20221208 14:26:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 201.05
[32m[20221208 14:26:31 @agent_ppo2.py:137][0m Total time:      10.43 min
[32m[20221208 14:26:31 @agent_ppo2.py:139][0m 849920 total steps have happened
[32m[20221208 14:26:31 @agent_ppo2.py:115][0m #------------------------ Iteration 415 --------------------------#
[32m[20221208 14:26:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:31 @agent_ppo2.py:179][0m |           0.0474 |          11.3704 |         -39.6978 |
[32m[20221208 14:26:31 @agent_ppo2.py:179][0m |           0.0178 |          10.2983 |         -36.1791 |
[32m[20221208 14:26:31 @agent_ppo2.py:179][0m |          -0.0207 |           9.8102 |         -39.0802 |
[32m[20221208 14:26:31 @agent_ppo2.py:179][0m |          -0.0306 |           9.4502 |         -40.5279 |
[32m[20221208 14:26:31 @agent_ppo2.py:179][0m |          -0.0447 |           9.1065 |         -41.8414 |
[32m[20221208 14:26:31 @agent_ppo2.py:179][0m |          -0.0514 |           8.9419 |         -42.7012 |
[32m[20221208 14:26:32 @agent_ppo2.py:179][0m |          -0.0555 |           8.7316 |         -43.6104 |
[32m[20221208 14:26:32 @agent_ppo2.py:179][0m |          -0.0613 |           8.5506 |         -44.2073 |
[32m[20221208 14:26:32 @agent_ppo2.py:179][0m |          -0.0633 |           8.4766 |         -43.2971 |
[32m[20221208 14:26:32 @agent_ppo2.py:179][0m |          -0.0642 |           8.3356 |         -45.0946 |
[32m[20221208 14:26:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.48
[32m[20221208 14:26:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.56
[32m[20221208 14:26:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.54
[32m[20221208 14:26:32 @agent_ppo2.py:137][0m Total time:      10.45 min
[32m[20221208 14:26:32 @agent_ppo2.py:139][0m 851968 total steps have happened
[32m[20221208 14:26:32 @agent_ppo2.py:115][0m #------------------------ Iteration 416 --------------------------#
[32m[20221208 14:26:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |           0.0869 |           6.7104 |         -30.7365 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |           0.0687 |           5.8020 |         -26.6842 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |           0.0473 |           5.5379 |         -24.8314 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |           0.0101 |           5.3966 |         -29.7856 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |          -0.0060 |           5.2775 |         -32.3793 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |          -0.0109 |           5.1708 |         -34.7494 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |          -0.0184 |           5.1301 |         -34.3361 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |          -0.0305 |           5.1060 |         -35.4359 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |          -0.0353 |           5.0066 |         -37.9190 |
[32m[20221208 14:26:33 @agent_ppo2.py:179][0m |          -0.0356 |           4.9576 |         -38.4383 |
[32m[20221208 14:26:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.27
[32m[20221208 14:26:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.45
[32m[20221208 14:26:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.92
[32m[20221208 14:26:34 @agent_ppo2.py:137][0m Total time:      10.48 min
[32m[20221208 14:26:34 @agent_ppo2.py:139][0m 854016 total steps have happened
[32m[20221208 14:26:34 @agent_ppo2.py:115][0m #------------------------ Iteration 417 --------------------------#
[32m[20221208 14:26:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:34 @agent_ppo2.py:179][0m |           0.0805 |          12.2071 |         -34.7017 |
[32m[20221208 14:26:34 @agent_ppo2.py:179][0m |           0.0437 |          11.0355 |         -29.7076 |
[32m[20221208 14:26:34 @agent_ppo2.py:179][0m |          -0.0023 |          10.6212 |         -37.0695 |
[32m[20221208 14:26:34 @agent_ppo2.py:179][0m |          -0.0262 |          10.2828 |         -40.1850 |
[32m[20221208 14:26:34 @agent_ppo2.py:179][0m |          -0.0330 |          10.0207 |         -41.1417 |
[32m[20221208 14:26:35 @agent_ppo2.py:179][0m |          -0.0425 |           9.8802 |         -41.5219 |
[32m[20221208 14:26:35 @agent_ppo2.py:179][0m |          -0.0478 |           9.7136 |         -43.8389 |
[32m[20221208 14:26:35 @agent_ppo2.py:179][0m |          -0.0521 |           9.6303 |         -44.4105 |
[32m[20221208 14:26:35 @agent_ppo2.py:179][0m |          -0.0560 |           9.4694 |         -45.5416 |
[32m[20221208 14:26:35 @agent_ppo2.py:179][0m |          -0.0608 |           9.3951 |         -47.2672 |
[32m[20221208 14:26:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:26:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 188.74
[32m[20221208 14:26:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 206.95
[32m[20221208 14:26:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.46
[32m[20221208 14:26:35 @agent_ppo2.py:137][0m Total time:      10.50 min
[32m[20221208 14:26:35 @agent_ppo2.py:139][0m 856064 total steps have happened
[32m[20221208 14:26:35 @agent_ppo2.py:115][0m #------------------------ Iteration 418 --------------------------#
[32m[20221208 14:26:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |           0.0658 |          10.0805 |         -38.2616 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |           0.0486 |           9.0209 |         -31.2926 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |           0.0123 |           8.5230 |         -33.9941 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0246 |           8.1482 |         -38.3525 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0365 |           7.8647 |         -41.3839 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0441 |           7.6098 |         -42.5071 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0490 |           7.4273 |         -43.9613 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0559 |           7.2177 |         -44.7326 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0529 |           7.0795 |         -44.7406 |
[32m[20221208 14:26:36 @agent_ppo2.py:179][0m |          -0.0565 |           6.8809 |         -45.8857 |
[32m[20221208 14:26:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.72
[32m[20221208 14:26:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.03
[32m[20221208 14:26:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.87
[32m[20221208 14:26:37 @agent_ppo2.py:137][0m Total time:      10.53 min
[32m[20221208 14:26:37 @agent_ppo2.py:139][0m 858112 total steps have happened
[32m[20221208 14:26:37 @agent_ppo2.py:115][0m #------------------------ Iteration 419 --------------------------#
[32m[20221208 14:26:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:37 @agent_ppo2.py:179][0m |           0.0281 |           3.9676 |         -25.4520 |
[32m[20221208 14:26:37 @agent_ppo2.py:179][0m |          -0.0148 |           3.5643 |         -14.9237 |
[32m[20221208 14:26:37 @agent_ppo2.py:179][0m |          -0.0186 |           3.5326 |         -14.9079 |
[32m[20221208 14:26:37 @agent_ppo2.py:179][0m |          -0.0161 |           3.5281 |         -14.2629 |
[32m[20221208 14:26:37 @agent_ppo2.py:179][0m |          -0.0235 |           3.4968 |         -14.8172 |
[32m[20221208 14:26:38 @agent_ppo2.py:179][0m |          -0.0263 |           3.4830 |         -15.3738 |
[32m[20221208 14:26:38 @agent_ppo2.py:179][0m |          -0.0246 |           3.4770 |         -15.6119 |
[32m[20221208 14:26:38 @agent_ppo2.py:179][0m |          -0.0209 |           3.4837 |         -15.2691 |
[32m[20221208 14:26:38 @agent_ppo2.py:179][0m |          -0.0235 |           3.5003 |         -15.5765 |
[32m[20221208 14:26:38 @agent_ppo2.py:179][0m |          -0.0238 |           3.4615 |         -15.9555 |
[32m[20221208 14:26:38 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:26:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.87
[32m[20221208 14:26:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.19
[32m[20221208 14:26:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.85
[32m[20221208 14:26:38 @agent_ppo2.py:137][0m Total time:      10.55 min
[32m[20221208 14:26:38 @agent_ppo2.py:139][0m 860160 total steps have happened
[32m[20221208 14:26:38 @agent_ppo2.py:115][0m #------------------------ Iteration 420 --------------------------#
[32m[20221208 14:26:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |           0.1094 |           5.7686 |         -46.9267 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |           0.0825 |           4.7671 |         -34.0502 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |           0.0204 |           4.1840 |         -39.3685 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0194 |           3.8469 |         -42.0951 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0357 |           3.5759 |         -44.4193 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0453 |           3.4143 |         -46.2795 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0490 |           3.2718 |         -46.4279 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0550 |           3.1671 |         -49.1907 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0618 |           3.0483 |         -52.2937 |
[32m[20221208 14:26:39 @agent_ppo2.py:179][0m |          -0.0671 |           2.9705 |         -55.9071 |
[32m[20221208 14:26:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.64
[32m[20221208 14:26:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.16
[32m[20221208 14:26:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.12
[32m[20221208 14:26:40 @agent_ppo2.py:137][0m Total time:      10.58 min
[32m[20221208 14:26:40 @agent_ppo2.py:139][0m 862208 total steps have happened
[32m[20221208 14:26:40 @agent_ppo2.py:115][0m #------------------------ Iteration 421 --------------------------#
[32m[20221208 14:26:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:40 @agent_ppo2.py:179][0m |           0.0385 |           5.7689 |         -50.2970 |
[32m[20221208 14:26:40 @agent_ppo2.py:179][0m |           0.0058 |           3.8221 |         -50.4356 |
[32m[20221208 14:26:40 @agent_ppo2.py:179][0m |          -0.0142 |           3.2405 |         -52.8841 |
[32m[20221208 14:26:40 @agent_ppo2.py:179][0m |          -0.0285 |           2.9250 |         -52.7117 |
[32m[20221208 14:26:40 @agent_ppo2.py:179][0m |          -0.0389 |           2.7111 |         -52.4469 |
[32m[20221208 14:26:40 @agent_ppo2.py:179][0m |          -0.0483 |           2.5550 |         -53.9392 |
[32m[20221208 14:26:41 @agent_ppo2.py:179][0m |          -0.0516 |           2.4198 |         -53.1382 |
[32m[20221208 14:26:41 @agent_ppo2.py:179][0m |          -0.0561 |           2.3123 |         -54.7591 |
[32m[20221208 14:26:41 @agent_ppo2.py:179][0m |          -0.0576 |           2.2408 |         -55.2488 |
[32m[20221208 14:26:41 @agent_ppo2.py:179][0m |          -0.0569 |           2.1481 |         -55.2693 |
[32m[20221208 14:26:41 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:26:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.77
[32m[20221208 14:26:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.62
[32m[20221208 14:26:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.07
[32m[20221208 14:26:41 @agent_ppo2.py:137][0m Total time:      10.60 min
[32m[20221208 14:26:41 @agent_ppo2.py:139][0m 864256 total steps have happened
[32m[20221208 14:26:41 @agent_ppo2.py:115][0m #------------------------ Iteration 422 --------------------------#
[32m[20221208 14:26:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |           0.0861 |           5.9129 |         -48.5173 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |           0.0650 |           5.0235 |         -44.5316 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |           0.0199 |           4.7466 |         -48.2623 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |           0.0201 |           4.5145 |         -44.9281 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |          -0.0040 |           4.3262 |         -46.2047 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |          -0.0205 |           4.1892 |         -50.3176 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |          -0.0358 |           4.0899 |         -52.3032 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |          -0.0355 |           3.9982 |         -54.7552 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |          -0.0427 |           3.9100 |         -55.1132 |
[32m[20221208 14:26:42 @agent_ppo2.py:179][0m |          -0.0512 |           3.8204 |         -58.1343 |
[32m[20221208 14:26:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.84
[32m[20221208 14:26:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.26
[32m[20221208 14:26:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.24
[32m[20221208 14:26:43 @agent_ppo2.py:137][0m Total time:      10.63 min
[32m[20221208 14:26:43 @agent_ppo2.py:139][0m 866304 total steps have happened
[32m[20221208 14:26:43 @agent_ppo2.py:115][0m #------------------------ Iteration 423 --------------------------#
[32m[20221208 14:26:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:43 @agent_ppo2.py:179][0m |           0.0794 |           5.5001 |         -48.0781 |
[32m[20221208 14:26:43 @agent_ppo2.py:179][0m |           0.0824 |           4.7929 |         -26.7325 |
[32m[20221208 14:26:43 @agent_ppo2.py:179][0m |           0.0025 |           4.4268 |         -31.1038 |
[32m[20221208 14:26:43 @agent_ppo2.py:179][0m |          -0.0299 |           4.2177 |         -35.9057 |
[32m[20221208 14:26:43 @agent_ppo2.py:179][0m |          -0.0437 |           4.0633 |         -37.1745 |
[32m[20221208 14:26:43 @agent_ppo2.py:179][0m |          -0.0505 |           3.9568 |         -37.7846 |
[32m[20221208 14:26:44 @agent_ppo2.py:179][0m |          -0.0543 |           3.8401 |         -39.5625 |
[32m[20221208 14:26:44 @agent_ppo2.py:179][0m |          -0.0601 |           3.7818 |         -39.3253 |
[32m[20221208 14:26:44 @agent_ppo2.py:179][0m |          -0.0649 |           3.6884 |         -40.1160 |
[32m[20221208 14:26:44 @agent_ppo2.py:179][0m |          -0.0684 |           3.6665 |         -40.8641 |
[32m[20221208 14:26:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.37
[32m[20221208 14:26:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.29
[32m[20221208 14:26:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.23
[32m[20221208 14:26:44 @agent_ppo2.py:137][0m Total time:      10.65 min
[32m[20221208 14:26:44 @agent_ppo2.py:139][0m 868352 total steps have happened
[32m[20221208 14:26:44 @agent_ppo2.py:115][0m #------------------------ Iteration 424 --------------------------#
[32m[20221208 14:26:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |           0.0696 |           4.1931 |         -39.0669 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |           0.0037 |           3.5809 |         -18.6361 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0216 |           3.2458 |         -19.3409 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0380 |           3.0185 |         -21.6115 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0495 |           2.8305 |         -22.7732 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0536 |           2.7368 |         -23.9328 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0605 |           2.6394 |         -24.4294 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0607 |           2.5468 |         -25.8951 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0634 |           2.4698 |         -26.7136 |
[32m[20221208 14:26:45 @agent_ppo2.py:179][0m |          -0.0696 |           2.4200 |         -27.4549 |
[32m[20221208 14:26:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:26:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.97
[32m[20221208 14:26:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.43
[32m[20221208 14:26:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.28
[32m[20221208 14:26:46 @agent_ppo2.py:137][0m Total time:      10.68 min
[32m[20221208 14:26:46 @agent_ppo2.py:139][0m 870400 total steps have happened
[32m[20221208 14:26:46 @agent_ppo2.py:115][0m #------------------------ Iteration 425 --------------------------#
[32m[20221208 14:26:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |           0.0273 |           1.0272 |         -61.1275 |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |           0.0626 |           0.8885 |         -48.8934 |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |          -0.0124 |           0.8508 |         -43.7421 |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |          -0.0062 |           0.8371 |         -44.1846 |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |          -0.0160 |           0.8162 |         -45.2766 |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |          -0.0181 |           0.8149 |         -46.0436 |
[32m[20221208 14:26:46 @agent_ppo2.py:179][0m |          -0.0236 |           0.8062 |         -46.2092 |
[32m[20221208 14:26:47 @agent_ppo2.py:179][0m |          -0.0329 |           0.8023 |         -47.2892 |
[32m[20221208 14:26:47 @agent_ppo2.py:179][0m |          -0.0080 |           0.8062 |         -45.5907 |
[32m[20221208 14:26:47 @agent_ppo2.py:179][0m |          -0.0246 |           0.7937 |         -47.4998 |
[32m[20221208 14:26:47 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:26:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.54
[32m[20221208 14:26:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.88
[32m[20221208 14:26:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.10
[32m[20221208 14:26:47 @agent_ppo2.py:137][0m Total time:      10.70 min
[32m[20221208 14:26:47 @agent_ppo2.py:139][0m 872448 total steps have happened
[32m[20221208 14:26:47 @agent_ppo2.py:115][0m #------------------------ Iteration 426 --------------------------#
[32m[20221208 14:26:48 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:26:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |           0.0818 |           8.2209 |         -63.4340 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |           0.0321 |           6.9860 |         -54.6775 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0064 |           6.4006 |         -58.4212 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0307 |           5.9938 |         -61.3973 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0496 |           5.7368 |         -64.1132 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0558 |           5.5225 |         -63.9822 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0555 |           5.3470 |         -64.3709 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0598 |           5.1411 |         -65.9538 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0682 |           5.0698 |         -68.0009 |
[32m[20221208 14:26:48 @agent_ppo2.py:179][0m |          -0.0749 |           4.9374 |         -69.5499 |
[32m[20221208 14:26:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.63
[32m[20221208 14:26:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.43
[32m[20221208 14:26:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.28
[32m[20221208 14:26:49 @agent_ppo2.py:137][0m Total time:      10.73 min
[32m[20221208 14:26:49 @agent_ppo2.py:139][0m 874496 total steps have happened
[32m[20221208 14:26:49 @agent_ppo2.py:115][0m #------------------------ Iteration 427 --------------------------#
[32m[20221208 14:26:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |           0.0523 |           5.7381 |         -39.9286 |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |           0.0198 |           5.1089 |         -18.6404 |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |          -0.0110 |           4.8060 |         -21.5916 |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |          -0.0328 |           4.6176 |         -24.6897 |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |          -0.0396 |           4.4756 |         -25.4923 |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |          -0.0484 |           4.3304 |         -26.9468 |
[32m[20221208 14:26:49 @agent_ppo2.py:179][0m |          -0.0552 |           4.2542 |         -28.1251 |
[32m[20221208 14:26:50 @agent_ppo2.py:179][0m |          -0.0595 |           4.1932 |         -28.7296 |
[32m[20221208 14:26:50 @agent_ppo2.py:179][0m |          -0.0556 |           4.0855 |         -28.5798 |
[32m[20221208 14:26:50 @agent_ppo2.py:179][0m |          -0.0409 |           4.0299 |         -27.2120 |
[32m[20221208 14:26:50 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:26:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.40
[32m[20221208 14:26:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.98
[32m[20221208 14:26:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.75
[32m[20221208 14:26:50 @agent_ppo2.py:137][0m Total time:      10.75 min
[32m[20221208 14:26:50 @agent_ppo2.py:139][0m 876544 total steps have happened
[32m[20221208 14:26:50 @agent_ppo2.py:115][0m #------------------------ Iteration 428 --------------------------#
[32m[20221208 14:26:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |           0.0866 |           8.2855 |         -68.0339 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |           0.0743 |           7.0020 |         -42.2607 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |           0.0026 |           6.3954 |         -53.3861 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0222 |           6.0428 |         -56.6576 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0483 |           5.8081 |         -60.4333 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0585 |           5.6160 |         -62.9316 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0622 |           5.4297 |         -63.7545 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0681 |           5.3083 |         -65.0065 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0716 |           5.2027 |         -66.4875 |
[32m[20221208 14:26:51 @agent_ppo2.py:179][0m |          -0.0781 |           5.0767 |         -68.2544 |
[32m[20221208 14:26:51 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.51
[32m[20221208 14:26:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.32
[32m[20221208 14:26:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 196.64
[32m[20221208 14:26:52 @agent_ppo2.py:137][0m Total time:      10.78 min
[32m[20221208 14:26:52 @agent_ppo2.py:139][0m 878592 total steps have happened
[32m[20221208 14:26:52 @agent_ppo2.py:115][0m #------------------------ Iteration 429 --------------------------#
[32m[20221208 14:26:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:26:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |           0.0659 |           6.3358 |         -55.5016 |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |           0.0445 |           5.5761 |         -46.1048 |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |           0.0045 |           5.1506 |         -56.3941 |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |           0.0079 |           4.9127 |         -62.9638 |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |          -0.0054 |           4.7567 |         -62.1615 |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |          -0.0255 |           4.6973 |         -64.0535 |
[32m[20221208 14:26:52 @agent_ppo2.py:179][0m |          -0.0339 |           4.5815 |         -69.2790 |
[32m[20221208 14:26:53 @agent_ppo2.py:179][0m |          -0.0455 |           4.4838 |         -72.4239 |
[32m[20221208 14:26:53 @agent_ppo2.py:179][0m |          -0.0490 |           4.4504 |         -73.2691 |
[32m[20221208 14:26:53 @agent_ppo2.py:179][0m |          -0.0503 |           4.3452 |         -73.6579 |
[32m[20221208 14:26:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.76
[32m[20221208 14:26:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.31
[32m[20221208 14:26:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.23
[32m[20221208 14:26:53 @agent_ppo2.py:137][0m Total time:      10.80 min
[32m[20221208 14:26:53 @agent_ppo2.py:139][0m 880640 total steps have happened
[32m[20221208 14:26:53 @agent_ppo2.py:115][0m #------------------------ Iteration 430 --------------------------#
[32m[20221208 14:26:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |           0.0750 |           9.1655 |         -77.8575 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |           0.0561 |           7.4649 |         -60.8622 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |           0.0025 |           7.0505 |         -77.7331 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0257 |           6.8093 |         -85.3978 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0362 |           6.6980 |         -89.1005 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0442 |           6.5860 |         -88.5345 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0501 |           6.4734 |         -91.5765 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0548 |           6.4100 |         -92.9411 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0563 |           6.3314 |         -93.1441 |
[32m[20221208 14:26:54 @agent_ppo2.py:179][0m |          -0.0590 |           6.2647 |         -94.9904 |
[32m[20221208 14:26:54 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:26:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.55
[32m[20221208 14:26:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.46
[32m[20221208 14:26:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.41
[32m[20221208 14:26:55 @agent_ppo2.py:137][0m Total time:      10.83 min
[32m[20221208 14:26:55 @agent_ppo2.py:139][0m 882688 total steps have happened
[32m[20221208 14:26:55 @agent_ppo2.py:115][0m #------------------------ Iteration 431 --------------------------#
[32m[20221208 14:26:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |           0.0723 |           5.1947 |         -78.7702 |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |           0.0388 |           4.6440 |         -73.0717 |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |          -0.0022 |           4.4319 |         -77.5415 |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |          -0.0299 |           4.2890 |         -81.1495 |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |          -0.0346 |           4.1969 |         -82.1500 |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |          -0.0445 |           4.0671 |         -84.7714 |
[32m[20221208 14:26:55 @agent_ppo2.py:179][0m |          -0.0562 |           3.9705 |         -86.3594 |
[32m[20221208 14:26:56 @agent_ppo2.py:179][0m |          -0.0537 |           3.8893 |         -87.2364 |
[32m[20221208 14:26:56 @agent_ppo2.py:179][0m |          -0.0658 |           3.8096 |         -87.3680 |
[32m[20221208 14:26:56 @agent_ppo2.py:179][0m |          -0.0648 |           3.7708 |         -89.0885 |
[32m[20221208 14:26:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.75
[32m[20221208 14:26:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 193.72
[32m[20221208 14:26:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.06
[32m[20221208 14:26:56 @agent_ppo2.py:137][0m Total time:      10.85 min
[32m[20221208 14:26:56 @agent_ppo2.py:139][0m 884736 total steps have happened
[32m[20221208 14:26:56 @agent_ppo2.py:115][0m #------------------------ Iteration 432 --------------------------#
[32m[20221208 14:26:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |           0.2246 |           5.0378 |         -84.0259 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |           0.0895 |           4.2232 |         -68.6422 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |           0.0299 |           3.8117 |         -74.2114 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0043 |           3.5724 |         -82.5000 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0239 |           3.3610 |         -87.5115 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0298 |           3.1540 |         -85.8232 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0359 |           3.0696 |         -88.1077 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0402 |           2.9548 |         -91.8626 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0436 |           2.8116 |         -90.9206 |
[32m[20221208 14:26:57 @agent_ppo2.py:179][0m |          -0.0486 |           2.6786 |         -96.1498 |
[32m[20221208 14:26:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.37
[32m[20221208 14:26:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.32
[32m[20221208 14:26:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.26
[32m[20221208 14:26:58 @agent_ppo2.py:137][0m Total time:      10.88 min
[32m[20221208 14:26:58 @agent_ppo2.py:139][0m 886784 total steps have happened
[32m[20221208 14:26:58 @agent_ppo2.py:115][0m #------------------------ Iteration 433 --------------------------#
[32m[20221208 14:26:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:26:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |           0.0546 |           8.3922 |         -70.3922 |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |           0.0441 |           7.3876 |         -63.7771 |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |          -0.0010 |           6.7945 |         -66.2228 |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |          -0.0266 |           6.5184 |         -68.0488 |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |          -0.0454 |           6.2336 |         -70.7881 |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |          -0.0494 |           6.0819 |         -68.9227 |
[32m[20221208 14:26:58 @agent_ppo2.py:179][0m |          -0.0542 |           5.9721 |         -71.9412 |
[32m[20221208 14:26:59 @agent_ppo2.py:179][0m |          -0.0624 |           5.8650 |         -74.1674 |
[32m[20221208 14:26:59 @agent_ppo2.py:179][0m |          -0.0656 |           5.7186 |         -76.4945 |
[32m[20221208 14:26:59 @agent_ppo2.py:179][0m |          -0.0702 |           5.6255 |         -77.9639 |
[32m[20221208 14:26:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:26:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.66
[32m[20221208 14:26:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.14
[32m[20221208 14:26:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.18
[32m[20221208 14:26:59 @agent_ppo2.py:137][0m Total time:      10.90 min
[32m[20221208 14:26:59 @agent_ppo2.py:139][0m 888832 total steps have happened
[32m[20221208 14:26:59 @agent_ppo2.py:115][0m #------------------------ Iteration 434 --------------------------#
[32m[20221208 14:26:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |           0.0448 |           8.2015 |         -85.8694 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |           0.0397 |           7.4342 |         -79.1541 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |           0.0018 |           7.1572 |         -88.0322 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0269 |           7.0103 |         -91.2040 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0334 |           6.8346 |         -93.9856 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0454 |           6.6650 |         -94.5825 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0502 |           6.5594 |         -96.5197 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0524 |           6.5261 |         -95.4094 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0556 |           6.4151 |         -97.9058 |
[32m[20221208 14:27:00 @agent_ppo2.py:179][0m |          -0.0568 |           6.3257 |         -99.1654 |
[32m[20221208 14:27:00 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:27:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.42
[32m[20221208 14:27:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 202.68
[32m[20221208 14:27:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.29
[32m[20221208 14:27:01 @agent_ppo2.py:137][0m Total time:      10.93 min
[32m[20221208 14:27:01 @agent_ppo2.py:139][0m 890880 total steps have happened
[32m[20221208 14:27:01 @agent_ppo2.py:115][0m #------------------------ Iteration 435 --------------------------#
[32m[20221208 14:27:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |           0.0562 |           2.3486 |         -88.7799 |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |           0.1073 |           1.7285 |         -41.4681 |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |           0.0833 |           1.5273 |         -37.5312 |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |           0.0014 |           1.4324 |         -87.7331 |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |          -0.0182 |           1.3568 |         -90.9498 |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |          -0.0178 |           1.2977 |         -91.8090 |
[32m[20221208 14:27:01 @agent_ppo2.py:179][0m |          -0.0179 |           1.2530 |         -91.0060 |
[32m[20221208 14:27:02 @agent_ppo2.py:179][0m |          -0.0178 |           1.2223 |         -88.9045 |
[32m[20221208 14:27:02 @agent_ppo2.py:179][0m |          -0.0344 |           1.1724 |         -93.0559 |
[32m[20221208 14:27:02 @agent_ppo2.py:179][0m |          -0.0385 |           1.1394 |         -94.7874 |
[32m[20221208 14:27:02 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.70
[32m[20221208 14:27:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.73
[32m[20221208 14:27:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.13
[32m[20221208 14:27:02 @agent_ppo2.py:137][0m Total time:      10.95 min
[32m[20221208 14:27:02 @agent_ppo2.py:139][0m 892928 total steps have happened
[32m[20221208 14:27:02 @agent_ppo2.py:115][0m #------------------------ Iteration 436 --------------------------#
[32m[20221208 14:27:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |           0.0979 |           4.8956 |         -34.3330 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |           0.0671 |           4.4338 |         -15.9908 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |           0.0170 |           4.2774 |         -21.3124 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0062 |           4.1982 |         -27.6226 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0194 |           4.1336 |         -29.9217 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0261 |           4.0852 |         -30.7793 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0282 |           4.0662 |         -32.5787 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0333 |           4.0739 |         -33.7585 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0394 |           3.9860 |         -35.1280 |
[32m[20221208 14:27:03 @agent_ppo2.py:179][0m |          -0.0402 |           3.9243 |         -35.7530 |
[32m[20221208 14:27:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.76
[32m[20221208 14:27:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.72
[32m[20221208 14:27:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.39
[32m[20221208 14:27:04 @agent_ppo2.py:137][0m Total time:      10.98 min
[32m[20221208 14:27:04 @agent_ppo2.py:139][0m 894976 total steps have happened
[32m[20221208 14:27:04 @agent_ppo2.py:115][0m #------------------------ Iteration 437 --------------------------#
[32m[20221208 14:27:04 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:27:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |           0.0746 |           4.5891 |         -93.4728 |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |           0.0658 |           3.9508 |         -77.5285 |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |           0.0164 |           3.6537 |         -91.8099 |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |          -0.0076 |           3.3871 |         -95.9817 |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |          -0.0214 |           3.2388 |        -100.0129 |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |          -0.0335 |           3.1083 |        -103.8510 |
[32m[20221208 14:27:04 @agent_ppo2.py:179][0m |          -0.0300 |           2.9841 |        -104.5065 |
[32m[20221208 14:27:05 @agent_ppo2.py:179][0m |          -0.0408 |           2.9274 |        -104.8279 |
[32m[20221208 14:27:05 @agent_ppo2.py:179][0m |          -0.0488 |           2.8355 |        -109.4053 |
[32m[20221208 14:27:05 @agent_ppo2.py:179][0m |          -0.0512 |           2.7937 |        -111.6611 |
[32m[20221208 14:27:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.64
[32m[20221208 14:27:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.73
[32m[20221208 14:27:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.70
[32m[20221208 14:27:05 @agent_ppo2.py:137][0m Total time:      11.00 min
[32m[20221208 14:27:05 @agent_ppo2.py:139][0m 897024 total steps have happened
[32m[20221208 14:27:05 @agent_ppo2.py:115][0m #------------------------ Iteration 438 --------------------------#
[32m[20221208 14:27:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |           0.0661 |           7.2625 |         -91.0118 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |           0.0424 |           6.7194 |         -82.6042 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0008 |           6.5069 |         -99.3043 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0131 |           6.3513 |        -103.5055 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0301 |           6.2907 |        -107.8983 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0374 |           6.1311 |        -112.0061 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0417 |           6.0502 |        -110.6715 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0443 |           5.9834 |        -114.0800 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0421 |           5.9249 |        -111.6568 |
[32m[20221208 14:27:06 @agent_ppo2.py:179][0m |          -0.0496 |           5.8757 |        -117.2743 |
[32m[20221208 14:27:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.25
[32m[20221208 14:27:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.82
[32m[20221208 14:27:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.80
[32m[20221208 14:27:07 @agent_ppo2.py:137][0m Total time:      11.03 min
[32m[20221208 14:27:07 @agent_ppo2.py:139][0m 899072 total steps have happened
[32m[20221208 14:27:07 @agent_ppo2.py:115][0m #------------------------ Iteration 439 --------------------------#
[32m[20221208 14:27:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |           0.0717 |           4.1360 |         -68.7252 |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |           0.0305 |           3.5899 |         -52.9229 |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |          -0.0042 |           3.3817 |         -66.1307 |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |          -0.0200 |           3.2530 |         -69.7666 |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |          -0.0296 |           3.1515 |         -73.5483 |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |          -0.0267 |           3.0641 |         -70.9283 |
[32m[20221208 14:27:07 @agent_ppo2.py:179][0m |          -0.0388 |           2.9932 |         -75.9010 |
[32m[20221208 14:27:08 @agent_ppo2.py:179][0m |          -0.0460 |           2.9261 |         -79.1728 |
[32m[20221208 14:27:08 @agent_ppo2.py:179][0m |          -0.0483 |           2.9150 |         -79.9998 |
[32m[20221208 14:27:08 @agent_ppo2.py:179][0m |          -0.0483 |           2.8082 |         -80.2283 |
[32m[20221208 14:27:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.02
[32m[20221208 14:27:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.78
[32m[20221208 14:27:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.04
[32m[20221208 14:27:08 @agent_ppo2.py:137][0m Total time:      11.05 min
[32m[20221208 14:27:08 @agent_ppo2.py:139][0m 901120 total steps have happened
[32m[20221208 14:27:08 @agent_ppo2.py:115][0m #------------------------ Iteration 440 --------------------------#
[32m[20221208 14:27:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0950 |           1.2090 |         -71.9761 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0635 |           1.1082 |         -34.2328 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0441 |           1.0880 |         -35.1494 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0446 |           1.0780 |         -35.6505 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0658 |           1.0822 |         -31.6319 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0485 |           1.0597 |         -34.2507 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0448 |           1.0769 |         -35.2578 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0421 |           1.0568 |         -34.4675 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0451 |           1.0788 |         -34.8383 |
[32m[20221208 14:27:09 @agent_ppo2.py:179][0m |           0.0453 |           1.0502 |         -35.3914 |
[32m[20221208 14:27:09 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.29
[32m[20221208 14:27:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.04
[32m[20221208 14:27:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.70
[32m[20221208 14:27:10 @agent_ppo2.py:137][0m Total time:      11.08 min
[32m[20221208 14:27:10 @agent_ppo2.py:139][0m 903168 total steps have happened
[32m[20221208 14:27:10 @agent_ppo2.py:115][0m #------------------------ Iteration 441 --------------------------#
[32m[20221208 14:27:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |           0.0270 |           0.9352 |         -43.0166 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0013 |           0.7618 |         -31.4928 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0089 |           0.7375 |         -32.2146 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0089 |           0.7298 |         -31.7393 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0093 |           0.7242 |         -33.0508 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0092 |           0.7220 |         -33.2381 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0129 |           0.7152 |         -32.9763 |
[32m[20221208 14:27:10 @agent_ppo2.py:179][0m |          -0.0143 |           0.7132 |         -33.1417 |
[32m[20221208 14:27:11 @agent_ppo2.py:179][0m |          -0.0104 |           0.7137 |         -33.4410 |
[32m[20221208 14:27:11 @agent_ppo2.py:179][0m |          -0.0106 |           0.7111 |         -33.1619 |
[32m[20221208 14:27:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.56
[32m[20221208 14:27:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.15
[32m[20221208 14:27:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.42
[32m[20221208 14:27:11 @agent_ppo2.py:137][0m Total time:      11.10 min
[32m[20221208 14:27:11 @agent_ppo2.py:139][0m 905216 total steps have happened
[32m[20221208 14:27:11 @agent_ppo2.py:115][0m #------------------------ Iteration 442 --------------------------#
[32m[20221208 14:27:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0357 |           1.2936 |         -97.6509 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0230 |           0.8003 |         -67.2106 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0121 |           0.7123 |         -68.8570 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0230 |           0.6861 |         -62.6947 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0126 |           0.6796 |         -69.1741 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0223 |           0.6731 |         -68.0148 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0114 |           0.6709 |         -67.8897 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0070 |           0.6686 |         -70.0431 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0060 |           0.6740 |         -71.5000 |
[32m[20221208 14:27:12 @agent_ppo2.py:179][0m |           0.0113 |           0.6690 |         -71.6109 |
[32m[20221208 14:27:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.41
[32m[20221208 14:27:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.72
[32m[20221208 14:27:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.87
[32m[20221208 14:27:12 @agent_ppo2.py:137][0m Total time:      11.13 min
[32m[20221208 14:27:12 @agent_ppo2.py:139][0m 907264 total steps have happened
[32m[20221208 14:27:12 @agent_ppo2.py:115][0m #------------------------ Iteration 443 --------------------------#
[32m[20221208 14:27:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |           0.0485 |           3.4421 |        -107.8216 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |           0.0389 |           2.9529 |         -72.4144 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |           0.0042 |           2.7853 |         -66.2700 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |          -0.0156 |           2.6852 |         -71.0209 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |          -0.0362 |           2.5935 |         -72.7761 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |          -0.0399 |           2.5164 |         -76.1661 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |          -0.0475 |           2.4673 |         -78.0689 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |          -0.0507 |           2.3949 |         -79.1678 |
[32m[20221208 14:27:13 @agent_ppo2.py:179][0m |          -0.0559 |           2.3581 |         -82.0030 |
[32m[20221208 14:27:14 @agent_ppo2.py:179][0m |          -0.0568 |           2.3361 |         -85.3827 |
[32m[20221208 14:27:14 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.11
[32m[20221208 14:27:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.34
[32m[20221208 14:27:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.41
[32m[20221208 14:27:14 @agent_ppo2.py:137][0m Total time:      11.15 min
[32m[20221208 14:27:14 @agent_ppo2.py:139][0m 909312 total steps have happened
[32m[20221208 14:27:14 @agent_ppo2.py:115][0m #------------------------ Iteration 444 --------------------------#
[32m[20221208 14:27:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |           0.0544 |           2.1890 |        -108.3835 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |           0.0908 |           1.7192 |         -73.1124 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |           0.0074 |           1.5197 |         -69.6965 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0173 |           1.4078 |         -72.9662 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0269 |           1.3384 |         -75.2228 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0402 |           1.2706 |         -80.2205 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0510 |           1.2281 |         -86.2299 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0571 |           1.1876 |         -86.8253 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0613 |           1.1549 |         -90.5501 |
[32m[20221208 14:27:15 @agent_ppo2.py:179][0m |          -0.0571 |           1.1332 |         -88.3729 |
[32m[20221208 14:27:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.19
[32m[20221208 14:27:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.31
[32m[20221208 14:27:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.08
[32m[20221208 14:27:15 @agent_ppo2.py:137][0m Total time:      11.18 min
[32m[20221208 14:27:15 @agent_ppo2.py:139][0m 911360 total steps have happened
[32m[20221208 14:27:15 @agent_ppo2.py:115][0m #------------------------ Iteration 445 --------------------------#
[32m[20221208 14:27:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |           0.0698 |           1.3863 |        -115.0363 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |           0.0360 |           1.2375 |         -79.1419 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0099 |           1.1624 |         -85.2473 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0245 |           1.1148 |         -89.6221 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0345 |           1.0668 |         -89.8509 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0395 |           1.0472 |         -93.9551 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0398 |           1.0194 |         -93.8962 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0452 |           0.9868 |         -98.7071 |
[32m[20221208 14:27:16 @agent_ppo2.py:179][0m |          -0.0446 |           0.9706 |         -96.5757 |
[32m[20221208 14:27:17 @agent_ppo2.py:179][0m |          -0.0488 |           0.9565 |        -100.3591 |
[32m[20221208 14:27:17 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.04
[32m[20221208 14:27:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.94
[32m[20221208 14:27:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.38
[32m[20221208 14:27:17 @agent_ppo2.py:137][0m Total time:      11.20 min
[32m[20221208 14:27:17 @agent_ppo2.py:139][0m 913408 total steps have happened
[32m[20221208 14:27:17 @agent_ppo2.py:115][0m #------------------------ Iteration 446 --------------------------#
[32m[20221208 14:27:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:17 @agent_ppo2.py:179][0m |           0.0816 |           4.3019 |        -116.6928 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |           0.0430 |           3.8874 |         -65.2201 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |           0.0270 |           3.7418 |         -43.5714 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |           0.0014 |           3.6054 |         -67.4166 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |           0.0143 |           3.5291 |         -69.5877 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |           0.0135 |           3.5773 |         -74.6400 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |          -0.0007 |           3.3996 |         -38.5048 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |          -0.0136 |           3.3567 |         -39.5940 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |          -0.0218 |           3.3189 |         -41.4362 |
[32m[20221208 14:27:18 @agent_ppo2.py:179][0m |          -0.0195 |           3.3035 |         -42.3884 |
[32m[20221208 14:27:18 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.76
[32m[20221208 14:27:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.58
[32m[20221208 14:27:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.52
[32m[20221208 14:27:18 @agent_ppo2.py:137][0m Total time:      11.23 min
[32m[20221208 14:27:18 @agent_ppo2.py:139][0m 915456 total steps have happened
[32m[20221208 14:27:18 @agent_ppo2.py:115][0m #------------------------ Iteration 447 --------------------------#
[32m[20221208 14:27:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0249 |           0.6242 |         -65.4082 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0215 |           0.5711 |         -39.3367 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0162 |           0.5567 |         -39.8971 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0024 |           0.5443 |         -41.2075 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0046 |           0.5392 |         -40.4427 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0019 |           0.5363 |         -41.5530 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0036 |           0.5315 |         -42.3781 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0006 |           0.5301 |         -42.9551 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0042 |           0.5285 |         -42.1902 |
[32m[20221208 14:27:19 @agent_ppo2.py:179][0m |           0.0020 |           0.5278 |         -44.5893 |
[32m[20221208 14:27:19 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:27:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.15
[32m[20221208 14:27:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.71
[32m[20221208 14:27:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.46
[32m[20221208 14:27:20 @agent_ppo2.py:137][0m Total time:      11.25 min
[32m[20221208 14:27:20 @agent_ppo2.py:139][0m 917504 total steps have happened
[32m[20221208 14:27:20 @agent_ppo2.py:115][0m #------------------------ Iteration 448 --------------------------#
[32m[20221208 14:27:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:20 @agent_ppo2.py:179][0m |           0.0884 |           0.6681 |         -73.4981 |
[32m[20221208 14:27:20 @agent_ppo2.py:179][0m |           0.0846 |           0.5935 |         -39.9791 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0739 |           0.5776 |         -40.8110 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0889 |           0.5718 |         -34.7774 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0666 |           0.5641 |         -44.1083 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0723 |           0.5569 |         -42.9411 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0645 |           0.5580 |         -44.2730 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0661 |           0.5562 |         -42.9864 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0869 |           0.5512 |         -36.2692 |
[32m[20221208 14:27:21 @agent_ppo2.py:179][0m |           0.0697 |           0.5489 |         -40.3109 |
[32m[20221208 14:27:21 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:27:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.70
[32m[20221208 14:27:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.88
[32m[20221208 14:27:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.28
[32m[20221208 14:27:21 @agent_ppo2.py:137][0m Total time:      11.27 min
[32m[20221208 14:27:21 @agent_ppo2.py:139][0m 919552 total steps have happened
[32m[20221208 14:27:21 @agent_ppo2.py:115][0m #------------------------ Iteration 449 --------------------------#
[32m[20221208 14:27:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |           0.0986 |           6.3923 |        -120.9067 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |           0.0662 |           4.9130 |         -82.3358 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |           0.0163 |           4.5149 |        -112.1673 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0150 |           4.2665 |        -129.0790 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0271 |           4.1853 |        -134.2480 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0318 |           3.9599 |        -137.4944 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0440 |           3.8979 |        -142.4554 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0413 |           3.7593 |        -142.9135 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0474 |           3.7124 |        -143.3915 |
[32m[20221208 14:27:22 @agent_ppo2.py:179][0m |          -0.0521 |           3.6479 |        -143.3902 |
[32m[20221208 14:27:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.04
[32m[20221208 14:27:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.43
[32m[20221208 14:27:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.78
[32m[20221208 14:27:23 @agent_ppo2.py:137][0m Total time:      11.30 min
[32m[20221208 14:27:23 @agent_ppo2.py:139][0m 921600 total steps have happened
[32m[20221208 14:27:23 @agent_ppo2.py:115][0m #------------------------ Iteration 450 --------------------------#
[32m[20221208 14:27:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:23 @agent_ppo2.py:179][0m |           0.1233 |           8.1798 |         -95.5644 |
[32m[20221208 14:27:23 @agent_ppo2.py:179][0m |           0.1152 |           6.5143 |         -49.0960 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |           0.0581 |           5.9429 |         -53.8731 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |           0.0247 |           5.5768 |         -53.1136 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |          -0.0006 |           5.3541 |         -64.5090 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |          -0.0145 |           5.1597 |         -67.7596 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |          -0.0302 |           4.9406 |         -75.6351 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |          -0.0351 |           4.8735 |         -77.5888 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |          -0.0437 |           4.6298 |         -78.2437 |
[32m[20221208 14:27:24 @agent_ppo2.py:179][0m |          -0.0481 |           4.5346 |         -83.7382 |
[32m[20221208 14:27:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.09
[32m[20221208 14:27:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.99
[32m[20221208 14:27:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.45
[32m[20221208 14:27:24 @agent_ppo2.py:137][0m Total time:      11.32 min
[32m[20221208 14:27:24 @agent_ppo2.py:139][0m 923648 total steps have happened
[32m[20221208 14:27:24 @agent_ppo2.py:115][0m #------------------------ Iteration 451 --------------------------#
[32m[20221208 14:27:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:27:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |           0.0765 |           5.6293 |        -127.2329 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |           0.0348 |           4.2052 |        -120.0067 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |           0.0264 |           3.7141 |        -123.7468 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0024 |           3.4237 |        -129.3807 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0247 |           3.2023 |        -134.5751 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0361 |           3.0459 |        -143.3366 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0391 |           2.9055 |        -143.1121 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0483 |           2.8065 |        -148.5747 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0525 |           2.7172 |        -150.1752 |
[32m[20221208 14:27:25 @agent_ppo2.py:179][0m |          -0.0502 |           2.6481 |        -149.4978 |
[32m[20221208 14:27:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.20
[32m[20221208 14:27:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.48
[32m[20221208 14:27:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.84
[32m[20221208 14:27:26 @agent_ppo2.py:137][0m Total time:      11.35 min
[32m[20221208 14:27:26 @agent_ppo2.py:139][0m 925696 total steps have happened
[32m[20221208 14:27:26 @agent_ppo2.py:115][0m #------------------------ Iteration 452 --------------------------#
[32m[20221208 14:27:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:26 @agent_ppo2.py:179][0m |           0.0680 |           6.8516 |        -131.2950 |
[32m[20221208 14:27:26 @agent_ppo2.py:179][0m |           0.0239 |           6.0659 |        -124.1184 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0044 |           5.8669 |        -128.0684 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0240 |           5.6966 |        -132.7471 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0256 |           5.5283 |        -132.7587 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0396 |           5.4444 |        -139.9077 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0481 |           5.3582 |        -141.4170 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0518 |           5.2966 |        -140.5439 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0547 |           5.2559 |        -143.2519 |
[32m[20221208 14:27:27 @agent_ppo2.py:179][0m |          -0.0546 |           5.2223 |        -145.1734 |
[32m[20221208 14:27:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:27:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.16
[32m[20221208 14:27:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.68
[32m[20221208 14:27:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.04
[32m[20221208 14:27:27 @agent_ppo2.py:137][0m Total time:      11.37 min
[32m[20221208 14:27:27 @agent_ppo2.py:139][0m 927744 total steps have happened
[32m[20221208 14:27:27 @agent_ppo2.py:115][0m #------------------------ Iteration 453 --------------------------#
[32m[20221208 14:27:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |           0.0665 |           4.3227 |        -126.7966 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |           0.0599 |           3.9501 |        -116.1110 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |           0.0092 |           3.7620 |        -130.0425 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0129 |           3.6411 |        -135.9669 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0253 |           3.5697 |        -136.7903 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0345 |           3.5002 |        -142.6782 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0330 |           3.4321 |        -146.9835 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0293 |           3.3936 |        -143.5079 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0403 |           3.3217 |        -147.3756 |
[32m[20221208 14:27:28 @agent_ppo2.py:179][0m |          -0.0465 |           3.2881 |        -147.8247 |
[32m[20221208 14:27:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.79
[32m[20221208 14:27:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.37
[32m[20221208 14:27:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.22
[32m[20221208 14:27:29 @agent_ppo2.py:137][0m Total time:      11.40 min
[32m[20221208 14:27:29 @agent_ppo2.py:139][0m 929792 total steps have happened
[32m[20221208 14:27:29 @agent_ppo2.py:115][0m #------------------------ Iteration 454 --------------------------#
[32m[20221208 14:27:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:29 @agent_ppo2.py:179][0m |           0.0527 |           4.2616 |        -122.1770 |
[32m[20221208 14:27:29 @agent_ppo2.py:179][0m |           0.0186 |           3.6277 |        -116.4218 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |           0.0064 |           3.4567 |        -123.3220 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0085 |           3.3613 |        -125.9568 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0117 |           3.2666 |        -112.5398 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0230 |           3.2087 |        -124.5814 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0395 |           3.1470 |        -134.6417 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0408 |           3.1080 |        -135.2966 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0418 |           3.0627 |        -132.0668 |
[32m[20221208 14:27:30 @agent_ppo2.py:179][0m |          -0.0482 |           3.0019 |        -139.4470 |
[32m[20221208 14:27:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.77
[32m[20221208 14:27:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.31
[32m[20221208 14:27:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.44
[32m[20221208 14:27:30 @agent_ppo2.py:137][0m Total time:      11.42 min
[32m[20221208 14:27:30 @agent_ppo2.py:139][0m 931840 total steps have happened
[32m[20221208 14:27:30 @agent_ppo2.py:115][0m #------------------------ Iteration 455 --------------------------#
[32m[20221208 14:27:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |           0.0767 |           6.7139 |        -123.0843 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |           0.0245 |           6.0673 |        -106.0087 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |           0.0204 |           5.9834 |        -103.0013 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0055 |           5.7445 |        -114.7435 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0191 |           5.6443 |        -116.6732 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0305 |           5.5558 |        -121.6574 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0342 |           5.4605 |        -119.9846 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0438 |           5.3960 |        -133.1189 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0406 |           5.3368 |        -130.0755 |
[32m[20221208 14:27:31 @agent_ppo2.py:179][0m |          -0.0495 |           5.2415 |        -133.6049 |
[32m[20221208 14:27:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.40
[32m[20221208 14:27:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.46
[32m[20221208 14:27:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.14
[32m[20221208 14:27:32 @agent_ppo2.py:137][0m Total time:      11.45 min
[32m[20221208 14:27:32 @agent_ppo2.py:139][0m 933888 total steps have happened
[32m[20221208 14:27:32 @agent_ppo2.py:115][0m #------------------------ Iteration 456 --------------------------#
[32m[20221208 14:27:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:32 @agent_ppo2.py:179][0m |           0.0333 |           1.1181 |        -139.1398 |
[32m[20221208 14:27:32 @agent_ppo2.py:179][0m |           0.0649 |           0.8483 |         -95.3804 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |           0.0682 |           0.7800 |        -112.4248 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |           0.0361 |           0.7606 |        -108.2108 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |           0.0429 |           0.7373 |        -106.4703 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |          -0.0029 |           0.7319 |        -137.0348 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |           0.0117 |           0.7149 |        -132.9666 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |           0.0143 |           0.7014 |        -119.9870 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |           0.0205 |           0.6940 |        -127.1064 |
[32m[20221208 14:27:33 @agent_ppo2.py:179][0m |          -0.0044 |           0.6872 |        -142.3730 |
[32m[20221208 14:27:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.27
[32m[20221208 14:27:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 27.97
[32m[20221208 14:27:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.98
[32m[20221208 14:27:33 @agent_ppo2.py:137][0m Total time:      11.47 min
[32m[20221208 14:27:33 @agent_ppo2.py:139][0m 935936 total steps have happened
[32m[20221208 14:27:33 @agent_ppo2.py:115][0m #------------------------ Iteration 457 --------------------------#
[32m[20221208 14:27:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |           0.0714 |           3.4006 |        -117.2632 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |           0.0805 |           2.4869 |         -95.1134 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |           0.0359 |           2.1856 |         -76.1639 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |           0.0086 |           2.0015 |         -86.1262 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |          -0.0149 |           1.8865 |         -99.6656 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |          -0.0368 |           1.7693 |        -113.8654 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |          -0.0332 |           1.7028 |        -121.0387 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |          -0.0392 |           1.6204 |        -124.6952 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |          -0.0497 |           1.5567 |        -127.6186 |
[32m[20221208 14:27:34 @agent_ppo2.py:179][0m |          -0.0512 |           1.5113 |        -132.6171 |
[32m[20221208 14:27:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.50
[32m[20221208 14:27:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.13
[32m[20221208 14:27:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.28
[32m[20221208 14:27:35 @agent_ppo2.py:137][0m Total time:      11.50 min
[32m[20221208 14:27:35 @agent_ppo2.py:139][0m 937984 total steps have happened
[32m[20221208 14:27:35 @agent_ppo2.py:115][0m #------------------------ Iteration 458 --------------------------#
[32m[20221208 14:27:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:35 @agent_ppo2.py:179][0m |           0.0745 |           4.1627 |        -117.5076 |
[32m[20221208 14:27:35 @agent_ppo2.py:179][0m |           0.0425 |           3.2374 |        -104.9420 |
[32m[20221208 14:27:35 @agent_ppo2.py:179][0m |           0.0031 |           2.9109 |        -127.2362 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0141 |           2.7476 |        -131.6786 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0244 |           2.6309 |        -132.3123 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0311 |           2.5444 |        -136.6062 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0342 |           2.4794 |        -135.1522 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0423 |           2.4207 |        -141.1625 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0398 |           2.3570 |        -141.4224 |
[32m[20221208 14:27:36 @agent_ppo2.py:179][0m |          -0.0427 |           2.2988 |        -140.5854 |
[32m[20221208 14:27:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.83
[32m[20221208 14:27:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.59
[32m[20221208 14:27:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.07
[32m[20221208 14:27:36 @agent_ppo2.py:137][0m Total time:      11.52 min
[32m[20221208 14:27:36 @agent_ppo2.py:139][0m 940032 total steps have happened
[32m[20221208 14:27:36 @agent_ppo2.py:115][0m #------------------------ Iteration 459 --------------------------#
[32m[20221208 14:27:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:27:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |           0.0682 |           4.7301 |        -132.3868 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |           0.0544 |           3.9552 |        -117.7277 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |           0.0040 |           3.6704 |        -133.1520 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0238 |           3.6180 |        -138.9859 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0378 |           3.5030 |        -141.7588 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0428 |           3.4224 |        -144.6783 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0508 |           3.3151 |        -146.1299 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0561 |           3.2835 |        -148.2787 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0605 |           3.2271 |        -146.8476 |
[32m[20221208 14:27:37 @agent_ppo2.py:179][0m |          -0.0595 |           3.1586 |        -151.6810 |
[32m[20221208 14:27:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.42
[32m[20221208 14:27:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.30
[32m[20221208 14:27:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.70
[32m[20221208 14:27:38 @agent_ppo2.py:137][0m Total time:      11.55 min
[32m[20221208 14:27:38 @agent_ppo2.py:139][0m 942080 total steps have happened
[32m[20221208 14:27:38 @agent_ppo2.py:115][0m #------------------------ Iteration 460 --------------------------#
[32m[20221208 14:27:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:38 @agent_ppo2.py:179][0m |           0.0598 |           4.5868 |        -128.0946 |
[32m[20221208 14:27:38 @agent_ppo2.py:179][0m |           0.0512 |           4.0271 |         -83.0996 |
[32m[20221208 14:27:38 @agent_ppo2.py:179][0m |           0.0084 |           3.7169 |         -91.5286 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0149 |           3.5800 |         -96.7203 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0289 |           3.4794 |        -100.4877 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0389 |           3.3526 |        -101.2561 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0424 |           3.3187 |        -104.4937 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0474 |           3.2719 |        -105.5492 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0542 |           3.1650 |        -106.1947 |
[32m[20221208 14:27:39 @agent_ppo2.py:179][0m |          -0.0536 |           3.1435 |        -108.5731 |
[32m[20221208 14:27:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.12
[32m[20221208 14:27:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.87
[32m[20221208 14:27:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.19
[32m[20221208 14:27:39 @agent_ppo2.py:137][0m Total time:      11.57 min
[32m[20221208 14:27:39 @agent_ppo2.py:139][0m 944128 total steps have happened
[32m[20221208 14:27:39 @agent_ppo2.py:115][0m #------------------------ Iteration 461 --------------------------#
[32m[20221208 14:27:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |           0.0722 |           4.1230 |        -130.3754 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |           0.2507 |           3.3122 |        -107.9527 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |           0.0148 |           2.9921 |        -100.9466 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0261 |           2.8024 |        -103.6485 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0349 |           2.6708 |        -109.3173 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0400 |           2.5072 |        -105.7878 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0472 |           2.4310 |        -112.4373 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0573 |           2.3263 |        -115.4297 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0621 |           2.2490 |        -113.6551 |
[32m[20221208 14:27:40 @agent_ppo2.py:179][0m |          -0.0634 |           2.1573 |        -115.5395 |
[32m[20221208 14:27:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.56
[32m[20221208 14:27:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.71
[32m[20221208 14:27:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.48
[32m[20221208 14:27:41 @agent_ppo2.py:137][0m Total time:      11.60 min
[32m[20221208 14:27:41 @agent_ppo2.py:139][0m 946176 total steps have happened
[32m[20221208 14:27:41 @agent_ppo2.py:115][0m #------------------------ Iteration 462 --------------------------#
[32m[20221208 14:27:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:41 @agent_ppo2.py:179][0m |           0.0624 |           4.2604 |        -134.4968 |
[32m[20221208 14:27:41 @agent_ppo2.py:179][0m |           0.0348 |           3.3950 |        -129.8547 |
[32m[20221208 14:27:41 @agent_ppo2.py:179][0m |           0.0104 |           3.1309 |        -124.1291 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0122 |           2.9626 |        -133.0766 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0189 |           2.8941 |        -127.8976 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0320 |           2.7829 |        -131.5221 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0379 |           2.7037 |        -140.6349 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0363 |           2.6627 |        -106.8321 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0464 |           2.5928 |         -97.8008 |
[32m[20221208 14:27:42 @agent_ppo2.py:179][0m |          -0.0464 |           2.5402 |         -86.2807 |
[32m[20221208 14:27:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.48
[32m[20221208 14:27:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.93
[32m[20221208 14:27:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.44
[32m[20221208 14:27:42 @agent_ppo2.py:137][0m Total time:      11.62 min
[32m[20221208 14:27:42 @agent_ppo2.py:139][0m 948224 total steps have happened
[32m[20221208 14:27:42 @agent_ppo2.py:115][0m #------------------------ Iteration 463 --------------------------#
[32m[20221208 14:27:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |           0.0810 |           6.6107 |        -130.1511 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |           0.0446 |           5.6155 |        -116.7834 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |           0.0043 |           5.2647 |        -116.6765 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0319 |           5.0232 |        -130.2771 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0402 |           4.8937 |        -131.7027 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0522 |           4.6945 |        -137.5205 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0523 |           4.5775 |        -141.1201 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0614 |           4.4836 |        -141.8683 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0557 |           4.4134 |        -140.1542 |
[32m[20221208 14:27:43 @agent_ppo2.py:179][0m |          -0.0660 |           4.3106 |        -146.3462 |
[32m[20221208 14:27:43 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:27:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.82
[32m[20221208 14:27:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.91
[32m[20221208 14:27:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.20
[32m[20221208 14:27:44 @agent_ppo2.py:137][0m Total time:      11.65 min
[32m[20221208 14:27:44 @agent_ppo2.py:139][0m 950272 total steps have happened
[32m[20221208 14:27:44 @agent_ppo2.py:115][0m #------------------------ Iteration 464 --------------------------#
[32m[20221208 14:27:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:44 @agent_ppo2.py:179][0m |           0.0617 |           8.2125 |        -148.5048 |
[32m[20221208 14:27:44 @agent_ppo2.py:179][0m |           0.0171 |           7.0676 |        -141.8255 |
[32m[20221208 14:27:44 @agent_ppo2.py:179][0m |          -0.0076 |           6.7527 |        -142.9823 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0192 |           6.6205 |        -141.4959 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0472 |           6.4966 |        -149.0384 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0584 |           6.3867 |        -151.2561 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0643 |           6.2650 |        -152.5844 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0725 |           6.2413 |        -156.6989 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0731 |           6.1844 |        -158.4148 |
[32m[20221208 14:27:45 @agent_ppo2.py:179][0m |          -0.0797 |           6.1169 |        -160.9865 |
[32m[20221208 14:27:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.21
[32m[20221208 14:27:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.00
[32m[20221208 14:27:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.53
[32m[20221208 14:27:45 @agent_ppo2.py:137][0m Total time:      11.67 min
[32m[20221208 14:27:45 @agent_ppo2.py:139][0m 952320 total steps have happened
[32m[20221208 14:27:45 @agent_ppo2.py:115][0m #------------------------ Iteration 465 --------------------------#
[32m[20221208 14:27:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |           0.0574 |           4.4665 |        -141.5768 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |           0.0438 |           4.1352 |        -123.3169 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0080 |           3.9419 |        -132.9377 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0283 |           3.8078 |        -140.5629 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0426 |           3.7307 |        -143.2345 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0533 |           3.6328 |        -143.9451 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0602 |           3.5774 |        -146.3495 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0598 |           3.5161 |        -148.0032 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0602 |           3.4735 |        -146.3711 |
[32m[20221208 14:27:46 @agent_ppo2.py:179][0m |          -0.0660 |           3.4188 |        -146.6119 |
[32m[20221208 14:27:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.34
[32m[20221208 14:27:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.92
[32m[20221208 14:27:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.11
[32m[20221208 14:27:47 @agent_ppo2.py:137][0m Total time:      11.70 min
[32m[20221208 14:27:47 @agent_ppo2.py:139][0m 954368 total steps have happened
[32m[20221208 14:27:47 @agent_ppo2.py:115][0m #------------------------ Iteration 466 --------------------------#
[32m[20221208 14:27:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:47 @agent_ppo2.py:179][0m |           0.0762 |           4.3694 |        -131.3986 |
[32m[20221208 14:27:47 @agent_ppo2.py:179][0m |           0.1019 |           3.9085 |         -70.9867 |
[32m[20221208 14:27:47 @agent_ppo2.py:179][0m |           0.0116 |           3.7042 |         -81.0498 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0200 |           3.5744 |         -85.2402 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0328 |           3.4410 |         -89.8217 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0492 |           3.3564 |         -94.1775 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0574 |           3.2647 |         -96.6579 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0601 |           3.1903 |         -98.0814 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0646 |           3.1359 |         -99.3858 |
[32m[20221208 14:27:48 @agent_ppo2.py:179][0m |          -0.0702 |           3.0706 |        -102.0329 |
[32m[20221208 14:27:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.47
[32m[20221208 14:27:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.52
[32m[20221208 14:27:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.94
[32m[20221208 14:27:48 @agent_ppo2.py:137][0m Total time:      11.72 min
[32m[20221208 14:27:48 @agent_ppo2.py:139][0m 956416 total steps have happened
[32m[20221208 14:27:48 @agent_ppo2.py:115][0m #------------------------ Iteration 467 --------------------------#
[32m[20221208 14:27:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |           0.0599 |           6.5185 |        -145.2557 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |           0.0892 |           5.8924 |        -101.7929 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |           0.0324 |           5.5673 |        -116.6879 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0033 |           5.3528 |        -131.9550 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0300 |           5.2561 |        -144.3737 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0420 |           5.0854 |        -154.6888 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0493 |           4.9855 |        -158.3618 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0560 |           4.9001 |        -165.7446 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0591 |           4.8168 |        -166.4823 |
[32m[20221208 14:27:49 @agent_ppo2.py:179][0m |          -0.0638 |           4.7372 |        -170.5555 |
[32m[20221208 14:27:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 169.91
[32m[20221208 14:27:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.93
[32m[20221208 14:27:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.59
[32m[20221208 14:27:50 @agent_ppo2.py:137][0m Total time:      11.75 min
[32m[20221208 14:27:50 @agent_ppo2.py:139][0m 958464 total steps have happened
[32m[20221208 14:27:50 @agent_ppo2.py:115][0m #------------------------ Iteration 468 --------------------------#
[32m[20221208 14:27:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:50 @agent_ppo2.py:179][0m |           0.0602 |           3.1497 |        -159.3912 |
[32m[20221208 14:27:50 @agent_ppo2.py:179][0m |           0.0365 |           2.8274 |        -144.6898 |
[32m[20221208 14:27:50 @agent_ppo2.py:179][0m |          -0.0016 |           2.6200 |        -158.8636 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0187 |           2.5111 |        -164.3968 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0203 |           2.4061 |        -166.2893 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0194 |           2.3274 |        -162.4438 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0377 |           2.2675 |        -174.9655 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0409 |           2.2153 |        -174.7173 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0430 |           2.1769 |        -174.0094 |
[32m[20221208 14:27:51 @agent_ppo2.py:179][0m |          -0.0469 |           2.1075 |        -177.7182 |
[32m[20221208 14:27:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.63
[32m[20221208 14:27:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.56
[32m[20221208 14:27:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.81
[32m[20221208 14:27:51 @agent_ppo2.py:137][0m Total time:      11.77 min
[32m[20221208 14:27:51 @agent_ppo2.py:139][0m 960512 total steps have happened
[32m[20221208 14:27:51 @agent_ppo2.py:115][0m #------------------------ Iteration 469 --------------------------#
[32m[20221208 14:27:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |           0.0725 |           5.1634 |        -163.1386 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |           0.0889 |           4.5483 |        -138.3579 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |           0.0296 |           4.1786 |        -148.0542 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |           0.0073 |           3.9269 |        -154.2065 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |          -0.0200 |           3.7336 |        -166.9335 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |          -0.0321 |           3.5801 |        -171.4167 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |          -0.0391 |           3.4123 |        -173.9395 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |          -0.0439 |           3.2863 |        -174.5273 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |          -0.0498 |           3.1782 |        -183.3844 |
[32m[20221208 14:27:52 @agent_ppo2.py:179][0m |          -0.0539 |           3.0842 |        -186.0227 |
[32m[20221208 14:27:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:27:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.60
[32m[20221208 14:27:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.58
[32m[20221208 14:27:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.24
[32m[20221208 14:27:53 @agent_ppo2.py:137][0m Total time:      11.80 min
[32m[20221208 14:27:53 @agent_ppo2.py:139][0m 962560 total steps have happened
[32m[20221208 14:27:53 @agent_ppo2.py:115][0m #------------------------ Iteration 470 --------------------------#
[32m[20221208 14:27:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:53 @agent_ppo2.py:179][0m |           0.1071 |           0.7156 |        -124.6741 |
[32m[20221208 14:27:53 @agent_ppo2.py:179][0m |           0.0130 |           0.6132 |         -53.6545 |
[32m[20221208 14:27:53 @agent_ppo2.py:179][0m |           0.0005 |           0.5846 |         -53.2133 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |           0.0002 |           0.5714 |         -52.4752 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |          -0.0036 |           0.5627 |         -53.0483 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |          -0.0042 |           0.5538 |         -53.7719 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |          -0.0058 |           0.5482 |         -54.4909 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |          -0.0018 |           0.5431 |         -55.2998 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |          -0.0059 |           0.5437 |         -55.9405 |
[32m[20221208 14:27:54 @agent_ppo2.py:179][0m |          -0.0083 |           0.5367 |         -56.4933 |
[32m[20221208 14:27:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.73
[32m[20221208 14:27:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.16
[32m[20221208 14:27:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.44
[32m[20221208 14:27:54 @agent_ppo2.py:137][0m Total time:      11.82 min
[32m[20221208 14:27:54 @agent_ppo2.py:139][0m 964608 total steps have happened
[32m[20221208 14:27:54 @agent_ppo2.py:115][0m #------------------------ Iteration 471 --------------------------#
[32m[20221208 14:27:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |           0.0752 |           3.1996 |        -169.7823 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |           0.0569 |           2.9422 |         -91.6482 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |           0.0147 |           2.7826 |        -106.2086 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |           0.0043 |           2.7509 |        -105.1423 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |          -0.0185 |           2.6693 |        -118.8975 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |          -0.0228 |           2.6126 |        -119.7138 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |          -0.0296 |           2.6156 |        -124.5680 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |          -0.0345 |           2.5933 |        -129.2633 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |          -0.0389 |           2.5165 |        -130.4246 |
[32m[20221208 14:27:55 @agent_ppo2.py:179][0m |          -0.0415 |           2.4958 |        -132.9931 |
[32m[20221208 14:27:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:27:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.11
[32m[20221208 14:27:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.41
[32m[20221208 14:27:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.95
[32m[20221208 14:27:56 @agent_ppo2.py:137][0m Total time:      11.85 min
[32m[20221208 14:27:56 @agent_ppo2.py:139][0m 966656 total steps have happened
[32m[20221208 14:27:56 @agent_ppo2.py:115][0m #------------------------ Iteration 472 --------------------------#
[32m[20221208 14:27:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:56 @agent_ppo2.py:179][0m |           0.0489 |           3.0026 |        -163.6966 |
[32m[20221208 14:27:56 @agent_ppo2.py:179][0m |           0.0142 |           2.5678 |        -124.8917 |
[32m[20221208 14:27:56 @agent_ppo2.py:179][0m |           0.0103 |           2.3392 |         -62.9582 |
[32m[20221208 14:27:56 @agent_ppo2.py:179][0m |          -0.0139 |           2.1767 |         -56.7349 |
[32m[20221208 14:27:57 @agent_ppo2.py:179][0m |          -0.0267 |           2.0224 |         -55.1580 |
[32m[20221208 14:27:57 @agent_ppo2.py:179][0m |          -0.0371 |           1.9071 |         -59.7797 |
[32m[20221208 14:27:57 @agent_ppo2.py:179][0m |          -0.0431 |           1.8006 |         -64.0890 |
[32m[20221208 14:27:57 @agent_ppo2.py:179][0m |          -0.0433 |           1.7120 |         -64.8012 |
[32m[20221208 14:27:57 @agent_ppo2.py:179][0m |          -0.0496 |           1.6224 |         -68.0390 |
[32m[20221208 14:27:57 @agent_ppo2.py:179][0m |          -0.0509 |           1.5945 |         -69.4862 |
[32m[20221208 14:27:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:27:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.47
[32m[20221208 14:27:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.76
[32m[20221208 14:27:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.73
[32m[20221208 14:27:57 @agent_ppo2.py:137][0m Total time:      11.87 min
[32m[20221208 14:27:57 @agent_ppo2.py:139][0m 968704 total steps have happened
[32m[20221208 14:27:57 @agent_ppo2.py:115][0m #------------------------ Iteration 473 --------------------------#
[32m[20221208 14:27:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:27:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |           0.0804 |           6.7564 |        -194.3045 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |           0.0677 |           5.6896 |        -158.9154 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |           0.0220 |           5.4602 |        -176.8506 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0046 |           5.3051 |        -197.7882 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0218 |           5.2199 |        -206.6390 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0328 |           5.1346 |        -211.6649 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0383 |           5.0942 |        -219.7573 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0467 |           4.9799 |        -223.1687 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0502 |           4.9435 |        -226.0852 |
[32m[20221208 14:27:58 @agent_ppo2.py:179][0m |          -0.0557 |           4.9148 |        -229.1451 |
[32m[20221208 14:27:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:27:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.22
[32m[20221208 14:27:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.64
[32m[20221208 14:27:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.59
[32m[20221208 14:27:59 @agent_ppo2.py:137][0m Total time:      11.90 min
[32m[20221208 14:27:59 @agent_ppo2.py:139][0m 970752 total steps have happened
[32m[20221208 14:27:59 @agent_ppo2.py:115][0m #------------------------ Iteration 474 --------------------------#
[32m[20221208 14:27:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:27:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:27:59 @agent_ppo2.py:179][0m |           0.0840 |           5.7019 |        -157.2409 |
[32m[20221208 14:27:59 @agent_ppo2.py:179][0m |           0.0576 |           4.9122 |        -118.2708 |
[32m[20221208 14:27:59 @agent_ppo2.py:179][0m |           0.0191 |           4.6271 |        -118.9547 |
[32m[20221208 14:27:59 @agent_ppo2.py:179][0m |          -0.0210 |           4.4941 |        -137.4883 |
[32m[20221208 14:28:00 @agent_ppo2.py:179][0m |          -0.0395 |           4.3903 |        -144.9531 |
[32m[20221208 14:28:00 @agent_ppo2.py:179][0m |          -0.0513 |           4.2694 |        -153.1352 |
[32m[20221208 14:28:00 @agent_ppo2.py:179][0m |          -0.0589 |           4.2231 |        -160.5772 |
[32m[20221208 14:28:00 @agent_ppo2.py:179][0m |          -0.0593 |           4.0619 |        -161.1003 |
[32m[20221208 14:28:00 @agent_ppo2.py:179][0m |          -0.0665 |           4.0091 |        -168.6504 |
[32m[20221208 14:28:00 @agent_ppo2.py:179][0m |          -0.0715 |           3.9527 |        -170.0921 |
[32m[20221208 14:28:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:28:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.75
[32m[20221208 14:28:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.02
[32m[20221208 14:28:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.73
[32m[20221208 14:28:00 @agent_ppo2.py:137][0m Total time:      11.92 min
[32m[20221208 14:28:00 @agent_ppo2.py:139][0m 972800 total steps have happened
[32m[20221208 14:28:00 @agent_ppo2.py:115][0m #------------------------ Iteration 475 --------------------------#
[32m[20221208 14:28:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |           0.0398 |           1.7937 |        -216.1462 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0054 |           1.1070 |        -216.1675 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0180 |           0.9789 |        -224.2868 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0205 |           0.9021 |        -223.6891 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0245 |           0.8474 |        -227.1882 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0222 |           0.8181 |        -226.6938 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0269 |           0.7859 |        -230.9027 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0211 |           0.7515 |        -225.6134 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0237 |           0.7364 |        -219.2872 |
[32m[20221208 14:28:01 @agent_ppo2.py:179][0m |          -0.0236 |           0.7212 |        -229.4078 |
[32m[20221208 14:28:01 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 14:28:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 29.74
[32m[20221208 14:28:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.91
[32m[20221208 14:28:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.05
[32m[20221208 14:28:02 @agent_ppo2.py:137][0m Total time:      11.95 min
[32m[20221208 14:28:02 @agent_ppo2.py:139][0m 974848 total steps have happened
[32m[20221208 14:28:02 @agent_ppo2.py:115][0m #------------------------ Iteration 476 --------------------------#
[32m[20221208 14:28:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:02 @agent_ppo2.py:179][0m |           0.0417 |           6.9583 |        -205.4064 |
[32m[20221208 14:28:02 @agent_ppo2.py:179][0m |           0.0931 |           5.9733 |        -168.3104 |
[32m[20221208 14:28:02 @agent_ppo2.py:179][0m |           0.0579 |           5.7851 |        -140.8786 |
[32m[20221208 14:28:02 @agent_ppo2.py:179][0m |           0.0030 |           5.6584 |        -141.6723 |
[32m[20221208 14:28:02 @agent_ppo2.py:179][0m |          -0.0352 |           5.5669 |        -140.4711 |
[32m[20221208 14:28:03 @agent_ppo2.py:179][0m |          -0.0517 |           5.4956 |        -147.1323 |
[32m[20221208 14:28:03 @agent_ppo2.py:179][0m |          -0.0643 |           5.4292 |        -154.7420 |
[32m[20221208 14:28:03 @agent_ppo2.py:179][0m |          -0.0664 |           5.3857 |        -156.4230 |
[32m[20221208 14:28:03 @agent_ppo2.py:179][0m |          -0.0793 |           5.2859 |        -168.8230 |
[32m[20221208 14:28:03 @agent_ppo2.py:179][0m |          -0.0814 |           5.2718 |        -169.0632 |
[32m[20221208 14:28:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:28:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.41
[32m[20221208 14:28:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.33
[32m[20221208 14:28:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.27
[32m[20221208 14:28:03 @agent_ppo2.py:137][0m Total time:      11.97 min
[32m[20221208 14:28:03 @agent_ppo2.py:139][0m 976896 total steps have happened
[32m[20221208 14:28:03 @agent_ppo2.py:115][0m #------------------------ Iteration 477 --------------------------#
[32m[20221208 14:28:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.1148 |           1.7649 |        -174.4563 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0639 |           1.2760 |         -91.1459 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0599 |           1.1581 |         -85.3212 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0459 |           1.1214 |         -92.4652 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0136 |           1.0775 |        -135.3860 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0018 |           1.0627 |        -151.1929 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |          -0.0050 |           1.0497 |        -154.4911 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0961 |           1.0648 |        -117.7168 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |           0.0457 |           1.0468 |         -92.4230 |
[32m[20221208 14:28:04 @agent_ppo2.py:179][0m |          -0.0019 |           1.0366 |        -143.8807 |
[32m[20221208 14:28:04 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:28:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.67
[32m[20221208 14:28:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 29.54
[32m[20221208 14:28:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.20
[32m[20221208 14:28:05 @agent_ppo2.py:137][0m Total time:      12.00 min
[32m[20221208 14:28:05 @agent_ppo2.py:139][0m 978944 total steps have happened
[32m[20221208 14:28:05 @agent_ppo2.py:115][0m #------------------------ Iteration 478 --------------------------#
[32m[20221208 14:28:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:05 @agent_ppo2.py:179][0m |           0.0936 |           5.9878 |        -233.3267 |
[32m[20221208 14:28:05 @agent_ppo2.py:179][0m |           0.0535 |           4.9592 |        -195.3983 |
[32m[20221208 14:28:05 @agent_ppo2.py:179][0m |          -0.0144 |           4.7533 |        -217.5593 |
[32m[20221208 14:28:05 @agent_ppo2.py:179][0m |          -0.0425 |           4.6357 |        -234.6765 |
[32m[20221208 14:28:05 @agent_ppo2.py:179][0m |          -0.0524 |           4.5293 |        -238.4865 |
[32m[20221208 14:28:06 @agent_ppo2.py:179][0m |          -0.0625 |           4.4582 |        -241.5862 |
[32m[20221208 14:28:06 @agent_ppo2.py:179][0m |          -0.0692 |           4.3752 |        -245.2159 |
[32m[20221208 14:28:06 @agent_ppo2.py:179][0m |          -0.0769 |           4.3126 |        -249.3750 |
[32m[20221208 14:28:06 @agent_ppo2.py:179][0m |          -0.0846 |           4.2839 |        -253.7036 |
[32m[20221208 14:28:06 @agent_ppo2.py:179][0m |          -0.0861 |           4.1792 |        -255.3800 |
[32m[20221208 14:28:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:28:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.88
[32m[20221208 14:28:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.88
[32m[20221208 14:28:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.14
[32m[20221208 14:28:06 @agent_ppo2.py:137][0m Total time:      12.02 min
[32m[20221208 14:28:06 @agent_ppo2.py:139][0m 980992 total steps have happened
[32m[20221208 14:28:06 @agent_ppo2.py:115][0m #------------------------ Iteration 479 --------------------------#
[32m[20221208 14:28:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.0778 |           0.9990 |        -237.8925 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1203 |           0.8434 |        -169.2472 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1484 |           0.8264 |         -59.2669 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1174 |           0.7981 |        -117.4716 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.0735 |           0.8045 |        -185.4487 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.0636 |           0.7881 |        -210.5041 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1595 |           0.7834 |         -81.8894 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1884 |           0.7819 |           0.5805 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1660 |           0.7781 |           0.4269 |
[32m[20221208 14:28:07 @agent_ppo2.py:179][0m |           0.1684 |           0.7810 |         -16.9874 |
[32m[20221208 14:28:07 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:28:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.94
[32m[20221208 14:28:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.16
[32m[20221208 14:28:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.30
[32m[20221208 14:28:08 @agent_ppo2.py:137][0m Total time:      12.05 min
[32m[20221208 14:28:08 @agent_ppo2.py:139][0m 983040 total steps have happened
[32m[20221208 14:28:08 @agent_ppo2.py:115][0m #------------------------ Iteration 480 --------------------------#
[32m[20221208 14:28:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:08 @agent_ppo2.py:179][0m |           0.0664 |           1.7803 |        -210.5601 |
[32m[20221208 14:28:08 @agent_ppo2.py:179][0m |           0.1204 |           1.4843 |        -103.3189 |
[32m[20221208 14:28:08 @agent_ppo2.py:179][0m |           0.0702 |           1.3560 |         -72.1546 |
[32m[20221208 14:28:08 @agent_ppo2.py:179][0m |           0.0291 |           1.2866 |        -111.8823 |
[32m[20221208 14:28:08 @agent_ppo2.py:179][0m |           0.0050 |           1.2310 |        -139.8407 |
[32m[20221208 14:28:09 @agent_ppo2.py:179][0m |          -0.0046 |           1.1767 |        -151.0177 |
[32m[20221208 14:28:09 @agent_ppo2.py:179][0m |          -0.0161 |           1.1386 |        -155.4423 |
[32m[20221208 14:28:09 @agent_ppo2.py:179][0m |          -0.0192 |           1.1166 |        -161.6294 |
[32m[20221208 14:28:09 @agent_ppo2.py:179][0m |          -0.0212 |           1.0841 |        -164.1372 |
[32m[20221208 14:28:09 @agent_ppo2.py:179][0m |          -0.0064 |           1.0640 |        -144.1908 |
[32m[20221208 14:28:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.98
[32m[20221208 14:28:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.70
[32m[20221208 14:28:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.62
[32m[20221208 14:28:09 @agent_ppo2.py:137][0m Total time:      12.07 min
[32m[20221208 14:28:09 @agent_ppo2.py:139][0m 985088 total steps have happened
[32m[20221208 14:28:09 @agent_ppo2.py:115][0m #------------------------ Iteration 481 --------------------------#
[32m[20221208 14:28:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |           0.0563 |           2.6322 |        -168.9413 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |           0.0119 |           2.2478 |        -150.2348 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0029 |           2.1646 |        -148.4171 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0084 |           2.1267 |        -150.8650 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0106 |           2.0901 |        -151.7295 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0255 |           2.0853 |        -157.5889 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0247 |           2.0908 |        -156.5992 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0315 |           2.0706 |        -162.9798 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0371 |           2.0567 |        -166.0419 |
[32m[20221208 14:28:10 @agent_ppo2.py:179][0m |          -0.0349 |           2.0261 |        -164.6683 |
[32m[20221208 14:28:10 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:28:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.66
[32m[20221208 14:28:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.24
[32m[20221208 14:28:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.00
[32m[20221208 14:28:11 @agent_ppo2.py:137][0m Total time:      12.10 min
[32m[20221208 14:28:11 @agent_ppo2.py:139][0m 987136 total steps have happened
[32m[20221208 14:28:11 @agent_ppo2.py:115][0m #------------------------ Iteration 482 --------------------------#
[32m[20221208 14:28:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:11 @agent_ppo2.py:179][0m |           0.0636 |           1.6126 |        -242.4573 |
[32m[20221208 14:28:11 @agent_ppo2.py:179][0m |           0.0227 |           1.2139 |        -193.4968 |
[32m[20221208 14:28:11 @agent_ppo2.py:179][0m |           0.0067 |           1.0967 |        -229.1153 |
[32m[20221208 14:28:11 @agent_ppo2.py:179][0m |           0.0039 |           1.0413 |        -202.6173 |
[32m[20221208 14:28:12 @agent_ppo2.py:179][0m |          -0.0185 |           0.9958 |        -153.1007 |
[32m[20221208 14:28:12 @agent_ppo2.py:179][0m |          -0.0311 |           0.9685 |        -157.1004 |
[32m[20221208 14:28:12 @agent_ppo2.py:179][0m |          -0.0383 |           0.9435 |        -158.9859 |
[32m[20221208 14:28:12 @agent_ppo2.py:179][0m |          -0.0477 |           0.9274 |        -160.4541 |
[32m[20221208 14:28:12 @agent_ppo2.py:179][0m |          -0.0503 |           0.9016 |        -164.9438 |
[32m[20221208 14:28:12 @agent_ppo2.py:179][0m |          -0.0523 |           0.8936 |        -164.1877 |
[32m[20221208 14:28:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.83
[32m[20221208 14:28:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.46
[32m[20221208 14:28:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.08
[32m[20221208 14:28:12 @agent_ppo2.py:137][0m Total time:      12.12 min
[32m[20221208 14:28:12 @agent_ppo2.py:139][0m 989184 total steps have happened
[32m[20221208 14:28:12 @agent_ppo2.py:115][0m #------------------------ Iteration 483 --------------------------#
[32m[20221208 14:28:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |           0.1223 |           4.5926 |        -225.2673 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |           0.1115 |           4.0372 |        -172.8909 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |           0.0301 |           3.8587 |        -185.7817 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0164 |           3.6576 |        -207.7413 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0415 |           3.5018 |        -217.9021 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0513 |           3.4255 |        -220.8509 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0611 |           3.3514 |        -227.1455 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0692 |           3.2940 |        -230.1805 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0739 |           3.1883 |        -240.0636 |
[32m[20221208 14:28:13 @agent_ppo2.py:179][0m |          -0.0799 |           3.1323 |        -241.3732 |
[32m[20221208 14:28:13 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:28:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.22
[32m[20221208 14:28:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.29
[32m[20221208 14:28:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.81
[32m[20221208 14:28:14 @agent_ppo2.py:137][0m Total time:      12.15 min
[32m[20221208 14:28:14 @agent_ppo2.py:139][0m 991232 total steps have happened
[32m[20221208 14:28:14 @agent_ppo2.py:115][0m #------------------------ Iteration 484 --------------------------#
[32m[20221208 14:28:14 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:14 @agent_ppo2.py:179][0m |           0.0649 |           1.7191 |        -207.8524 |
[32m[20221208 14:28:14 @agent_ppo2.py:179][0m |           0.0381 |           1.4696 |        -181.0645 |
[32m[20221208 14:28:14 @agent_ppo2.py:179][0m |           0.0137 |           1.3542 |        -160.7054 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0164 |           1.2803 |        -165.5494 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0302 |           1.2284 |        -171.2457 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0408 |           1.2026 |        -172.5871 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0458 |           1.1740 |        -178.4474 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0481 |           1.1491 |        -174.1342 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0468 |           1.1193 |        -178.6075 |
[32m[20221208 14:28:15 @agent_ppo2.py:179][0m |          -0.0501 |           1.1031 |        -179.0217 |
[32m[20221208 14:28:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.87
[32m[20221208 14:28:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.58
[32m[20221208 14:28:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.29
[32m[20221208 14:28:15 @agent_ppo2.py:137][0m Total time:      12.17 min
[32m[20221208 14:28:15 @agent_ppo2.py:139][0m 993280 total steps have happened
[32m[20221208 14:28:15 @agent_ppo2.py:115][0m #------------------------ Iteration 485 --------------------------#
[32m[20221208 14:28:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |           0.1376 |           2.8615 |        -245.5313 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |           0.0995 |           2.4681 |         -84.6512 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |           0.0346 |           2.3079 |         -73.7231 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |           0.0076 |           2.1519 |        -100.5733 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |          -0.0115 |           2.0360 |        -115.5848 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |          -0.0228 |           1.9613 |        -136.0625 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |          -0.0297 |           1.9007 |        -119.7231 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |          -0.0358 |           1.8311 |        -135.9845 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |          -0.0373 |           1.7730 |        -162.9276 |
[32m[20221208 14:28:16 @agent_ppo2.py:179][0m |          -0.0397 |           1.7342 |        -169.3609 |
[32m[20221208 14:28:16 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:28:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.92
[32m[20221208 14:28:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.58
[32m[20221208 14:28:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.80
[32m[20221208 14:28:17 @agent_ppo2.py:137][0m Total time:      12.20 min
[32m[20221208 14:28:17 @agent_ppo2.py:139][0m 995328 total steps have happened
[32m[20221208 14:28:17 @agent_ppo2.py:115][0m #------------------------ Iteration 486 --------------------------#
[32m[20221208 14:28:17 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:17 @agent_ppo2.py:179][0m |           0.0734 |           3.5622 |        -237.2478 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |           0.0533 |           3.0712 |        -215.7144 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |           0.0064 |           2.9587 |        -226.0237 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0245 |           2.7949 |        -230.0456 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0383 |           2.6901 |        -230.9808 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0524 |           2.6313 |        -234.2468 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0585 |           2.5744 |        -230.5517 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0635 |           2.5501 |        -229.8192 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0590 |           2.5031 |        -219.3555 |
[32m[20221208 14:28:18 @agent_ppo2.py:179][0m |          -0.0707 |           2.4836 |        -226.2524 |
[32m[20221208 14:28:18 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.66
[32m[20221208 14:28:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.05
[32m[20221208 14:28:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.34
[32m[20221208 14:28:18 @agent_ppo2.py:137][0m Total time:      12.23 min
[32m[20221208 14:28:18 @agent_ppo2.py:139][0m 997376 total steps have happened
[32m[20221208 14:28:18 @agent_ppo2.py:115][0m #------------------------ Iteration 487 --------------------------#
[32m[20221208 14:28:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |           0.0543 |           1.5016 |        -234.3420 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |           0.0234 |           1.1701 |        -226.5457 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |           0.0036 |           1.0342 |        -234.9217 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |           0.0012 |           0.9612 |        -228.2591 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |          -0.0135 |           0.9079 |        -238.3817 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |          -0.0117 |           0.8772 |        -234.3057 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |           0.0298 |           0.8493 |        -191.1243 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |          -0.0148 |           0.8299 |        -222.1269 |
[32m[20221208 14:28:19 @agent_ppo2.py:179][0m |          -0.0197 |           0.8110 |        -238.8667 |
[32m[20221208 14:28:20 @agent_ppo2.py:179][0m |          -0.0272 |           0.8035 |        -237.5143 |
[32m[20221208 14:28:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 33.33
[32m[20221208 14:28:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.85
[32m[20221208 14:28:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.77
[32m[20221208 14:28:20 @agent_ppo2.py:137][0m Total time:      12.25 min
[32m[20221208 14:28:20 @agent_ppo2.py:139][0m 999424 total steps have happened
[32m[20221208 14:28:20 @agent_ppo2.py:115][0m #------------------------ Iteration 488 --------------------------#
[32m[20221208 14:28:20 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.1064 |           1.2187 |        -178.2082 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0982 |           1.0325 |        -116.5372 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0452 |           0.9588 |        -200.5164 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0378 |           0.8850 |        -182.0632 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0225 |           0.8411 |        -192.6620 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0035 |           0.7898 |        -233.4976 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0621 |           0.7628 |        -192.1193 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |           0.0266 |           0.7364 |        -228.8484 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |          -0.0169 |           0.7286 |        -251.6484 |
[32m[20221208 14:28:21 @agent_ppo2.py:179][0m |          -0.0111 |           0.6927 |        -249.7345 |
[32m[20221208 14:28:21 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 29.36
[32m[20221208 14:28:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 39.87
[32m[20221208 14:28:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.22
[32m[20221208 14:28:21 @agent_ppo2.py:137][0m Total time:      12.28 min
[32m[20221208 14:28:21 @agent_ppo2.py:139][0m 1001472 total steps have happened
[32m[20221208 14:28:21 @agent_ppo2.py:115][0m #------------------------ Iteration 489 --------------------------#
[32m[20221208 14:28:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |           0.0597 |           1.0693 |        -151.7084 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |           0.0289 |           0.8952 |         -88.1430 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |          -0.0007 |           0.8752 |        -136.7674 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |          -0.0073 |           0.8905 |        -153.2613 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |          -0.0227 |           0.8485 |        -162.7503 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |          -0.0180 |           0.8335 |        -161.4201 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |          -0.0227 |           0.8400 |        -167.6286 |
[32m[20221208 14:28:22 @agent_ppo2.py:179][0m |          -0.0167 |           0.8403 |        -169.2673 |
[32m[20221208 14:28:23 @agent_ppo2.py:179][0m |          -0.0060 |           0.8371 |        -154.7064 |
[32m[20221208 14:28:23 @agent_ppo2.py:179][0m |           0.0141 |           0.8269 |        -130.4127 |
[32m[20221208 14:28:23 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:28:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 20.16
[32m[20221208 14:28:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.75
[32m[20221208 14:28:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.53
[32m[20221208 14:28:23 @agent_ppo2.py:137][0m Total time:      12.30 min
[32m[20221208 14:28:23 @agent_ppo2.py:139][0m 1003520 total steps have happened
[32m[20221208 14:28:23 @agent_ppo2.py:115][0m #------------------------ Iteration 490 --------------------------#
[32m[20221208 14:28:24 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |           0.0613 |           3.0624 |        -176.6508 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |           0.0339 |           2.3027 |        -144.4647 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0072 |           2.1345 |        -160.3535 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0315 |           2.0766 |        -170.0582 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0437 |           2.0274 |        -170.6868 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0574 |           1.9862 |        -182.2004 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0626 |           1.9454 |        -182.4322 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0696 |           1.8967 |        -188.7319 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0715 |           1.8854 |        -192.0851 |
[32m[20221208 14:28:24 @agent_ppo2.py:179][0m |          -0.0729 |           1.8393 |        -190.7792 |
[32m[20221208 14:28:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:28:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.14
[32m[20221208 14:28:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.46
[32m[20221208 14:28:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.63
[32m[20221208 14:28:25 @agent_ppo2.py:137][0m Total time:      12.33 min
[32m[20221208 14:28:25 @agent_ppo2.py:139][0m 1005568 total steps have happened
[32m[20221208 14:28:25 @agent_ppo2.py:115][0m #------------------------ Iteration 491 --------------------------#
[32m[20221208 14:28:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:25 @agent_ppo2.py:179][0m |           0.0427 |           1.1633 |        -289.1202 |
[32m[20221208 14:28:25 @agent_ppo2.py:179][0m |           0.0344 |           0.9394 |        -266.5575 |
[32m[20221208 14:28:25 @agent_ppo2.py:179][0m |           0.0095 |           0.8309 |        -259.6145 |
[32m[20221208 14:28:25 @agent_ppo2.py:179][0m |          -0.0159 |           0.7605 |        -271.9110 |
[32m[20221208 14:28:25 @agent_ppo2.py:179][0m |          -0.0301 |           0.7114 |        -285.7182 |
[32m[20221208 14:28:25 @agent_ppo2.py:179][0m |          -0.0316 |           0.6793 |        -283.6329 |
[32m[20221208 14:28:26 @agent_ppo2.py:179][0m |          -0.0364 |           0.6489 |        -276.9756 |
[32m[20221208 14:28:26 @agent_ppo2.py:179][0m |          -0.0410 |           0.6239 |        -288.7698 |
[32m[20221208 14:28:26 @agent_ppo2.py:179][0m |          -0.0406 |           0.6050 |        -289.1590 |
[32m[20221208 14:28:26 @agent_ppo2.py:179][0m |          -0.0428 |           0.5940 |        -295.2601 |
[32m[20221208 14:28:26 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:28:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.49
[32m[20221208 14:28:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.90
[32m[20221208 14:28:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.57
[32m[20221208 14:28:26 @agent_ppo2.py:137][0m Total time:      12.35 min
[32m[20221208 14:28:26 @agent_ppo2.py:139][0m 1007616 total steps have happened
[32m[20221208 14:28:26 @agent_ppo2.py:115][0m #------------------------ Iteration 492 --------------------------#
[32m[20221208 14:28:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |           0.1433 |           4.1830 |        -226.9760 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |           0.0508 |           3.0824 |        -196.0510 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |           0.0085 |           2.8821 |        -206.0943 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0119 |           2.8085 |        -211.8289 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0307 |           2.6766 |        -224.8263 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0424 |           2.6080 |        -235.3215 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0501 |           2.5636 |        -236.3420 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0556 |           2.4979 |        -240.1898 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0611 |           2.4757 |        -239.1148 |
[32m[20221208 14:28:27 @agent_ppo2.py:179][0m |          -0.0619 |           2.3901 |        -245.9658 |
[32m[20221208 14:28:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.35
[32m[20221208 14:28:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.90
[32m[20221208 14:28:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.27
[32m[20221208 14:28:28 @agent_ppo2.py:137][0m Total time:      12.38 min
[32m[20221208 14:28:28 @agent_ppo2.py:139][0m 1009664 total steps have happened
[32m[20221208 14:28:28 @agent_ppo2.py:115][0m #------------------------ Iteration 493 --------------------------#
[32m[20221208 14:28:28 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:28:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:28 @agent_ppo2.py:179][0m |           0.0602 |           2.5165 |        -138.7032 |
[32m[20221208 14:28:28 @agent_ppo2.py:179][0m |           0.0317 |           2.2441 |        -105.7957 |
[32m[20221208 14:28:28 @agent_ppo2.py:179][0m |           0.0156 |           2.2153 |        -109.5932 |
[32m[20221208 14:28:28 @agent_ppo2.py:179][0m |           0.0121 |           2.2070 |        -112.3882 |
[32m[20221208 14:28:29 @agent_ppo2.py:179][0m |           0.0099 |           2.1568 |        -110.6626 |
[32m[20221208 14:28:29 @agent_ppo2.py:179][0m |           0.0068 |           2.1734 |        -112.4159 |
[32m[20221208 14:28:29 @agent_ppo2.py:179][0m |           0.0027 |           2.1424 |        -119.2770 |
[32m[20221208 14:28:29 @agent_ppo2.py:179][0m |           0.0066 |           2.1483 |        -113.8934 |
[32m[20221208 14:28:29 @agent_ppo2.py:179][0m |           0.0016 |           2.1707 |        -115.2563 |
[32m[20221208 14:28:29 @agent_ppo2.py:179][0m |           0.0022 |           2.1445 |        -113.0696 |
[32m[20221208 14:28:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.20
[32m[20221208 14:28:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.44
[32m[20221208 14:28:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.45
[32m[20221208 14:28:29 @agent_ppo2.py:137][0m Total time:      12.41 min
[32m[20221208 14:28:29 @agent_ppo2.py:139][0m 1011712 total steps have happened
[32m[20221208 14:28:29 @agent_ppo2.py:115][0m #------------------------ Iteration 494 --------------------------#
[32m[20221208 14:28:30 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |           0.0617 |           2.7431 |        -240.0764 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |           0.0282 |           2.3144 |        -205.8981 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0096 |           2.1593 |        -213.2263 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0341 |           2.0752 |        -226.3263 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0458 |           2.0095 |        -223.1121 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0567 |           1.9485 |        -233.8545 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0669 |           1.9243 |        -236.6147 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0745 |           1.8705 |        -242.4906 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0721 |           1.8872 |        -242.6859 |
[32m[20221208 14:28:30 @agent_ppo2.py:179][0m |          -0.0734 |           1.8438 |        -242.6192 |
[32m[20221208 14:28:30 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.60
[32m[20221208 14:28:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.88
[32m[20221208 14:28:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.42
[32m[20221208 14:28:31 @agent_ppo2.py:137][0m Total time:      12.43 min
[32m[20221208 14:28:31 @agent_ppo2.py:139][0m 1013760 total steps have happened
[32m[20221208 14:28:31 @agent_ppo2.py:115][0m #------------------------ Iteration 495 --------------------------#
[32m[20221208 14:28:31 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:31 @agent_ppo2.py:179][0m |           0.0769 |           2.7432 |        -270.9745 |
[32m[20221208 14:28:31 @agent_ppo2.py:179][0m |           0.0861 |           2.3901 |        -212.8376 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |           0.0755 |           2.2458 |        -186.1268 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |           0.0049 |           2.1199 |        -180.1431 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |          -0.0162 |           2.0673 |        -191.0221 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |          -0.0292 |           2.0122 |        -190.0188 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |          -0.0454 |           1.9335 |        -197.5803 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |          -0.0552 |           1.9173 |        -205.2190 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |          -0.0635 |           1.8731 |        -206.9265 |
[32m[20221208 14:28:32 @agent_ppo2.py:179][0m |          -0.0662 |           1.8351 |        -210.8470 |
[32m[20221208 14:28:32 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:28:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.44
[32m[20221208 14:28:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.25
[32m[20221208 14:28:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.32
[32m[20221208 14:28:32 @agent_ppo2.py:137][0m Total time:      12.46 min
[32m[20221208 14:28:32 @agent_ppo2.py:139][0m 1015808 total steps have happened
[32m[20221208 14:28:32 @agent_ppo2.py:115][0m #------------------------ Iteration 496 --------------------------#
[32m[20221208 14:28:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |           0.0716 |           0.6328 |        -219.6336 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |           0.0814 |           0.5615 |        -136.9249 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |           0.0045 |           0.5405 |        -183.9442 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |           0.0578 |           0.5319 |        -142.1409 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |           0.0090 |           0.5254 |        -179.5882 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |          -0.0183 |           0.5341 |        -195.8734 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |          -0.0193 |           0.5222 |        -193.6778 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |          -0.0164 |           0.5256 |        -199.2385 |
[32m[20221208 14:28:33 @agent_ppo2.py:179][0m |          -0.0108 |           0.5242 |        -197.1591 |
[32m[20221208 14:28:34 @agent_ppo2.py:179][0m |          -0.0234 |           0.5225 |        -200.9824 |
[32m[20221208 14:28:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:28:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.79
[32m[20221208 14:28:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.95
[32m[20221208 14:28:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.60
[32m[20221208 14:28:34 @agent_ppo2.py:137][0m Total time:      12.48 min
[32m[20221208 14:28:34 @agent_ppo2.py:139][0m 1017856 total steps have happened
[32m[20221208 14:28:34 @agent_ppo2.py:115][0m #------------------------ Iteration 497 --------------------------#
[32m[20221208 14:28:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:34 @agent_ppo2.py:179][0m |           0.0308 |           1.6148 |        -232.6620 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0162 |           1.2049 |        -211.3726 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0311 |           1.0528 |        -222.9426 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0262 |           0.9452 |        -217.3179 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0381 |           0.8923 |        -228.9999 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0372 |           0.8564 |        -230.2659 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0403 |           0.8452 |        -232.1726 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0443 |           0.8253 |        -234.6276 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0485 |           0.8132 |        -237.1427 |
[32m[20221208 14:28:35 @agent_ppo2.py:179][0m |          -0.0458 |           0.8125 |        -237.3414 |
[32m[20221208 14:28:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.64
[32m[20221208 14:28:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.85
[32m[20221208 14:28:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.24
[32m[20221208 14:28:35 @agent_ppo2.py:137][0m Total time:      12.51 min
[32m[20221208 14:28:35 @agent_ppo2.py:139][0m 1019904 total steps have happened
[32m[20221208 14:28:35 @agent_ppo2.py:115][0m #------------------------ Iteration 498 --------------------------#
[32m[20221208 14:28:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |           0.0885 |           1.5087 |        -293.1114 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |           0.0588 |           1.1459 |        -238.4585 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |           0.0054 |           0.9698 |        -248.1283 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |          -0.0207 |           0.8955 |        -261.0740 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |          -0.0306 |           0.8294 |        -265.9408 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |          -0.0336 |           0.7942 |        -263.0493 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |          -0.0375 |           0.7588 |        -269.0557 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |          -0.0430 |           0.7395 |        -274.0535 |
[32m[20221208 14:28:36 @agent_ppo2.py:179][0m |          -0.0491 |           0.7105 |        -280.9767 |
[32m[20221208 14:28:37 @agent_ppo2.py:179][0m |          -0.0512 |           0.6867 |        -279.8204 |
[32m[20221208 14:28:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:28:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.60
[32m[20221208 14:28:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 39.75
[32m[20221208 14:28:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.71
[32m[20221208 14:28:37 @agent_ppo2.py:137][0m Total time:      12.53 min
[32m[20221208 14:28:37 @agent_ppo2.py:139][0m 1021952 total steps have happened
[32m[20221208 14:28:37 @agent_ppo2.py:115][0m #------------------------ Iteration 499 --------------------------#
[32m[20221208 14:28:37 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:28:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0247 |           0.5155 |        -297.0256 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0173 |           0.4115 |        -287.8275 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0149 |           0.3968 |        -278.1983 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0187 |           0.3871 |        -283.2159 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0026 |           0.3844 |        -288.5427 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |          -0.0017 |           0.3825 |        -297.4830 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0004 |           0.3802 |        -295.1398 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0388 |           0.3816 |        -247.3724 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0183 |           0.3770 |        -267.9823 |
[32m[20221208 14:28:38 @agent_ppo2.py:179][0m |           0.0040 |           0.3772 |        -281.0477 |
[32m[20221208 14:28:38 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:28:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.82
[32m[20221208 14:28:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.19
[32m[20221208 14:28:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.68
[32m[20221208 14:28:39 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 209.75
[32m[20221208 14:28:39 @agent_ppo2.py:137][0m Total time:      12.56 min
[32m[20221208 14:28:39 @agent_ppo2.py:139][0m 1024000 total steps have happened
[32m[20221208 14:28:39 @agent_ppo2.py:115][0m #------------------------ Iteration 500 --------------------------#
[32m[20221208 14:28:39 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |           0.0464 |           1.5045 |        -165.4399 |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |          -0.0089 |           1.3414 |        -159.4966 |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |          -0.0326 |           1.2622 |        -167.4044 |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |          -0.0498 |           1.2322 |        -174.8106 |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |          -0.0586 |           1.2221 |        -181.9317 |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |          -0.0638 |           1.1997 |        -186.6264 |
[32m[20221208 14:28:39 @agent_ppo2.py:179][0m |          -0.0687 |           1.1799 |        -189.1943 |
[32m[20221208 14:28:40 @agent_ppo2.py:179][0m |          -0.0704 |           1.1658 |        -189.7188 |
[32m[20221208 14:28:40 @agent_ppo2.py:179][0m |          -0.0738 |           1.1594 |        -191.7580 |
[32m[20221208 14:28:40 @agent_ppo2.py:179][0m |          -0.0720 |           1.1565 |        -196.0956 |
[32m[20221208 14:28:40 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.95
[32m[20221208 14:28:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.23
[32m[20221208 14:28:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.26
[32m[20221208 14:28:40 @agent_ppo2.py:137][0m Total time:      12.59 min
[32m[20221208 14:28:40 @agent_ppo2.py:139][0m 1026048 total steps have happened
[32m[20221208 14:28:40 @agent_ppo2.py:115][0m #------------------------ Iteration 501 --------------------------#
[32m[20221208 14:28:41 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |           0.1194 |           2.0266 |        -299.4705 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |           0.0885 |           1.6612 |        -255.7131 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |           0.0079 |           1.5759 |        -272.3658 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0227 |           1.5167 |        -304.2502 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0352 |           1.4859 |        -305.8934 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0476 |           1.4494 |        -309.4437 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0555 |           1.4282 |        -319.3479 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0580 |           1.4330 |        -325.4529 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0641 |           1.3869 |        -328.2815 |
[32m[20221208 14:28:41 @agent_ppo2.py:179][0m |          -0.0717 |           1.3686 |        -331.9418 |
[32m[20221208 14:28:41 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.46
[32m[20221208 14:28:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.70
[32m[20221208 14:28:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.35
[32m[20221208 14:28:42 @agent_ppo2.py:137][0m Total time:      12.61 min
[32m[20221208 14:28:42 @agent_ppo2.py:139][0m 1028096 total steps have happened
[32m[20221208 14:28:42 @agent_ppo2.py:115][0m #------------------------ Iteration 502 --------------------------#
[32m[20221208 14:28:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:28:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:42 @agent_ppo2.py:179][0m |           0.0756 |           0.5844 |        -308.7853 |
[32m[20221208 14:28:42 @agent_ppo2.py:179][0m |           0.1004 |           0.4890 |        -253.0116 |
[32m[20221208 14:28:42 @agent_ppo2.py:179][0m |           0.0096 |           0.4666 |        -289.0145 |
[32m[20221208 14:28:42 @agent_ppo2.py:179][0m |          -0.0041 |           0.4493 |        -299.0988 |
[32m[20221208 14:28:42 @agent_ppo2.py:179][0m |          -0.0077 |           0.4370 |        -292.1414 |
[32m[20221208 14:28:43 @agent_ppo2.py:179][0m |          -0.0101 |           0.4293 |        -299.2810 |
[32m[20221208 14:28:43 @agent_ppo2.py:179][0m |          -0.0147 |           0.4219 |        -301.4005 |
[32m[20221208 14:28:43 @agent_ppo2.py:179][0m |          -0.0126 |           0.4220 |        -291.3407 |
[32m[20221208 14:28:43 @agent_ppo2.py:179][0m |          -0.0089 |           0.4180 |        -297.5027 |
[32m[20221208 14:28:43 @agent_ppo2.py:179][0m |          -0.0174 |           0.4140 |        -296.4174 |
[32m[20221208 14:28:43 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:28:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 26.30
[32m[20221208 14:28:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 33.14
[32m[20221208 14:28:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.73
[32m[20221208 14:28:43 @agent_ppo2.py:137][0m Total time:      12.64 min
[32m[20221208 14:28:43 @agent_ppo2.py:139][0m 1030144 total steps have happened
[32m[20221208 14:28:43 @agent_ppo2.py:115][0m #------------------------ Iteration 503 --------------------------#
[32m[20221208 14:28:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |           0.1258 |           0.9880 |        -270.5865 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |           0.1153 |           0.7905 |        -123.9595 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |           0.0705 |           0.7247 |        -128.3139 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |           0.0190 |           0.6830 |        -187.1868 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |          -0.0126 |           0.6505 |        -221.9922 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |          -0.0271 |           0.6353 |        -266.7071 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |          -0.0396 |           0.6114 |        -285.7733 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |          -0.0391 |           0.5985 |        -297.6263 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |          -0.0447 |           0.5861 |        -295.9245 |
[32m[20221208 14:28:44 @agent_ppo2.py:179][0m |          -0.0514 |           0.5690 |        -301.4958 |
[32m[20221208 14:28:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:28:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.17
[32m[20221208 14:28:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.05
[32m[20221208 14:28:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.26
[32m[20221208 14:28:45 @agent_ppo2.py:137][0m Total time:      12.66 min
[32m[20221208 14:28:45 @agent_ppo2.py:139][0m 1032192 total steps have happened
[32m[20221208 14:28:45 @agent_ppo2.py:115][0m #------------------------ Iteration 504 --------------------------#
[32m[20221208 14:28:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:45 @agent_ppo2.py:179][0m |           0.0613 |           1.4233 |        -281.8640 |
[32m[20221208 14:28:45 @agent_ppo2.py:179][0m |           0.0450 |           1.1956 |        -268.6244 |
[32m[20221208 14:28:45 @agent_ppo2.py:179][0m |          -0.0048 |           1.1160 |        -294.4427 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0267 |           1.0529 |        -298.9876 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0330 |           1.0068 |        -300.1203 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0375 |           0.9751 |        -307.1699 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0522 |           0.9370 |        -309.7318 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0526 |           0.8987 |        -308.4103 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0527 |           0.8761 |        -309.0143 |
[32m[20221208 14:28:46 @agent_ppo2.py:179][0m |          -0.0589 |           0.8582 |        -321.7316 |
[32m[20221208 14:28:46 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:28:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.20
[32m[20221208 14:28:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.03
[32m[20221208 14:28:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.64
[32m[20221208 14:28:46 @agent_ppo2.py:137][0m Total time:      12.69 min
[32m[20221208 14:28:46 @agent_ppo2.py:139][0m 1034240 total steps have happened
[32m[20221208 14:28:46 @agent_ppo2.py:115][0m #------------------------ Iteration 505 --------------------------#
[32m[20221208 14:28:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |           0.1398 |           2.8165 |        -229.6751 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |           0.0812 |           2.2351 |        -177.3904 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |           0.0051 |           2.0057 |        -216.5622 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0196 |           1.9618 |        -235.9416 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0382 |           1.8702 |        -247.9981 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0473 |           1.8041 |        -251.3363 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0564 |           1.7675 |        -253.6759 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0644 |           1.7278 |        -262.7133 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0724 |           1.7204 |        -265.5610 |
[32m[20221208 14:28:47 @agent_ppo2.py:179][0m |          -0.0706 |           1.6633 |        -269.0770 |
[32m[20221208 14:28:47 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.00
[32m[20221208 14:28:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.18
[32m[20221208 14:28:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.33
[32m[20221208 14:28:48 @agent_ppo2.py:137][0m Total time:      12.72 min
[32m[20221208 14:28:48 @agent_ppo2.py:139][0m 1036288 total steps have happened
[32m[20221208 14:28:48 @agent_ppo2.py:115][0m #------------------------ Iteration 506 --------------------------#
[32m[20221208 14:28:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:48 @agent_ppo2.py:179][0m |           0.0714 |           1.2107 |        -305.1192 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |           0.0191 |           0.9039 |        -280.5594 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0007 |           0.8042 |        -289.6804 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0350 |           0.7510 |        -311.1574 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0448 |           0.7112 |        -317.8575 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0532 |           0.6851 |        -318.7521 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0613 |           0.6657 |        -328.8533 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0638 |           0.6507 |        -334.4851 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0599 |           0.6408 |        -328.5999 |
[32m[20221208 14:28:49 @agent_ppo2.py:179][0m |          -0.0639 |           0.6300 |        -329.3236 |
[32m[20221208 14:28:49 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:28:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.78
[32m[20221208 14:28:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.60
[32m[20221208 14:28:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.60
[32m[20221208 14:28:49 @agent_ppo2.py:137][0m Total time:      12.74 min
[32m[20221208 14:28:49 @agent_ppo2.py:139][0m 1038336 total steps have happened
[32m[20221208 14:28:49 @agent_ppo2.py:115][0m #------------------------ Iteration 507 --------------------------#
[32m[20221208 14:28:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |           0.1065 |           3.5461 |        -263.6815 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |           0.0241 |           2.8549 |        -198.3738 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |          -0.0274 |           2.5854 |        -217.3550 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |          -0.0542 |           2.4129 |        -227.9530 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |          -0.0618 |           2.3026 |        -233.7339 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |          -0.0681 |           2.2161 |        -235.7599 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |          -0.0790 |           2.1602 |        -248.7499 |
[32m[20221208 14:28:50 @agent_ppo2.py:179][0m |          -0.0806 |           2.0778 |        -251.1518 |
[32m[20221208 14:28:51 @agent_ppo2.py:179][0m |          -0.0891 |           2.0520 |        -261.8203 |
[32m[20221208 14:28:51 @agent_ppo2.py:179][0m |          -0.0901 |           2.0002 |        -266.3902 |
[32m[20221208 14:28:51 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:28:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.20
[32m[20221208 14:28:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.06
[32m[20221208 14:28:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.11
[32m[20221208 14:28:51 @agent_ppo2.py:137][0m Total time:      12.77 min
[32m[20221208 14:28:51 @agent_ppo2.py:139][0m 1040384 total steps have happened
[32m[20221208 14:28:51 @agent_ppo2.py:115][0m #------------------------ Iteration 508 --------------------------#
[32m[20221208 14:28:51 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |           0.0937 |           2.6797 |        -302.4049 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |           0.0727 |           2.3287 |        -278.4351 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |           0.0004 |           2.1561 |        -291.4060 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0305 |           2.0373 |        -311.0164 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0409 |           1.9463 |        -317.2453 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0544 |           1.8774 |        -322.7822 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0640 |           1.8076 |        -326.8859 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0664 |           1.7413 |        -329.7132 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0745 |           1.7046 |        -330.1858 |
[32m[20221208 14:28:52 @agent_ppo2.py:179][0m |          -0.0769 |           1.6410 |        -334.8177 |
[32m[20221208 14:28:52 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:28:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.42
[32m[20221208 14:28:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.58
[32m[20221208 14:28:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.77
[32m[20221208 14:28:53 @agent_ppo2.py:137][0m Total time:      12.80 min
[32m[20221208 14:28:53 @agent_ppo2.py:139][0m 1042432 total steps have happened
[32m[20221208 14:28:53 @agent_ppo2.py:115][0m #------------------------ Iteration 509 --------------------------#
[32m[20221208 14:28:53 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:28:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:53 @agent_ppo2.py:179][0m |           0.1034 |           1.2415 |        -219.5159 |
[32m[20221208 14:28:53 @agent_ppo2.py:179][0m |           0.0757 |           1.1614 |        -145.1033 |
[32m[20221208 14:28:53 @agent_ppo2.py:179][0m |           0.0372 |           1.1654 |        -113.9630 |
[32m[20221208 14:28:53 @agent_ppo2.py:179][0m |           0.0136 |           1.1576 |        -127.3333 |
[32m[20221208 14:28:53 @agent_ppo2.py:179][0m |          -0.0127 |           1.1599 |        -142.9238 |
[32m[20221208 14:28:54 @agent_ppo2.py:179][0m |          -0.0163 |           1.1390 |        -145.3833 |
[32m[20221208 14:28:54 @agent_ppo2.py:179][0m |          -0.0130 |           1.1466 |        -144.1110 |
[32m[20221208 14:28:54 @agent_ppo2.py:179][0m |          -0.0122 |           1.1505 |        -144.1224 |
[32m[20221208 14:28:54 @agent_ppo2.py:179][0m |          -0.0164 |           1.1356 |        -149.4730 |
[32m[20221208 14:28:54 @agent_ppo2.py:179][0m |          -0.0171 |           1.1277 |        -150.0283 |
[32m[20221208 14:28:54 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:28:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.02
[32m[20221208 14:28:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.69
[32m[20221208 14:28:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.90
[32m[20221208 14:28:54 @agent_ppo2.py:137][0m Total time:      12.82 min
[32m[20221208 14:28:54 @agent_ppo2.py:139][0m 1044480 total steps have happened
[32m[20221208 14:28:54 @agent_ppo2.py:115][0m #------------------------ Iteration 510 --------------------------#
[32m[20221208 14:28:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |           0.0949 |           1.6235 |        -276.8372 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |           0.0747 |           1.3889 |        -137.5819 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0070 |           1.2852 |        -134.3088 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0406 |           1.2286 |        -140.8505 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0553 |           1.1692 |        -152.5143 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0656 |           1.1446 |        -151.8997 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0775 |           1.1294 |        -158.1536 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0805 |           1.0960 |        -162.8154 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0831 |           1.0761 |        -165.5181 |
[32m[20221208 14:28:55 @agent_ppo2.py:179][0m |          -0.0873 |           1.0587 |        -171.9078 |
[32m[20221208 14:28:55 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:28:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.75
[32m[20221208 14:28:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.97
[32m[20221208 14:28:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 26.31
[32m[20221208 14:28:56 @agent_ppo2.py:137][0m Total time:      12.85 min
[32m[20221208 14:28:56 @agent_ppo2.py:139][0m 1046528 total steps have happened
[32m[20221208 14:28:56 @agent_ppo2.py:115][0m #------------------------ Iteration 511 --------------------------#
[32m[20221208 14:28:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:56 @agent_ppo2.py:179][0m |           0.0700 |           2.5893 |        -340.0308 |
[32m[20221208 14:28:56 @agent_ppo2.py:179][0m |           0.0582 |           2.0314 |        -304.5511 |
[32m[20221208 14:28:56 @agent_ppo2.py:179][0m |          -0.0002 |           1.8763 |        -317.9740 |
[32m[20221208 14:28:56 @agent_ppo2.py:179][0m |          -0.0219 |           1.7839 |        -332.3813 |
[32m[20221208 14:28:57 @agent_ppo2.py:179][0m |          -0.0391 |           1.7302 |        -342.3265 |
[32m[20221208 14:28:57 @agent_ppo2.py:179][0m |          -0.0489 |           1.6973 |        -346.4022 |
[32m[20221208 14:28:57 @agent_ppo2.py:179][0m |          -0.0513 |           1.6476 |        -353.0887 |
[32m[20221208 14:28:57 @agent_ppo2.py:179][0m |          -0.0539 |           1.5966 |        -356.1572 |
[32m[20221208 14:28:57 @agent_ppo2.py:179][0m |          -0.0575 |           1.5994 |        -353.4692 |
[32m[20221208 14:28:57 @agent_ppo2.py:179][0m |          -0.0552 |           1.5260 |        -358.9627 |
[32m[20221208 14:28:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:28:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.20
[32m[20221208 14:28:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.96
[32m[20221208 14:28:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.69
[32m[20221208 14:28:57 @agent_ppo2.py:137][0m Total time:      12.87 min
[32m[20221208 14:28:57 @agent_ppo2.py:139][0m 1048576 total steps have happened
[32m[20221208 14:28:57 @agent_ppo2.py:115][0m #------------------------ Iteration 512 --------------------------#
[32m[20221208 14:28:58 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:28:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |           0.0575 |           2.7277 |        -334.2841 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |           0.0667 |           2.0235 |        -294.2703 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |           0.0693 |           1.8869 |        -238.8489 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |           0.0085 |           1.7968 |        -188.3114 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |          -0.0263 |           1.7536 |        -200.7116 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |          -0.0501 |           1.7020 |        -213.7789 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |          -0.0647 |           1.6694 |        -220.1515 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |          -0.0678 |           1.6534 |        -224.1902 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |          -0.0749 |           1.6262 |        -233.7082 |
[32m[20221208 14:28:58 @agent_ppo2.py:179][0m |          -0.0770 |           1.5917 |        -235.0838 |
[32m[20221208 14:28:58 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:28:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.33
[32m[20221208 14:28:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.80
[32m[20221208 14:28:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.73
[32m[20221208 14:28:59 @agent_ppo2.py:137][0m Total time:      12.90 min
[32m[20221208 14:28:59 @agent_ppo2.py:139][0m 1050624 total steps have happened
[32m[20221208 14:28:59 @agent_ppo2.py:115][0m #------------------------ Iteration 513 --------------------------#
[32m[20221208 14:28:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:28:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:28:59 @agent_ppo2.py:179][0m |           0.0741 |           2.6123 |        -255.5739 |
[32m[20221208 14:28:59 @agent_ppo2.py:179][0m |           0.0297 |           2.1322 |        -154.4181 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0148 |           1.9663 |        -160.6337 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0395 |           1.8743 |        -175.7029 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0477 |           1.8207 |        -181.7382 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0594 |           1.7823 |        -192.6389 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0640 |           1.7564 |        -199.1490 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0683 |           1.7099 |        -203.1867 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0710 |           1.7034 |        -206.7286 |
[32m[20221208 14:29:00 @agent_ppo2.py:179][0m |          -0.0749 |           1.6833 |        -212.7559 |
[32m[20221208 14:29:00 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:29:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.36
[32m[20221208 14:29:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.96
[32m[20221208 14:29:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.38
[32m[20221208 14:29:00 @agent_ppo2.py:137][0m Total time:      12.92 min
[32m[20221208 14:29:00 @agent_ppo2.py:139][0m 1052672 total steps have happened
[32m[20221208 14:29:00 @agent_ppo2.py:115][0m #------------------------ Iteration 514 --------------------------#
[32m[20221208 14:29:01 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |           0.1697 |           3.1469 |        -292.5190 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |           0.1025 |           2.4475 |        -209.7291 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |           0.0375 |           2.2089 |        -261.5426 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |          -0.0006 |           2.0829 |        -301.7114 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |          -0.0220 |           1.9866 |        -314.3283 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |          -0.0348 |           1.9050 |        -326.0918 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |          -0.0447 |           1.8649 |        -333.7030 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |          -0.0532 |           1.7962 |        -338.2980 |
[32m[20221208 14:29:01 @agent_ppo2.py:179][0m |          -0.0586 |           1.7488 |        -344.6058 |
[32m[20221208 14:29:02 @agent_ppo2.py:179][0m |          -0.0599 |           1.7118 |        -346.1326 |
[32m[20221208 14:29:02 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:29:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.21
[32m[20221208 14:29:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.43
[32m[20221208 14:29:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.40
[32m[20221208 14:29:02 @agent_ppo2.py:137][0m Total time:      12.95 min
[32m[20221208 14:29:02 @agent_ppo2.py:139][0m 1054720 total steps have happened
[32m[20221208 14:29:02 @agent_ppo2.py:115][0m #------------------------ Iteration 515 --------------------------#
[32m[20221208 14:29:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |           0.4531 |           2.0800 |        -314.5896 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |           0.0404 |           1.3531 |        -236.4623 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0032 |           1.1964 |        -246.0348 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0274 |           1.1309 |        -262.9012 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0386 |           1.0757 |        -268.9760 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0452 |           1.0321 |        -273.1285 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0480 |           1.0191 |        -279.2888 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0562 |           0.9929 |        -283.6839 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0567 |           0.9820 |        -291.2871 |
[32m[20221208 14:29:03 @agent_ppo2.py:179][0m |          -0.0590 |           0.9682 |        -288.6486 |
[32m[20221208 14:29:03 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:29:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.90
[32m[20221208 14:29:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.91
[32m[20221208 14:29:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.86
[32m[20221208 14:29:03 @agent_ppo2.py:137][0m Total time:      12.98 min
[32m[20221208 14:29:03 @agent_ppo2.py:139][0m 1056768 total steps have happened
[32m[20221208 14:29:03 @agent_ppo2.py:115][0m #------------------------ Iteration 516 --------------------------#
[32m[20221208 14:29:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |           0.0891 |           1.6465 |        -317.7873 |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |           0.0726 |           1.3574 |        -206.9508 |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |           0.1316 |           1.2427 |        -215.9787 |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |          -0.0026 |           1.1549 |        -222.7889 |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |          -0.0300 |           1.0927 |        -238.6080 |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |          -0.0413 |           1.0546 |        -246.4623 |
[32m[20221208 14:29:04 @agent_ppo2.py:179][0m |          -0.0464 |           1.0287 |        -252.3897 |
[32m[20221208 14:29:05 @agent_ppo2.py:179][0m |          -0.0538 |           0.9874 |        -261.8712 |
[32m[20221208 14:29:05 @agent_ppo2.py:179][0m |          -0.0569 |           0.9603 |        -261.5191 |
[32m[20221208 14:29:05 @agent_ppo2.py:179][0m |          -0.0619 |           0.9304 |        -263.8368 |
[32m[20221208 14:29:05 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:29:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.27
[32m[20221208 14:29:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.08
[32m[20221208 14:29:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.42
[32m[20221208 14:29:05 @agent_ppo2.py:137][0m Total time:      13.00 min
[32m[20221208 14:29:05 @agent_ppo2.py:139][0m 1058816 total steps have happened
[32m[20221208 14:29:05 @agent_ppo2.py:115][0m #------------------------ Iteration 517 --------------------------#
[32m[20221208 14:29:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |           0.0690 |           1.7718 |        -395.8386 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |           0.0486 |           1.4930 |        -355.2067 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0023 |           1.4036 |        -367.4314 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0216 |           1.3361 |        -380.2169 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0357 |           1.2989 |        -384.2108 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0412 |           1.2423 |        -392.4582 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0483 |           1.2210 |        -396.2050 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0497 |           1.1775 |        -402.4554 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0551 |           1.1527 |        -410.4372 |
[32m[20221208 14:29:06 @agent_ppo2.py:179][0m |          -0.0578 |           1.1290 |        -412.9937 |
[32m[20221208 14:29:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:29:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.09
[32m[20221208 14:29:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.79
[32m[20221208 14:29:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.20
[32m[20221208 14:29:07 @agent_ppo2.py:137][0m Total time:      13.03 min
[32m[20221208 14:29:07 @agent_ppo2.py:139][0m 1060864 total steps have happened
[32m[20221208 14:29:07 @agent_ppo2.py:115][0m #------------------------ Iteration 518 --------------------------#
[32m[20221208 14:29:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:07 @agent_ppo2.py:179][0m |           0.1234 |           2.7473 |        -305.2953 |
[32m[20221208 14:29:07 @agent_ppo2.py:179][0m |           0.0903 |           2.4194 |        -168.7611 |
[32m[20221208 14:29:07 @agent_ppo2.py:179][0m |           0.0374 |           2.3287 |        -249.3758 |
[32m[20221208 14:29:07 @agent_ppo2.py:179][0m |           0.0018 |           2.2173 |        -302.0185 |
[32m[20221208 14:29:07 @agent_ppo2.py:179][0m |          -0.0180 |           2.1681 |        -319.9631 |
[32m[20221208 14:29:07 @agent_ppo2.py:179][0m |          -0.0351 |           2.1321 |        -330.2741 |
[32m[20221208 14:29:08 @agent_ppo2.py:179][0m |          -0.0390 |           2.0900 |        -334.2218 |
[32m[20221208 14:29:08 @agent_ppo2.py:179][0m |          -0.0477 |           2.0560 |        -346.5121 |
[32m[20221208 14:29:08 @agent_ppo2.py:179][0m |          -0.0578 |           2.0656 |        -360.5314 |
[32m[20221208 14:29:08 @agent_ppo2.py:179][0m |          -0.0540 |           2.0196 |        -352.6735 |
[32m[20221208 14:29:08 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 14:29:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.72
[32m[20221208 14:29:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.45
[32m[20221208 14:29:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.44
[32m[20221208 14:29:08 @agent_ppo2.py:137][0m Total time:      13.06 min
[32m[20221208 14:29:08 @agent_ppo2.py:139][0m 1062912 total steps have happened
[32m[20221208 14:29:08 @agent_ppo2.py:115][0m #------------------------ Iteration 519 --------------------------#
[32m[20221208 14:29:09 @agent_ppo2.py:121][0m Sampling time: 0.50 s by 1 slaves
[32m[20221208 14:29:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |           0.0727 |           2.3227 |        -391.8497 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |           0.0300 |           2.0365 |        -379.6955 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |           0.0128 |           1.9080 |        -369.2890 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0061 |           1.8483 |        -395.3317 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0271 |           1.7597 |        -413.4864 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0413 |           1.6827 |        -428.2923 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0473 |           1.6475 |        -426.2166 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0520 |           1.6203 |        -428.9377 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0561 |           1.5808 |        -428.8894 |
[32m[20221208 14:29:09 @agent_ppo2.py:179][0m |          -0.0604 |           1.5618 |        -434.3658 |
[32m[20221208 14:29:09 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:29:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.58
[32m[20221208 14:29:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.26
[32m[20221208 14:29:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 20.64
[32m[20221208 14:29:10 @agent_ppo2.py:137][0m Total time:      13.08 min
[32m[20221208 14:29:10 @agent_ppo2.py:139][0m 1064960 total steps have happened
[32m[20221208 14:29:10 @agent_ppo2.py:115][0m #------------------------ Iteration 520 --------------------------#
[32m[20221208 14:29:10 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:29:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:10 @agent_ppo2.py:179][0m |           0.1163 |           4.1627 |        -344.0394 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |           0.0607 |           3.5241 |        -298.4782 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |           0.0203 |           3.2820 |        -337.8175 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0160 |           3.1205 |        -337.1033 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0400 |           2.9970 |        -342.8920 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0481 |           2.9096 |        -359.0364 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0549 |           2.8410 |        -356.1551 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0589 |           2.7556 |        -357.9986 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0684 |           2.7664 |        -360.8830 |
[32m[20221208 14:29:11 @agent_ppo2.py:179][0m |          -0.0689 |           2.6375 |        -368.0162 |
[32m[20221208 14:29:11 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:29:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.01
[32m[20221208 14:29:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.64
[32m[20221208 14:29:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.79
[32m[20221208 14:29:12 @agent_ppo2.py:137][0m Total time:      13.11 min
[32m[20221208 14:29:12 @agent_ppo2.py:139][0m 1067008 total steps have happened
[32m[20221208 14:29:12 @agent_ppo2.py:115][0m #------------------------ Iteration 521 --------------------------#
[32m[20221208 14:29:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |           0.0787 |           3.2351 |        -357.4680 |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |           0.0811 |           2.6347 |        -312.7679 |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |           0.0173 |           2.4216 |        -324.8877 |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |          -0.0221 |           2.2729 |        -343.6176 |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |          -0.0423 |           2.1729 |        -370.8459 |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |          -0.0511 |           2.0660 |        -370.7625 |
[32m[20221208 14:29:12 @agent_ppo2.py:179][0m |          -0.0495 |           1.9986 |        -365.2731 |
[32m[20221208 14:29:13 @agent_ppo2.py:179][0m |          -0.0582 |           1.9324 |        -375.7153 |
[32m[20221208 14:29:13 @agent_ppo2.py:179][0m |          -0.0622 |           1.8674 |        -372.1835 |
[32m[20221208 14:29:13 @agent_ppo2.py:179][0m |          -0.0638 |           1.8378 |        -374.2779 |
[32m[20221208 14:29:13 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:29:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.56
[32m[20221208 14:29:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.70
[32m[20221208 14:29:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.63
[32m[20221208 14:29:13 @agent_ppo2.py:137][0m Total time:      13.14 min
[32m[20221208 14:29:13 @agent_ppo2.py:139][0m 1069056 total steps have happened
[32m[20221208 14:29:13 @agent_ppo2.py:115][0m #------------------------ Iteration 522 --------------------------#
[32m[20221208 14:29:14 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:29:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |           0.1371 |           4.7207 |        -361.4200 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |           0.1053 |           4.1159 |        -216.9644 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |           0.0127 |           3.8861 |        -280.2556 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0233 |           3.7990 |        -315.9389 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0434 |           3.7006 |        -328.7766 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0537 |           3.6285 |        -339.0941 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0606 |           3.6134 |        -343.2472 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0681 |           3.5584 |        -357.9382 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0719 |           3.5474 |        -360.1516 |
[32m[20221208 14:29:14 @agent_ppo2.py:179][0m |          -0.0771 |           3.4710 |        -361.5052 |
[32m[20221208 14:29:14 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:29:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.35
[32m[20221208 14:29:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.70
[32m[20221208 14:29:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.28
[32m[20221208 14:29:15 @agent_ppo2.py:137][0m Total time:      13.16 min
[32m[20221208 14:29:15 @agent_ppo2.py:139][0m 1071104 total steps have happened
[32m[20221208 14:29:15 @agent_ppo2.py:115][0m #------------------------ Iteration 523 --------------------------#
[32m[20221208 14:29:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:15 @agent_ppo2.py:179][0m |           0.1163 |           3.7353 |        -369.7605 |
[32m[20221208 14:29:15 @agent_ppo2.py:179][0m |           0.2187 |           3.2550 |        -264.1364 |
[32m[20221208 14:29:15 @agent_ppo2.py:179][0m |           0.0725 |           3.0488 |        -240.9036 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |           0.0163 |           2.9111 |        -288.4871 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |          -0.0155 |           2.8198 |        -313.0545 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |          -0.0296 |           2.7316 |        -323.3536 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |          -0.0428 |           2.6882 |        -331.5405 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |          -0.0533 |           2.6265 |        -336.0037 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |          -0.0548 |           2.5962 |        -348.5547 |
[32m[20221208 14:29:16 @agent_ppo2.py:179][0m |          -0.0645 |           2.5622 |        -349.8790 |
[32m[20221208 14:29:16 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:29:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.35
[32m[20221208 14:29:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.92
[32m[20221208 14:29:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.56
[32m[20221208 14:29:16 @agent_ppo2.py:137][0m Total time:      13.19 min
[32m[20221208 14:29:16 @agent_ppo2.py:139][0m 1073152 total steps have happened
[32m[20221208 14:29:16 @agent_ppo2.py:115][0m #------------------------ Iteration 524 --------------------------#
[32m[20221208 14:29:17 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:29:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |           0.0768 |           2.3628 |        -368.0819 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |           2.0533 |           2.0201 |        -223.7701 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |           0.0301 |           1.9309 |        -226.3066 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |          -0.0007 |           1.8731 |        -232.8789 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |          -0.0203 |           1.8368 |        -251.9773 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |          -0.0298 |           1.8041 |        -267.8614 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |          -0.0464 |           1.7979 |        -288.1321 |
[32m[20221208 14:29:17 @agent_ppo2.py:179][0m |          -0.0540 |           1.7624 |        -297.0119 |
[32m[20221208 14:29:18 @agent_ppo2.py:179][0m |          -0.0609 |           1.7545 |        -307.5917 |
[32m[20221208 14:29:18 @agent_ppo2.py:179][0m |          -0.0644 |           1.7353 |        -304.7679 |
[32m[20221208 14:29:18 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:29:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.60
[32m[20221208 14:29:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.36
[32m[20221208 14:29:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.95
[32m[20221208 14:29:18 @agent_ppo2.py:137][0m Total time:      13.22 min
[32m[20221208 14:29:18 @agent_ppo2.py:139][0m 1075200 total steps have happened
[32m[20221208 14:29:18 @agent_ppo2.py:115][0m #------------------------ Iteration 525 --------------------------#
[32m[20221208 14:29:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |           0.1707 |           2.6723 |        -284.4964 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |           0.0353 |           2.3641 |        -258.3962 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |           0.0203 |           2.2177 |        -274.0078 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0009 |           2.0718 |        -284.0576 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0106 |           1.9921 |        -290.8477 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0160 |           1.9059 |        -295.1309 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0212 |           1.8490 |        -296.5240 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0262 |           1.8065 |        -301.4170 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0276 |           1.7631 |        -304.2594 |
[32m[20221208 14:29:19 @agent_ppo2.py:179][0m |          -0.0266 |           1.7432 |        -304.5660 |
[32m[20221208 14:29:19 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:29:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.48
[32m[20221208 14:29:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.96
[32m[20221208 14:29:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.27
[32m[20221208 14:29:20 @agent_ppo2.py:137][0m Total time:      13.25 min
[32m[20221208 14:29:20 @agent_ppo2.py:139][0m 1077248 total steps have happened
[32m[20221208 14:29:20 @agent_ppo2.py:115][0m #------------------------ Iteration 526 --------------------------#
[32m[20221208 14:29:20 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:29:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:20 @agent_ppo2.py:179][0m |           0.0823 |           2.0604 |        -392.6138 |
[32m[20221208 14:29:20 @agent_ppo2.py:179][0m |           0.0155 |           1.5484 |        -372.6987 |
[32m[20221208 14:29:20 @agent_ppo2.py:179][0m |          -0.0101 |           1.4097 |        -346.5910 |
[32m[20221208 14:29:20 @agent_ppo2.py:179][0m |          -0.0242 |           1.3242 |        -385.6120 |
[32m[20221208 14:29:21 @agent_ppo2.py:179][0m |          -0.0458 |           1.2542 |        -396.9979 |
[32m[20221208 14:29:21 @agent_ppo2.py:179][0m |          -0.0535 |           1.2226 |        -385.4361 |
[32m[20221208 14:29:21 @agent_ppo2.py:179][0m |          -0.0632 |           1.1920 |        -390.5301 |
[32m[20221208 14:29:21 @agent_ppo2.py:179][0m |          -0.0713 |           1.1614 |        -403.0250 |
[32m[20221208 14:29:21 @agent_ppo2.py:179][0m |          -0.0683 |           1.1279 |        -407.1632 |
[32m[20221208 14:29:21 @agent_ppo2.py:179][0m |          -0.0694 |           1.1063 |        -399.0396 |
[32m[20221208 14:29:21 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:29:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.56
[32m[20221208 14:29:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.50
[32m[20221208 14:29:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.26
[32m[20221208 14:29:21 @agent_ppo2.py:137][0m Total time:      13.27 min
[32m[20221208 14:29:21 @agent_ppo2.py:139][0m 1079296 total steps have happened
[32m[20221208 14:29:21 @agent_ppo2.py:115][0m #------------------------ Iteration 527 --------------------------#
[32m[20221208 14:29:22 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |           0.1338 |           3.9615 |        -385.4965 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |           0.0526 |           3.6237 |        -352.2570 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |           0.0007 |           3.4437 |        -384.3786 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |          -0.0187 |           3.3804 |        -402.0192 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |          -0.0372 |           3.2814 |        -405.5866 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |          -0.0477 |           3.2506 |        -422.7508 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |          -0.0490 |           3.2001 |        -425.7584 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |          -0.0554 |           3.1511 |        -427.9643 |
[32m[20221208 14:29:22 @agent_ppo2.py:179][0m |          -0.0649 |           3.1331 |        -434.5578 |
[32m[20221208 14:29:23 @agent_ppo2.py:179][0m |          -0.0673 |           3.1107 |        -443.0254 |
[32m[20221208 14:29:23 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:29:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.24
[32m[20221208 14:29:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.10
[32m[20221208 14:29:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.50
[32m[20221208 14:29:23 @agent_ppo2.py:137][0m Total time:      13.30 min
[32m[20221208 14:29:23 @agent_ppo2.py:139][0m 1081344 total steps have happened
[32m[20221208 14:29:23 @agent_ppo2.py:115][0m #------------------------ Iteration 528 --------------------------#
[32m[20221208 14:29:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |           0.0884 |           5.2729 |        -365.3457 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |           0.0931 |           4.9538 |        -251.6043 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |           0.0404 |           4.8469 |        -296.2390 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |           0.0041 |           4.7391 |        -335.6938 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |          -0.0183 |           4.6918 |        -353.7213 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |          -0.0311 |           4.5637 |        -367.9368 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |          -0.0359 |           4.5437 |        -382.2450 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |          -0.0441 |           4.4391 |        -388.2636 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |          -0.0549 |           4.3721 |        -402.2661 |
[32m[20221208 14:29:24 @agent_ppo2.py:179][0m |          -0.0584 |           4.3835 |        -405.6018 |
[32m[20221208 14:29:24 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 14:29:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.01
[32m[20221208 14:29:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.06
[32m[20221208 14:29:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.63
[32m[20221208 14:29:25 @agent_ppo2.py:137][0m Total time:      13.33 min
[32m[20221208 14:29:25 @agent_ppo2.py:139][0m 1083392 total steps have happened
[32m[20221208 14:29:25 @agent_ppo2.py:115][0m #------------------------ Iteration 529 --------------------------#
[32m[20221208 14:29:25 @agent_ppo2.py:121][0m Sampling time: 0.51 s by 1 slaves
[32m[20221208 14:29:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:25 @agent_ppo2.py:179][0m |           0.0511 |           1.2658 |        -343.4236 |
[32m[20221208 14:29:25 @agent_ppo2.py:179][0m |          -0.0185 |           1.0486 |        -262.0066 |
[32m[20221208 14:29:25 @agent_ppo2.py:179][0m |          -0.0344 |           1.0123 |        -271.3489 |
[32m[20221208 14:29:25 @agent_ppo2.py:179][0m |          -0.0330 |           0.9930 |        -275.9451 |
[32m[20221208 14:29:26 @agent_ppo2.py:179][0m |          -0.0367 |           0.9870 |        -276.2458 |
[32m[20221208 14:29:26 @agent_ppo2.py:179][0m |          -0.0428 |           0.9933 |        -289.5107 |
[32m[20221208 14:29:26 @agent_ppo2.py:179][0m |          -0.0382 |           0.9890 |        -285.0016 |
[32m[20221208 14:29:26 @agent_ppo2.py:179][0m |          -0.0250 |           0.9595 |        -284.0209 |
[32m[20221208 14:29:26 @agent_ppo2.py:179][0m |          -0.0383 |           0.9880 |        -286.7787 |
[32m[20221208 14:29:26 @agent_ppo2.py:179][0m |          -0.0411 |           0.9841 |        -292.0100 |
[32m[20221208 14:29:26 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:29:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 19.41
[32m[20221208 14:29:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.02
[32m[20221208 14:29:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.31
[32m[20221208 14:29:26 @agent_ppo2.py:137][0m Total time:      13.36 min
[32m[20221208 14:29:26 @agent_ppo2.py:139][0m 1085440 total steps have happened
[32m[20221208 14:29:26 @agent_ppo2.py:115][0m #------------------------ Iteration 530 --------------------------#
[32m[20221208 14:29:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |           0.0905 |           3.4243 |        -355.7014 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |           0.0610 |           3.0946 |        -352.6294 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |           0.0224 |           2.9985 |        -372.9844 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0077 |           2.9240 |        -405.0277 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0194 |           2.8464 |        -411.8394 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0326 |           2.7939 |        -425.0148 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0433 |           2.7642 |        -443.9191 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0487 |           2.7222 |        -453.2031 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0550 |           2.7108 |        -454.2879 |
[32m[20221208 14:29:27 @agent_ppo2.py:179][0m |          -0.0593 |           2.6498 |        -467.0123 |
[32m[20221208 14:29:27 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:29:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.13
[32m[20221208 14:29:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.13
[32m[20221208 14:29:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.29
[32m[20221208 14:29:28 @agent_ppo2.py:137][0m Total time:      13.38 min
[32m[20221208 14:29:28 @agent_ppo2.py:139][0m 1087488 total steps have happened
[32m[20221208 14:29:28 @agent_ppo2.py:115][0m #------------------------ Iteration 531 --------------------------#
[32m[20221208 14:29:28 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:29:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:28 @agent_ppo2.py:179][0m |           0.0594 |           3.9160 |        -412.5969 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |           0.0907 |           3.6892 |        -323.1059 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |           0.0449 |           3.5256 |        -347.1037 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0033 |           3.4765 |        -375.2646 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0294 |           3.3605 |        -400.8846 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0378 |           3.3154 |        -403.5132 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0482 |           3.3048 |        -415.2200 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0520 |           3.2376 |        -419.1776 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0520 |           3.2463 |        -418.6554 |
[32m[20221208 14:29:29 @agent_ppo2.py:179][0m |          -0.0601 |           3.1682 |        -428.8888 |
[32m[20221208 14:29:29 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:29:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.20
[32m[20221208 14:29:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.44
[32m[20221208 14:29:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.21
[32m[20221208 14:29:30 @agent_ppo2.py:137][0m Total time:      13.41 min
[32m[20221208 14:29:30 @agent_ppo2.py:139][0m 1089536 total steps have happened
[32m[20221208 14:29:30 @agent_ppo2.py:115][0m #------------------------ Iteration 532 --------------------------#
[32m[20221208 14:29:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |           0.0841 |           2.1893 |        -382.0566 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |           0.0367 |           1.7916 |        -362.2198 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |           0.0039 |           1.6704 |        -384.7307 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |          -0.0197 |           1.5755 |        -411.5669 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |          -0.0346 |           1.5265 |        -419.0491 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |          -0.0340 |           1.4499 |        -417.0300 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |          -0.0462 |           1.4306 |        -433.5090 |
[32m[20221208 14:29:30 @agent_ppo2.py:179][0m |          -0.0484 |           1.3831 |        -431.4231 |
[32m[20221208 14:29:31 @agent_ppo2.py:179][0m |          -0.0514 |           1.3531 |        -436.1961 |
[32m[20221208 14:29:31 @agent_ppo2.py:179][0m |          -0.0508 |           1.3242 |        -439.3462 |
[32m[20221208 14:29:31 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:29:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.41
[32m[20221208 14:29:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.97
[32m[20221208 14:29:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.18
[32m[20221208 14:29:31 @agent_ppo2.py:137][0m Total time:      13.44 min
[32m[20221208 14:29:31 @agent_ppo2.py:139][0m 1091584 total steps have happened
[32m[20221208 14:29:31 @agent_ppo2.py:115][0m #------------------------ Iteration 533 --------------------------#
[32m[20221208 14:29:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |           1.0797 |           1.1198 |        -270.5686 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |           0.1324 |           0.8983 |        -171.8626 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |           0.0279 |           0.8549 |        -229.3193 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |           0.0037 |           0.8119 |        -244.5385 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |          -0.0220 |           0.7928 |        -254.4726 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |          -0.0414 |           0.7676 |        -276.5008 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |          -0.0369 |           0.7464 |        -270.1594 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |          -0.0483 |           0.7400 |        -275.6045 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |          -0.0541 |           0.7319 |        -273.2621 |
[32m[20221208 14:29:32 @agent_ppo2.py:179][0m |          -0.0602 |           0.7173 |        -282.7003 |
[32m[20221208 14:29:32 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:29:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.43
[32m[20221208 14:29:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.93
[32m[20221208 14:29:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.16
[32m[20221208 14:29:33 @agent_ppo2.py:137][0m Total time:      13.46 min
[32m[20221208 14:29:33 @agent_ppo2.py:139][0m 1093632 total steps have happened
[32m[20221208 14:29:33 @agent_ppo2.py:115][0m #------------------------ Iteration 534 --------------------------#
[32m[20221208 14:29:33 @agent_ppo2.py:121][0m Sampling time: 0.50 s by 1 slaves
[32m[20221208 14:29:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:33 @agent_ppo2.py:179][0m |           0.0908 |           2.6719 |        -348.8233 |
[32m[20221208 14:29:33 @agent_ppo2.py:179][0m |           0.0866 |           2.1880 |        -229.1796 |
[32m[20221208 14:29:33 @agent_ppo2.py:179][0m |           0.0047 |           2.0265 |        -261.9867 |
[32m[20221208 14:29:33 @agent_ppo2.py:179][0m |          -0.0183 |           1.9464 |        -275.9397 |
[32m[20221208 14:29:34 @agent_ppo2.py:179][0m |          -0.0396 |           1.9103 |        -289.2082 |
[32m[20221208 14:29:34 @agent_ppo2.py:179][0m |          -0.0540 |           1.8469 |        -302.0612 |
[32m[20221208 14:29:34 @agent_ppo2.py:179][0m |          -0.0556 |           1.7904 |        -304.5862 |
[32m[20221208 14:29:34 @agent_ppo2.py:179][0m |          -0.0641 |           1.7545 |        -308.9423 |
[32m[20221208 14:29:34 @agent_ppo2.py:179][0m |          -0.0713 |           1.7370 |        -309.9129 |
[32m[20221208 14:29:34 @agent_ppo2.py:179][0m |          -0.0748 |           1.6976 |        -320.4899 |
[32m[20221208 14:29:34 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 14:29:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.56
[32m[20221208 14:29:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.90
[32m[20221208 14:29:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.19
[32m[20221208 14:29:34 @agent_ppo2.py:137][0m Total time:      13.49 min
[32m[20221208 14:29:34 @agent_ppo2.py:139][0m 1095680 total steps have happened
[32m[20221208 14:29:34 @agent_ppo2.py:115][0m #------------------------ Iteration 535 --------------------------#
[32m[20221208 14:29:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |           0.0945 |           2.0399 |        -409.3246 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |           0.0429 |           1.3847 |        -362.3913 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |           0.0295 |           1.2178 |        -306.6305 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |          -0.0087 |           1.1244 |        -293.3620 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |          -0.0282 |           1.0644 |        -304.6084 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |          -0.0409 |           1.0239 |        -309.3121 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |          -0.0514 |           0.9853 |        -313.8880 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |          -0.0547 |           0.9586 |        -317.0203 |
[32m[20221208 14:29:35 @agent_ppo2.py:179][0m |          -0.0605 |           0.9335 |        -329.1792 |
[32m[20221208 14:29:36 @agent_ppo2.py:179][0m |          -0.0668 |           0.9174 |        -333.3609 |
[32m[20221208 14:29:36 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:29:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.85
[32m[20221208 14:29:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.99
[32m[20221208 14:29:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.49
[32m[20221208 14:29:36 @agent_ppo2.py:137][0m Total time:      13.52 min
[32m[20221208 14:29:36 @agent_ppo2.py:139][0m 1097728 total steps have happened
[32m[20221208 14:29:36 @agent_ppo2.py:115][0m #------------------------ Iteration 536 --------------------------#
[32m[20221208 14:29:36 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:29:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |           0.0436 |           4.7945 |        -476.5620 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |           0.0412 |           4.0354 |        -389.1718 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0005 |           3.8392 |        -360.3032 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0311 |           3.6023 |        -412.7936 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0407 |           3.5082 |        -432.0386 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0463 |           3.4065 |        -425.8371 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0539 |           3.2724 |        -403.4358 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0606 |           3.1854 |        -422.5828 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0652 |           3.1088 |        -418.7626 |
[32m[20221208 14:29:37 @agent_ppo2.py:179][0m |          -0.0685 |           3.0549 |        -426.8406 |
[32m[20221208 14:29:37 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:29:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.87
[32m[20221208 14:29:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.26
[32m[20221208 14:29:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.36
[32m[20221208 14:29:38 @agent_ppo2.py:137][0m Total time:      13.55 min
[32m[20221208 14:29:38 @agent_ppo2.py:139][0m 1099776 total steps have happened
[32m[20221208 14:29:38 @agent_ppo2.py:115][0m #------------------------ Iteration 537 --------------------------#
[32m[20221208 14:29:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:38 @agent_ppo2.py:179][0m |           0.0569 |           2.1722 |        -408.5154 |
[32m[20221208 14:29:38 @agent_ppo2.py:179][0m |           0.0367 |           1.6683 |        -357.0689 |
[32m[20221208 14:29:38 @agent_ppo2.py:179][0m |           0.0298 |           1.4488 |        -300.9919 |
[32m[20221208 14:29:38 @agent_ppo2.py:179][0m |           0.0240 |           1.3023 |        -251.3722 |
[32m[20221208 14:29:38 @agent_ppo2.py:179][0m |           0.0038 |           1.2303 |        -231.7303 |
[32m[20221208 14:29:39 @agent_ppo2.py:179][0m |          -0.0088 |           1.1383 |        -248.3311 |
[32m[20221208 14:29:39 @agent_ppo2.py:179][0m |          -0.0153 |           1.0883 |        -272.1655 |
[32m[20221208 14:29:39 @agent_ppo2.py:179][0m |          -0.0261 |           1.0543 |        -268.4384 |
[32m[20221208 14:29:39 @agent_ppo2.py:179][0m |          -0.0302 |           1.0014 |        -279.9016 |
[32m[20221208 14:29:39 @agent_ppo2.py:179][0m |          -0.0275 |           0.9709 |        -297.8885 |
[32m[20221208 14:29:39 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:29:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.21
[32m[20221208 14:29:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.46
[32m[20221208 14:29:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.30
[32m[20221208 14:29:39 @agent_ppo2.py:137][0m Total time:      13.57 min
[32m[20221208 14:29:39 @agent_ppo2.py:139][0m 1101824 total steps have happened
[32m[20221208 14:29:39 @agent_ppo2.py:115][0m #------------------------ Iteration 538 --------------------------#
[32m[20221208 14:29:40 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:29:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |           0.2654 |           1.3892 |        -225.8376 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |           0.0923 |           1.0545 |        -193.1081 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |           0.0232 |           1.0199 |        -258.4206 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |           0.0132 |           0.9997 |        -261.2071 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |          -0.0141 |           0.9697 |        -272.8023 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |          -0.0119 |           0.9688 |        -274.9212 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |          -0.0104 |           0.9528 |        -276.5564 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |          -0.0329 |           0.9475 |        -283.6727 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |          -0.0144 |           0.9403 |        -276.2139 |
[32m[20221208 14:29:40 @agent_ppo2.py:179][0m |          -0.0191 |           0.9427 |        -282.3268 |
[32m[20221208 14:29:40 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:29:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.70
[32m[20221208 14:29:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 28.75
[32m[20221208 14:29:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.47
[32m[20221208 14:29:41 @agent_ppo2.py:137][0m Total time:      13.60 min
[32m[20221208 14:29:41 @agent_ppo2.py:139][0m 1103872 total steps have happened
[32m[20221208 14:29:41 @agent_ppo2.py:115][0m #------------------------ Iteration 539 --------------------------#
[32m[20221208 14:29:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:41 @agent_ppo2.py:179][0m |           0.0792 |           2.0660 |        -376.6280 |
[32m[20221208 14:29:41 @agent_ppo2.py:179][0m |           0.0463 |           1.5158 |        -340.7568 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0073 |           1.3946 |        -354.7444 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0407 |           1.3471 |        -366.3519 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0485 |           1.2799 |        -370.5169 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0580 |           1.2646 |        -356.5800 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0652 |           1.2127 |        -373.1338 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0721 |           1.2042 |        -374.9490 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0717 |           1.1957 |        -375.2111 |
[32m[20221208 14:29:42 @agent_ppo2.py:179][0m |          -0.0814 |           1.1404 |        -383.1079 |
[32m[20221208 14:29:42 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:29:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.30
[32m[20221208 14:29:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.00
[32m[20221208 14:29:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.41
[32m[20221208 14:29:42 @agent_ppo2.py:137][0m Total time:      13.62 min
[32m[20221208 14:29:42 @agent_ppo2.py:139][0m 1105920 total steps have happened
[32m[20221208 14:29:42 @agent_ppo2.py:115][0m #------------------------ Iteration 540 --------------------------#
[32m[20221208 14:29:43 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |           0.0725 |           2.0227 |        -349.0728 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |           0.0494 |           1.6732 |        -310.2097 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |           0.0079 |           1.5500 |        -331.0218 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0137 |           1.4437 |        -349.7698 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0315 |           1.3768 |        -370.1573 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0402 |           1.3238 |        -375.9941 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0473 |           1.2778 |        -375.5250 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0526 |           1.2428 |        -377.9374 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0565 |           1.2058 |        -380.4952 |
[32m[20221208 14:29:43 @agent_ppo2.py:179][0m |          -0.0580 |           1.1696 |        -385.0296 |
[32m[20221208 14:29:43 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:29:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.50
[32m[20221208 14:29:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.29
[32m[20221208 14:29:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.29
[32m[20221208 14:29:44 @agent_ppo2.py:137][0m Total time:      13.65 min
[32m[20221208 14:29:44 @agent_ppo2.py:139][0m 1107968 total steps have happened
[32m[20221208 14:29:44 @agent_ppo2.py:115][0m #------------------------ Iteration 541 --------------------------#
[32m[20221208 14:29:44 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:44 @agent_ppo2.py:179][0m |           0.0678 |           2.8033 |        -369.3098 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |           0.0429 |           2.4038 |        -312.0695 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0006 |           2.2332 |        -357.9957 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0353 |           2.1082 |        -388.3513 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0516 |           2.0459 |        -390.7076 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0613 |           1.9564 |        -405.0980 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0686 |           1.9140 |        -402.7595 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0716 |           1.8515 |        -410.6167 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0786 |           1.8192 |        -417.5477 |
[32m[20221208 14:29:45 @agent_ppo2.py:179][0m |          -0.0803 |           1.7743 |        -416.5965 |
[32m[20221208 14:29:45 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:29:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.39
[32m[20221208 14:29:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.22
[32m[20221208 14:29:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.38
[32m[20221208 14:29:46 @agent_ppo2.py:137][0m Total time:      13.68 min
[32m[20221208 14:29:46 @agent_ppo2.py:139][0m 1110016 total steps have happened
[32m[20221208 14:29:46 @agent_ppo2.py:115][0m #------------------------ Iteration 542 --------------------------#
[32m[20221208 14:29:46 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:29:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:46 @agent_ppo2.py:179][0m |           0.0904 |           2.4230 |        -350.7979 |
[32m[20221208 14:29:46 @agent_ppo2.py:179][0m |           0.0620 |           2.0576 |        -267.0121 |
[32m[20221208 14:29:46 @agent_ppo2.py:179][0m |           0.0118 |           1.8845 |        -291.3082 |
[32m[20221208 14:29:46 @agent_ppo2.py:179][0m |          -0.0187 |           1.7635 |        -327.6061 |
[32m[20221208 14:29:46 @agent_ppo2.py:179][0m |          -0.0398 |           1.6800 |        -349.3181 |
[32m[20221208 14:29:46 @agent_ppo2.py:179][0m |          -0.0528 |           1.6212 |        -351.7948 |
[32m[20221208 14:29:47 @agent_ppo2.py:179][0m |          -0.0592 |           1.5556 |        -360.6970 |
[32m[20221208 14:29:47 @agent_ppo2.py:179][0m |          -0.0637 |           1.4983 |        -366.6553 |
[32m[20221208 14:29:47 @agent_ppo2.py:179][0m |          -0.0677 |           1.4422 |        -371.4050 |
[32m[20221208 14:29:47 @agent_ppo2.py:179][0m |          -0.0731 |           1.3941 |        -381.7269 |
[32m[20221208 14:29:47 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:29:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.40
[32m[20221208 14:29:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.02
[32m[20221208 14:29:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.40
[32m[20221208 14:29:47 @agent_ppo2.py:137][0m Total time:      13.70 min
[32m[20221208 14:29:47 @agent_ppo2.py:139][0m 1112064 total steps have happened
[32m[20221208 14:29:47 @agent_ppo2.py:115][0m #------------------------ Iteration 543 --------------------------#
[32m[20221208 14:29:48 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |           0.0880 |           3.1146 |        -389.8183 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |           0.0780 |           2.6374 |        -349.0078 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |           0.0348 |           2.3786 |        -345.5705 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0025 |           2.2116 |        -393.2007 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0216 |           2.0557 |        -408.1075 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0361 |           1.9727 |        -409.1951 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0485 |           1.8820 |        -416.9425 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0474 |           1.7913 |        -421.0287 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0544 |           1.7322 |        -424.4174 |
[32m[20221208 14:29:48 @agent_ppo2.py:179][0m |          -0.0598 |           1.6641 |        -428.5298 |
[32m[20221208 14:29:48 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:29:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.76
[32m[20221208 14:29:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.43
[32m[20221208 14:29:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.11
[32m[20221208 14:29:49 @agent_ppo2.py:137][0m Total time:      13.73 min
[32m[20221208 14:29:49 @agent_ppo2.py:139][0m 1114112 total steps have happened
[32m[20221208 14:29:49 @agent_ppo2.py:115][0m #------------------------ Iteration 544 --------------------------#
[32m[20221208 14:29:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:49 @agent_ppo2.py:179][0m |           0.0553 |           2.2268 |        -388.9304 |
[32m[20221208 14:29:49 @agent_ppo2.py:179][0m |           0.0216 |           1.6183 |        -362.0613 |
[32m[20221208 14:29:49 @agent_ppo2.py:179][0m |          -0.0130 |           1.4115 |        -353.2375 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0362 |           1.3089 |        -394.2815 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0511 |           1.2102 |        -407.8352 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0640 |           1.1636 |        -421.9566 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0663 |           1.1120 |        -420.4349 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0705 |           1.0687 |        -421.4633 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0697 |           1.0454 |        -417.7010 |
[32m[20221208 14:29:50 @agent_ppo2.py:179][0m |          -0.0727 |           1.0047 |        -415.3643 |
[32m[20221208 14:29:50 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:29:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.39
[32m[20221208 14:29:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.72
[32m[20221208 14:29:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.98
[32m[20221208 14:29:50 @agent_ppo2.py:137][0m Total time:      13.76 min
[32m[20221208 14:29:50 @agent_ppo2.py:139][0m 1116160 total steps have happened
[32m[20221208 14:29:50 @agent_ppo2.py:115][0m #------------------------ Iteration 545 --------------------------#
[32m[20221208 14:29:51 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:29:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |           0.0708 |           2.6811 |        -371.0388 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |           0.0412 |           1.9010 |        -349.9189 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0115 |           1.6871 |        -361.0677 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0269 |           1.5387 |        -369.3668 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0333 |           1.4320 |        -356.2921 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0280 |           1.3644 |        -292.1366 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0404 |           1.2907 |        -294.0184 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0560 |           1.2535 |        -348.0059 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0628 |           1.2116 |        -375.6544 |
[32m[20221208 14:29:51 @agent_ppo2.py:179][0m |          -0.0653 |           1.1762 |        -369.5712 |
[32m[20221208 14:29:51 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:29:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.46
[32m[20221208 14:29:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.07
[32m[20221208 14:29:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.81
[32m[20221208 14:29:52 @agent_ppo2.py:137][0m Total time:      13.78 min
[32m[20221208 14:29:52 @agent_ppo2.py:139][0m 1118208 total steps have happened
[32m[20221208 14:29:52 @agent_ppo2.py:115][0m #------------------------ Iteration 546 --------------------------#
[32m[20221208 14:29:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:29:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:52 @agent_ppo2.py:179][0m |           0.1006 |           2.4384 |        -312.2794 |
[32m[20221208 14:29:52 @agent_ppo2.py:179][0m |           0.1028 |           2.0141 |        -163.2944 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |           0.0476 |           1.8225 |        -158.7433 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |           0.0299 |           1.7386 |        -181.8619 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |           0.0321 |           1.6662 |        -149.3912 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |           0.0105 |           1.6035 |        -181.1062 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |           0.0006 |           1.5809 |        -205.9585 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |          -0.0115 |           1.5454 |        -210.0686 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |          -0.0114 |           1.5264 |        -221.2105 |
[32m[20221208 14:29:53 @agent_ppo2.py:179][0m |          -0.0078 |           1.4935 |        -227.7710 |
[32m[20221208 14:29:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:29:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.93
[32m[20221208 14:29:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.45
[32m[20221208 14:29:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.71
[32m[20221208 14:29:53 @agent_ppo2.py:137][0m Total time:      13.81 min
[32m[20221208 14:29:53 @agent_ppo2.py:139][0m 1120256 total steps have happened
[32m[20221208 14:29:53 @agent_ppo2.py:115][0m #------------------------ Iteration 547 --------------------------#
[32m[20221208 14:29:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:29:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |           0.1081 |           3.4187 |        -347.0874 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |           0.1318 |           2.8934 |        -230.5942 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |           0.0836 |           2.6972 |        -249.6846 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |           0.0345 |           2.5912 |        -291.9462 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |          -0.0047 |           2.4900 |        -313.5654 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |          -0.0170 |           2.4252 |        -332.9482 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |          -0.0366 |           2.3641 |        -355.8900 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |          -0.0364 |           2.3096 |        -354.1030 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |          -0.0488 |           2.2622 |        -363.2852 |
[32m[20221208 14:29:54 @agent_ppo2.py:179][0m |          -0.0567 |           2.2344 |        -377.0213 |
[32m[20221208 14:29:54 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:29:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.21
[32m[20221208 14:29:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.78
[32m[20221208 14:29:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.81
[32m[20221208 14:29:55 @agent_ppo2.py:137][0m Total time:      13.83 min
[32m[20221208 14:29:55 @agent_ppo2.py:139][0m 1122304 total steps have happened
[32m[20221208 14:29:55 @agent_ppo2.py:115][0m #------------------------ Iteration 548 --------------------------#
[32m[20221208 14:29:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:55 @agent_ppo2.py:179][0m |           0.0886 |           2.9362 |        -362.7896 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |           0.0690 |           2.5718 |        -310.1120 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |           0.0759 |           2.4367 |        -277.8779 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |           0.0342 |           2.3462 |        -252.0613 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |          -0.0066 |           2.2728 |        -278.9761 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |          -0.0252 |           2.2521 |        -294.6302 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |          -0.0390 |           2.1982 |        -304.6092 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |          -0.0460 |           2.1805 |        -303.7938 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |          -0.0498 |           2.1657 |        -317.6264 |
[32m[20221208 14:29:56 @agent_ppo2.py:179][0m |          -0.0552 |           2.1216 |        -325.6588 |
[32m[20221208 14:29:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:29:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.64
[32m[20221208 14:29:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.00
[32m[20221208 14:29:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.82
[32m[20221208 14:29:56 @agent_ppo2.py:137][0m Total time:      13.86 min
[32m[20221208 14:29:56 @agent_ppo2.py:139][0m 1124352 total steps have happened
[32m[20221208 14:29:56 @agent_ppo2.py:115][0m #------------------------ Iteration 549 --------------------------#
[32m[20221208 14:29:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:29:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |           0.0622 |           2.4977 |        -359.1285 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |           0.0603 |           2.2408 |        -280.1196 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |           0.0102 |           2.1315 |        -275.7092 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |          -0.0063 |           2.0576 |        -262.7318 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |          -0.0278 |           2.0275 |        -291.7569 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |          -0.0429 |           1.9684 |        -300.9282 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |          -0.0515 |           1.9299 |        -312.5738 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |          -0.0201 |           1.8681 |        -293.6745 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |          -0.0018 |           1.8373 |        -272.8574 |
[32m[20221208 14:29:57 @agent_ppo2.py:179][0m |           0.0587 |           1.8010 |        -185.6409 |
[32m[20221208 14:29:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:29:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.50
[32m[20221208 14:29:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.27
[32m[20221208 14:29:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.91
[32m[20221208 14:29:58 @agent_ppo2.py:137][0m Total time:      13.88 min
[32m[20221208 14:29:58 @agent_ppo2.py:139][0m 1126400 total steps have happened
[32m[20221208 14:29:58 @agent_ppo2.py:115][0m #------------------------ Iteration 550 --------------------------#
[32m[20221208 14:29:58 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:29:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:29:58 @agent_ppo2.py:179][0m |           0.0679 |           3.3609 |        -360.6005 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |           0.0285 |           3.1396 |        -341.8384 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0082 |           3.0295 |        -367.5487 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0258 |           2.9744 |        -376.0952 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0381 |           2.9323 |        -388.7133 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0486 |           2.8946 |        -393.6791 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0568 |           2.8283 |        -395.2248 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0582 |           2.8129 |        -405.5382 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0639 |           2.7994 |        -406.9624 |
[32m[20221208 14:29:59 @agent_ppo2.py:179][0m |          -0.0622 |           2.7698 |        -404.5009 |
[32m[20221208 14:29:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:29:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.91
[32m[20221208 14:29:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.68
[32m[20221208 14:29:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.42
[32m[20221208 14:29:59 @agent_ppo2.py:137][0m Total time:      13.91 min
[32m[20221208 14:29:59 @agent_ppo2.py:139][0m 1128448 total steps have happened
[32m[20221208 14:29:59 @agent_ppo2.py:115][0m #------------------------ Iteration 551 --------------------------#
[32m[20221208 14:30:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |           0.1367 |           4.3731 |        -348.8582 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |           0.1165 |           4.0390 |        -301.9781 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |           0.0418 |           3.8736 |        -302.5120 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |          -0.0029 |           3.8077 |        -344.5415 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |          -0.0258 |           3.7191 |        -367.1875 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |          -0.0341 |           3.6665 |        -371.3865 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |          -0.0489 |           3.6124 |        -386.6040 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |          -0.0559 |           3.5791 |        -389.4648 |
[32m[20221208 14:30:00 @agent_ppo2.py:179][0m |          -0.0610 |           3.5285 |        -395.3263 |
[32m[20221208 14:30:01 @agent_ppo2.py:179][0m |          -0.0655 |           3.4821 |        -395.6847 |
[32m[20221208 14:30:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.04
[32m[20221208 14:30:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.71
[32m[20221208 14:30:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.43
[32m[20221208 14:30:01 @agent_ppo2.py:137][0m Total time:      13.93 min
[32m[20221208 14:30:01 @agent_ppo2.py:139][0m 1130496 total steps have happened
[32m[20221208 14:30:01 @agent_ppo2.py:115][0m #------------------------ Iteration 552 --------------------------#
[32m[20221208 14:30:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |           0.0399 |           4.1832 |        -394.6785 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |           0.0502 |           3.8418 |        -346.5864 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |           0.0228 |           3.7028 |        -349.8217 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0198 |           3.6065 |        -372.1544 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0250 |           3.5339 |        -370.1789 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0394 |           3.5028 |        -382.3090 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0510 |           3.4546 |        -392.0631 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0558 |           3.3892 |        -402.9161 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0640 |           3.3602 |        -408.2347 |
[32m[20221208 14:30:02 @agent_ppo2.py:179][0m |          -0.0676 |           3.3078 |        -413.8858 |
[32m[20221208 14:30:02 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:30:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.62
[32m[20221208 14:30:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.61
[32m[20221208 14:30:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.63
[32m[20221208 14:30:03 @agent_ppo2.py:137][0m Total time:      13.96 min
[32m[20221208 14:30:03 @agent_ppo2.py:139][0m 1132544 total steps have happened
[32m[20221208 14:30:03 @agent_ppo2.py:115][0m #------------------------ Iteration 553 --------------------------#
[32m[20221208 14:30:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |           0.0620 |           3.0544 |        -360.3844 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |           0.0119 |           2.7552 |        -342.8121 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |          -0.0011 |           2.6014 |        -356.9870 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |          -0.0368 |           2.4882 |        -379.4416 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |          -0.0479 |           2.4187 |        -387.5247 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |          -0.0550 |           2.3677 |        -390.4277 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |          -0.0555 |           2.3231 |        -386.5755 |
[32m[20221208 14:30:03 @agent_ppo2.py:179][0m |          -0.0629 |           2.3091 |        -393.6232 |
[32m[20221208 14:30:04 @agent_ppo2.py:179][0m |          -0.0620 |           2.3034 |        -400.0454 |
[32m[20221208 14:30:04 @agent_ppo2.py:179][0m |          -0.0650 |           2.2655 |        -397.2264 |
[32m[20221208 14:30:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:30:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.66
[32m[20221208 14:30:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.09
[32m[20221208 14:30:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.23
[32m[20221208 14:30:04 @agent_ppo2.py:137][0m Total time:      13.99 min
[32m[20221208 14:30:04 @agent_ppo2.py:139][0m 1134592 total steps have happened
[32m[20221208 14:30:04 @agent_ppo2.py:115][0m #------------------------ Iteration 554 --------------------------#
[32m[20221208 14:30:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:30:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |           0.0794 |           3.1169 |        -399.7398 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |           0.0752 |           2.6272 |        -311.1199 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |           0.0185 |           2.3948 |        -251.7909 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0283 |           2.2597 |        -279.4303 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0468 |           2.1901 |        -300.2695 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0571 |           2.1342 |        -303.9464 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0645 |           2.0656 |        -312.7057 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0732 |           2.0295 |        -322.3576 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0742 |           1.9874 |        -328.4989 |
[32m[20221208 14:30:05 @agent_ppo2.py:179][0m |          -0.0724 |           1.9552 |        -322.6313 |
[32m[20221208 14:30:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.37
[32m[20221208 14:30:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.80
[32m[20221208 14:30:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.49
[32m[20221208 14:30:06 @agent_ppo2.py:137][0m Total time:      14.01 min
[32m[20221208 14:30:06 @agent_ppo2.py:139][0m 1136640 total steps have happened
[32m[20221208 14:30:06 @agent_ppo2.py:115][0m #------------------------ Iteration 555 --------------------------#
[32m[20221208 14:30:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |           0.0987 |           2.7544 |        -392.9302 |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |           0.0912 |           2.3404 |        -284.7518 |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |           0.0006 |           2.1541 |        -254.4073 |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |          -0.0321 |           2.0573 |        -279.3698 |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |          -0.0534 |           2.0101 |        -287.6118 |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |          -0.0652 |           1.9555 |        -301.1137 |
[32m[20221208 14:30:06 @agent_ppo2.py:179][0m |          -0.0735 |           1.9324 |        -305.6587 |
[32m[20221208 14:30:07 @agent_ppo2.py:179][0m |          -0.0773 |           1.8833 |        -316.5132 |
[32m[20221208 14:30:07 @agent_ppo2.py:179][0m |          -0.0796 |           1.8656 |        -317.3755 |
[32m[20221208 14:30:07 @agent_ppo2.py:179][0m |          -0.0846 |           1.8362 |        -320.8795 |
[32m[20221208 14:30:07 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:30:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.69
[32m[20221208 14:30:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.03
[32m[20221208 14:30:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.91
[32m[20221208 14:30:07 @agent_ppo2.py:137][0m Total time:      14.04 min
[32m[20221208 14:30:07 @agent_ppo2.py:139][0m 1138688 total steps have happened
[32m[20221208 14:30:07 @agent_ppo2.py:115][0m #------------------------ Iteration 556 --------------------------#
[32m[20221208 14:30:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:30:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |           0.0980 |           2.7617 |        -370.9207 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |           0.0309 |           2.4665 |        -238.8743 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0032 |           2.3709 |        -248.5186 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0276 |           2.2747 |        -253.6146 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0444 |           2.2294 |        -280.4106 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0545 |           2.1854 |        -290.1368 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0555 |           2.1291 |        -295.6755 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0588 |           2.1160 |        -296.6508 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0661 |           2.0909 |        -310.0979 |
[32m[20221208 14:30:08 @agent_ppo2.py:179][0m |          -0.0700 |           2.0536 |        -314.5630 |
[32m[20221208 14:30:08 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:30:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.76
[32m[20221208 14:30:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.39
[32m[20221208 14:30:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.35
[32m[20221208 14:30:09 @agent_ppo2.py:137][0m Total time:      14.06 min
[32m[20221208 14:30:09 @agent_ppo2.py:139][0m 1140736 total steps have happened
[32m[20221208 14:30:09 @agent_ppo2.py:115][0m #------------------------ Iteration 557 --------------------------#
[32m[20221208 14:30:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:30:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |           0.0956 |           2.8967 |        -471.5596 |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |           0.0725 |           2.4573 |        -338.1384 |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |           0.0026 |           2.2837 |        -314.7930 |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |          -0.0258 |           2.1750 |        -333.5339 |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |          -0.0360 |           2.1039 |        -339.5434 |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |          -0.0525 |           2.0572 |        -355.6230 |
[32m[20221208 14:30:09 @agent_ppo2.py:179][0m |          -0.0574 |           2.0051 |        -369.2446 |
[32m[20221208 14:30:10 @agent_ppo2.py:179][0m |          -0.0662 |           1.9746 |        -377.2781 |
[32m[20221208 14:30:10 @agent_ppo2.py:179][0m |          -0.0691 |           1.9669 |        -381.3577 |
[32m[20221208 14:30:10 @agent_ppo2.py:179][0m |          -0.0740 |           1.9104 |        -389.1865 |
[32m[20221208 14:30:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.21
[32m[20221208 14:30:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.92
[32m[20221208 14:30:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.93
[32m[20221208 14:30:10 @agent_ppo2.py:137][0m Total time:      14.09 min
[32m[20221208 14:30:10 @agent_ppo2.py:139][0m 1142784 total steps have happened
[32m[20221208 14:30:10 @agent_ppo2.py:115][0m #------------------------ Iteration 558 --------------------------#
[32m[20221208 14:30:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |           0.1106 |           3.6188 |        -472.4036 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |           0.0716 |           3.0260 |        -382.6169 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |           0.0080 |           2.8747 |        -433.4127 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0140 |           2.7728 |        -466.6636 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0231 |           2.7097 |        -476.7454 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0381 |           2.6674 |        -488.9719 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0430 |           2.6001 |        -495.8775 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0535 |           2.5697 |        -510.5137 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0582 |           2.5459 |        -517.2569 |
[32m[20221208 14:30:11 @agent_ppo2.py:179][0m |          -0.0650 |           2.5385 |        -527.1849 |
[32m[20221208 14:30:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:30:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.75
[32m[20221208 14:30:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.12
[32m[20221208 14:30:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.82
[32m[20221208 14:30:12 @agent_ppo2.py:137][0m Total time:      14.11 min
[32m[20221208 14:30:12 @agent_ppo2.py:139][0m 1144832 total steps have happened
[32m[20221208 14:30:12 @agent_ppo2.py:115][0m #------------------------ Iteration 559 --------------------------#
[32m[20221208 14:30:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |           0.0932 |           4.1566 |        -474.9679 |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |           0.0699 |           3.6207 |        -347.9862 |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |           0.0040 |           3.4486 |        -429.9699 |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |          -0.0250 |           3.3308 |        -462.1806 |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |          -0.0423 |           3.2621 |        -467.0205 |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |          -0.0461 |           3.1983 |        -478.9638 |
[32m[20221208 14:30:12 @agent_ppo2.py:179][0m |          -0.0558 |           3.1683 |        -481.6137 |
[32m[20221208 14:30:13 @agent_ppo2.py:179][0m |          -0.0635 |           3.1264 |        -500.2237 |
[32m[20221208 14:30:13 @agent_ppo2.py:179][0m |          -0.0678 |           3.0854 |        -506.1170 |
[32m[20221208 14:30:13 @agent_ppo2.py:179][0m |          -0.0716 |           3.0686 |        -510.7724 |
[32m[20221208 14:30:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.58
[32m[20221208 14:30:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.29
[32m[20221208 14:30:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.61
[32m[20221208 14:30:13 @agent_ppo2.py:137][0m Total time:      14.14 min
[32m[20221208 14:30:13 @agent_ppo2.py:139][0m 1146880 total steps have happened
[32m[20221208 14:30:13 @agent_ppo2.py:115][0m #------------------------ Iteration 560 --------------------------#
[32m[20221208 14:30:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |           0.0874 |           4.6545 |        -500.5590 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |           0.0607 |           4.2797 |        -412.5787 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |           0.0099 |           4.1533 |        -452.9450 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0183 |           4.0523 |        -479.3178 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0335 |           3.9642 |        -492.4067 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0483 |           3.8872 |        -507.3976 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0587 |           3.8383 |        -519.6268 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0642 |           3.8044 |        -528.2118 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0697 |           3.7591 |        -536.4017 |
[32m[20221208 14:30:14 @agent_ppo2.py:179][0m |          -0.0711 |           3.7019 |        -534.7049 |
[32m[20221208 14:30:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:30:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.39
[32m[20221208 14:30:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.78
[32m[20221208 14:30:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.68
[32m[20221208 14:30:15 @agent_ppo2.py:137][0m Total time:      14.16 min
[32m[20221208 14:30:15 @agent_ppo2.py:139][0m 1148928 total steps have happened
[32m[20221208 14:30:15 @agent_ppo2.py:115][0m #------------------------ Iteration 561 --------------------------#
[32m[20221208 14:30:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |           0.0514 |           3.3613 |        -477.0235 |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |           0.0601 |           2.6920 |        -404.1551 |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |           0.0096 |           2.3913 |        -424.5959 |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |          -0.0220 |           2.2484 |        -454.2195 |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |          -0.0385 |           2.1013 |        -469.2008 |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |          -0.0482 |           2.0198 |        -471.5984 |
[32m[20221208 14:30:15 @agent_ppo2.py:179][0m |          -0.0561 |           1.9533 |        -476.5540 |
[32m[20221208 14:30:16 @agent_ppo2.py:179][0m |          -0.0648 |           1.8909 |        -481.1508 |
[32m[20221208 14:30:16 @agent_ppo2.py:179][0m |          -0.0707 |           1.8366 |        -490.4932 |
[32m[20221208 14:30:16 @agent_ppo2.py:179][0m |          -0.0690 |           1.8130 |        -495.6789 |
[32m[20221208 14:30:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:30:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.19
[32m[20221208 14:30:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.82
[32m[20221208 14:30:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.16
[32m[20221208 14:30:16 @agent_ppo2.py:137][0m Total time:      14.19 min
[32m[20221208 14:30:16 @agent_ppo2.py:139][0m 1150976 total steps have happened
[32m[20221208 14:30:16 @agent_ppo2.py:115][0m #------------------------ Iteration 562 --------------------------#
[32m[20221208 14:30:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:30:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |           0.0687 |           3.0689 |        -484.4389 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |           0.0397 |           2.8088 |        -423.8957 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0106 |           2.7093 |        -460.2425 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0198 |           2.5994 |        -457.3247 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0351 |           2.5599 |        -472.2394 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0488 |           2.5030 |        -481.4586 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0570 |           2.4685 |        -502.7356 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0577 |           2.4338 |        -497.2468 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0658 |           2.4203 |        -503.3987 |
[32m[20221208 14:30:17 @agent_ppo2.py:179][0m |          -0.0701 |           2.3917 |        -514.1701 |
[32m[20221208 14:30:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:30:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.67
[32m[20221208 14:30:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.32
[32m[20221208 14:30:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.43
[32m[20221208 14:30:18 @agent_ppo2.py:137][0m Total time:      14.21 min
[32m[20221208 14:30:18 @agent_ppo2.py:139][0m 1153024 total steps have happened
[32m[20221208 14:30:18 @agent_ppo2.py:115][0m #------------------------ Iteration 563 --------------------------#
[32m[20221208 14:30:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:18 @agent_ppo2.py:179][0m |           0.0694 |           2.2592 |        -358.2468 |
[32m[20221208 14:30:18 @agent_ppo2.py:179][0m |           0.0388 |           1.5915 |        -273.2144 |
[32m[20221208 14:30:18 @agent_ppo2.py:179][0m |          -0.0176 |           1.4105 |        -296.5152 |
[32m[20221208 14:30:18 @agent_ppo2.py:179][0m |          -0.0453 |           1.2988 |        -325.1008 |
[32m[20221208 14:30:18 @agent_ppo2.py:179][0m |          -0.0586 |           1.2041 |        -339.0810 |
[32m[20221208 14:30:18 @agent_ppo2.py:179][0m |          -0.0732 |           1.1483 |        -344.5476 |
[32m[20221208 14:30:19 @agent_ppo2.py:179][0m |          -0.0824 |           1.1156 |        -351.3887 |
[32m[20221208 14:30:19 @agent_ppo2.py:179][0m |          -0.0836 |           1.0813 |        -359.0225 |
[32m[20221208 14:30:19 @agent_ppo2.py:179][0m |          -0.0861 |           1.0632 |        -365.7588 |
[32m[20221208 14:30:19 @agent_ppo2.py:179][0m |          -0.0902 |           1.0381 |        -371.4481 |
[32m[20221208 14:30:19 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.07
[32m[20221208 14:30:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.80
[32m[20221208 14:30:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.72
[32m[20221208 14:30:19 @agent_ppo2.py:137][0m Total time:      14.24 min
[32m[20221208 14:30:19 @agent_ppo2.py:139][0m 1155072 total steps have happened
[32m[20221208 14:30:19 @agent_ppo2.py:115][0m #------------------------ Iteration 564 --------------------------#
[32m[20221208 14:30:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |           0.0660 |           0.6274 |        -508.9601 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |           0.0074 |           0.4564 |        -491.4909 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |           0.0074 |           0.4119 |        -474.2568 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |          -0.0140 |           0.3957 |        -483.5893 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |          -0.0048 |           0.3807 |        -472.6102 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |           0.0021 |           0.3742 |        -485.9750 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |          -0.0024 |           0.3671 |        -463.1488 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |          -0.0146 |           0.3647 |        -486.6766 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |          -0.0158 |           0.3580 |        -492.5455 |
[32m[20221208 14:30:20 @agent_ppo2.py:179][0m |          -0.0196 |           0.3577 |        -485.9813 |
[32m[20221208 14:30:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.93
[32m[20221208 14:30:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.56
[32m[20221208 14:30:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.77
[32m[20221208 14:30:21 @agent_ppo2.py:137][0m Total time:      14.26 min
[32m[20221208 14:30:21 @agent_ppo2.py:139][0m 1157120 total steps have happened
[32m[20221208 14:30:21 @agent_ppo2.py:115][0m #------------------------ Iteration 565 --------------------------#
[32m[20221208 14:30:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:21 @agent_ppo2.py:179][0m |           0.1077 |           3.8409 |        -452.4434 |
[32m[20221208 14:30:21 @agent_ppo2.py:179][0m |           0.0905 |           3.0806 |        -383.7767 |
[32m[20221208 14:30:21 @agent_ppo2.py:179][0m |           0.0120 |           2.7905 |        -415.2314 |
[32m[20221208 14:30:21 @agent_ppo2.py:179][0m |          -0.0215 |           2.6199 |        -452.5830 |
[32m[20221208 14:30:21 @agent_ppo2.py:179][0m |          -0.0352 |           2.5150 |        -468.0385 |
[32m[20221208 14:30:21 @agent_ppo2.py:179][0m |          -0.0496 |           2.4532 |        -476.4531 |
[32m[20221208 14:30:22 @agent_ppo2.py:179][0m |          -0.0570 |           2.3238 |        -478.7430 |
[32m[20221208 14:30:22 @agent_ppo2.py:179][0m |          -0.0540 |           2.2382 |        -486.6113 |
[32m[20221208 14:30:22 @agent_ppo2.py:179][0m |          -0.0196 |           2.1710 |        -463.8599 |
[32m[20221208 14:30:22 @agent_ppo2.py:179][0m |          -0.0602 |           2.1344 |        -483.3880 |
[32m[20221208 14:30:22 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:30:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.16
[32m[20221208 14:30:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.32
[32m[20221208 14:30:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.86
[32m[20221208 14:30:22 @agent_ppo2.py:137][0m Total time:      14.29 min
[32m[20221208 14:30:22 @agent_ppo2.py:139][0m 1159168 total steps have happened
[32m[20221208 14:30:22 @agent_ppo2.py:115][0m #------------------------ Iteration 566 --------------------------#
[32m[20221208 14:30:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:30:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |           0.1925 |           1.3276 |        -436.3482 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |           0.1158 |           1.0803 |        -259.5860 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |           0.0764 |           0.9804 |        -265.9134 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |           0.0356 |           0.9375 |        -222.2660 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |           0.0069 |           0.8943 |        -212.5998 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |          -0.0109 |           0.8669 |        -226.6606 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |          -0.0221 |           0.8455 |        -228.3629 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |          -0.0258 |           0.8261 |        -239.8823 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |          -0.0340 |           0.8085 |        -232.1253 |
[32m[20221208 14:30:23 @agent_ppo2.py:179][0m |          -0.0410 |           0.7895 |        -247.8947 |
[32m[20221208 14:30:23 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:30:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.95
[32m[20221208 14:30:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.66
[32m[20221208 14:30:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.56
[32m[20221208 14:30:24 @agent_ppo2.py:137][0m Total time:      14.31 min
[32m[20221208 14:30:24 @agent_ppo2.py:139][0m 1161216 total steps have happened
[32m[20221208 14:30:24 @agent_ppo2.py:115][0m #------------------------ Iteration 567 --------------------------#
[32m[20221208 14:30:24 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:30:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |           0.0814 |           1.3031 |        -441.0455 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |           0.0485 |           1.0643 |        -416.3662 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |           0.0107 |           0.9806 |        -443.8564 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |          -0.0121 |           0.9219 |        -456.6118 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |          -0.0262 |           0.8808 |        -470.3105 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |          -0.0325 |           0.8570 |        -481.0258 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |          -0.0428 |           0.8274 |        -488.2532 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |          -0.0459 |           0.7974 |        -493.9631 |
[32m[20221208 14:30:25 @agent_ppo2.py:179][0m |          -0.0407 |           0.7806 |        -488.3706 |
[32m[20221208 14:30:26 @agent_ppo2.py:179][0m |          -0.0502 |           0.7794 |        -499.9082 |
[32m[20221208 14:30:26 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221208 14:30:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.78
[32m[20221208 14:30:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.74
[32m[20221208 14:30:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.51
[32m[20221208 14:30:26 @agent_ppo2.py:137][0m Total time:      14.35 min
[32m[20221208 14:30:26 @agent_ppo2.py:139][0m 1163264 total steps have happened
[32m[20221208 14:30:26 @agent_ppo2.py:115][0m #------------------------ Iteration 568 --------------------------#
[32m[20221208 14:30:26 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |           0.0640 |           4.1765 |        -460.1658 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |           0.0347 |           3.5611 |        -414.3206 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0160 |           3.3432 |        -437.6275 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0361 |           3.1716 |        -459.3591 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0494 |           3.0479 |        -461.1451 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0549 |           2.9769 |        -465.0527 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0581 |           2.9151 |        -462.4780 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0674 |           2.8073 |        -464.3013 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0708 |           2.7685 |        -475.7744 |
[32m[20221208 14:30:27 @agent_ppo2.py:179][0m |          -0.0787 |           2.7080 |        -481.5453 |
[32m[20221208 14:30:27 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:30:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.75
[32m[20221208 14:30:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.58
[32m[20221208 14:30:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.80
[32m[20221208 14:30:28 @agent_ppo2.py:137][0m Total time:      14.38 min
[32m[20221208 14:30:28 @agent_ppo2.py:139][0m 1165312 total steps have happened
[32m[20221208 14:30:28 @agent_ppo2.py:115][0m #------------------------ Iteration 569 --------------------------#
[32m[20221208 14:30:28 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:28 @agent_ppo2.py:179][0m |           0.1102 |           4.2218 |        -440.7475 |
[32m[20221208 14:30:28 @agent_ppo2.py:179][0m |           0.0925 |           3.9714 |        -356.4929 |
[32m[20221208 14:30:28 @agent_ppo2.py:179][0m |           0.0207 |           3.8067 |        -429.6562 |
[32m[20221208 14:30:28 @agent_ppo2.py:179][0m |          -0.0129 |           3.6803 |        -462.3552 |
[32m[20221208 14:30:28 @agent_ppo2.py:179][0m |          -0.0342 |           3.6292 |        -492.2419 |
[32m[20221208 14:30:29 @agent_ppo2.py:179][0m |          -0.0397 |           3.5501 |        -494.7625 |
[32m[20221208 14:30:29 @agent_ppo2.py:179][0m |          -0.0485 |           3.4746 |        -501.0962 |
[32m[20221208 14:30:29 @agent_ppo2.py:179][0m |          -0.0549 |           3.4529 |        -515.7946 |
[32m[20221208 14:30:29 @agent_ppo2.py:179][0m |          -0.0615 |           3.4032 |        -513.0361 |
[32m[20221208 14:30:29 @agent_ppo2.py:179][0m |          -0.0673 |           3.3784 |        -528.3115 |
[32m[20221208 14:30:29 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.57
[32m[20221208 14:30:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.70
[32m[20221208 14:30:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.10
[32m[20221208 14:30:29 @agent_ppo2.py:137][0m Total time:      14.41 min
[32m[20221208 14:30:29 @agent_ppo2.py:139][0m 1167360 total steps have happened
[32m[20221208 14:30:29 @agent_ppo2.py:115][0m #------------------------ Iteration 570 --------------------------#
[32m[20221208 14:30:30 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |           0.0601 |           2.4234 |        -473.6331 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |           0.0321 |           2.1855 |        -460.2972 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0052 |           2.0667 |        -469.4940 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0151 |           2.0298 |        -474.3320 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0367 |           1.9825 |        -474.1165 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0416 |           1.9244 |        -492.4751 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0469 |           1.8894 |        -492.1259 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0531 |           1.8631 |        -501.6053 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0544 |           1.8335 |        -504.3675 |
[32m[20221208 14:30:30 @agent_ppo2.py:179][0m |          -0.0599 |           1.8141 |        -507.4146 |
[32m[20221208 14:30:30 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.99
[32m[20221208 14:30:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.55
[32m[20221208 14:30:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.79
[32m[20221208 14:30:31 @agent_ppo2.py:137][0m Total time:      14.43 min
[32m[20221208 14:30:31 @agent_ppo2.py:139][0m 1169408 total steps have happened
[32m[20221208 14:30:31 @agent_ppo2.py:115][0m #------------------------ Iteration 571 --------------------------#
[32m[20221208 14:30:31 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:30:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:31 @agent_ppo2.py:179][0m |           0.0986 |           1.4024 |        -440.5267 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.1313 |           1.2341 |        -288.2666 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0704 |           1.1833 |        -329.9508 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0590 |           1.1464 |        -347.1550 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0471 |           1.1375 |        -354.9348 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0448 |           1.0970 |        -353.4308 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0406 |           1.0859 |        -356.5262 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0423 |           1.0859 |        -357.6414 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0395 |           1.0682 |        -358.3021 |
[32m[20221208 14:30:32 @agent_ppo2.py:179][0m |           0.0388 |           1.0541 |        -350.4536 |
[32m[20221208 14:30:32 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.82
[32m[20221208 14:30:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.81
[32m[20221208 14:30:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.73
[32m[20221208 14:30:32 @agent_ppo2.py:137][0m Total time:      14.46 min
[32m[20221208 14:30:32 @agent_ppo2.py:139][0m 1171456 total steps have happened
[32m[20221208 14:30:32 @agent_ppo2.py:115][0m #------------------------ Iteration 572 --------------------------#
[32m[20221208 14:30:33 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |           0.0675 |           4.6556 |        -429.1931 |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |           0.0681 |           3.9234 |        -321.4641 |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |           0.0218 |           3.7043 |        -361.2492 |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |          -0.0026 |           3.5813 |        -384.1436 |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |          -0.0352 |           3.4875 |        -417.9392 |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |          -0.0469 |           3.4300 |        -428.1376 |
[32m[20221208 14:30:33 @agent_ppo2.py:179][0m |          -0.0542 |           3.3736 |        -445.8348 |
[32m[20221208 14:30:34 @agent_ppo2.py:179][0m |          -0.0601 |           3.3183 |        -451.5620 |
[32m[20221208 14:30:34 @agent_ppo2.py:179][0m |          -0.0665 |           3.3044 |        -464.1141 |
[32m[20221208 14:30:34 @agent_ppo2.py:179][0m |          -0.0668 |           3.2243 |        -463.7131 |
[32m[20221208 14:30:34 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:30:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.92
[32m[20221208 14:30:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.19
[32m[20221208 14:30:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.43
[32m[20221208 14:30:34 @agent_ppo2.py:137][0m Total time:      14.49 min
[32m[20221208 14:30:34 @agent_ppo2.py:139][0m 1173504 total steps have happened
[32m[20221208 14:30:34 @agent_ppo2.py:115][0m #------------------------ Iteration 573 --------------------------#
[32m[20221208 14:30:35 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |           0.1217 |           3.5842 |        -442.4187 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |           0.0881 |           3.2448 |        -292.4868 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |           0.0409 |           3.1569 |        -336.2735 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |           0.0067 |           3.0756 |        -394.5822 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |          -0.0142 |           3.0359 |        -437.3989 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |          -0.0237 |           2.9924 |        -450.9414 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |          -0.0322 |           2.9609 |        -460.9279 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |          -0.0401 |           2.9425 |        -465.0655 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |          -0.0430 |           2.9011 |        -474.5873 |
[32m[20221208 14:30:35 @agent_ppo2.py:179][0m |          -0.0488 |           2.8592 |        -485.2051 |
[32m[20221208 14:30:35 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:30:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.06
[32m[20221208 14:30:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.25
[32m[20221208 14:30:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.37
[32m[20221208 14:30:36 @agent_ppo2.py:137][0m Total time:      14.51 min
[32m[20221208 14:30:36 @agent_ppo2.py:139][0m 1175552 total steps have happened
[32m[20221208 14:30:36 @agent_ppo2.py:115][0m #------------------------ Iteration 574 --------------------------#
[32m[20221208 14:30:36 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:36 @agent_ppo2.py:179][0m |           0.0783 |           4.7644 |        -425.4932 |
[32m[20221208 14:30:36 @agent_ppo2.py:179][0m |           0.0504 |           4.1607 |        -368.0137 |
[32m[20221208 14:30:36 @agent_ppo2.py:179][0m |           0.0011 |           3.9768 |        -418.3156 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0254 |           3.8003 |        -450.1108 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0394 |           3.7158 |        -464.3485 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0518 |           3.5956 |        -480.4434 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0563 |           3.5363 |        -491.5284 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0588 |           3.4762 |        -498.5846 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0624 |           3.3956 |        -500.3853 |
[32m[20221208 14:30:37 @agent_ppo2.py:179][0m |          -0.0656 |           3.3836 |        -506.2864 |
[32m[20221208 14:30:37 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:30:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.17
[32m[20221208 14:30:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.97
[32m[20221208 14:30:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.06
[32m[20221208 14:30:37 @agent_ppo2.py:137][0m Total time:      14.54 min
[32m[20221208 14:30:37 @agent_ppo2.py:139][0m 1177600 total steps have happened
[32m[20221208 14:30:37 @agent_ppo2.py:115][0m #------------------------ Iteration 575 --------------------------#
[32m[20221208 14:30:38 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |           0.0784 |           2.7947 |        -418.7069 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |           2.0405 |           2.5341 |        -248.8534 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |           0.0180 |           2.4417 |        -223.4075 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |          -0.0215 |           2.3643 |        -260.2322 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |          -0.0342 |           2.3160 |        -270.7106 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |          -0.0541 |           2.2458 |        -287.5373 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |          -0.0643 |           2.1895 |        -306.6425 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |          -0.0694 |           2.1302 |        -308.1786 |
[32m[20221208 14:30:38 @agent_ppo2.py:179][0m |          -0.0708 |           2.1084 |        -317.2140 |
[32m[20221208 14:30:39 @agent_ppo2.py:179][0m |          -0.0768 |           2.0737 |        -314.5501 |
[32m[20221208 14:30:39 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.99
[32m[20221208 14:30:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.19
[32m[20221208 14:30:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.73
[32m[20221208 14:30:39 @agent_ppo2.py:137][0m Total time:      14.57 min
[32m[20221208 14:30:39 @agent_ppo2.py:139][0m 1179648 total steps have happened
[32m[20221208 14:30:39 @agent_ppo2.py:115][0m #------------------------ Iteration 576 --------------------------#
[32m[20221208 14:30:39 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |           0.0554 |           3.7953 |        -458.9493 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |           0.0803 |           3.4628 |        -394.3260 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |           0.0056 |           3.3361 |        -445.3069 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0219 |           3.2111 |        -471.8472 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0318 |           3.1382 |        -474.4924 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0397 |           3.0822 |        -486.5129 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0432 |           3.0152 |        -504.3797 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0468 |           2.9521 |        -504.8345 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0524 |           2.9164 |        -511.5472 |
[32m[20221208 14:30:40 @agent_ppo2.py:179][0m |          -0.0526 |           2.8936 |        -509.3405 |
[32m[20221208 14:30:40 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:30:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.62
[32m[20221208 14:30:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.40
[32m[20221208 14:30:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.93
[32m[20221208 14:30:41 @agent_ppo2.py:137][0m Total time:      14.60 min
[32m[20221208 14:30:41 @agent_ppo2.py:139][0m 1181696 total steps have happened
[32m[20221208 14:30:41 @agent_ppo2.py:115][0m #------------------------ Iteration 577 --------------------------#
[32m[20221208 14:30:41 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:41 @agent_ppo2.py:179][0m |           0.0868 |           1.2407 |        -382.4661 |
[32m[20221208 14:30:41 @agent_ppo2.py:179][0m |           0.0774 |           0.9584 |        -123.7782 |
[32m[20221208 14:30:41 @agent_ppo2.py:179][0m |           0.0035 |           0.8657 |        -127.3068 |
[32m[20221208 14:30:41 @agent_ppo2.py:179][0m |          -0.0296 |           0.8221 |        -156.7064 |
[32m[20221208 14:30:41 @agent_ppo2.py:179][0m |          -0.0485 |           0.7900 |        -169.7520 |
[32m[20221208 14:30:42 @agent_ppo2.py:179][0m |          -0.0583 |           0.7737 |        -180.4224 |
[32m[20221208 14:30:42 @agent_ppo2.py:179][0m |          -0.0608 |           0.7608 |        -189.6947 |
[32m[20221208 14:30:42 @agent_ppo2.py:179][0m |          -0.0661 |           0.7367 |        -194.9834 |
[32m[20221208 14:30:42 @agent_ppo2.py:179][0m |          -0.0633 |           0.7224 |        -190.9019 |
[32m[20221208 14:30:42 @agent_ppo2.py:179][0m |          -0.0746 |           0.7152 |        -208.8592 |
[32m[20221208 14:30:42 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:30:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.63
[32m[20221208 14:30:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.28
[32m[20221208 14:30:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.88
[32m[20221208 14:30:42 @agent_ppo2.py:137][0m Total time:      14.62 min
[32m[20221208 14:30:42 @agent_ppo2.py:139][0m 1183744 total steps have happened
[32m[20221208 14:30:42 @agent_ppo2.py:115][0m #------------------------ Iteration 578 --------------------------#
[32m[20221208 14:30:43 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:30:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |           0.0670 |           2.7368 |        -458.7282 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |           0.0573 |           2.5254 |        -444.6577 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |           0.0250 |           2.3918 |        -366.3354 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |           0.0042 |           2.2832 |        -373.0429 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |           0.0048 |           2.2302 |        -378.1562 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |          -0.0241 |           2.1751 |        -429.8857 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |          -0.0431 |           2.1405 |        -475.5840 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |          -0.0490 |           2.0953 |        -491.1087 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |          -0.0495 |           2.0655 |        -510.5213 |
[32m[20221208 14:30:43 @agent_ppo2.py:179][0m |          -0.0535 |           2.0173 |        -498.3236 |
[32m[20221208 14:30:43 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:30:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.30
[32m[20221208 14:30:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.54
[32m[20221208 14:30:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.52
[32m[20221208 14:30:44 @agent_ppo2.py:137][0m Total time:      14.65 min
[32m[20221208 14:30:44 @agent_ppo2.py:139][0m 1185792 total steps have happened
[32m[20221208 14:30:44 @agent_ppo2.py:115][0m #------------------------ Iteration 579 --------------------------#
[32m[20221208 14:30:44 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:44 @agent_ppo2.py:179][0m |           0.0525 |           3.8286 |        -446.3692 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |           0.0512 |           3.5494 |        -363.5590 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |           0.0301 |           3.3983 |        -297.8246 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0218 |           3.3020 |        -308.5569 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0391 |           3.2402 |        -324.1684 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0483 |           3.2101 |        -328.0048 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0573 |           3.1334 |        -343.8572 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0586 |           3.1106 |        -346.9732 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0640 |           3.0576 |        -356.1520 |
[32m[20221208 14:30:45 @agent_ppo2.py:179][0m |          -0.0657 |           3.0467 |        -359.4569 |
[32m[20221208 14:30:45 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:30:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.62
[32m[20221208 14:30:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.66
[32m[20221208 14:30:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.69
[32m[20221208 14:30:46 @agent_ppo2.py:137][0m Total time:      14.68 min
[32m[20221208 14:30:46 @agent_ppo2.py:139][0m 1187840 total steps have happened
[32m[20221208 14:30:46 @agent_ppo2.py:115][0m #------------------------ Iteration 580 --------------------------#
[32m[20221208 14:30:46 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:46 @agent_ppo2.py:179][0m |           0.1006 |           3.9983 |        -442.6627 |
[32m[20221208 14:30:46 @agent_ppo2.py:179][0m |           0.1083 |           3.6312 |        -345.6605 |
[32m[20221208 14:30:46 @agent_ppo2.py:179][0m |           0.0312 |           3.5117 |        -376.4882 |
[32m[20221208 14:30:46 @agent_ppo2.py:179][0m |           0.0137 |           3.4195 |        -396.1739 |
[32m[20221208 14:30:46 @agent_ppo2.py:179][0m |          -0.0161 |           3.3975 |        -453.3002 |
[32m[20221208 14:30:46 @agent_ppo2.py:179][0m |          -0.0232 |           3.2964 |        -466.2258 |
[32m[20221208 14:30:47 @agent_ppo2.py:179][0m |          -0.0361 |           3.2590 |        -487.2038 |
[32m[20221208 14:30:47 @agent_ppo2.py:179][0m |          -0.0466 |           3.2067 |        -506.4885 |
[32m[20221208 14:30:47 @agent_ppo2.py:179][0m |          -0.0515 |           3.1550 |        -510.6838 |
[32m[20221208 14:30:47 @agent_ppo2.py:179][0m |          -0.0482 |           3.1319 |        -519.1146 |
[32m[20221208 14:30:47 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:30:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.29
[32m[20221208 14:30:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.37
[32m[20221208 14:30:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.03
[32m[20221208 14:30:47 @agent_ppo2.py:137][0m Total time:      14.70 min
[32m[20221208 14:30:47 @agent_ppo2.py:139][0m 1189888 total steps have happened
[32m[20221208 14:30:47 @agent_ppo2.py:115][0m #------------------------ Iteration 581 --------------------------#
[32m[20221208 14:30:48 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |           0.0701 |           2.6391 |        -470.0710 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |           0.0585 |           2.3238 |        -291.8934 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |           0.0180 |           2.2140 |        -255.4668 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0052 |           2.1066 |        -287.0495 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0072 |           2.0238 |        -251.8464 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0235 |           1.9375 |        -280.4798 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0269 |           1.8993 |        -321.4538 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0327 |           1.7992 |        -311.9712 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0382 |           1.7574 |        -353.3928 |
[32m[20221208 14:30:48 @agent_ppo2.py:179][0m |          -0.0417 |           1.7086 |        -430.7774 |
[32m[20221208 14:30:48 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:30:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.30
[32m[20221208 14:30:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.13
[32m[20221208 14:30:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.57
[32m[20221208 14:30:49 @agent_ppo2.py:137][0m Total time:      14.73 min
[32m[20221208 14:30:49 @agent_ppo2.py:139][0m 1191936 total steps have happened
[32m[20221208 14:30:49 @agent_ppo2.py:115][0m #------------------------ Iteration 582 --------------------------#
[32m[20221208 14:30:49 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:49 @agent_ppo2.py:179][0m |           0.0608 |           3.3497 |        -478.2839 |
[32m[20221208 14:30:49 @agent_ppo2.py:179][0m |           0.0128 |           3.1274 |        -451.5039 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |           0.0015 |           2.9868 |        -462.8809 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0004 |           2.9141 |        -445.3039 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0246 |           2.8741 |        -460.5326 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0372 |           2.7888 |        -468.8411 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0471 |           2.7623 |        -479.0725 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0468 |           2.7236 |        -479.6880 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0553 |           2.7130 |        -483.2837 |
[32m[20221208 14:30:50 @agent_ppo2.py:179][0m |          -0.0553 |           2.6614 |        -486.5485 |
[32m[20221208 14:30:50 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.11
[32m[20221208 14:30:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.96
[32m[20221208 14:30:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.31
[32m[20221208 14:30:50 @agent_ppo2.py:137][0m Total time:      14.76 min
[32m[20221208 14:30:50 @agent_ppo2.py:139][0m 1193984 total steps have happened
[32m[20221208 14:30:50 @agent_ppo2.py:115][0m #------------------------ Iteration 583 --------------------------#
[32m[20221208 14:30:51 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |           0.2364 |           4.0765 |        -434.2248 |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |           0.0848 |           3.6744 |        -370.7932 |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |           0.0414 |           3.5554 |        -360.9804 |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |          -0.0005 |           3.4368 |        -417.0779 |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |          -0.0096 |           3.3608 |        -431.6564 |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |          -0.0262 |           3.3019 |        -449.5051 |
[32m[20221208 14:30:51 @agent_ppo2.py:179][0m |          -0.0390 |           3.2362 |        -476.1305 |
[32m[20221208 14:30:52 @agent_ppo2.py:179][0m |          -0.0439 |           3.1879 |        -486.8156 |
[32m[20221208 14:30:52 @agent_ppo2.py:179][0m |          -0.0335 |           3.1438 |        -489.0291 |
[32m[20221208 14:30:52 @agent_ppo2.py:179][0m |          -0.0388 |           3.0647 |        -488.7464 |
[32m[20221208 14:30:52 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:30:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.48
[32m[20221208 14:30:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.08
[32m[20221208 14:30:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.79
[32m[20221208 14:30:52 @agent_ppo2.py:137][0m Total time:      14.79 min
[32m[20221208 14:30:52 @agent_ppo2.py:139][0m 1196032 total steps have happened
[32m[20221208 14:30:52 @agent_ppo2.py:115][0m #------------------------ Iteration 584 --------------------------#
[32m[20221208 14:30:53 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:30:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |           0.1405 |           4.2019 |        -364.5434 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |           0.1403 |           3.5478 |        -294.8124 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |           0.0563 |           3.3429 |        -258.2326 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |           0.0142 |           3.1926 |        -334.8273 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |          -0.0019 |           3.1254 |        -373.9928 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |           0.0047 |           3.0016 |        -345.1699 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |          -0.0150 |           2.9543 |        -364.2672 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |          -0.0244 |           2.8860 |        -391.4540 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |          -0.0345 |           2.7980 |        -421.1505 |
[32m[20221208 14:30:53 @agent_ppo2.py:179][0m |          -0.0349 |           2.7575 |        -388.8738 |
[32m[20221208 14:30:53 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.07
[32m[20221208 14:30:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.65
[32m[20221208 14:30:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.07
[32m[20221208 14:30:54 @agent_ppo2.py:137][0m Total time:      14.81 min
[32m[20221208 14:30:54 @agent_ppo2.py:139][0m 1198080 total steps have happened
[32m[20221208 14:30:54 @agent_ppo2.py:115][0m #------------------------ Iteration 585 --------------------------#
[32m[20221208 14:30:54 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:54 @agent_ppo2.py:179][0m |           0.0473 |           5.7399 |        -512.0895 |
[32m[20221208 14:30:54 @agent_ppo2.py:179][0m |           0.0378 |           5.2555 |        -452.9653 |
[32m[20221208 14:30:54 @agent_ppo2.py:179][0m |           0.0031 |           5.0302 |        -491.3168 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0268 |           4.9119 |        -506.3218 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0388 |           4.7697 |        -526.4174 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0474 |           4.6839 |        -537.1437 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0576 |           4.6312 |        -543.2303 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0593 |           4.5661 |        -546.1751 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0652 |           4.5404 |        -558.3313 |
[32m[20221208 14:30:55 @agent_ppo2.py:179][0m |          -0.0679 |           4.4416 |        -567.2884 |
[32m[20221208 14:30:55 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:30:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.05
[32m[20221208 14:30:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.31
[32m[20221208 14:30:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.61
[32m[20221208 14:30:55 @agent_ppo2.py:137][0m Total time:      14.84 min
[32m[20221208 14:30:55 @agent_ppo2.py:139][0m 1200128 total steps have happened
[32m[20221208 14:30:55 @agent_ppo2.py:115][0m #------------------------ Iteration 586 --------------------------#
[32m[20221208 14:30:56 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |           0.0719 |           2.3574 |        -441.7972 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |           0.0961 |           1.8021 |        -178.2320 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |           0.0224 |           1.5301 |        -198.1444 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |           0.0100 |           1.3350 |        -205.1159 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |           0.0074 |           1.1941 |        -206.7392 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |          -0.0150 |           1.1072 |        -221.7819 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |          -0.0208 |           1.0190 |        -234.3584 |
[32m[20221208 14:30:56 @agent_ppo2.py:179][0m |          -0.0244 |           0.9713 |        -241.4429 |
[32m[20221208 14:30:57 @agent_ppo2.py:179][0m |          -0.0249 |           0.9056 |        -239.5663 |
[32m[20221208 14:30:57 @agent_ppo2.py:179][0m |          -0.0307 |           0.8714 |        -252.9533 |
[32m[20221208 14:30:57 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:30:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.62
[32m[20221208 14:30:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.07
[32m[20221208 14:30:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.66
[32m[20221208 14:30:57 @agent_ppo2.py:137][0m Total time:      14.87 min
[32m[20221208 14:30:57 @agent_ppo2.py:139][0m 1202176 total steps have happened
[32m[20221208 14:30:57 @agent_ppo2.py:115][0m #------------------------ Iteration 587 --------------------------#
[32m[20221208 14:30:57 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |           0.0958 |           6.5637 |        -451.7523 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |           0.0891 |           5.8394 |        -352.6214 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |           0.0599 |           5.6251 |        -335.8836 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |           0.0020 |           5.5737 |        -425.7435 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |          -0.0159 |           5.4687 |        -468.9433 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |          -0.0261 |           5.4290 |        -477.5219 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |          -0.0205 |           5.3771 |        -472.8779 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |          -0.0316 |           5.3488 |        -487.6876 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |          -0.0452 |           5.2819 |        -511.3160 |
[32m[20221208 14:30:58 @agent_ppo2.py:179][0m |          -0.0536 |           5.2431 |        -522.5992 |
[32m[20221208 14:30:58 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:30:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.94
[32m[20221208 14:30:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.52
[32m[20221208 14:30:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.18
[32m[20221208 14:30:59 @agent_ppo2.py:137][0m Total time:      14.90 min
[32m[20221208 14:30:59 @agent_ppo2.py:139][0m 1204224 total steps have happened
[32m[20221208 14:30:59 @agent_ppo2.py:115][0m #------------------------ Iteration 588 --------------------------#
[32m[20221208 14:30:59 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:30:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:30:59 @agent_ppo2.py:179][0m |           0.0734 |           4.5216 |        -486.6276 |
[32m[20221208 14:30:59 @agent_ppo2.py:179][0m |           0.0573 |           4.0869 |        -385.3319 |
[32m[20221208 14:30:59 @agent_ppo2.py:179][0m |           0.0239 |           3.9342 |        -424.8559 |
[32m[20221208 14:30:59 @agent_ppo2.py:179][0m |          -0.0091 |           3.7668 |        -483.1796 |
[32m[20221208 14:30:59 @agent_ppo2.py:179][0m |          -0.0251 |           3.6973 |        -515.7690 |
[32m[20221208 14:31:00 @agent_ppo2.py:179][0m |          -0.0376 |           3.6422 |        -531.8853 |
[32m[20221208 14:31:00 @agent_ppo2.py:179][0m |          -0.0368 |           3.6020 |        -522.1953 |
[32m[20221208 14:31:00 @agent_ppo2.py:179][0m |          -0.0318 |           3.5420 |        -534.2335 |
[32m[20221208 14:31:00 @agent_ppo2.py:179][0m |          -0.0457 |           3.5267 |        -546.6759 |
[32m[20221208 14:31:00 @agent_ppo2.py:179][0m |          -0.0527 |           3.4822 |        -549.1929 |
[32m[20221208 14:31:00 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.08
[32m[20221208 14:31:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.60
[32m[20221208 14:31:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.38
[32m[20221208 14:31:00 @agent_ppo2.py:137][0m Total time:      14.92 min
[32m[20221208 14:31:00 @agent_ppo2.py:139][0m 1206272 total steps have happened
[32m[20221208 14:31:00 @agent_ppo2.py:115][0m #------------------------ Iteration 589 --------------------------#
[32m[20221208 14:31:01 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |           0.1992 |           4.3223 |        -439.9322 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |           0.0987 |           4.0472 |        -340.0832 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |           0.0481 |           3.9192 |        -410.3222 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |           0.0199 |           3.7648 |        -465.1782 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |          -0.0038 |           3.6791 |        -487.6871 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |          -0.0224 |           3.5844 |        -502.0661 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |          -0.0290 |           3.5586 |        -526.0712 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |          -0.0365 |           3.4812 |        -526.0523 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |          -0.0433 |           3.4472 |        -545.2897 |
[32m[20221208 14:31:01 @agent_ppo2.py:179][0m |          -0.0458 |           3.4021 |        -543.9772 |
[32m[20221208 14:31:01 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.81
[32m[20221208 14:31:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.58
[32m[20221208 14:31:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.98
[32m[20221208 14:31:02 @agent_ppo2.py:137][0m Total time:      14.95 min
[32m[20221208 14:31:02 @agent_ppo2.py:139][0m 1208320 total steps have happened
[32m[20221208 14:31:02 @agent_ppo2.py:115][0m #------------------------ Iteration 590 --------------------------#
[32m[20221208 14:31:02 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:02 @agent_ppo2.py:179][0m |           0.0578 |           2.6627 |        -461.2099 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |           0.0416 |           2.2993 |        -395.2908 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0008 |           2.2020 |        -441.2464 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0280 |           2.1201 |        -478.2918 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0329 |           2.0740 |        -477.9740 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0429 |           2.0430 |        -493.9069 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0483 |           2.0115 |        -496.4004 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0512 |           1.9800 |        -508.9664 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0487 |           1.9568 |        -499.8575 |
[32m[20221208 14:31:03 @agent_ppo2.py:179][0m |          -0.0501 |           1.9173 |        -504.2190 |
[32m[20221208 14:31:03 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.70
[32m[20221208 14:31:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.81
[32m[20221208 14:31:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.18
[32m[20221208 14:31:04 @agent_ppo2.py:137][0m Total time:      14.98 min
[32m[20221208 14:31:04 @agent_ppo2.py:139][0m 1210368 total steps have happened
[32m[20221208 14:31:04 @agent_ppo2.py:115][0m #------------------------ Iteration 591 --------------------------#
[32m[20221208 14:31:04 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:04 @agent_ppo2.py:179][0m |           0.0501 |           0.7506 |        -473.4528 |
[32m[20221208 14:31:04 @agent_ppo2.py:179][0m |           0.0124 |           0.5781 |        -455.2064 |
[32m[20221208 14:31:04 @agent_ppo2.py:179][0m |           0.0038 |           0.5514 |        -459.5522 |
[32m[20221208 14:31:04 @agent_ppo2.py:179][0m |           0.0394 |           0.5339 |        -338.3572 |
[32m[20221208 14:31:04 @agent_ppo2.py:179][0m |           0.0020 |           0.5163 |        -448.0490 |
[32m[20221208 14:31:04 @agent_ppo2.py:179][0m |           0.0194 |           0.5096 |        -420.8688 |
[32m[20221208 14:31:05 @agent_ppo2.py:179][0m |          -0.0075 |           0.5003 |        -474.2681 |
[32m[20221208 14:31:05 @agent_ppo2.py:179][0m |          -0.0070 |           0.4968 |        -449.8602 |
[32m[20221208 14:31:05 @agent_ppo2.py:179][0m |          -0.0134 |           0.4857 |        -465.6210 |
[32m[20221208 14:31:05 @agent_ppo2.py:179][0m |          -0.0110 |           0.4799 |        -470.2250 |
[32m[20221208 14:31:05 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.12
[32m[20221208 14:31:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.60
[32m[20221208 14:31:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.04
[32m[20221208 14:31:05 @agent_ppo2.py:137][0m Total time:      15.00 min
[32m[20221208 14:31:05 @agent_ppo2.py:139][0m 1212416 total steps have happened
[32m[20221208 14:31:05 @agent_ppo2.py:115][0m #------------------------ Iteration 592 --------------------------#
[32m[20221208 14:31:06 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:31:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |           0.0824 |           4.6160 |        -460.0521 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |           0.0445 |           4.0020 |        -394.3255 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |           0.0005 |           3.7421 |        -433.0807 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0168 |           3.6623 |        -450.7813 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0197 |           3.5278 |        -462.9176 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0370 |           3.4478 |        -471.5155 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0476 |           3.3431 |        -486.3999 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0525 |           3.2879 |        -495.2040 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0540 |           3.1748 |        -499.0815 |
[32m[20221208 14:31:06 @agent_ppo2.py:179][0m |          -0.0528 |           3.1194 |        -505.2022 |
[32m[20221208 14:31:06 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:31:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.96
[32m[20221208 14:31:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.74
[32m[20221208 14:31:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.17
[32m[20221208 14:31:07 @agent_ppo2.py:137][0m Total time:      15.03 min
[32m[20221208 14:31:07 @agent_ppo2.py:139][0m 1214464 total steps have happened
[32m[20221208 14:31:07 @agent_ppo2.py:115][0m #------------------------ Iteration 593 --------------------------#
[32m[20221208 14:31:07 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:31:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:07 @agent_ppo2.py:179][0m |           0.0802 |           4.9629 |        -432.7042 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |           0.0385 |           4.5532 |        -417.4341 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |           0.0156 |           4.3600 |        -452.3164 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0172 |           4.2367 |        -474.2114 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0319 |           4.1980 |        -490.7360 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0406 |           4.0868 |        -504.4038 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0423 |           4.0010 |        -503.1719 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0509 |           3.9818 |        -521.0214 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0463 |           3.9691 |        -511.5994 |
[32m[20221208 14:31:08 @agent_ppo2.py:179][0m |          -0.0490 |           3.9068 |        -519.9885 |
[32m[20221208 14:31:08 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.17
[32m[20221208 14:31:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.06
[32m[20221208 14:31:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.56
[32m[20221208 14:31:08 @agent_ppo2.py:137][0m Total time:      15.06 min
[32m[20221208 14:31:08 @agent_ppo2.py:139][0m 1216512 total steps have happened
[32m[20221208 14:31:08 @agent_ppo2.py:115][0m #------------------------ Iteration 594 --------------------------#
[32m[20221208 14:31:09 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:09 @agent_ppo2.py:179][0m |           0.0980 |           4.3516 |        -452.6089 |
[32m[20221208 14:31:09 @agent_ppo2.py:179][0m |           0.0506 |           3.9830 |        -373.0456 |
[32m[20221208 14:31:09 @agent_ppo2.py:179][0m |           0.0136 |           3.8838 |        -410.4552 |
[32m[20221208 14:31:09 @agent_ppo2.py:179][0m |          -0.0039 |           3.8311 |        -434.9209 |
[32m[20221208 14:31:09 @agent_ppo2.py:179][0m |          -0.0129 |           3.7354 |        -432.3377 |
[32m[20221208 14:31:09 @agent_ppo2.py:179][0m |          -0.0133 |           3.6672 |        -412.2101 |
[32m[20221208 14:31:10 @agent_ppo2.py:179][0m |          -0.0340 |           3.6616 |        -449.9163 |
[32m[20221208 14:31:10 @agent_ppo2.py:179][0m |          -0.0422 |           3.6013 |        -460.7831 |
[32m[20221208 14:31:10 @agent_ppo2.py:179][0m |          -0.0473 |           3.5944 |        -469.7001 |
[32m[20221208 14:31:10 @agent_ppo2.py:179][0m |          -0.0534 |           3.5271 |        -486.2143 |
[32m[20221208 14:31:10 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:31:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.93
[32m[20221208 14:31:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.31
[32m[20221208 14:31:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.32
[32m[20221208 14:31:10 @agent_ppo2.py:137][0m Total time:      15.09 min
[32m[20221208 14:31:10 @agent_ppo2.py:139][0m 1218560 total steps have happened
[32m[20221208 14:31:10 @agent_ppo2.py:115][0m #------------------------ Iteration 595 --------------------------#
[32m[20221208 14:31:11 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |           0.0573 |           7.2664 |        -419.1288 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |           0.0293 |           6.6977 |        -380.2065 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |           0.0232 |           6.4914 |        -416.4492 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0059 |           6.2459 |        -444.5535 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0141 |           6.0712 |        -463.5129 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0310 |           5.9488 |        -491.9919 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0426 |           5.7417 |        -505.4472 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0501 |           5.6125 |        -516.8281 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0533 |           5.4878 |        -528.7835 |
[32m[20221208 14:31:11 @agent_ppo2.py:179][0m |          -0.0573 |           5.3992 |        -535.8427 |
[32m[20221208 14:31:11 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.41
[32m[20221208 14:31:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.89
[32m[20221208 14:31:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.84
[32m[20221208 14:31:12 @agent_ppo2.py:137][0m Total time:      15.11 min
[32m[20221208 14:31:12 @agent_ppo2.py:139][0m 1220608 total steps have happened
[32m[20221208 14:31:12 @agent_ppo2.py:115][0m #------------------------ Iteration 596 --------------------------#
[32m[20221208 14:31:12 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:12 @agent_ppo2.py:179][0m |           0.0516 |           5.4746 |        -488.9967 |
[32m[20221208 14:31:12 @agent_ppo2.py:179][0m |           0.0597 |           4.8785 |        -369.1381 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |           0.0545 |           4.7735 |        -362.3000 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |           0.0055 |           4.6003 |        -433.9240 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |          -0.0179 |           4.4972 |        -464.8135 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |          -0.0280 |           4.4634 |        -473.9934 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |          -0.0401 |           4.3668 |        -489.0958 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |          -0.0452 |           4.3309 |        -495.5142 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |          -0.0500 |           4.3196 |        -503.9761 |
[32m[20221208 14:31:13 @agent_ppo2.py:179][0m |          -0.0503 |           4.2997 |        -513.5203 |
[32m[20221208 14:31:13 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.96
[32m[20221208 14:31:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.15
[32m[20221208 14:31:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.07
[32m[20221208 14:31:13 @agent_ppo2.py:137][0m Total time:      15.14 min
[32m[20221208 14:31:13 @agent_ppo2.py:139][0m 1222656 total steps have happened
[32m[20221208 14:31:13 @agent_ppo2.py:115][0m #------------------------ Iteration 597 --------------------------#
[32m[20221208 14:31:14 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |           0.0701 |           5.8820 |        -425.6790 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |           0.0651 |           5.3911 |        -354.3016 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |           0.0203 |           5.1533 |        -376.0567 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |          -0.0127 |           4.9725 |        -422.1896 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |          -0.0209 |           4.8538 |        -429.5993 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |          -0.0354 |           4.7620 |        -448.1299 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |          -0.0438 |           4.6838 |        -455.6021 |
[32m[20221208 14:31:14 @agent_ppo2.py:179][0m |          -0.0503 |           4.6416 |        -467.9681 |
[32m[20221208 14:31:15 @agent_ppo2.py:179][0m |          -0.0532 |           4.5466 |        -470.4450 |
[32m[20221208 14:31:15 @agent_ppo2.py:179][0m |          -0.0544 |           4.5199 |        -488.8253 |
[32m[20221208 14:31:15 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:31:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.33
[32m[20221208 14:31:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.57
[32m[20221208 14:31:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.81
[32m[20221208 14:31:15 @agent_ppo2.py:137][0m Total time:      15.17 min
[32m[20221208 14:31:15 @agent_ppo2.py:139][0m 1224704 total steps have happened
[32m[20221208 14:31:15 @agent_ppo2.py:115][0m #------------------------ Iteration 598 --------------------------#
[32m[20221208 14:31:16 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |           0.0805 |           4.4568 |        -426.8623 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |           0.0587 |           3.9097 |        -393.3552 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |           0.0261 |           3.6564 |        -422.7039 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0103 |           3.4827 |        -432.9308 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0326 |           3.3265 |        -461.2174 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0372 |           3.2535 |        -469.6361 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0445 |           3.1623 |        -482.9798 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0397 |           3.1003 |        -474.2179 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0503 |           3.0285 |        -490.7335 |
[32m[20221208 14:31:16 @agent_ppo2.py:179][0m |          -0.0561 |           2.9903 |        -496.5109 |
[32m[20221208 14:31:16 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.82
[32m[20221208 14:31:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.34
[32m[20221208 14:31:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.65
[32m[20221208 14:31:17 @agent_ppo2.py:137][0m Total time:      15.20 min
[32m[20221208 14:31:17 @agent_ppo2.py:139][0m 1226752 total steps have happened
[32m[20221208 14:31:17 @agent_ppo2.py:115][0m #------------------------ Iteration 599 --------------------------#
[32m[20221208 14:31:17 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:17 @agent_ppo2.py:179][0m |           0.0288 |           2.6643 |        -509.3323 |
[32m[20221208 14:31:17 @agent_ppo2.py:179][0m |           0.0095 |           2.3867 |        -497.8688 |
[32m[20221208 14:31:17 @agent_ppo2.py:179][0m |          -0.0067 |           2.2762 |        -498.3735 |
[32m[20221208 14:31:17 @agent_ppo2.py:179][0m |          -0.0198 |           2.2077 |        -514.4305 |
[32m[20221208 14:31:18 @agent_ppo2.py:179][0m |          -0.0281 |           2.1566 |        -528.0618 |
[32m[20221208 14:31:18 @agent_ppo2.py:179][0m |          -0.0316 |           2.0651 |        -522.0162 |
[32m[20221208 14:31:18 @agent_ppo2.py:179][0m |          -0.0325 |           2.0153 |        -527.9475 |
[32m[20221208 14:31:18 @agent_ppo2.py:179][0m |          -0.0353 |           1.9669 |        -533.5494 |
[32m[20221208 14:31:18 @agent_ppo2.py:179][0m |          -0.0370 |           1.9714 |        -537.3203 |
[32m[20221208 14:31:18 @agent_ppo2.py:179][0m |          -0.0405 |           1.9156 |        -547.8848 |
[32m[20221208 14:31:18 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.14
[32m[20221208 14:31:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.68
[32m[20221208 14:31:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.43
[32m[20221208 14:31:18 @agent_ppo2.py:137][0m Total time:      15.22 min
[32m[20221208 14:31:18 @agent_ppo2.py:139][0m 1228800 total steps have happened
[32m[20221208 14:31:18 @agent_ppo2.py:115][0m #------------------------ Iteration 600 --------------------------#
[32m[20221208 14:31:19 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |           0.0853 |           5.1578 |        -468.7200 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |           0.0373 |           4.4239 |        -456.2094 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |           0.0111 |           4.1704 |        -477.4686 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |          -0.0108 |           3.9678 |        -497.3911 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |          -0.0257 |           3.7945 |        -506.2582 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |          -0.0388 |           3.6675 |        -516.3342 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |          -0.0445 |           3.5421 |        -520.3753 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |          -0.0500 |           3.4497 |        -531.0421 |
[32m[20221208 14:31:19 @agent_ppo2.py:179][0m |          -0.0524 |           3.3465 |        -543.4867 |
[32m[20221208 14:31:20 @agent_ppo2.py:179][0m |          -0.0561 |           3.2727 |        -550.1283 |
[32m[20221208 14:31:20 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:31:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.50
[32m[20221208 14:31:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.37
[32m[20221208 14:31:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.95
[32m[20221208 14:31:20 @agent_ppo2.py:137][0m Total time:      15.25 min
[32m[20221208 14:31:20 @agent_ppo2.py:139][0m 1230848 total steps have happened
[32m[20221208 14:31:20 @agent_ppo2.py:115][0m #------------------------ Iteration 601 --------------------------#
[32m[20221208 14:31:20 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |           0.1186 |           5.1808 |        -451.5360 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |           0.1255 |           4.4403 |        -323.4249 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |           0.0574 |           4.1450 |        -353.2492 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |           0.0141 |           3.9892 |        -406.1040 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |          -0.0116 |           3.7948 |        -439.6012 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |          -0.0253 |           3.6438 |        -458.0701 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |          -0.0348 |           3.5339 |        -468.8541 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |          -0.0424 |           3.4152 |        -478.7859 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |          -0.0521 |           3.3411 |        -492.7465 |
[32m[20221208 14:31:21 @agent_ppo2.py:179][0m |          -0.0529 |           3.2674 |        -493.9388 |
[32m[20221208 14:31:21 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.11
[32m[20221208 14:31:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.77
[32m[20221208 14:31:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.67
[32m[20221208 14:31:22 @agent_ppo2.py:137][0m Total time:      15.28 min
[32m[20221208 14:31:22 @agent_ppo2.py:139][0m 1232896 total steps have happened
[32m[20221208 14:31:22 @agent_ppo2.py:115][0m #------------------------ Iteration 602 --------------------------#
[32m[20221208 14:31:22 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:22 @agent_ppo2.py:179][0m |           0.0839 |           8.0065 |        -459.7714 |
[32m[20221208 14:31:22 @agent_ppo2.py:179][0m |           0.0653 |           7.1688 |        -352.2414 |
[32m[20221208 14:31:22 @agent_ppo2.py:179][0m |           0.0060 |           6.8102 |        -442.4648 |
[32m[20221208 14:31:22 @agent_ppo2.py:179][0m |          -0.0278 |           6.6555 |        -481.5630 |
[32m[20221208 14:31:22 @agent_ppo2.py:179][0m |          -0.0377 |           6.4196 |        -487.8742 |
[32m[20221208 14:31:23 @agent_ppo2.py:179][0m |          -0.0488 |           6.3629 |        -503.9895 |
[32m[20221208 14:31:23 @agent_ppo2.py:179][0m |          -0.0565 |           6.2782 |        -518.4738 |
[32m[20221208 14:31:23 @agent_ppo2.py:179][0m |          -0.0547 |           6.1335 |        -509.7098 |
[32m[20221208 14:31:23 @agent_ppo2.py:179][0m |          -0.0615 |           5.9645 |        -520.8400 |
[32m[20221208 14:31:23 @agent_ppo2.py:179][0m |          -0.0671 |           5.9725 |        -527.3615 |
[32m[20221208 14:31:23 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:31:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.07
[32m[20221208 14:31:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.13
[32m[20221208 14:31:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.39
[32m[20221208 14:31:23 @agent_ppo2.py:137][0m Total time:      15.31 min
[32m[20221208 14:31:23 @agent_ppo2.py:139][0m 1234944 total steps have happened
[32m[20221208 14:31:23 @agent_ppo2.py:115][0m #------------------------ Iteration 603 --------------------------#
[32m[20221208 14:31:24 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:31:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |           0.0917 |           5.1656 |        -467.7535 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |           0.1150 |           4.7474 |        -378.9634 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |           0.0436 |           4.5580 |        -383.2331 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0030 |           4.4531 |        -436.6859 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0210 |           4.3868 |        -469.6000 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0323 |           4.3023 |        -479.5567 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0402 |           4.2277 |        -496.1292 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0469 |           4.1717 |        -505.8919 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0537 |           4.1571 |        -515.1074 |
[32m[20221208 14:31:24 @agent_ppo2.py:179][0m |          -0.0542 |           4.1152 |        -519.6692 |
[32m[20221208 14:31:24 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.28
[32m[20221208 14:31:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.80
[32m[20221208 14:31:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.37
[32m[20221208 14:31:25 @agent_ppo2.py:137][0m Total time:      15.33 min
[32m[20221208 14:31:25 @agent_ppo2.py:139][0m 1236992 total steps have happened
[32m[20221208 14:31:25 @agent_ppo2.py:115][0m #------------------------ Iteration 604 --------------------------#
[32m[20221208 14:31:25 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:25 @agent_ppo2.py:179][0m |           0.2287 |           4.4862 |        -459.6494 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |           0.0978 |           4.2066 |        -351.0931 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |           0.0421 |           4.0505 |        -391.3964 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |           0.0019 |           3.9530 |        -449.7145 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |          -0.0166 |           3.8762 |        -472.8328 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |          -0.0291 |           3.7386 |        -483.6261 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |          -0.0358 |           3.6928 |        -500.2945 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |          -0.0440 |           3.5978 |        -509.7618 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |          -0.0459 |           3.5437 |        -515.3209 |
[32m[20221208 14:31:26 @agent_ppo2.py:179][0m |          -0.0500 |           3.4970 |        -524.5342 |
[32m[20221208 14:31:26 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.04
[32m[20221208 14:31:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.58
[32m[20221208 14:31:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.57
[32m[20221208 14:31:27 @agent_ppo2.py:137][0m Total time:      15.36 min
[32m[20221208 14:31:27 @agent_ppo2.py:139][0m 1239040 total steps have happened
[32m[20221208 14:31:27 @agent_ppo2.py:115][0m #------------------------ Iteration 605 --------------------------#
[32m[20221208 14:31:27 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |           0.0701 |           0.9825 |        -474.2089 |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |           0.0378 |           0.7728 |        -402.1717 |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |           0.0152 |           0.7037 |        -420.9762 |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |           0.0061 |           0.6608 |        -415.4336 |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |          -0.0079 |           0.6256 |        -430.9604 |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |          -0.0213 |           0.5982 |        -451.4259 |
[32m[20221208 14:31:27 @agent_ppo2.py:179][0m |          -0.0284 |           0.5799 |        -464.3683 |
[32m[20221208 14:31:28 @agent_ppo2.py:179][0m |          -0.0317 |           0.5643 |        -464.6686 |
[32m[20221208 14:31:28 @agent_ppo2.py:179][0m |          -0.0325 |           0.5520 |        -473.0529 |
[32m[20221208 14:31:28 @agent_ppo2.py:179][0m |          -0.0297 |           0.5361 |        -471.7376 |
[32m[20221208 14:31:28 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.07
[32m[20221208 14:31:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.02
[32m[20221208 14:31:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.26
[32m[20221208 14:31:28 @agent_ppo2.py:137][0m Total time:      15.39 min
[32m[20221208 14:31:28 @agent_ppo2.py:139][0m 1241088 total steps have happened
[32m[20221208 14:31:28 @agent_ppo2.py:115][0m #------------------------ Iteration 606 --------------------------#
[32m[20221208 14:31:29 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |           0.0797 |           6.3609 |        -458.6259 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |           0.0905 |           5.8633 |        -345.1528 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |           0.0477 |           5.7689 |        -363.8566 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |           0.0139 |           5.6256 |        -400.9065 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |          -0.0150 |           5.4751 |        -426.7766 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |          -0.0280 |           5.4287 |        -442.1646 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |          -0.0404 |           5.3541 |        -453.7843 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |          -0.0440 |           5.2838 |        -457.6769 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |          -0.0474 |           5.2410 |        -473.1239 |
[32m[20221208 14:31:29 @agent_ppo2.py:179][0m |          -0.0572 |           5.1860 |        -481.3191 |
[32m[20221208 14:31:29 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.42
[32m[20221208 14:31:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.64
[32m[20221208 14:31:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.00
[32m[20221208 14:31:30 @agent_ppo2.py:137][0m Total time:      15.41 min
[32m[20221208 14:31:30 @agent_ppo2.py:139][0m 1243136 total steps have happened
[32m[20221208 14:31:30 @agent_ppo2.py:115][0m #------------------------ Iteration 607 --------------------------#
[32m[20221208 14:31:30 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:30 @agent_ppo2.py:179][0m |           0.0625 |           5.7805 |        -475.5318 |
[32m[20221208 14:31:30 @agent_ppo2.py:179][0m |           0.0368 |           5.1271 |        -434.0968 |
[32m[20221208 14:31:30 @agent_ppo2.py:179][0m |          -0.0120 |           4.8983 |        -465.4930 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0354 |           4.7587 |        -479.7294 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0454 |           4.6953 |        -500.6160 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0528 |           4.5248 |        -502.6962 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0556 |           4.4634 |        -507.3319 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0603 |           4.3913 |        -504.4714 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0700 |           4.3570 |        -521.1077 |
[32m[20221208 14:31:31 @agent_ppo2.py:179][0m |          -0.0726 |           4.2783 |        -529.9968 |
[32m[20221208 14:31:31 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:31:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.30
[32m[20221208 14:31:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.36
[32m[20221208 14:31:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.51
[32m[20221208 14:31:31 @agent_ppo2.py:137][0m Total time:      15.44 min
[32m[20221208 14:31:31 @agent_ppo2.py:139][0m 1245184 total steps have happened
[32m[20221208 14:31:31 @agent_ppo2.py:115][0m #------------------------ Iteration 608 --------------------------#
[32m[20221208 14:31:32 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:31:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |           0.1469 |           5.7867 |        -411.8938 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |           0.1478 |           4.9230 |        -295.9817 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |           0.0440 |           4.6152 |        -356.5198 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |           0.0105 |           4.3747 |        -424.7231 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |          -0.0099 |           4.2693 |        -438.0703 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |          -0.0223 |           4.2138 |        -444.0753 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |          -0.0340 |           4.0697 |        -452.6393 |
[32m[20221208 14:31:32 @agent_ppo2.py:179][0m |          -0.0391 |           4.0578 |        -470.0249 |
[32m[20221208 14:31:33 @agent_ppo2.py:179][0m |          -0.0478 |           3.9494 |        -449.2787 |
[32m[20221208 14:31:33 @agent_ppo2.py:179][0m |          -0.0535 |           3.8852 |        -493.5784 |
[32m[20221208 14:31:33 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.54
[32m[20221208 14:31:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.02
[32m[20221208 14:31:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.80
[32m[20221208 14:31:33 @agent_ppo2.py:137][0m Total time:      15.47 min
[32m[20221208 14:31:33 @agent_ppo2.py:139][0m 1247232 total steps have happened
[32m[20221208 14:31:33 @agent_ppo2.py:115][0m #------------------------ Iteration 609 --------------------------#
[32m[20221208 14:31:34 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |           0.0563 |           3.9680 |        -392.1299 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |           0.0438 |           3.4038 |        -241.0266 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0093 |           3.3105 |        -272.7002 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0352 |           3.2316 |        -310.2761 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0449 |           3.1659 |        -332.3239 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0585 |           3.1280 |        -343.0000 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0643 |           3.0884 |        -360.9184 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0590 |           3.0819 |        -362.2031 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0672 |           3.0206 |        -366.0764 |
[32m[20221208 14:31:34 @agent_ppo2.py:179][0m |          -0.0745 |           2.9948 |        -378.1249 |
[32m[20221208 14:31:34 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.73
[32m[20221208 14:31:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.92
[32m[20221208 14:31:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.94
[32m[20221208 14:31:35 @agent_ppo2.py:137][0m Total time:      15.50 min
[32m[20221208 14:31:35 @agent_ppo2.py:139][0m 1249280 total steps have happened
[32m[20221208 14:31:35 @agent_ppo2.py:115][0m #------------------------ Iteration 610 --------------------------#
[32m[20221208 14:31:35 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:31:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:35 @agent_ppo2.py:179][0m |           0.0916 |           4.5462 |        -485.7197 |
[32m[20221208 14:31:35 @agent_ppo2.py:179][0m |           0.1407 |           4.2859 |        -313.8266 |
[32m[20221208 14:31:35 @agent_ppo2.py:179][0m |           0.0552 |           4.1191 |        -253.5919 |
[32m[20221208 14:31:35 @agent_ppo2.py:179][0m |           0.0190 |           4.0547 |        -307.0989 |
[32m[20221208 14:31:36 @agent_ppo2.py:179][0m |           0.0023 |           3.9648 |        -358.6248 |
[32m[20221208 14:31:36 @agent_ppo2.py:179][0m |          -0.0178 |           3.8928 |        -452.4096 |
[32m[20221208 14:31:36 @agent_ppo2.py:179][0m |          -0.0296 |           3.8666 |        -505.0856 |
[32m[20221208 14:31:36 @agent_ppo2.py:179][0m |          -0.0359 |           3.8299 |        -508.5485 |
[32m[20221208 14:31:36 @agent_ppo2.py:179][0m |          -0.0382 |           3.8041 |        -504.2233 |
[32m[20221208 14:31:36 @agent_ppo2.py:179][0m |          -0.0404 |           3.7904 |        -510.9519 |
[32m[20221208 14:31:36 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:31:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.72
[32m[20221208 14:31:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.35
[32m[20221208 14:31:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.54
[32m[20221208 14:31:36 @agent_ppo2.py:137][0m Total time:      15.52 min
[32m[20221208 14:31:36 @agent_ppo2.py:139][0m 1251328 total steps have happened
[32m[20221208 14:31:36 @agent_ppo2.py:115][0m #------------------------ Iteration 611 --------------------------#
[32m[20221208 14:31:37 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:31:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |           0.0671 |           2.6931 |        -328.7828 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |           0.0314 |           2.3722 |        -151.2436 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |           0.0004 |           2.2524 |        -150.4902 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |          -0.0214 |           2.1753 |        -163.1046 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |          -0.0377 |           2.1313 |        -181.8072 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |          -0.0331 |           2.0801 |        -180.4599 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |          -0.0453 |           2.0464 |        -191.3521 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |          -0.0492 |           2.0128 |        -196.2452 |
[32m[20221208 14:31:37 @agent_ppo2.py:179][0m |          -0.0531 |           1.9740 |        -203.9480 |
[32m[20221208 14:31:38 @agent_ppo2.py:179][0m |          -0.0610 |           1.9654 |        -213.0100 |
[32m[20221208 14:31:38 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.92
[32m[20221208 14:31:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.63
[32m[20221208 14:31:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.43
[32m[20221208 14:31:38 @agent_ppo2.py:137][0m Total time:      15.55 min
[32m[20221208 14:31:38 @agent_ppo2.py:139][0m 1253376 total steps have happened
[32m[20221208 14:31:38 @agent_ppo2.py:115][0m #------------------------ Iteration 612 --------------------------#
[32m[20221208 14:31:38 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:31:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |           0.0454 |           4.8437 |        -522.8464 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |           0.0415 |           4.5480 |        -430.8326 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |           0.0165 |           4.3446 |        -432.7359 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0152 |           4.2144 |        -439.5393 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0314 |           4.1504 |        -466.2419 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0418 |           4.0865 |        -478.1768 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0439 |           4.0078 |        -487.0351 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0421 |           4.0088 |        -483.8187 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0435 |           3.9154 |        -482.6037 |
[32m[20221208 14:31:39 @agent_ppo2.py:179][0m |          -0.0499 |           3.9051 |        -484.6529 |
[32m[20221208 14:31:39 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:31:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.62
[32m[20221208 14:31:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.25
[32m[20221208 14:31:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.56
[32m[20221208 14:31:40 @agent_ppo2.py:137][0m Total time:      15.58 min
[32m[20221208 14:31:40 @agent_ppo2.py:139][0m 1255424 total steps have happened
[32m[20221208 14:31:40 @agent_ppo2.py:115][0m #------------------------ Iteration 613 --------------------------#
[32m[20221208 14:31:40 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:31:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:40 @agent_ppo2.py:179][0m |           0.1078 |           2.9218 |        -445.1675 |
[32m[20221208 14:31:40 @agent_ppo2.py:179][0m |           0.0884 |           2.5710 |        -334.4569 |
[32m[20221208 14:31:40 @agent_ppo2.py:179][0m |           0.0482 |           2.4686 |        -282.4968 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0047 |           2.4165 |        -250.9352 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0222 |           2.3982 |        -263.4733 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0362 |           2.3481 |        -279.5497 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0530 |           2.3157 |        -295.8245 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0580 |           2.2810 |        -298.6735 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0665 |           2.2693 |        -313.6752 |
[32m[20221208 14:31:41 @agent_ppo2.py:179][0m |          -0.0704 |           2.2736 |        -321.1714 |
[32m[20221208 14:31:41 @agent_ppo2.py:124][0m Policy update time: 0.82 s
[32m[20221208 14:31:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.74
[32m[20221208 14:31:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.21
[32m[20221208 14:31:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.04
[32m[20221208 14:31:41 @agent_ppo2.py:137][0m Total time:      15.61 min
[32m[20221208 14:31:41 @agent_ppo2.py:139][0m 1257472 total steps have happened
[32m[20221208 14:31:41 @agent_ppo2.py:115][0m #------------------------ Iteration 614 --------------------------#
[32m[20221208 14:31:42 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:31:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |           0.0730 |           2.4963 |        -389.9691 |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |           0.0468 |           2.1634 |        -217.0231 |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |           0.0043 |           1.9819 |        -219.9666 |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |          -0.0106 |           1.8824 |        -246.4585 |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |          -0.0176 |           1.8103 |        -251.1680 |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |          -0.0292 |           1.7400 |        -270.2440 |
[32m[20221208 14:31:42 @agent_ppo2.py:179][0m |          -0.0308 |           1.6756 |        -267.7492 |
[32m[20221208 14:31:43 @agent_ppo2.py:179][0m |          -0.0384 |           1.6043 |        -269.2995 |
[32m[20221208 14:31:43 @agent_ppo2.py:179][0m |          -0.0323 |           1.5476 |        -263.1028 |
[32m[20221208 14:31:43 @agent_ppo2.py:179][0m |          -0.0417 |           1.5388 |        -278.0739 |
[32m[20221208 14:31:43 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:31:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.39
[32m[20221208 14:31:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.35
[32m[20221208 14:31:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.62
[32m[20221208 14:31:43 @agent_ppo2.py:137][0m Total time:      15.64 min
[32m[20221208 14:31:43 @agent_ppo2.py:139][0m 1259520 total steps have happened
[32m[20221208 14:31:43 @agent_ppo2.py:115][0m #------------------------ Iteration 615 --------------------------#
[32m[20221208 14:31:44 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:31:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |           0.0963 |           6.5467 |        -477.1638 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |           0.1063 |           5.9826 |        -253.6647 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |           0.0552 |           5.7774 |        -313.3475 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |           0.0275 |           5.5893 |        -380.4942 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |           0.0099 |           5.4992 |        -409.0106 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |          -0.0077 |           5.3587 |        -440.3325 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |          -0.0242 |           5.2826 |        -468.8117 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |          -0.0265 |           5.2135 |        -477.9859 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |          -0.0323 |           5.1615 |        -490.3924 |
[32m[20221208 14:31:44 @agent_ppo2.py:179][0m |          -0.0173 |           5.1086 |        -484.3843 |
[32m[20221208 14:31:44 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:31:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.61
[32m[20221208 14:31:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.84
[32m[20221208 14:31:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.00
[32m[20221208 14:31:45 @agent_ppo2.py:137][0m Total time:      15.67 min
[32m[20221208 14:31:45 @agent_ppo2.py:139][0m 1261568 total steps have happened
[32m[20221208 14:31:45 @agent_ppo2.py:115][0m #------------------------ Iteration 616 --------------------------#
[32m[20221208 14:31:45 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:45 @agent_ppo2.py:179][0m |           0.0581 |           2.4082 |        -458.3320 |
[32m[20221208 14:31:45 @agent_ppo2.py:179][0m |           0.0541 |           2.1435 |        -308.6500 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |           0.0124 |           2.0156 |        -270.8013 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0113 |           1.9170 |        -345.0240 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0267 |           1.8568 |        -383.3337 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0388 |           1.7942 |        -399.0197 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0434 |           1.7518 |        -406.6270 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0450 |           1.7053 |        -406.9600 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0527 |           1.6929 |        -419.4899 |
[32m[20221208 14:31:46 @agent_ppo2.py:179][0m |          -0.0528 |           1.6671 |        -418.8503 |
[32m[20221208 14:31:46 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:31:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.09
[32m[20221208 14:31:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.93
[32m[20221208 14:31:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.18
[32m[20221208 14:31:46 @agent_ppo2.py:137][0m Total time:      15.69 min
[32m[20221208 14:31:46 @agent_ppo2.py:139][0m 1263616 total steps have happened
[32m[20221208 14:31:46 @agent_ppo2.py:115][0m #------------------------ Iteration 617 --------------------------#
[32m[20221208 14:31:47 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:31:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |           0.1003 |           4.0077 |        -466.6248 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |           0.0795 |           3.4318 |        -334.7727 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |           0.0278 |           3.2495 |        -378.2979 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |           0.0099 |           3.1086 |        -391.5101 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |          -0.0079 |           2.9477 |        -419.2142 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |          -0.0252 |           2.8568 |        -442.1232 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |          -0.0338 |           2.8022 |        -456.4264 |
[32m[20221208 14:31:47 @agent_ppo2.py:179][0m |          -0.0370 |           2.7170 |        -456.3690 |
[32m[20221208 14:31:48 @agent_ppo2.py:179][0m |          -0.0415 |           2.6764 |        -466.2893 |
[32m[20221208 14:31:48 @agent_ppo2.py:179][0m |          -0.0427 |           2.6059 |        -467.1536 |
[32m[20221208 14:31:48 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:31:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.31
[32m[20221208 14:31:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.62
[32m[20221208 14:31:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.00
[32m[20221208 14:31:48 @agent_ppo2.py:137][0m Total time:      15.72 min
[32m[20221208 14:31:48 @agent_ppo2.py:139][0m 1265664 total steps have happened
[32m[20221208 14:31:48 @agent_ppo2.py:115][0m #------------------------ Iteration 618 --------------------------#
[32m[20221208 14:31:49 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |           0.0843 |           7.0463 |        -551.4391 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |           0.0504 |           6.2556 |        -461.5314 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0061 |           5.9911 |        -522.1137 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0336 |           5.8180 |        -564.0307 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0455 |           5.6646 |        -571.2424 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0590 |           5.5261 |        -591.8384 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0662 |           5.4301 |        -613.2766 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0672 |           5.3308 |        -620.4034 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0741 |           5.2861 |        -619.4500 |
[32m[20221208 14:31:49 @agent_ppo2.py:179][0m |          -0.0778 |           5.1866 |        -642.0487 |
[32m[20221208 14:31:49 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:31:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.86
[32m[20221208 14:31:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.90
[32m[20221208 14:31:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.91
[32m[20221208 14:31:50 @agent_ppo2.py:137][0m Total time:      15.75 min
[32m[20221208 14:31:50 @agent_ppo2.py:139][0m 1267712 total steps have happened
[32m[20221208 14:31:50 @agent_ppo2.py:115][0m #------------------------ Iteration 619 --------------------------#
[32m[20221208 14:31:50 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:50 @agent_ppo2.py:179][0m |           0.0997 |           4.8988 |        -396.4484 |
[32m[20221208 14:31:50 @agent_ppo2.py:179][0m |           0.0323 |           4.3286 |        -330.5828 |
[32m[20221208 14:31:50 @agent_ppo2.py:179][0m |           0.0050 |           4.1515 |        -346.9990 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0104 |           4.0061 |        -353.6666 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0236 |           3.9076 |        -369.6605 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0351 |           3.8070 |        -378.8818 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0402 |           3.7614 |        -373.0420 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0575 |           3.6936 |        -403.6356 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0579 |           3.6488 |        -404.3251 |
[32m[20221208 14:31:51 @agent_ppo2.py:179][0m |          -0.0663 |           3.5852 |        -415.4618 |
[32m[20221208 14:31:51 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:31:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.32
[32m[20221208 14:31:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.87
[32m[20221208 14:31:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.99
[32m[20221208 14:31:51 @agent_ppo2.py:137][0m Total time:      15.77 min
[32m[20221208 14:31:51 @agent_ppo2.py:139][0m 1269760 total steps have happened
[32m[20221208 14:31:51 @agent_ppo2.py:115][0m #------------------------ Iteration 620 --------------------------#
[32m[20221208 14:31:52 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |           0.1028 |           6.2261 |        -518.4081 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |           0.1068 |           5.7735 |        -377.8519 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |           0.0548 |           5.6252 |        -403.9337 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |           0.0131 |           5.5231 |        -491.8471 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |          -0.0120 |           5.4360 |        -531.5110 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |          -0.0270 |           5.3473 |        -547.9404 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |          -0.0392 |           5.3222 |        -573.0423 |
[32m[20221208 14:31:52 @agent_ppo2.py:179][0m |          -0.0434 |           5.3107 |        -587.2502 |
[32m[20221208 14:31:53 @agent_ppo2.py:179][0m |          -0.0496 |           5.1989 |        -599.1290 |
[32m[20221208 14:31:53 @agent_ppo2.py:179][0m |          -0.0501 |           5.1889 |        -598.9972 |
[32m[20221208 14:31:53 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:31:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.25
[32m[20221208 14:31:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.23
[32m[20221208 14:31:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.99
[32m[20221208 14:31:53 @agent_ppo2.py:137][0m Total time:      15.80 min
[32m[20221208 14:31:53 @agent_ppo2.py:139][0m 1271808 total steps have happened
[32m[20221208 14:31:53 @agent_ppo2.py:115][0m #------------------------ Iteration 621 --------------------------#
[32m[20221208 14:31:53 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |           0.0712 |           4.9362 |        -553.7892 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |           0.1348 |           4.3290 |        -336.9903 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |           0.0351 |           4.0434 |        -305.6772 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0097 |           3.8288 |        -375.4113 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0258 |           3.7059 |        -388.0441 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0419 |           3.5496 |        -414.4788 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0480 |           3.4470 |        -419.5037 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0573 |           3.3595 |        -437.9894 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0640 |           3.2967 |        -452.8648 |
[32m[20221208 14:31:54 @agent_ppo2.py:179][0m |          -0.0650 |           3.1962 |        -455.8598 |
[32m[20221208 14:31:54 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:31:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.25
[32m[20221208 14:31:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.97
[32m[20221208 14:31:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.89
[32m[20221208 14:31:55 @agent_ppo2.py:137][0m Total time:      15.83 min
[32m[20221208 14:31:55 @agent_ppo2.py:139][0m 1273856 total steps have happened
[32m[20221208 14:31:55 @agent_ppo2.py:115][0m #------------------------ Iteration 622 --------------------------#
[32m[20221208 14:31:55 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:31:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:55 @agent_ppo2.py:179][0m |           0.0754 |           4.5089 |        -533.2019 |
[32m[20221208 14:31:55 @agent_ppo2.py:179][0m |           0.0400 |           3.9745 |        -399.0978 |
[32m[20221208 14:31:55 @agent_ppo2.py:179][0m |           0.0050 |           3.7143 |        -409.6463 |
[32m[20221208 14:31:55 @agent_ppo2.py:179][0m |          -0.0141 |           3.5419 |        -416.6202 |
[32m[20221208 14:31:55 @agent_ppo2.py:179][0m |          -0.0308 |           3.4026 |        -443.6499 |
[32m[20221208 14:31:56 @agent_ppo2.py:179][0m |          -0.0393 |           3.2853 |        -475.4081 |
[32m[20221208 14:31:56 @agent_ppo2.py:179][0m |          -0.0479 |           3.2106 |        -497.3929 |
[32m[20221208 14:31:56 @agent_ppo2.py:179][0m |          -0.0558 |           3.1193 |        -467.5387 |
[32m[20221208 14:31:56 @agent_ppo2.py:179][0m |          -0.0613 |           3.0409 |        -482.0635 |
[32m[20221208 14:31:56 @agent_ppo2.py:179][0m |          -0.0656 |           2.9920 |        -504.1997 |
[32m[20221208 14:31:56 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:31:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.24
[32m[20221208 14:31:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.61
[32m[20221208 14:31:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.59
[32m[20221208 14:31:56 @agent_ppo2.py:137][0m Total time:      15.86 min
[32m[20221208 14:31:56 @agent_ppo2.py:139][0m 1275904 total steps have happened
[32m[20221208 14:31:56 @agent_ppo2.py:115][0m #------------------------ Iteration 623 --------------------------#
[32m[20221208 14:31:57 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:31:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |           0.1297 |           2.7396 |        -315.8684 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |           0.0628 |           2.4519 |        -132.0091 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |           0.0478 |           2.3517 |        -142.7388 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |           0.0187 |           2.3133 |        -178.6184 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |          -0.0022 |           2.2568 |        -195.6030 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |          -0.0149 |           2.1751 |        -205.1876 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |          -0.0252 |           2.1343 |        -223.5013 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |          -0.0301 |           2.1366 |        -236.9636 |
[32m[20221208 14:31:57 @agent_ppo2.py:179][0m |          -0.0362 |           2.0844 |        -237.4071 |
[32m[20221208 14:31:58 @agent_ppo2.py:179][0m |          -0.0404 |           2.0444 |        -246.4562 |
[32m[20221208 14:31:58 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:31:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.81
[32m[20221208 14:31:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.76
[32m[20221208 14:31:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.51
[32m[20221208 14:31:58 @agent_ppo2.py:137][0m Total time:      15.88 min
[32m[20221208 14:31:58 @agent_ppo2.py:139][0m 1277952 total steps have happened
[32m[20221208 14:31:58 @agent_ppo2.py:115][0m #------------------------ Iteration 624 --------------------------#
[32m[20221208 14:31:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:31:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |           0.0596 |           6.8297 |        -607.2035 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |           0.0711 |           6.3198 |        -514.0848 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |           0.0476 |           6.0150 |        -475.2967 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0078 |           5.9117 |        -569.6488 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0236 |           5.7795 |        -579.6759 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0374 |           5.6613 |        -585.3256 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0455 |           5.5603 |        -612.1578 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0469 |           5.5101 |        -610.4557 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0553 |           5.4328 |        -624.3036 |
[32m[20221208 14:31:59 @agent_ppo2.py:179][0m |          -0.0619 |           5.3913 |        -635.2831 |
[32m[20221208 14:31:59 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:32:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.10
[32m[20221208 14:32:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.11
[32m[20221208 14:32:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.38
[32m[20221208 14:32:00 @agent_ppo2.py:137][0m Total time:      15.91 min
[32m[20221208 14:32:00 @agent_ppo2.py:139][0m 1280000 total steps have happened
[32m[20221208 14:32:00 @agent_ppo2.py:115][0m #------------------------ Iteration 625 --------------------------#
[32m[20221208 14:32:00 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:32:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |           0.1805 |           4.2180 |        -512.7700 |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |           0.1103 |           3.7516 |        -212.4662 |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |           0.0618 |           3.5135 |        -272.1965 |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |           0.0124 |           3.4055 |        -363.7932 |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |          -0.0153 |           3.3045 |        -404.6090 |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |          -0.0307 |           3.2671 |        -429.0278 |
[32m[20221208 14:32:00 @agent_ppo2.py:179][0m |          -0.0393 |           3.1697 |        -435.9601 |
[32m[20221208 14:32:01 @agent_ppo2.py:179][0m |          -0.0492 |           3.1216 |        -451.2355 |
[32m[20221208 14:32:01 @agent_ppo2.py:179][0m |          -0.0527 |           3.0657 |        -463.6914 |
[32m[20221208 14:32:01 @agent_ppo2.py:179][0m |          -0.0550 |           3.0507 |        -475.5733 |
[32m[20221208 14:32:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
