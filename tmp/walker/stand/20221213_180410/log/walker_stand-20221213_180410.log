[32m[20221213 18:04:10 @logger.py:105][0m Log file set to ./tmp/walker/stand/20221213_180410/log/walker_stand-20221213_180410.log
[32m[20221213 18:04:10 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 18:04:10 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 18:04:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:10 @agent_ppo2.py:185][0m |          -0.0006 |           6.7484 |           0.2228 |
[32m[20221213 18:04:10 @agent_ppo2.py:185][0m |           0.0015 |           3.6279 |           0.2228 |
[32m[20221213 18:04:10 @agent_ppo2.py:185][0m |          -0.0081 |           1.7849 |           0.2229 |
[32m[20221213 18:04:10 @agent_ppo2.py:185][0m |          -0.0026 |           1.2636 |           0.2228 |
[32m[20221213 18:04:11 @agent_ppo2.py:185][0m |          -0.0016 |           1.1916 |           0.2228 |
[32m[20221213 18:04:11 @agent_ppo2.py:185][0m |          -0.0029 |           1.1707 |           0.2228 |
[32m[20221213 18:04:11 @agent_ppo2.py:185][0m |          -0.0166 |           1.2191 |           0.2228 |
[32m[20221213 18:04:11 @agent_ppo2.py:185][0m |          -0.0029 |           1.1462 |           0.2228 |
[32m[20221213 18:04:11 @agent_ppo2.py:185][0m |          -0.0053 |           1.1321 |           0.2227 |
[32m[20221213 18:04:11 @agent_ppo2.py:185][0m |          -0.0019 |           1.1282 |           0.2227 |
[32m[20221213 18:04:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:04:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.76
[32m[20221213 18:04:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 164.41
[32m[20221213 18:04:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.07
[32m[20221213 18:04:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 60.07
[32m[20221213 18:04:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 60.07
[32m[20221213 18:04:11 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 18:04:11 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 18:04:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 18:04:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0025 |           1.0444 |           0.2221 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0005 |           0.4461 |           0.2221 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0049 |           0.4191 |           0.2220 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0038 |           0.4065 |           0.2221 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0150 |           0.4017 |           0.2221 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0094 |           0.3943 |           0.2221 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0160 |           0.3893 |           0.2222 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0090 |           0.3834 |           0.2222 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0135 |           0.3789 |           0.2222 |
[32m[20221213 18:04:12 @agent_ppo2.py:185][0m |          -0.0115 |           0.3743 |           0.2222 |
[32m[20221213 18:04:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.27
[32m[20221213 18:04:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.67
[32m[20221213 18:04:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.85
[32m[20221213 18:04:12 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 133.85
[32m[20221213 18:04:12 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 133.85
[32m[20221213 18:04:12 @agent_ppo2.py:143][0m Total time:       0.05 min
[32m[20221213 18:04:12 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 18:04:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 18:04:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |           0.0041 |           1.6096 |           0.2239 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |           0.0008 |           1.1452 |           0.2239 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |          -0.0042 |           1.1042 |           0.2240 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |          -0.0080 |           1.0813 |           0.2240 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |          -0.0089 |           1.0554 |           0.2240 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |          -0.0111 |           1.0288 |           0.2239 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |          -0.0119 |           1.0056 |           0.2239 |
[32m[20221213 18:04:13 @agent_ppo2.py:185][0m |          -0.0112 |           0.9725 |           0.2239 |
[32m[20221213 18:04:14 @agent_ppo2.py:185][0m |          -0.0175 |           0.9446 |           0.2238 |
[32m[20221213 18:04:14 @agent_ppo2.py:185][0m |          -0.0164 |           0.9159 |           0.2239 |
[32m[20221213 18:04:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:04:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.71
[32m[20221213 18:04:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.80
[32m[20221213 18:04:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.61
[32m[20221213 18:04:14 @agent_ppo2.py:143][0m Total time:       0.07 min
[32m[20221213 18:04:14 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 18:04:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 18:04:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:14 @agent_ppo2.py:185][0m |           0.0021 |           1.7688 |           0.2243 |
[32m[20221213 18:04:14 @agent_ppo2.py:185][0m |          -0.0025 |           1.3547 |           0.2240 |
[32m[20221213 18:04:14 @agent_ppo2.py:185][0m |          -0.0174 |           1.3213 |           0.2238 |
[32m[20221213 18:04:14 @agent_ppo2.py:185][0m |          -0.0120 |           1.2789 |           0.2235 |
[32m[20221213 18:04:15 @agent_ppo2.py:185][0m |          -0.0103 |           1.2013 |           0.2233 |
[32m[20221213 18:04:15 @agent_ppo2.py:185][0m |           0.0004 |           1.2268 |           0.2232 |
[32m[20221213 18:04:15 @agent_ppo2.py:185][0m |          -0.0119 |           1.1583 |           0.2231 |
[32m[20221213 18:04:15 @agent_ppo2.py:185][0m |          -0.0135 |           1.1146 |           0.2230 |
[32m[20221213 18:04:15 @agent_ppo2.py:185][0m |          -0.0223 |           1.0999 |           0.2230 |
[32m[20221213 18:04:15 @agent_ppo2.py:185][0m |          -0.0133 |           1.0837 |           0.2230 |
[32m[20221213 18:04:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:04:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.74
[32m[20221213 18:04:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.35
[32m[20221213 18:04:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.71
[32m[20221213 18:04:15 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 135.71
[32m[20221213 18:04:15 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 135.71
[32m[20221213 18:04:15 @agent_ppo2.py:143][0m Total time:       0.09 min
[32m[20221213 18:04:15 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 18:04:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 18:04:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |           0.0013 |           1.8020 |           0.2228 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0045 |           1.5089 |           0.2228 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0061 |           1.4672 |           0.2228 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0082 |           1.4238 |           0.2228 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0083 |           1.4074 |           0.2228 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0094 |           1.3914 |           0.2228 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0072 |           1.3570 |           0.2229 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0027 |           1.3880 |           0.2231 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0142 |           1.3274 |           0.2232 |
[32m[20221213 18:04:16 @agent_ppo2.py:185][0m |          -0.0104 |           1.3105 |           0.2233 |
[32m[20221213 18:04:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.64
[32m[20221213 18:04:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.59
[32m[20221213 18:04:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.87
[32m[20221213 18:04:16 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 136.87
[32m[20221213 18:04:16 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 136.87
[32m[20221213 18:04:16 @agent_ppo2.py:143][0m Total time:       0.11 min
[32m[20221213 18:04:16 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 18:04:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 18:04:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |           0.0061 |           1.9398 |           0.2309 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |           0.0088 |           1.7416 |           0.2309 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |          -0.0049 |           1.5844 |           0.2306 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |          -0.0058 |           1.5252 |           0.2305 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |          -0.0067 |           1.5037 |           0.2304 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |          -0.0052 |           1.4664 |           0.2304 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |          -0.0054 |           1.4665 |           0.2304 |
[32m[20221213 18:04:17 @agent_ppo2.py:185][0m |          -0.0052 |           1.4234 |           0.2304 |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0109 |           1.3951 |           0.2304 |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0137 |           1.3910 |           0.2304 |
[32m[20221213 18:04:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.18
[32m[20221213 18:04:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 159.32
[32m[20221213 18:04:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.36
[32m[20221213 18:04:18 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221213 18:04:18 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 18:04:18 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 18:04:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0039 |           2.8054 |           0.2284 |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0032 |           2.4522 |           0.2283 |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0017 |           2.3656 |           0.2280 |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0127 |           2.3182 |           0.2276 |
[32m[20221213 18:04:18 @agent_ppo2.py:185][0m |          -0.0031 |           2.3704 |           0.2276 |
[32m[20221213 18:04:19 @agent_ppo2.py:185][0m |          -0.0132 |           2.2327 |           0.2274 |
[32m[20221213 18:04:19 @agent_ppo2.py:185][0m |          -0.0165 |           2.1690 |           0.2273 |
[32m[20221213 18:04:19 @agent_ppo2.py:185][0m |          -0.0075 |           2.2072 |           0.2273 |
[32m[20221213 18:04:19 @agent_ppo2.py:185][0m |          -0.0091 |           2.1309 |           0.2272 |
[32m[20221213 18:04:19 @agent_ppo2.py:185][0m |          -0.0124 |           2.1225 |           0.2272 |
[32m[20221213 18:04:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 160.57
[32m[20221213 18:04:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.11
[32m[20221213 18:04:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.67
[32m[20221213 18:04:19 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 183.67
[32m[20221213 18:04:19 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 183.67
[32m[20221213 18:04:19 @agent_ppo2.py:143][0m Total time:       0.16 min
[32m[20221213 18:04:19 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 18:04:19 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 18:04:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:19 @agent_ppo2.py:185][0m |           0.0051 |           2.9795 |           0.2285 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0013 |           2.5511 |           0.2284 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0031 |           2.4585 |           0.2283 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0033 |           2.3911 |           0.2283 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0063 |           2.3569 |           0.2282 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0104 |           2.2970 |           0.2282 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0094 |           2.2840 |           0.2282 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0093 |           2.2143 |           0.2283 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0096 |           2.1763 |           0.2282 |
[32m[20221213 18:04:20 @agent_ppo2.py:185][0m |          -0.0056 |           2.1826 |           0.2283 |
[32m[20221213 18:04:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 157.28
[32m[20221213 18:04:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.47
[32m[20221213 18:04:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.13
[32m[20221213 18:04:20 @agent_ppo2.py:143][0m Total time:       0.18 min
[32m[20221213 18:04:20 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 18:04:20 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 18:04:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |           0.0024 |           5.1217 |           0.2327 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0040 |           4.4128 |           0.2327 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0037 |           4.0814 |           0.2326 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0077 |           3.9239 |           0.2324 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0014 |           3.7860 |           0.2323 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0080 |           3.6936 |           0.2323 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0157 |           3.6278 |           0.2322 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0120 |           3.5970 |           0.2322 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0172 |           3.5238 |           0.2322 |
[32m[20221213 18:04:21 @agent_ppo2.py:185][0m |          -0.0105 |           3.4527 |           0.2320 |
[32m[20221213 18:04:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:04:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 185.21
[32m[20221213 18:04:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.33
[32m[20221213 18:04:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.17
[32m[20221213 18:04:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 186.17
[32m[20221213 18:04:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 186.17
[32m[20221213 18:04:22 @agent_ppo2.py:143][0m Total time:       0.20 min
[32m[20221213 18:04:22 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 18:04:22 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 18:04:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:22 @agent_ppo2.py:185][0m |           0.0025 |           4.5085 |           0.2252 |
[32m[20221213 18:04:22 @agent_ppo2.py:185][0m |          -0.0038 |           4.0802 |           0.2251 |
[32m[20221213 18:04:22 @agent_ppo2.py:185][0m |           0.0039 |           4.1503 |           0.2250 |
[32m[20221213 18:04:22 @agent_ppo2.py:185][0m |          -0.0085 |           3.8293 |           0.2249 |
[32m[20221213 18:04:22 @agent_ppo2.py:185][0m |          -0.0063 |           3.7306 |           0.2248 |
[32m[20221213 18:04:22 @agent_ppo2.py:185][0m |          -0.0170 |           3.6962 |           0.2248 |
[32m[20221213 18:04:23 @agent_ppo2.py:185][0m |          -0.0110 |           3.6366 |           0.2248 |
[32m[20221213 18:04:23 @agent_ppo2.py:185][0m |          -0.0086 |           3.5627 |           0.2247 |
[32m[20221213 18:04:23 @agent_ppo2.py:185][0m |          -0.0102 |           3.5078 |           0.2247 |
[32m[20221213 18:04:23 @agent_ppo2.py:185][0m |          -0.0157 |           3.4764 |           0.2247 |
[32m[20221213 18:04:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:04:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 164.00
[32m[20221213 18:04:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 176.48
[32m[20221213 18:04:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.84
[32m[20221213 18:04:23 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.84
[32m[20221213 18:04:23 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.84
[32m[20221213 18:04:23 @agent_ppo2.py:143][0m Total time:       0.22 min
[32m[20221213 18:04:23 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 18:04:23 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 18:04:23 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:04:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:23 @agent_ppo2.py:185][0m |           0.0003 |           4.8106 |           0.2282 |
[32m[20221213 18:04:23 @agent_ppo2.py:185][0m |           0.0029 |           4.3875 |           0.2283 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |           0.0031 |           4.3301 |           0.2284 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |          -0.0054 |           4.2552 |           0.2285 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |           0.0047 |           4.3893 |           0.2284 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |          -0.0053 |           4.2301 |           0.2285 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |          -0.0120 |           4.0165 |           0.2286 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |          -0.0001 |           4.0932 |           0.2287 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |          -0.0152 |           3.9290 |           0.2287 |
[32m[20221213 18:04:24 @agent_ppo2.py:185][0m |          -0.0123 |           3.8759 |           0.2289 |
[32m[20221213 18:04:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 166.83
[32m[20221213 18:04:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 191.39
[32m[20221213 18:04:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.39
[32m[20221213 18:04:24 @agent_ppo2.py:143][0m Total time:       0.24 min
[32m[20221213 18:04:24 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 18:04:24 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 18:04:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0009 |           5.4838 |           0.2346 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |           0.0026 |           5.0353 |           0.2347 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0049 |           4.7684 |           0.2345 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0046 |           4.6470 |           0.2345 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0047 |           4.5553 |           0.2346 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0053 |           4.4939 |           0.2346 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0067 |           4.4192 |           0.2348 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0120 |           4.3868 |           0.2349 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0116 |           4.3120 |           0.2350 |
[32m[20221213 18:04:25 @agent_ppo2.py:185][0m |          -0.0089 |           4.3002 |           0.2352 |
[32m[20221213 18:04:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:04:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.59
[32m[20221213 18:04:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 217.63
[32m[20221213 18:04:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.32
[32m[20221213 18:04:26 @agent_ppo2.py:143][0m Total time:       0.26 min
[32m[20221213 18:04:26 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 18:04:26 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 18:04:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |           0.0016 |           5.8011 |           0.2299 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0026 |           5.4533 |           0.2298 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0062 |           5.2858 |           0.2298 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0093 |           5.2289 |           0.2296 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0023 |           5.1819 |           0.2295 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0115 |           5.0863 |           0.2294 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0179 |           5.1653 |           0.2293 |
[32m[20221213 18:04:26 @agent_ppo2.py:185][0m |          -0.0126 |           4.9488 |           0.2292 |
[32m[20221213 18:04:27 @agent_ppo2.py:185][0m |          -0.0169 |           4.9369 |           0.2292 |
[32m[20221213 18:04:27 @agent_ppo2.py:185][0m |          -0.0092 |           4.8371 |           0.2291 |
[32m[20221213 18:04:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 171.66
[32m[20221213 18:04:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 200.68
[32m[20221213 18:04:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.08
[32m[20221213 18:04:27 @agent_ppo2.py:143][0m Total time:       0.28 min
[32m[20221213 18:04:27 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 18:04:27 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 18:04:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:27 @agent_ppo2.py:185][0m |          -0.0026 |           7.0665 |           0.2354 |
[32m[20221213 18:04:27 @agent_ppo2.py:185][0m |           0.0037 |           6.8553 |           0.2353 |
[32m[20221213 18:04:27 @agent_ppo2.py:185][0m |           0.0005 |           6.5369 |           0.2352 |
[32m[20221213 18:04:27 @agent_ppo2.py:185][0m |          -0.0122 |           6.2925 |           0.2354 |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |          -0.0089 |           6.1967 |           0.2355 |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |          -0.0136 |           6.1365 |           0.2355 |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |          -0.0103 |           6.0440 |           0.2356 |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |          -0.0105 |           5.9936 |           0.2358 |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |          -0.0202 |           5.9678 |           0.2358 |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |          -0.0158 |           5.8878 |           0.2360 |
[32m[20221213 18:04:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.33
[32m[20221213 18:04:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.34
[32m[20221213 18:04:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.23
[32m[20221213 18:04:28 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 263.23
[32m[20221213 18:04:28 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 263.23
[32m[20221213 18:04:28 @agent_ppo2.py:143][0m Total time:       0.31 min
[32m[20221213 18:04:28 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 18:04:28 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 18:04:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:28 @agent_ppo2.py:185][0m |           0.0179 |           5.2556 |           0.2374 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0043 |           4.3935 |           0.2372 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0047 |           4.3023 |           0.2369 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0032 |           4.3175 |           0.2369 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0099 |           4.2153 |           0.2366 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0061 |           4.2345 |           0.2365 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0045 |           4.1977 |           0.2366 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0124 |           4.1237 |           0.2363 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0112 |           4.1141 |           0.2363 |
[32m[20221213 18:04:29 @agent_ppo2.py:185][0m |          -0.0125 |           4.0821 |           0.2362 |
[32m[20221213 18:04:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:04:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.61
[32m[20221213 18:04:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 188.83
[32m[20221213 18:04:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.47
[32m[20221213 18:04:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 264.47
[32m[20221213 18:04:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 264.47
[32m[20221213 18:04:30 @agent_ppo2.py:143][0m Total time:       0.33 min
[32m[20221213 18:04:30 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 18:04:30 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 18:04:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |           0.0067 |           6.6187 |           0.2356 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0028 |           5.8892 |           0.2355 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0090 |           5.7322 |           0.2354 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0065 |           5.7940 |           0.2353 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0148 |           5.5898 |           0.2353 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0031 |           5.9245 |           0.2355 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0096 |           5.4909 |           0.2353 |
[32m[20221213 18:04:30 @agent_ppo2.py:185][0m |          -0.0126 |           5.4710 |           0.2353 |
[32m[20221213 18:04:31 @agent_ppo2.py:185][0m |          -0.0114 |           5.4069 |           0.2354 |
[32m[20221213 18:04:31 @agent_ppo2.py:185][0m |          -0.0110 |           5.3860 |           0.2355 |
[32m[20221213 18:04:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:04:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 175.19
[32m[20221213 18:04:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.13
[32m[20221213 18:04:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.36
[32m[20221213 18:04:31 @agent_ppo2.py:143][0m Total time:       0.35 min
[32m[20221213 18:04:31 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 18:04:31 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 18:04:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:31 @agent_ppo2.py:185][0m |          -0.0031 |           7.9375 |           0.2371 |
[32m[20221213 18:04:31 @agent_ppo2.py:185][0m |          -0.0033 |           7.2776 |           0.2370 |
[32m[20221213 18:04:31 @agent_ppo2.py:185][0m |          -0.0061 |           7.0431 |           0.2368 |
[32m[20221213 18:04:31 @agent_ppo2.py:185][0m |          -0.0079 |           6.9828 |           0.2367 |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |          -0.0107 |           6.8816 |           0.2365 |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |          -0.0015 |           7.2594 |           0.2364 |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |          -0.0161 |           6.7717 |           0.2363 |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |          -0.0163 |           6.6712 |           0.2364 |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |          -0.0116 |           6.6431 |           0.2363 |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |          -0.0135 |           6.5570 |           0.2364 |
[32m[20221213 18:04:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.45
[32m[20221213 18:04:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.51
[32m[20221213 18:04:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.25
[32m[20221213 18:04:32 @agent_ppo2.py:143][0m Total time:       0.37 min
[32m[20221213 18:04:32 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 18:04:32 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 18:04:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:04:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:32 @agent_ppo2.py:185][0m |           0.0004 |           7.7765 |           0.2359 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0050 |           7.3003 |           0.2355 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0103 |           7.1393 |           0.2353 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0078 |           6.9892 |           0.2354 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0034 |           7.0413 |           0.2354 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0114 |           6.8369 |           0.2357 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0120 |           6.6910 |           0.2360 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0117 |           6.6398 |           0.2361 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0125 |           6.5405 |           0.2363 |
[32m[20221213 18:04:33 @agent_ppo2.py:185][0m |          -0.0133 |           6.4613 |           0.2366 |
[32m[20221213 18:04:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:04:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.01
[32m[20221213 18:04:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.29
[32m[20221213 18:04:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.46
[32m[20221213 18:04:33 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 18:04:33 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 18:04:33 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 18:04:34 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:04:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:34 @agent_ppo2.py:185][0m |           0.0084 |           8.9988 |           0.2461 |
[32m[20221213 18:04:34 @agent_ppo2.py:185][0m |          -0.0015 |           7.7648 |           0.2456 |
[32m[20221213 18:04:34 @agent_ppo2.py:185][0m |          -0.0044 |           7.5131 |           0.2453 |
[32m[20221213 18:04:34 @agent_ppo2.py:185][0m |          -0.0068 |           7.3806 |           0.2449 |
[32m[20221213 18:04:34 @agent_ppo2.py:185][0m |          -0.0087 |           7.2451 |           0.2446 |
[32m[20221213 18:04:34 @agent_ppo2.py:185][0m |          -0.0135 |           7.2320 |           0.2444 |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0088 |           7.0471 |           0.2442 |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0127 |           7.0173 |           0.2442 |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0157 |           6.9228 |           0.2440 |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0106 |           7.1324 |           0.2439 |
[32m[20221213 18:04:35 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 18:04:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.05
[32m[20221213 18:04:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.98
[32m[20221213 18:04:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.61
[32m[20221213 18:04:35 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 265.61
[32m[20221213 18:04:35 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 265.61
[32m[20221213 18:04:35 @agent_ppo2.py:143][0m Total time:       0.42 min
[32m[20221213 18:04:35 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 18:04:35 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 18:04:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0006 |           6.9033 |           0.2419 |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0087 |           6.4986 |           0.2419 |
[32m[20221213 18:04:35 @agent_ppo2.py:185][0m |          -0.0022 |           6.3655 |           0.2419 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0054 |           6.5290 |           0.2421 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0125 |           6.2452 |           0.2422 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0100 |           6.1537 |           0.2421 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0130 |           6.1144 |           0.2423 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0111 |           6.0375 |           0.2424 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0125 |           5.9905 |           0.2425 |
[32m[20221213 18:04:36 @agent_ppo2.py:185][0m |          -0.0104 |           5.9506 |           0.2425 |
[32m[20221213 18:04:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:04:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.78
[32m[20221213 18:04:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 215.97
[32m[20221213 18:04:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.84
[32m[20221213 18:04:36 @agent_ppo2.py:143][0m Total time:       0.44 min
[32m[20221213 18:04:36 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 18:04:36 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 18:04:36 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:04:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0027 |           7.5623 |           0.2493 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |           0.0077 |           8.0234 |           0.2493 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |           0.0068 |           8.0180 |           0.2490 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0059 |           7.2521 |           0.2488 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0079 |           7.2159 |           0.2488 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0077 |           7.1897 |           0.2488 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0080 |           7.1861 |           0.2489 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0078 |           7.1590 |           0.2489 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |           0.0046 |           8.0338 |           0.2489 |
[32m[20221213 18:04:37 @agent_ppo2.py:185][0m |          -0.0099 |           7.1543 |           0.2489 |
[32m[20221213 18:04:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:04:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.54
[32m[20221213 18:04:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.68
[32m[20221213 18:04:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.19
[32m[20221213 18:04:38 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 278.19
[32m[20221213 18:04:38 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 278.19
[32m[20221213 18:04:38 @agent_ppo2.py:143][0m Total time:       0.46 min
[32m[20221213 18:04:38 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 18:04:38 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 18:04:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0029 |           7.8158 |           0.2427 |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0045 |           7.6656 |           0.2425 |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0055 |           7.6330 |           0.2423 |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0055 |           7.6146 |           0.2421 |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0084 |           7.5920 |           0.2419 |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0065 |           7.5742 |           0.2419 |
[32m[20221213 18:04:38 @agent_ppo2.py:185][0m |          -0.0107 |           7.5657 |           0.2419 |
[32m[20221213 18:04:39 @agent_ppo2.py:185][0m |          -0.0090 |           7.5623 |           0.2418 |
[32m[20221213 18:04:39 @agent_ppo2.py:185][0m |          -0.0108 |           7.5349 |           0.2417 |
[32m[20221213 18:04:39 @agent_ppo2.py:185][0m |          -0.0109 |           7.5060 |           0.2417 |
[32m[20221213 18:04:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.89
[32m[20221213 18:04:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.83
[32m[20221213 18:04:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.51
[32m[20221213 18:04:39 @agent_ppo2.py:143][0m Total time:       0.49 min
[32m[20221213 18:04:39 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 18:04:39 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 18:04:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:39 @agent_ppo2.py:185][0m |          -0.0058 |          10.4337 |           0.2474 |
[32m[20221213 18:04:39 @agent_ppo2.py:185][0m |          -0.0026 |           9.4897 |           0.2473 |
[32m[20221213 18:04:39 @agent_ppo2.py:185][0m |          -0.0049 |           9.3132 |           0.2472 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0070 |           9.1630 |           0.2471 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0066 |           9.0258 |           0.2471 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0039 |           8.9266 |           0.2471 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0125 |           8.8159 |           0.2471 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0086 |           8.7442 |           0.2471 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0101 |           8.6874 |           0.2472 |
[32m[20221213 18:04:40 @agent_ppo2.py:185][0m |          -0.0046 |           8.6958 |           0.2472 |
[32m[20221213 18:04:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:04:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.71
[32m[20221213 18:04:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.25
[32m[20221213 18:04:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.10
[32m[20221213 18:04:40 @agent_ppo2.py:143][0m Total time:       0.51 min
[32m[20221213 18:04:40 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 18:04:40 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 18:04:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0000 |           9.1136 |           0.2390 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0035 |           8.6705 |           0.2386 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |           0.0050 |           9.1223 |           0.2384 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0064 |           8.4212 |           0.2379 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0088 |           8.3647 |           0.2380 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0067 |           8.2824 |           0.2379 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0119 |           8.2968 |           0.2377 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0097 |           8.1818 |           0.2376 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0093 |           8.1426 |           0.2374 |
[32m[20221213 18:04:41 @agent_ppo2.py:185][0m |          -0.0107 |           8.0855 |           0.2374 |
[32m[20221213 18:04:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.36
[32m[20221213 18:04:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.97
[32m[20221213 18:04:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.86
[32m[20221213 18:04:42 @agent_ppo2.py:143][0m Total time:       0.53 min
[32m[20221213 18:04:42 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 18:04:42 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 18:04:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0018 |          10.5227 |           0.2425 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0042 |          10.0930 |           0.2424 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0074 |           9.9277 |           0.2422 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0041 |           9.7688 |           0.2421 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0065 |           9.7254 |           0.2421 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0116 |           9.4827 |           0.2420 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0115 |           9.3386 |           0.2421 |
[32m[20221213 18:04:42 @agent_ppo2.py:185][0m |          -0.0137 |           9.2209 |           0.2421 |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |          -0.0124 |           9.1340 |           0.2421 |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |          -0.0061 |           9.2197 |           0.2422 |
[32m[20221213 18:04:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.11
[32m[20221213 18:04:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.09
[32m[20221213 18:04:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.40
[32m[20221213 18:04:43 @agent_ppo2.py:143][0m Total time:       0.55 min
[32m[20221213 18:04:43 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 18:04:43 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 18:04:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |           0.0005 |          11.9480 |           0.2512 |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |          -0.0085 |          10.9098 |           0.2509 |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |          -0.0243 |          10.6136 |           0.2506 |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |          -0.0113 |          10.3807 |           0.2506 |
[32m[20221213 18:04:43 @agent_ppo2.py:185][0m |          -0.0129 |          10.2217 |           0.2506 |
[32m[20221213 18:04:44 @agent_ppo2.py:185][0m |          -0.0160 |          10.1027 |           0.2506 |
[32m[20221213 18:04:44 @agent_ppo2.py:185][0m |          -0.0051 |          10.6492 |           0.2508 |
[32m[20221213 18:04:44 @agent_ppo2.py:185][0m |          -0.0129 |           9.9004 |           0.2508 |
[32m[20221213 18:04:44 @agent_ppo2.py:185][0m |          -0.0207 |           9.8365 |           0.2508 |
[32m[20221213 18:04:44 @agent_ppo2.py:185][0m |          -0.0107 |          10.0962 |           0.2509 |
[32m[20221213 18:04:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:04:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.49
[32m[20221213 18:04:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.47
[32m[20221213 18:04:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.25
[32m[20221213 18:04:44 @agent_ppo2.py:143][0m Total time:       0.57 min
[32m[20221213 18:04:44 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 18:04:44 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 18:04:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:44 @agent_ppo2.py:185][0m |          -0.0012 |          13.7395 |           0.2437 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0087 |          12.8043 |           0.2440 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0084 |          12.6241 |           0.2439 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0101 |          12.5564 |           0.2440 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0128 |          12.4442 |           0.2441 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |           0.0008 |          13.7109 |           0.2441 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0109 |          12.1615 |           0.2441 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0163 |          12.0563 |           0.2442 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0129 |          12.0638 |           0.2443 |
[32m[20221213 18:04:45 @agent_ppo2.py:185][0m |          -0.0136 |          11.8931 |           0.2443 |
[32m[20221213 18:04:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:04:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.62
[32m[20221213 18:04:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.02
[32m[20221213 18:04:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.61
[32m[20221213 18:04:46 @agent_ppo2.py:143][0m Total time:       0.60 min
[32m[20221213 18:04:46 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 18:04:46 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 18:04:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |          -0.0024 |          10.9619 |           0.2441 |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |           0.0014 |          10.7863 |           0.2441 |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |          -0.0045 |          10.5216 |           0.2441 |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |           0.0017 |          10.9492 |           0.2439 |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |          -0.0079 |          10.3799 |           0.2439 |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |           0.0042 |          11.2399 |           0.2439 |
[32m[20221213 18:04:46 @agent_ppo2.py:185][0m |          -0.0081 |          10.2551 |           0.2437 |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0093 |          10.1622 |           0.2437 |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0086 |          10.1257 |           0.2437 |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0080 |          10.0926 |           0.2437 |
[32m[20221213 18:04:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:04:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.38
[32m[20221213 18:04:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.11
[32m[20221213 18:04:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.92
[32m[20221213 18:04:47 @agent_ppo2.py:143][0m Total time:       0.62 min
[32m[20221213 18:04:47 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 18:04:47 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 18:04:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0030 |          11.2027 |           0.2438 |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0012 |          11.1415 |           0.2438 |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0034 |          10.9437 |           0.2435 |
[32m[20221213 18:04:47 @agent_ppo2.py:185][0m |          -0.0058 |          10.7833 |           0.2435 |
[32m[20221213 18:04:48 @agent_ppo2.py:185][0m |          -0.0086 |          10.6949 |           0.2434 |
[32m[20221213 18:04:48 @agent_ppo2.py:185][0m |          -0.0094 |          10.6161 |           0.2434 |
[32m[20221213 18:04:48 @agent_ppo2.py:185][0m |          -0.0038 |          10.8821 |           0.2434 |
[32m[20221213 18:04:48 @agent_ppo2.py:185][0m |          -0.0087 |          10.5282 |           0.2434 |
[32m[20221213 18:04:48 @agent_ppo2.py:185][0m |          -0.0095 |          10.4038 |           0.2434 |
[32m[20221213 18:04:48 @agent_ppo2.py:185][0m |          -0.0110 |          10.3374 |           0.2435 |
[32m[20221213 18:04:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:04:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.77
[32m[20221213 18:04:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.18
[32m[20221213 18:04:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 178.05
[32m[20221213 18:04:48 @agent_ppo2.py:143][0m Total time:       0.64 min
[32m[20221213 18:04:48 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 18:04:48 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 18:04:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |           0.0000 |          11.9379 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |           0.0099 |          13.3044 |           0.2484 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0056 |          11.6914 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0059 |          11.6382 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0017 |          11.9261 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0087 |          11.5719 |           0.2484 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0088 |          11.5004 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0098 |          11.4475 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0093 |          11.4327 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:185][0m |          -0.0108 |          11.4155 |           0.2483 |
[32m[20221213 18:04:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.06
[32m[20221213 18:04:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.07
[32m[20221213 18:04:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.45
[32m[20221213 18:04:49 @agent_ppo2.py:143][0m Total time:       0.66 min
[32m[20221213 18:04:49 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 18:04:49 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 18:04:50 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:04:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0004 |          10.9844 |           0.2498 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0035 |           9.8620 |           0.2497 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0051 |           9.4197 |           0.2495 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0117 |           9.1098 |           0.2495 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0041 |           8.9009 |           0.2495 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0106 |           8.6615 |           0.2494 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0088 |           8.5008 |           0.2495 |
[32m[20221213 18:04:50 @agent_ppo2.py:185][0m |          -0.0143 |           8.3629 |           0.2493 |
[32m[20221213 18:04:51 @agent_ppo2.py:185][0m |          -0.0126 |           8.2924 |           0.2494 |
[32m[20221213 18:04:51 @agent_ppo2.py:185][0m |          -0.0115 |           8.1895 |           0.2493 |
[32m[20221213 18:04:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:04:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.62
[32m[20221213 18:04:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.95
[32m[20221213 18:04:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.93
[32m[20221213 18:04:51 @agent_ppo2.py:143][0m Total time:       0.68 min
[32m[20221213 18:04:51 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 18:04:51 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 18:04:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:04:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:51 @agent_ppo2.py:185][0m |          -0.0010 |          15.6090 |           0.2519 |
[32m[20221213 18:04:51 @agent_ppo2.py:185][0m |           0.0063 |          15.9947 |           0.2518 |
[32m[20221213 18:04:51 @agent_ppo2.py:185][0m |          -0.0072 |          14.4535 |           0.2515 |
[32m[20221213 18:04:51 @agent_ppo2.py:185][0m |          -0.0043 |          14.4541 |           0.2514 |
[32m[20221213 18:04:52 @agent_ppo2.py:185][0m |          -0.0011 |          14.8877 |           0.2513 |
[32m[20221213 18:04:52 @agent_ppo2.py:185][0m |          -0.0095 |          13.9738 |           0.2512 |
[32m[20221213 18:04:52 @agent_ppo2.py:185][0m |          -0.0118 |          13.6636 |           0.2512 |
[32m[20221213 18:04:52 @agent_ppo2.py:185][0m |          -0.0112 |          13.5210 |           0.2511 |
[32m[20221213 18:04:52 @agent_ppo2.py:185][0m |          -0.0114 |          13.3832 |           0.2511 |
[32m[20221213 18:04:52 @agent_ppo2.py:185][0m |          -0.0056 |          13.5462 |           0.2512 |
[32m[20221213 18:04:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:04:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.30
[32m[20221213 18:04:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.69
[32m[20221213 18:04:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.80
[32m[20221213 18:04:52 @agent_ppo2.py:143][0m Total time:       0.71 min
[32m[20221213 18:04:52 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 18:04:52 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 18:04:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0009 |          14.2550 |           0.2420 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0053 |          12.9446 |           0.2418 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0083 |          12.4498 |           0.2415 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0071 |          12.0993 |           0.2414 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0158 |          11.7837 |           0.2413 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0096 |          11.5628 |           0.2414 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0112 |          11.3586 |           0.2413 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0110 |          11.2226 |           0.2413 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0110 |          11.0506 |           0.2413 |
[32m[20221213 18:04:53 @agent_ppo2.py:185][0m |          -0.0124 |          11.0237 |           0.2413 |
[32m[20221213 18:04:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:04:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.47
[32m[20221213 18:04:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.44
[32m[20221213 18:04:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.80
[32m[20221213 18:04:53 @agent_ppo2.py:143][0m Total time:       0.73 min
[32m[20221213 18:04:53 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 18:04:53 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 18:04:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0015 |          14.0528 |           0.2533 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0053 |          13.2720 |           0.2532 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0053 |          13.0666 |           0.2528 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0053 |          13.0716 |           0.2526 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0062 |          12.7666 |           0.2524 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0098 |          12.6202 |           0.2522 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0051 |          12.8028 |           0.2521 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0007 |          13.2913 |           0.2521 |
[32m[20221213 18:04:54 @agent_ppo2.py:185][0m |          -0.0102 |          12.2996 |           0.2520 |
[32m[20221213 18:04:55 @agent_ppo2.py:185][0m |          -0.0006 |          13.0053 |           0.2518 |
[32m[20221213 18:04:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.78
[32m[20221213 18:04:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.84
[32m[20221213 18:04:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.11
[32m[20221213 18:04:55 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 18:04:55 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 18:04:55 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 18:04:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:55 @agent_ppo2.py:185][0m |          -0.0008 |          16.2843 |           0.2511 |
[32m[20221213 18:04:55 @agent_ppo2.py:185][0m |          -0.0027 |          14.6810 |           0.2512 |
[32m[20221213 18:04:55 @agent_ppo2.py:185][0m |          -0.0053 |          14.3328 |           0.2511 |
[32m[20221213 18:04:55 @agent_ppo2.py:185][0m |          -0.0061 |          14.0691 |           0.2510 |
[32m[20221213 18:04:55 @agent_ppo2.py:185][0m |          -0.0003 |          14.1981 |           0.2511 |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0082 |          13.7447 |           0.2510 |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0107 |          13.6636 |           0.2509 |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0126 |          13.5075 |           0.2510 |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0122 |          13.3758 |           0.2510 |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0180 |          13.6034 |           0.2510 |
[32m[20221213 18:04:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:04:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.76
[32m[20221213 18:04:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.45
[32m[20221213 18:04:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.51
[32m[20221213 18:04:56 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 279.51
[32m[20221213 18:04:56 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 279.51
[32m[20221213 18:04:56 @agent_ppo2.py:143][0m Total time:       0.77 min
[32m[20221213 18:04:56 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 18:04:56 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 18:04:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0036 |          15.4907 |           0.2453 |
[32m[20221213 18:04:56 @agent_ppo2.py:185][0m |          -0.0034 |          15.0169 |           0.2451 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0076 |          14.8575 |           0.2450 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0057 |          14.7589 |           0.2448 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0089 |          14.6656 |           0.2444 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0103 |          14.6154 |           0.2442 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0083 |          14.5626 |           0.2439 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0061 |          14.9528 |           0.2439 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0106 |          14.5059 |           0.2437 |
[32m[20221213 18:04:57 @agent_ppo2.py:185][0m |          -0.0116 |          14.4420 |           0.2435 |
[32m[20221213 18:04:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:04:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.34
[32m[20221213 18:04:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.99
[32m[20221213 18:04:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.29
[32m[20221213 18:04:57 @agent_ppo2.py:143][0m Total time:       0.79 min
[32m[20221213 18:04:57 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 18:04:57 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 18:04:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |           0.0092 |          15.3774 |           0.2450 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0057 |          14.4313 |           0.2446 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0081 |          14.3128 |           0.2444 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0078 |          14.1911 |           0.2444 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0096 |          14.1200 |           0.2443 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0048 |          14.2430 |           0.2443 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0078 |          14.0203 |           0.2442 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0110 |          13.9496 |           0.2442 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0128 |          13.9279 |           0.2442 |
[32m[20221213 18:04:58 @agent_ppo2.py:185][0m |          -0.0060 |          14.1009 |           0.2440 |
[32m[20221213 18:04:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:04:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.48
[32m[20221213 18:04:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.53
[32m[20221213 18:04:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.99
[32m[20221213 18:04:59 @agent_ppo2.py:143][0m Total time:       0.81 min
[32m[20221213 18:04:59 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 18:04:59 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 18:04:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:04:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |           0.0001 |          15.3634 |           0.2426 |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |          -0.0050 |          14.8335 |           0.2427 |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |          -0.0033 |          14.6598 |           0.2425 |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |          -0.0044 |          14.4817 |           0.2424 |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |          -0.0063 |          14.4157 |           0.2422 |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |          -0.0123 |          14.3300 |           0.2421 |
[32m[20221213 18:04:59 @agent_ppo2.py:185][0m |          -0.0078 |          14.2318 |           0.2424 |
[32m[20221213 18:05:00 @agent_ppo2.py:185][0m |          -0.0058 |          14.8652 |           0.2423 |
[32m[20221213 18:05:00 @agent_ppo2.py:185][0m |          -0.0123 |          14.1291 |           0.2422 |
[32m[20221213 18:05:00 @agent_ppo2.py:185][0m |          -0.0046 |          14.0951 |           0.2423 |
[32m[20221213 18:05:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.59
[32m[20221213 18:05:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.15
[32m[20221213 18:05:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.30
[32m[20221213 18:05:00 @agent_ppo2.py:143][0m Total time:       0.84 min
[32m[20221213 18:05:00 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 18:05:00 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 18:05:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:00 @agent_ppo2.py:185][0m |          -0.0014 |          17.2325 |           0.2461 |
[32m[20221213 18:05:00 @agent_ppo2.py:185][0m |          -0.0099 |          15.6224 |           0.2459 |
[32m[20221213 18:05:00 @agent_ppo2.py:185][0m |          -0.0055 |          14.9908 |           0.2456 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0099 |          14.5358 |           0.2454 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0071 |          14.1967 |           0.2453 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0074 |          13.9644 |           0.2450 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0118 |          13.7077 |           0.2448 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0060 |          13.5011 |           0.2447 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0147 |          13.3408 |           0.2446 |
[32m[20221213 18:05:01 @agent_ppo2.py:185][0m |          -0.0182 |          13.2763 |           0.2444 |
[32m[20221213 18:05:01 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 18:05:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.99
[32m[20221213 18:05:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.05
[32m[20221213 18:05:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 181.90
[32m[20221213 18:05:01 @agent_ppo2.py:143][0m Total time:       0.86 min
[32m[20221213 18:05:01 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 18:05:01 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 18:05:02 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:02 @agent_ppo2.py:185][0m |           0.0046 |          17.7948 |           0.2392 |
[32m[20221213 18:05:02 @agent_ppo2.py:185][0m |          -0.0022 |          17.3388 |           0.2391 |
[32m[20221213 18:05:02 @agent_ppo2.py:185][0m |          -0.0099 |          16.8042 |           0.2389 |
[32m[20221213 18:05:02 @agent_ppo2.py:185][0m |          -0.0075 |          16.4884 |           0.2389 |
[32m[20221213 18:05:02 @agent_ppo2.py:185][0m |          -0.0098 |          16.3120 |           0.2387 |
[32m[20221213 18:05:03 @agent_ppo2.py:185][0m |          -0.0098 |          16.1432 |           0.2387 |
[32m[20221213 18:05:03 @agent_ppo2.py:185][0m |          -0.0126 |          16.0725 |           0.2386 |
[32m[20221213 18:05:03 @agent_ppo2.py:185][0m |          -0.0112 |          15.9098 |           0.2386 |
[32m[20221213 18:05:03 @agent_ppo2.py:185][0m |          -0.0102 |          15.7999 |           0.2386 |
[32m[20221213 18:05:03 @agent_ppo2.py:185][0m |          -0.0143 |          15.6783 |           0.2386 |
[32m[20221213 18:05:03 @agent_ppo2.py:130][0m Policy update time: 1.38 s
[32m[20221213 18:05:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.69
[32m[20221213 18:05:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.13
[32m[20221213 18:05:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.25
[32m[20221213 18:05:03 @agent_ppo2.py:143][0m Total time:       0.89 min
[32m[20221213 18:05:03 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 18:05:03 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 18:05:03 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 18:05:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0025 |          16.2862 |           0.2459 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0028 |          15.9083 |           0.2457 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0055 |          15.7770 |           0.2456 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0056 |          15.6706 |           0.2456 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0063 |          15.5850 |           0.2454 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |           0.0038 |          16.3172 |           0.2453 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0086 |          15.4710 |           0.2452 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0079 |          15.4020 |           0.2451 |
[32m[20221213 18:05:04 @agent_ppo2.py:185][0m |          -0.0022 |          15.6053 |           0.2451 |
[32m[20221213 18:05:05 @agent_ppo2.py:185][0m |          -0.0087 |          15.3273 |           0.2449 |
[32m[20221213 18:05:05 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:05:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.27
[32m[20221213 18:05:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.20
[32m[20221213 18:05:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.31
[32m[20221213 18:05:05 @agent_ppo2.py:143][0m Total time:       0.92 min
[32m[20221213 18:05:05 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 18:05:05 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 18:05:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:05:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:05 @agent_ppo2.py:185][0m |          -0.0057 |          20.3558 |           0.2439 |
[32m[20221213 18:05:05 @agent_ppo2.py:185][0m |          -0.0073 |          19.3670 |           0.2439 |
[32m[20221213 18:05:05 @agent_ppo2.py:185][0m |          -0.0026 |          19.2371 |           0.2439 |
[32m[20221213 18:05:05 @agent_ppo2.py:185][0m |          -0.0104 |          18.7839 |           0.2438 |
[32m[20221213 18:05:06 @agent_ppo2.py:185][0m |          -0.0118 |          18.5580 |           0.2438 |
[32m[20221213 18:05:06 @agent_ppo2.py:185][0m |          -0.0129 |          18.4426 |           0.2439 |
[32m[20221213 18:05:06 @agent_ppo2.py:185][0m |          -0.0132 |          18.3217 |           0.2438 |
[32m[20221213 18:05:06 @agent_ppo2.py:185][0m |          -0.0131 |          18.1244 |           0.2439 |
[32m[20221213 18:05:06 @agent_ppo2.py:185][0m |          -0.0139 |          18.0378 |           0.2440 |
[32m[20221213 18:05:06 @agent_ppo2.py:185][0m |          -0.0163 |          17.8731 |           0.2440 |
[32m[20221213 18:05:06 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 18:05:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.76
[32m[20221213 18:05:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.49
[32m[20221213 18:05:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.08
[32m[20221213 18:05:06 @agent_ppo2.py:143][0m Total time:       0.94 min
[32m[20221213 18:05:06 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 18:05:06 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 18:05:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:05:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |          -0.0001 |          22.6680 |           0.2479 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |           0.0016 |          21.6044 |           0.2478 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |          -0.0004 |          21.4023 |           0.2479 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |           0.0008 |          22.1381 |           0.2478 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |          -0.0070 |          20.6144 |           0.2478 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |          -0.0098 |          20.4069 |           0.2477 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |          -0.0115 |          20.2550 |           0.2477 |
[32m[20221213 18:05:07 @agent_ppo2.py:185][0m |          -0.0102 |          20.1391 |           0.2477 |
[32m[20221213 18:05:08 @agent_ppo2.py:185][0m |          -0.0076 |          20.0467 |           0.2477 |
[32m[20221213 18:05:08 @agent_ppo2.py:185][0m |          -0.0145 |          19.8423 |           0.2476 |
[32m[20221213 18:05:08 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:05:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.62
[32m[20221213 18:05:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.96
[32m[20221213 18:05:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 178.97
[32m[20221213 18:05:08 @agent_ppo2.py:143][0m Total time:       0.97 min
[32m[20221213 18:05:08 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 18:05:08 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 18:05:08 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:08 @agent_ppo2.py:185][0m |          -0.0056 |          19.9147 |           0.2461 |
[32m[20221213 18:05:08 @agent_ppo2.py:185][0m |          -0.0060 |          19.1063 |           0.2460 |
[32m[20221213 18:05:08 @agent_ppo2.py:185][0m |          -0.0121 |          18.6184 |           0.2459 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0114 |          18.4342 |           0.2460 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0085 |          17.9912 |           0.2459 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0107 |          17.9207 |           0.2460 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0093 |          17.6095 |           0.2462 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0130 |          17.4995 |           0.2462 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0137 |          17.3123 |           0.2461 |
[32m[20221213 18:05:09 @agent_ppo2.py:185][0m |          -0.0109 |          17.2073 |           0.2461 |
[32m[20221213 18:05:09 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:05:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.21
[32m[20221213 18:05:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.90
[32m[20221213 18:05:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.00
[32m[20221213 18:05:09 @agent_ppo2.py:143][0m Total time:       0.99 min
[32m[20221213 18:05:09 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 18:05:09 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 18:05:10 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0022 |          18.8652 |           0.2474 |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0050 |          18.1272 |           0.2473 |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0055 |          17.9451 |           0.2470 |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0085 |          17.7936 |           0.2468 |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0096 |          17.7710 |           0.2466 |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0110 |          17.6198 |           0.2465 |
[32m[20221213 18:05:10 @agent_ppo2.py:185][0m |          -0.0068 |          17.7375 |           0.2463 |
[32m[20221213 18:05:11 @agent_ppo2.py:185][0m |          -0.0096 |          17.4650 |           0.2460 |
[32m[20221213 18:05:11 @agent_ppo2.py:185][0m |          -0.0104 |          17.4899 |           0.2458 |
[32m[20221213 18:05:11 @agent_ppo2.py:185][0m |           0.0015 |          18.7443 |           0.2456 |
[32m[20221213 18:05:11 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:05:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.27
[32m[20221213 18:05:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.90
[32m[20221213 18:05:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.33
[32m[20221213 18:05:11 @agent_ppo2.py:143][0m Total time:       1.02 min
[32m[20221213 18:05:11 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 18:05:11 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 18:05:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:05:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:11 @agent_ppo2.py:185][0m |          -0.0017 |          17.7976 |           0.2468 |
[32m[20221213 18:05:11 @agent_ppo2.py:185][0m |          -0.0008 |          17.2068 |           0.2463 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0097 |          16.8277 |           0.2458 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0123 |          16.6873 |           0.2460 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0099 |          16.5328 |           0.2459 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0063 |          16.4866 |           0.2458 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0119 |          16.3294 |           0.2458 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0114 |          16.2279 |           0.2458 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0087 |          16.1992 |           0.2456 |
[32m[20221213 18:05:12 @agent_ppo2.py:185][0m |          -0.0108 |          16.0278 |           0.2456 |
[32m[20221213 18:05:12 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:05:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.55
[32m[20221213 18:05:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.10
[32m[20221213 18:05:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.34
[32m[20221213 18:05:12 @agent_ppo2.py:143][0m Total time:       1.04 min
[32m[20221213 18:05:12 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 18:05:12 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 18:05:13 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |           0.0004 |          16.4359 |           0.2373 |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |          -0.0051 |          16.1463 |           0.2370 |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |           0.0010 |          16.8489 |           0.2368 |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |          -0.0072 |          16.0514 |           0.2366 |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |          -0.0076 |          16.0102 |           0.2366 |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |          -0.0011 |          16.4936 |           0.2365 |
[32m[20221213 18:05:13 @agent_ppo2.py:185][0m |          -0.0112 |          15.9521 |           0.2366 |
[32m[20221213 18:05:14 @agent_ppo2.py:185][0m |          -0.0111 |          15.9285 |           0.2366 |
[32m[20221213 18:05:14 @agent_ppo2.py:185][0m |          -0.0104 |          15.8829 |           0.2366 |
[32m[20221213 18:05:14 @agent_ppo2.py:185][0m |          -0.0017 |          16.8399 |           0.2366 |
[32m[20221213 18:05:14 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 18:05:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.78
[32m[20221213 18:05:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.29
[32m[20221213 18:05:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.16
[32m[20221213 18:05:14 @agent_ppo2.py:143][0m Total time:       1.07 min
[32m[20221213 18:05:14 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 18:05:14 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 18:05:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:05:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:14 @agent_ppo2.py:185][0m |          -0.0002 |          18.0208 |           0.2431 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0046 |          17.6263 |           0.2429 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0069 |          17.4629 |           0.2426 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0121 |          17.4616 |           0.2424 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0102 |          17.3481 |           0.2424 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0102 |          17.2531 |           0.2422 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0076 |          17.2390 |           0.2422 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0123 |          17.2107 |           0.2422 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0101 |          17.0750 |           0.2421 |
[32m[20221213 18:05:15 @agent_ppo2.py:185][0m |          -0.0115 |          17.0459 |           0.2422 |
[32m[20221213 18:05:15 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 18:05:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.77
[32m[20221213 18:05:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.50
[32m[20221213 18:05:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.07
[32m[20221213 18:05:15 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 18:05:15 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 18:05:16 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 18:05:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0024 |          23.7480 |           0.2480 |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0044 |          22.7384 |           0.2476 |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0081 |          22.3326 |           0.2474 |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0055 |          21.7960 |           0.2471 |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0096 |          21.5214 |           0.2468 |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0124 |          21.1865 |           0.2468 |
[32m[20221213 18:05:16 @agent_ppo2.py:185][0m |          -0.0106 |          20.6088 |           0.2467 |
[32m[20221213 18:05:17 @agent_ppo2.py:185][0m |          -0.0147 |          20.3143 |           0.2466 |
[32m[20221213 18:05:17 @agent_ppo2.py:185][0m |          -0.0162 |          20.3723 |           0.2465 |
[32m[20221213 18:05:17 @agent_ppo2.py:185][0m |          -0.0100 |          19.7772 |           0.2464 |
[32m[20221213 18:05:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:05:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.45
[32m[20221213 18:05:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.30
[32m[20221213 18:05:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.50
[32m[20221213 18:05:17 @agent_ppo2.py:143][0m Total time:       1.12 min
[32m[20221213 18:05:17 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 18:05:17 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 18:05:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:17 @agent_ppo2.py:185][0m |          -0.0020 |          25.9866 |           0.2323 |
[32m[20221213 18:05:17 @agent_ppo2.py:185][0m |          -0.0075 |          24.4284 |           0.2318 |
[32m[20221213 18:05:17 @agent_ppo2.py:185][0m |          -0.0067 |          23.4053 |           0.2316 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |          -0.0066 |          23.2155 |           0.2315 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |          -0.0123 |          22.4647 |           0.2312 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |           0.0030 |          22.1621 |           0.2313 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |          -0.0143 |          21.8732 |           0.2313 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |          -0.0131 |          21.5656 |           0.2312 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |          -0.0144 |          21.2775 |           0.2311 |
[32m[20221213 18:05:18 @agent_ppo2.py:185][0m |          -0.0147 |          21.1113 |           0.2310 |
[32m[20221213 18:05:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:05:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.98
[32m[20221213 18:05:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 346.23
[32m[20221213 18:05:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.34
[32m[20221213 18:05:18 @agent_ppo2.py:143][0m Total time:       1.14 min
[32m[20221213 18:05:18 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 18:05:18 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 18:05:18 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |           0.0067 |          24.6500 |           0.2406 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0074 |          23.1153 |           0.2405 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0085 |          22.5911 |           0.2407 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0087 |          22.2028 |           0.2408 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0093 |          21.8906 |           0.2409 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0120 |          21.6530 |           0.2410 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0141 |          21.5660 |           0.2411 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0097 |          21.2834 |           0.2414 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0144 |          21.1275 |           0.2417 |
[32m[20221213 18:05:19 @agent_ppo2.py:185][0m |          -0.0090 |          20.8375 |           0.2418 |
[32m[20221213 18:05:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:05:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.89
[32m[20221213 18:05:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.68
[32m[20221213 18:05:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.86
[32m[20221213 18:05:20 @agent_ppo2.py:143][0m Total time:       1.16 min
[32m[20221213 18:05:20 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 18:05:20 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 18:05:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:05:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:20 @agent_ppo2.py:185][0m |           0.0021 |          27.1673 |           0.2421 |
[32m[20221213 18:05:20 @agent_ppo2.py:185][0m |          -0.0081 |          25.9853 |           0.2419 |
[32m[20221213 18:05:20 @agent_ppo2.py:185][0m |          -0.0077 |          25.3095 |           0.2418 |
[32m[20221213 18:05:20 @agent_ppo2.py:185][0m |          -0.0074 |          24.9238 |           0.2417 |
[32m[20221213 18:05:20 @agent_ppo2.py:185][0m |           0.0014 |          25.6811 |           0.2416 |
[32m[20221213 18:05:20 @agent_ppo2.py:185][0m |          -0.0070 |          24.2656 |           0.2417 |
[32m[20221213 18:05:21 @agent_ppo2.py:185][0m |          -0.0114 |          23.8556 |           0.2417 |
[32m[20221213 18:05:21 @agent_ppo2.py:185][0m |          -0.0049 |          24.7619 |           0.2417 |
[32m[20221213 18:05:21 @agent_ppo2.py:185][0m |          -0.0133 |          23.2862 |           0.2417 |
[32m[20221213 18:05:21 @agent_ppo2.py:185][0m |          -0.0109 |          22.7773 |           0.2417 |
[32m[20221213 18:05:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 18:05:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.14
[32m[20221213 18:05:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.94
[32m[20221213 18:05:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.66
[32m[20221213 18:05:21 @agent_ppo2.py:143][0m Total time:       1.19 min
[32m[20221213 18:05:21 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 18:05:21 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 18:05:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:21 @agent_ppo2.py:185][0m |          -0.0022 |          25.2207 |           0.2439 |
[32m[20221213 18:05:21 @agent_ppo2.py:185][0m |          -0.0035 |          23.6041 |           0.2440 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0044 |          23.0574 |           0.2439 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0069 |          22.5271 |           0.2440 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0086 |          22.1629 |           0.2441 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0012 |          23.0995 |           0.2443 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0126 |          21.7204 |           0.2443 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |           0.0067 |          24.4474 |           0.2445 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0061 |          21.5203 |           0.2446 |
[32m[20221213 18:05:22 @agent_ppo2.py:185][0m |          -0.0123 |          20.5918 |           0.2447 |
[32m[20221213 18:05:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:05:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.68
[32m[20221213 18:05:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.91
[32m[20221213 18:05:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.36
[32m[20221213 18:05:22 @agent_ppo2.py:143][0m Total time:       1.21 min
[32m[20221213 18:05:22 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 18:05:22 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 18:05:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |           0.0013 |          24.5163 |           0.2525 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0022 |          22.7505 |           0.2524 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0028 |          22.8011 |           0.2522 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0116 |          21.8634 |           0.2522 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0114 |          21.3040 |           0.2523 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0113 |          21.0815 |           0.2523 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0105 |          20.7789 |           0.2523 |
[32m[20221213 18:05:23 @agent_ppo2.py:185][0m |          -0.0116 |          20.6482 |           0.2524 |
[32m[20221213 18:05:24 @agent_ppo2.py:185][0m |          -0.0125 |          20.4247 |           0.2526 |
[32m[20221213 18:05:24 @agent_ppo2.py:185][0m |          -0.0149 |          20.2436 |           0.2527 |
[32m[20221213 18:05:24 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 18:05:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.43
[32m[20221213 18:05:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.39
[32m[20221213 18:05:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 175.64
[32m[20221213 18:05:24 @agent_ppo2.py:143][0m Total time:       1.24 min
[32m[20221213 18:05:24 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 18:05:24 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 18:05:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:05:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:24 @agent_ppo2.py:185][0m |           0.0016 |          30.8079 |           0.2531 |
[32m[20221213 18:05:24 @agent_ppo2.py:185][0m |          -0.0026 |          29.2252 |           0.2530 |
[32m[20221213 18:05:24 @agent_ppo2.py:185][0m |          -0.0068 |          28.5682 |           0.2530 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0084 |          28.0184 |           0.2530 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0083 |          27.4921 |           0.2529 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0084 |          27.2063 |           0.2529 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0152 |          27.0009 |           0.2529 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0120 |          26.6948 |           0.2529 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0084 |          26.8382 |           0.2529 |
[32m[20221213 18:05:25 @agent_ppo2.py:185][0m |          -0.0167 |          26.2191 |           0.2528 |
[32m[20221213 18:05:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:05:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.94
[32m[20221213 18:05:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.69
[32m[20221213 18:05:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.09
[32m[20221213 18:05:25 @agent_ppo2.py:143][0m Total time:       1.26 min
[32m[20221213 18:05:25 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 18:05:25 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 18:05:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |           0.0133 |          30.8731 |           0.2567 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0041 |          26.8281 |           0.2563 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0116 |          26.2886 |           0.2559 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0095 |          25.6448 |           0.2556 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0079 |          25.1287 |           0.2556 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0114 |          24.7951 |           0.2553 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0124 |          24.4358 |           0.2552 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0117 |          24.1292 |           0.2550 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0118 |          23.9747 |           0.2549 |
[32m[20221213 18:05:26 @agent_ppo2.py:185][0m |          -0.0122 |          23.6301 |           0.2548 |
[32m[20221213 18:05:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.07
[32m[20221213 18:05:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.12
[32m[20221213 18:05:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.18
[32m[20221213 18:05:26 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 375.18
[32m[20221213 18:05:26 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 375.18
[32m[20221213 18:05:26 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 18:05:26 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 18:05:26 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 18:05:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0059 |          34.0065 |           0.2550 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0056 |          31.6987 |           0.2548 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0086 |          30.7960 |           0.2547 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0068 |          29.9256 |           0.2546 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0087 |          29.3795 |           0.2545 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0110 |          28.8053 |           0.2542 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0109 |          28.2202 |           0.2540 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0058 |          27.8598 |           0.2539 |
[32m[20221213 18:05:27 @agent_ppo2.py:185][0m |          -0.0121 |          27.4776 |           0.2538 |
[32m[20221213 18:05:28 @agent_ppo2.py:185][0m |          -0.0128 |          27.0188 |           0.2538 |
[32m[20221213 18:05:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.58
[32m[20221213 18:05:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.79
[32m[20221213 18:05:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.79
[32m[20221213 18:05:28 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221213 18:05:28 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 18:05:28 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 18:05:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:28 @agent_ppo2.py:185][0m |          -0.0029 |          27.8253 |           0.2552 |
[32m[20221213 18:05:28 @agent_ppo2.py:185][0m |          -0.0016 |          26.5053 |           0.2548 |
[32m[20221213 18:05:28 @agent_ppo2.py:185][0m |          -0.0051 |          26.0029 |           0.2546 |
[32m[20221213 18:05:28 @agent_ppo2.py:185][0m |          -0.0095 |          25.5620 |           0.2542 |
[32m[20221213 18:05:28 @agent_ppo2.py:185][0m |          -0.0108 |          25.3177 |           0.2540 |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0096 |          25.2018 |           0.2538 |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0145 |          24.8930 |           0.2536 |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0118 |          24.6937 |           0.2533 |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0085 |          24.5020 |           0.2532 |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0131 |          24.2982 |           0.2531 |
[32m[20221213 18:05:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:05:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.05
[32m[20221213 18:05:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 337.01
[32m[20221213 18:05:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.57
[32m[20221213 18:05:29 @agent_ppo2.py:143][0m Total time:       1.32 min
[32m[20221213 18:05:29 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 18:05:29 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 18:05:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0088 |          31.5923 |           0.2397 |
[32m[20221213 18:05:29 @agent_ppo2.py:185][0m |          -0.0036 |          30.3570 |           0.2393 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0042 |          29.7900 |           0.2389 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0077 |          29.3897 |           0.2387 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0042 |          29.1912 |           0.2384 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0073 |          28.6804 |           0.2384 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0141 |          28.2772 |           0.2379 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0147 |          28.1869 |           0.2380 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0097 |          27.6937 |           0.2379 |
[32m[20221213 18:05:30 @agent_ppo2.py:185][0m |          -0.0135 |          27.5377 |           0.2379 |
[32m[20221213 18:05:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.01
[32m[20221213 18:05:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.55
[32m[20221213 18:05:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.39
[32m[20221213 18:05:30 @agent_ppo2.py:143][0m Total time:       1.34 min
[32m[20221213 18:05:30 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 18:05:30 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 18:05:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0023 |          31.4362 |           0.2489 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0042 |          29.8876 |           0.2488 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0026 |          29.1147 |           0.2487 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0105 |          28.5457 |           0.2485 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0130 |          28.2210 |           0.2484 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0092 |          27.7824 |           0.2482 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0089 |          27.5286 |           0.2481 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0107 |          27.1973 |           0.2479 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0121 |          27.0988 |           0.2476 |
[32m[20221213 18:05:31 @agent_ppo2.py:185][0m |          -0.0141 |          26.8207 |           0.2475 |
[32m[20221213 18:05:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:05:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.84
[32m[20221213 18:05:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.08
[32m[20221213 18:05:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.06
[32m[20221213 18:05:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 402.06
[32m[20221213 18:05:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 402.06
[32m[20221213 18:05:32 @agent_ppo2.py:143][0m Total time:       1.36 min
[32m[20221213 18:05:32 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 18:05:32 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 18:05:32 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0007 |          31.8820 |           0.2463 |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0063 |          30.0610 |           0.2462 |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0083 |          29.2524 |           0.2459 |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0065 |          28.6361 |           0.2458 |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0122 |          27.9481 |           0.2456 |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0092 |          27.2680 |           0.2455 |
[32m[20221213 18:05:32 @agent_ppo2.py:185][0m |          -0.0133 |          26.9053 |           0.2454 |
[32m[20221213 18:05:33 @agent_ppo2.py:185][0m |          -0.0126 |          26.3290 |           0.2456 |
[32m[20221213 18:05:33 @agent_ppo2.py:185][0m |          -0.0128 |          25.8990 |           0.2457 |
[32m[20221213 18:05:33 @agent_ppo2.py:185][0m |          -0.0123 |          25.7996 |           0.2457 |
[32m[20221213 18:05:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.46
[32m[20221213 18:05:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.59
[32m[20221213 18:05:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 199.39
[32m[20221213 18:05:33 @agent_ppo2.py:143][0m Total time:       1.39 min
[32m[20221213 18:05:33 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 18:05:33 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 18:05:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:05:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:33 @agent_ppo2.py:185][0m |          -0.0010 |          32.9346 |           0.2418 |
[32m[20221213 18:05:33 @agent_ppo2.py:185][0m |          -0.0035 |          30.8522 |           0.2416 |
[32m[20221213 18:05:33 @agent_ppo2.py:185][0m |          -0.0104 |          30.0625 |           0.2415 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |          -0.0129 |          29.3169 |           0.2415 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |           0.0124 |          31.2413 |           0.2414 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |          -0.0048 |          29.8774 |           0.2414 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |          -0.0107 |          27.6160 |           0.2414 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |          -0.0121 |          27.2068 |           0.2413 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |          -0.0067 |          27.5776 |           0.2412 |
[32m[20221213 18:05:34 @agent_ppo2.py:185][0m |          -0.0132 |          26.4887 |           0.2412 |
[32m[20221213 18:05:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.45
[32m[20221213 18:05:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.04
[32m[20221213 18:05:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.81
[32m[20221213 18:05:34 @agent_ppo2.py:143][0m Total time:       1.41 min
[32m[20221213 18:05:34 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 18:05:34 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 18:05:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0033 |          30.9807 |           0.2391 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |           0.0040 |          31.4718 |           0.2387 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0121 |          28.8171 |           0.2385 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0080 |          28.2009 |           0.2383 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0036 |          29.5397 |           0.2384 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0127 |          27.3899 |           0.2384 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0143 |          27.0413 |           0.2383 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0141 |          26.6169 |           0.2384 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0123 |          26.3586 |           0.2384 |
[32m[20221213 18:05:35 @agent_ppo2.py:185][0m |          -0.0128 |          26.0261 |           0.2384 |
[32m[20221213 18:05:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:05:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.93
[32m[20221213 18:05:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.63
[32m[20221213 18:05:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.02
[32m[20221213 18:05:35 @agent_ppo2.py:143][0m Total time:       1.43 min
[32m[20221213 18:05:35 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 18:05:35 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 18:05:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0007 |          27.9935 |           0.2469 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0038 |          26.9094 |           0.2469 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0057 |          26.3638 |           0.2469 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0117 |          26.0579 |           0.2470 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0102 |          25.6646 |           0.2471 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0112 |          25.4848 |           0.2472 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0162 |          25.2222 |           0.2471 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0133 |          24.9031 |           0.2474 |
[32m[20221213 18:05:36 @agent_ppo2.py:185][0m |          -0.0117 |          24.8269 |           0.2474 |
[32m[20221213 18:05:37 @agent_ppo2.py:185][0m |          -0.0136 |          24.6096 |           0.2475 |
[32m[20221213 18:05:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:05:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.10
[32m[20221213 18:05:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.41
[32m[20221213 18:05:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 180.59
[32m[20221213 18:05:37 @agent_ppo2.py:143][0m Total time:       1.45 min
[32m[20221213 18:05:37 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 18:05:37 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 18:05:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:37 @agent_ppo2.py:185][0m |          -0.0016 |          28.4251 |           0.2551 |
[32m[20221213 18:05:37 @agent_ppo2.py:185][0m |          -0.0063 |          26.9850 |           0.2547 |
[32m[20221213 18:05:37 @agent_ppo2.py:185][0m |          -0.0027 |          26.3877 |           0.2547 |
[32m[20221213 18:05:37 @agent_ppo2.py:185][0m |          -0.0105 |          25.1406 |           0.2545 |
[32m[20221213 18:05:37 @agent_ppo2.py:185][0m |          -0.0135 |          24.5657 |           0.2546 |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |          -0.0154 |          24.0125 |           0.2546 |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |          -0.0115 |          23.5216 |           0.2545 |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |          -0.0096 |          22.9868 |           0.2545 |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |          -0.0115 |          22.6320 |           0.2546 |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |          -0.0142 |          22.1726 |           0.2545 |
[32m[20221213 18:05:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.40
[32m[20221213 18:05:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.40
[32m[20221213 18:05:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.24
[32m[20221213 18:05:38 @agent_ppo2.py:143][0m Total time:       1.47 min
[32m[20221213 18:05:38 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 18:05:38 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 18:05:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |           0.0014 |          32.1947 |           0.2477 |
[32m[20221213 18:05:38 @agent_ppo2.py:185][0m |           0.0020 |          30.8491 |           0.2472 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0089 |          29.2283 |           0.2468 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0091 |          28.4184 |           0.2465 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0047 |          27.9063 |           0.2463 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0115 |          27.2321 |           0.2460 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0103 |          26.6330 |           0.2459 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0002 |          29.0368 |           0.2457 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0100 |          25.7297 |           0.2456 |
[32m[20221213 18:05:39 @agent_ppo2.py:185][0m |          -0.0160 |          25.4978 |           0.2453 |
[32m[20221213 18:05:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.41
[32m[20221213 18:05:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.41
[32m[20221213 18:05:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.94
[32m[20221213 18:05:39 @agent_ppo2.py:143][0m Total time:       1.49 min
[32m[20221213 18:05:39 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 18:05:39 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 18:05:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0011 |          32.0552 |           0.2451 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0075 |          30.2967 |           0.2452 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |           0.0014 |          31.2356 |           0.2451 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0114 |          28.6275 |           0.2455 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0084 |          27.8942 |           0.2455 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0018 |          30.5543 |           0.2458 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0106 |          26.9712 |           0.2460 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0186 |          26.6746 |           0.2461 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0112 |          26.1475 |           0.2462 |
[32m[20221213 18:05:40 @agent_ppo2.py:185][0m |          -0.0147 |          25.6748 |           0.2465 |
[32m[20221213 18:05:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:05:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.00
[32m[20221213 18:05:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.36
[32m[20221213 18:05:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 174.02
[32m[20221213 18:05:41 @agent_ppo2.py:143][0m Total time:       1.51 min
[32m[20221213 18:05:41 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 18:05:41 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 18:05:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |           0.0005 |          34.6745 |           0.2488 |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |          -0.0058 |          33.3102 |           0.2487 |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |          -0.0105 |          32.8632 |           0.2485 |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |          -0.0072 |          32.5293 |           0.2483 |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |          -0.0060 |          31.8658 |           0.2482 |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |          -0.0072 |          32.2211 |           0.2481 |
[32m[20221213 18:05:41 @agent_ppo2.py:185][0m |          -0.0067 |          32.9593 |           0.2480 |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0145 |          30.7838 |           0.2477 |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0089 |          31.0878 |           0.2477 |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0126 |          30.2427 |           0.2476 |
[32m[20221213 18:05:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:05:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.64
[32m[20221213 18:05:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.17
[32m[20221213 18:05:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.99
[32m[20221213 18:05:42 @agent_ppo2.py:143][0m Total time:       1.54 min
[32m[20221213 18:05:42 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 18:05:42 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 18:05:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0010 |          37.0175 |           0.2478 |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0051 |          35.4961 |           0.2478 |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0039 |          34.7887 |           0.2477 |
[32m[20221213 18:05:42 @agent_ppo2.py:185][0m |          -0.0079 |          34.4150 |           0.2476 |
[32m[20221213 18:05:43 @agent_ppo2.py:185][0m |          -0.0033 |          34.3774 |           0.2474 |
[32m[20221213 18:05:43 @agent_ppo2.py:185][0m |          -0.0090 |          33.7642 |           0.2474 |
[32m[20221213 18:05:43 @agent_ppo2.py:185][0m |          -0.0124 |          33.6858 |           0.2473 |
[32m[20221213 18:05:43 @agent_ppo2.py:185][0m |          -0.0094 |          33.1547 |           0.2473 |
[32m[20221213 18:05:43 @agent_ppo2.py:185][0m |          -0.0012 |          35.1156 |           0.2473 |
[32m[20221213 18:05:43 @agent_ppo2.py:185][0m |          -0.0168 |          32.8163 |           0.2471 |
[32m[20221213 18:05:43 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 18:05:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.95
[32m[20221213 18:05:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.83
[32m[20221213 18:05:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.30
[32m[20221213 18:05:43 @agent_ppo2.py:143][0m Total time:       1.56 min
[32m[20221213 18:05:43 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 18:05:43 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 18:05:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:05:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |           0.0070 |          34.3783 |           0.2498 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0060 |          31.8924 |           0.2497 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0007 |          32.2599 |           0.2495 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0060 |          30.5094 |           0.2494 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |           0.0024 |          32.5908 |           0.2493 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0015 |          30.9464 |           0.2494 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0080 |          29.1174 |           0.2492 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0081 |          28.7351 |           0.2494 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0108 |          28.2284 |           0.2494 |
[32m[20221213 18:05:44 @agent_ppo2.py:185][0m |          -0.0109 |          27.8905 |           0.2493 |
[32m[20221213 18:05:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:05:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.25
[32m[20221213 18:05:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.00
[32m[20221213 18:05:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.21
[32m[20221213 18:05:45 @agent_ppo2.py:143][0m Total time:       1.58 min
[32m[20221213 18:05:45 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 18:05:45 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 18:05:45 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:45 @agent_ppo2.py:185][0m |           0.0034 |          33.4479 |           0.2506 |
[32m[20221213 18:05:45 @agent_ppo2.py:185][0m |          -0.0023 |          31.9922 |           0.2503 |
[32m[20221213 18:05:45 @agent_ppo2.py:185][0m |          -0.0079 |          31.2103 |           0.2502 |
[32m[20221213 18:05:45 @agent_ppo2.py:185][0m |          -0.0098 |          30.7654 |           0.2503 |
[32m[20221213 18:05:45 @agent_ppo2.py:185][0m |          -0.0077 |          30.3220 |           0.2502 |
[32m[20221213 18:05:45 @agent_ppo2.py:185][0m |          -0.0152 |          29.8287 |           0.2503 |
[32m[20221213 18:05:46 @agent_ppo2.py:185][0m |          -0.0125 |          29.2353 |           0.2501 |
[32m[20221213 18:05:46 @agent_ppo2.py:185][0m |          -0.0115 |          28.8851 |           0.2503 |
[32m[20221213 18:05:46 @agent_ppo2.py:185][0m |          -0.0120 |          28.5351 |           0.2502 |
[32m[20221213 18:05:46 @agent_ppo2.py:185][0m |          -0.0158 |          27.9563 |           0.2501 |
[32m[20221213 18:05:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:05:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.47
[32m[20221213 18:05:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.78
[32m[20221213 18:05:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.30
[32m[20221213 18:05:46 @agent_ppo2.py:143][0m Total time:       1.60 min
[32m[20221213 18:05:46 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 18:05:46 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 18:05:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:46 @agent_ppo2.py:185][0m |          -0.0043 |          35.4462 |           0.2589 |
[32m[20221213 18:05:46 @agent_ppo2.py:185][0m |          -0.0074 |          33.7502 |           0.2586 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0052 |          33.0530 |           0.2581 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0095 |          32.3882 |           0.2580 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0123 |          31.9854 |           0.2579 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0092 |          31.6089 |           0.2578 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0037 |          31.8908 |           0.2575 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0116 |          30.7535 |           0.2576 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0146 |          30.4867 |           0.2576 |
[32m[20221213 18:05:47 @agent_ppo2.py:185][0m |          -0.0127 |          30.1220 |           0.2574 |
[32m[20221213 18:05:47 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 18:05:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.51
[32m[20221213 18:05:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.31
[32m[20221213 18:05:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.58
[32m[20221213 18:05:48 @agent_ppo2.py:143][0m Total time:       1.64 min
[32m[20221213 18:05:48 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 18:05:48 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 18:05:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:05:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:48 @agent_ppo2.py:185][0m |           0.0002 |          33.0903 |           0.2473 |
[32m[20221213 18:05:48 @agent_ppo2.py:185][0m |          -0.0062 |          31.4210 |           0.2472 |
[32m[20221213 18:05:48 @agent_ppo2.py:185][0m |           0.0087 |          34.2652 |           0.2471 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0076 |          30.4207 |           0.2470 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0078 |          30.0313 |           0.2470 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0090 |          29.7048 |           0.2469 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0091 |          29.4200 |           0.2471 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0109 |          29.0985 |           0.2470 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0145 |          28.7740 |           0.2471 |
[32m[20221213 18:05:49 @agent_ppo2.py:185][0m |          -0.0137 |          28.4650 |           0.2470 |
[32m[20221213 18:05:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:05:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.07
[32m[20221213 18:05:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 360.21
[32m[20221213 18:05:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.52
[32m[20221213 18:05:49 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221213 18:05:49 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 18:05:49 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 18:05:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0042 |          33.2326 |           0.2513 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0036 |          31.6272 |           0.2514 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |           0.0050 |          34.2200 |           0.2514 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0095 |          30.3389 |           0.2514 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0102 |          29.8645 |           0.2515 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0099 |          29.5429 |           0.2516 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0054 |          29.8049 |           0.2516 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0068 |          29.2603 |           0.2516 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0161 |          28.5369 |           0.2518 |
[32m[20221213 18:05:50 @agent_ppo2.py:185][0m |          -0.0144 |          28.1825 |           0.2518 |
[32m[20221213 18:05:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:05:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.23
[32m[20221213 18:05:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.07
[32m[20221213 18:05:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.04
[32m[20221213 18:05:51 @agent_ppo2.py:143][0m Total time:       1.68 min
[32m[20221213 18:05:51 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 18:05:51 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 18:05:51 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:05:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:51 @agent_ppo2.py:185][0m |          -0.0034 |          37.2072 |           0.2539 |
[32m[20221213 18:05:51 @agent_ppo2.py:185][0m |          -0.0057 |          35.7684 |           0.2534 |
[32m[20221213 18:05:51 @agent_ppo2.py:185][0m |          -0.0106 |          34.3995 |           0.2532 |
[32m[20221213 18:05:51 @agent_ppo2.py:185][0m |           0.0000 |          36.1246 |           0.2530 |
[32m[20221213 18:05:51 @agent_ppo2.py:185][0m |          -0.0125 |          33.1941 |           0.2528 |
[32m[20221213 18:05:52 @agent_ppo2.py:185][0m |          -0.0132 |          32.5654 |           0.2530 |
[32m[20221213 18:05:52 @agent_ppo2.py:185][0m |          -0.0123 |          32.2434 |           0.2530 |
[32m[20221213 18:05:52 @agent_ppo2.py:185][0m |          -0.0145 |          31.7243 |           0.2531 |
[32m[20221213 18:05:52 @agent_ppo2.py:185][0m |          -0.0149 |          31.2529 |           0.2529 |
[32m[20221213 18:05:52 @agent_ppo2.py:185][0m |          -0.0118 |          30.9328 |           0.2531 |
[32m[20221213 18:05:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:05:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.43
[32m[20221213 18:05:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.77
[32m[20221213 18:05:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.48
[32m[20221213 18:05:52 @agent_ppo2.py:143][0m Total time:       1.71 min
[32m[20221213 18:05:52 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 18:05:52 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 18:05:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:52 @agent_ppo2.py:185][0m |           0.0102 |          41.8760 |           0.2544 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0044 |          38.9529 |           0.2544 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0026 |          40.1255 |           0.2542 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0086 |          37.9587 |           0.2542 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0026 |          38.2583 |           0.2541 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0093 |          37.4534 |           0.2541 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0146 |          37.2392 |           0.2540 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0108 |          37.0094 |           0.2540 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0103 |          37.0153 |           0.2537 |
[32m[20221213 18:05:53 @agent_ppo2.py:185][0m |          -0.0131 |          36.7239 |           0.2538 |
[32m[20221213 18:05:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:05:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.77
[32m[20221213 18:05:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.57
[32m[20221213 18:05:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.18
[32m[20221213 18:05:53 @agent_ppo2.py:143][0m Total time:       1.73 min
[32m[20221213 18:05:53 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 18:05:53 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 18:05:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0033 |          35.0069 |           0.2550 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0030 |          33.2039 |           0.2545 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0084 |          32.3521 |           0.2543 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0106 |          31.6082 |           0.2542 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0058 |          31.2573 |           0.2541 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |           0.0013 |          32.3069 |           0.2540 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0126 |          30.1458 |           0.2539 |
[32m[20221213 18:05:54 @agent_ppo2.py:185][0m |          -0.0046 |          30.3522 |           0.2538 |
[32m[20221213 18:05:55 @agent_ppo2.py:185][0m |          -0.0162 |          29.2274 |           0.2539 |
[32m[20221213 18:05:55 @agent_ppo2.py:185][0m |          -0.0168 |          28.6684 |           0.2539 |
[32m[20221213 18:05:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:05:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.81
[32m[20221213 18:05:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.86
[32m[20221213 18:05:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 175.41
[32m[20221213 18:05:55 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 18:05:55 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 18:05:55 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 18:05:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:55 @agent_ppo2.py:185][0m |           0.0003 |          39.3969 |           0.2671 |
[32m[20221213 18:05:55 @agent_ppo2.py:185][0m |          -0.0019 |          37.4991 |           0.2671 |
[32m[20221213 18:05:55 @agent_ppo2.py:185][0m |          -0.0081 |          36.6197 |           0.2671 |
[32m[20221213 18:05:55 @agent_ppo2.py:185][0m |          -0.0017 |          37.8881 |           0.2670 |
[32m[20221213 18:05:56 @agent_ppo2.py:185][0m |          -0.0054 |          34.8416 |           0.2669 |
[32m[20221213 18:05:56 @agent_ppo2.py:185][0m |          -0.0087 |          34.0681 |           0.2669 |
[32m[20221213 18:05:56 @agent_ppo2.py:185][0m |          -0.0071 |          33.5768 |           0.2668 |
[32m[20221213 18:05:56 @agent_ppo2.py:185][0m |          -0.0025 |          33.5360 |           0.2667 |
[32m[20221213 18:05:56 @agent_ppo2.py:185][0m |          -0.0091 |          32.4619 |           0.2667 |
[32m[20221213 18:05:56 @agent_ppo2.py:185][0m |          -0.0106 |          32.0322 |           0.2666 |
[32m[20221213 18:05:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:05:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.93
[32m[20221213 18:05:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.19
[32m[20221213 18:05:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.10
[32m[20221213 18:05:56 @agent_ppo2.py:143][0m Total time:       1.77 min
[32m[20221213 18:05:56 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 18:05:56 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 18:05:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0026 |          37.6949 |           0.2565 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0072 |          35.5995 |           0.2558 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0056 |          34.4915 |           0.2560 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0110 |          33.6874 |           0.2559 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0118 |          32.9752 |           0.2559 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0123 |          32.3185 |           0.2560 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0108 |          31.7275 |           0.2562 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0134 |          31.4960 |           0.2560 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0108 |          30.5298 |           0.2560 |
[32m[20221213 18:05:57 @agent_ppo2.py:185][0m |          -0.0193 |          29.8839 |           0.2558 |
[32m[20221213 18:05:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:05:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.09
[32m[20221213 18:05:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.85
[32m[20221213 18:05:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.59
[32m[20221213 18:05:58 @agent_ppo2.py:143][0m Total time:       1.80 min
[32m[20221213 18:05:58 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 18:05:58 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 18:05:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:05:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:58 @agent_ppo2.py:185][0m |          -0.0059 |          40.4803 |           0.2551 |
[32m[20221213 18:05:58 @agent_ppo2.py:185][0m |          -0.0063 |          38.8092 |           0.2547 |
[32m[20221213 18:05:58 @agent_ppo2.py:185][0m |          -0.0086 |          38.2957 |           0.2548 |
[32m[20221213 18:05:58 @agent_ppo2.py:185][0m |          -0.0100 |          37.7461 |           0.2547 |
[32m[20221213 18:05:58 @agent_ppo2.py:185][0m |          -0.0104 |          37.3365 |           0.2547 |
[32m[20221213 18:05:58 @agent_ppo2.py:185][0m |          -0.0098 |          36.8624 |           0.2548 |
[32m[20221213 18:05:59 @agent_ppo2.py:185][0m |          -0.0141 |          37.0828 |           0.2546 |
[32m[20221213 18:05:59 @agent_ppo2.py:185][0m |          -0.0083 |          36.5852 |           0.2546 |
[32m[20221213 18:05:59 @agent_ppo2.py:185][0m |          -0.0142 |          36.0268 |           0.2547 |
[32m[20221213 18:05:59 @agent_ppo2.py:185][0m |          -0.0133 |          35.6440 |           0.2547 |
[32m[20221213 18:05:59 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 18:05:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.95
[32m[20221213 18:05:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.43
[32m[20221213 18:05:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.00
[32m[20221213 18:05:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 450.00
[32m[20221213 18:05:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 450.00
[32m[20221213 18:05:59 @agent_ppo2.py:143][0m Total time:       1.82 min
[32m[20221213 18:05:59 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 18:05:59 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 18:05:59 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:05:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:05:59 @agent_ppo2.py:185][0m |          -0.0019 |          36.9139 |           0.2599 |
[32m[20221213 18:05:59 @agent_ppo2.py:185][0m |          -0.0058 |          34.8632 |           0.2598 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0088 |          34.0183 |           0.2597 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0064 |          33.4751 |           0.2597 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0114 |          32.9672 |           0.2597 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0077 |          32.3496 |           0.2597 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0119 |          31.9971 |           0.2599 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0113 |          31.5563 |           0.2599 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0146 |          31.2530 |           0.2601 |
[32m[20221213 18:06:00 @agent_ppo2.py:185][0m |          -0.0149 |          30.9279 |           0.2601 |
[32m[20221213 18:06:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:06:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.55
[32m[20221213 18:06:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.89
[32m[20221213 18:06:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 169.50
[32m[20221213 18:06:00 @agent_ppo2.py:143][0m Total time:       1.84 min
[32m[20221213 18:06:00 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 18:06:00 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 18:06:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0063 |          42.3537 |           0.2598 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0038 |          39.5845 |           0.2599 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0076 |          38.5096 |           0.2600 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0148 |          37.3317 |           0.2598 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0131 |          36.4752 |           0.2597 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0073 |          36.4878 |           0.2600 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0094 |          35.1963 |           0.2600 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0102 |          34.7443 |           0.2603 |
[32m[20221213 18:06:01 @agent_ppo2.py:185][0m |          -0.0158 |          33.7152 |           0.2602 |
[32m[20221213 18:06:02 @agent_ppo2.py:185][0m |          -0.0135 |          33.1418 |           0.2604 |
[32m[20221213 18:06:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:06:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.19
[32m[20221213 18:06:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.29
[32m[20221213 18:06:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.58
[32m[20221213 18:06:02 @agent_ppo2.py:143][0m Total time:       1.87 min
[32m[20221213 18:06:02 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 18:06:02 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 18:06:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:06:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:02 @agent_ppo2.py:185][0m |          -0.0039 |          42.2384 |           0.2595 |
[32m[20221213 18:06:02 @agent_ppo2.py:185][0m |          -0.0039 |          39.9448 |           0.2591 |
[32m[20221213 18:06:02 @agent_ppo2.py:185][0m |          -0.0099 |          39.2134 |           0.2590 |
[32m[20221213 18:06:02 @agent_ppo2.py:185][0m |          -0.0119 |          38.6639 |           0.2588 |
[32m[20221213 18:06:02 @agent_ppo2.py:185][0m |          -0.0115 |          38.1540 |           0.2587 |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |          -0.0120 |          37.5311 |           0.2585 |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |          -0.0123 |          37.3486 |           0.2585 |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |          -0.0151 |          36.8982 |           0.2584 |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |          -0.0120 |          36.5933 |           0.2582 |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |          -0.0127 |          36.1104 |           0.2581 |
[32m[20221213 18:06:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.39
[32m[20221213 18:06:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.10
[32m[20221213 18:06:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.13
[32m[20221213 18:06:03 @agent_ppo2.py:143][0m Total time:       1.89 min
[32m[20221213 18:06:03 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 18:06:03 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 18:06:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |           0.0083 |          40.9928 |           0.2570 |
[32m[20221213 18:06:03 @agent_ppo2.py:185][0m |          -0.0052 |          37.9716 |           0.2573 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0065 |          37.1150 |           0.2573 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0065 |          36.6293 |           0.2574 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0090 |          36.0511 |           0.2574 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0090 |          35.4548 |           0.2575 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0127 |          35.0562 |           0.2578 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0130 |          34.9502 |           0.2579 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0140 |          34.5837 |           0.2581 |
[32m[20221213 18:06:04 @agent_ppo2.py:185][0m |          -0.0086 |          34.4436 |           0.2582 |
[32m[20221213 18:06:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.35
[32m[20221213 18:06:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.93
[32m[20221213 18:06:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.23
[32m[20221213 18:06:04 @agent_ppo2.py:143][0m Total time:       1.91 min
[32m[20221213 18:06:04 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 18:06:04 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 18:06:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0028 |          43.8298 |           0.2648 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0080 |          41.2176 |           0.2645 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0094 |          40.5556 |           0.2643 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0088 |          39.3014 |           0.2640 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0120 |          38.7607 |           0.2639 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0132 |          38.2170 |           0.2637 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0150 |          37.5277 |           0.2637 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0159 |          37.0555 |           0.2636 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0160 |          36.6865 |           0.2635 |
[32m[20221213 18:06:05 @agent_ppo2.py:185][0m |          -0.0153 |          36.1074 |           0.2636 |
[32m[20221213 18:06:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:06:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.44
[32m[20221213 18:06:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.23
[32m[20221213 18:06:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.05
[32m[20221213 18:06:06 @agent_ppo2.py:143][0m Total time:       1.93 min
[32m[20221213 18:06:06 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 18:06:06 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 18:06:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:06 @agent_ppo2.py:185][0m |           0.0096 |          45.8583 |           0.2662 |
[32m[20221213 18:06:06 @agent_ppo2.py:185][0m |          -0.0031 |          40.2273 |           0.2659 |
[32m[20221213 18:06:06 @agent_ppo2.py:185][0m |          -0.0092 |          38.6371 |           0.2658 |
[32m[20221213 18:06:06 @agent_ppo2.py:185][0m |          -0.0128 |          38.0128 |           0.2659 |
[32m[20221213 18:06:06 @agent_ppo2.py:185][0m |          -0.0124 |          37.2147 |           0.2661 |
[32m[20221213 18:06:06 @agent_ppo2.py:185][0m |          -0.0123 |          37.0153 |           0.2661 |
[32m[20221213 18:06:07 @agent_ppo2.py:185][0m |          -0.0150 |          36.2113 |           0.2663 |
[32m[20221213 18:06:07 @agent_ppo2.py:185][0m |          -0.0021 |          38.7236 |           0.2663 |
[32m[20221213 18:06:07 @agent_ppo2.py:185][0m |          -0.0169 |          35.2361 |           0.2664 |
[32m[20221213 18:06:07 @agent_ppo2.py:185][0m |          -0.0104 |          35.3992 |           0.2665 |
[32m[20221213 18:06:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:06:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.23
[32m[20221213 18:06:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.75
[32m[20221213 18:06:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.98
[32m[20221213 18:06:07 @agent_ppo2.py:143][0m Total time:       1.95 min
[32m[20221213 18:06:07 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 18:06:07 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 18:06:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:07 @agent_ppo2.py:185][0m |          -0.0014 |          42.6583 |           0.2734 |
[32m[20221213 18:06:07 @agent_ppo2.py:185][0m |          -0.0034 |          40.7441 |           0.2734 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0055 |          39.3554 |           0.2734 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |           0.0067 |          43.0227 |           0.2731 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0125 |          37.4731 |           0.2731 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0102 |          36.2914 |           0.2733 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0128 |          35.4105 |           0.2732 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0117 |          34.7581 |           0.2732 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0124 |          33.9442 |           0.2733 |
[32m[20221213 18:06:08 @agent_ppo2.py:185][0m |          -0.0138 |          33.1965 |           0.2735 |
[32m[20221213 18:06:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:06:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.27
[32m[20221213 18:06:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.45
[32m[20221213 18:06:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.64
[32m[20221213 18:06:08 @agent_ppo2.py:143][0m Total time:       1.98 min
[32m[20221213 18:06:08 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 18:06:08 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 18:06:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:06:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |           0.0058 |          41.9602 |           0.2698 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |          -0.0060 |          39.4196 |           0.2695 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |          -0.0072 |          38.5220 |           0.2696 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |          -0.0084 |          37.9371 |           0.2695 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |          -0.0102 |          37.3177 |           0.2697 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |           0.0025 |          41.6495 |           0.2700 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |          -0.0132 |          36.9165 |           0.2700 |
[32m[20221213 18:06:09 @agent_ppo2.py:185][0m |          -0.0122 |          36.4240 |           0.2701 |
[32m[20221213 18:06:10 @agent_ppo2.py:185][0m |          -0.0127 |          36.1972 |           0.2705 |
[32m[20221213 18:06:10 @agent_ppo2.py:185][0m |          -0.0122 |          35.7230 |           0.2705 |
[32m[20221213 18:06:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:06:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.50
[32m[20221213 18:06:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.57
[32m[20221213 18:06:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.84
[32m[20221213 18:06:10 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 512.84
[32m[20221213 18:06:10 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 512.84
[32m[20221213 18:06:10 @agent_ppo2.py:143][0m Total time:       2.00 min
[32m[20221213 18:06:10 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 18:06:10 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 18:06:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:10 @agent_ppo2.py:185][0m |          -0.0014 |          40.6870 |           0.2709 |
[32m[20221213 18:06:10 @agent_ppo2.py:185][0m |          -0.0060 |          39.1603 |           0.2706 |
[32m[20221213 18:06:10 @agent_ppo2.py:185][0m |          -0.0075 |          38.3938 |           0.2704 |
[32m[20221213 18:06:10 @agent_ppo2.py:185][0m |          -0.0073 |          37.7651 |           0.2703 |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0032 |          38.7350 |           0.2702 |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0115 |          36.5740 |           0.2702 |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0057 |          37.8444 |           0.2704 |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0147 |          35.8730 |           0.2704 |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0029 |          40.4186 |           0.2704 |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0134 |          34.9539 |           0.2702 |
[32m[20221213 18:06:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:06:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.18
[32m[20221213 18:06:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.50
[32m[20221213 18:06:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.07
[32m[20221213 18:06:11 @agent_ppo2.py:143][0m Total time:       2.02 min
[32m[20221213 18:06:11 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 18:06:11 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 18:06:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:11 @agent_ppo2.py:185][0m |          -0.0006 |          44.9792 |           0.2745 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0027 |          42.8924 |           0.2746 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0090 |          41.1665 |           0.2744 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0118 |          40.1190 |           0.2743 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0110 |          38.7773 |           0.2743 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0103 |          37.8504 |           0.2744 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0068 |          38.0503 |           0.2744 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0111 |          36.4290 |           0.2742 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0056 |          36.4477 |           0.2742 |
[32m[20221213 18:06:12 @agent_ppo2.py:185][0m |          -0.0138 |          35.0147 |           0.2743 |
[32m[20221213 18:06:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:06:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.15
[32m[20221213 18:06:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.83
[32m[20221213 18:06:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.32
[32m[20221213 18:06:12 @agent_ppo2.py:143][0m Total time:       2.04 min
[32m[20221213 18:06:12 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 18:06:12 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 18:06:13 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:06:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |           0.0003 |          41.3707 |           0.2764 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0076 |          37.8261 |           0.2763 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0091 |          36.5457 |           0.2762 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0112 |          35.1669 |           0.2762 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0119 |          34.4165 |           0.2764 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0126 |          33.4529 |           0.2764 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0022 |          35.5361 |           0.2764 |
[32m[20221213 18:06:13 @agent_ppo2.py:185][0m |          -0.0141 |          32.2938 |           0.2767 |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |          -0.0172 |          31.3713 |           0.2766 |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |          -0.0143 |          30.9398 |           0.2766 |
[32m[20221213 18:06:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:06:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.78
[32m[20221213 18:06:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.20
[32m[20221213 18:06:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.98
[32m[20221213 18:06:14 @agent_ppo2.py:143][0m Total time:       2.07 min
[32m[20221213 18:06:14 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 18:06:14 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 18:06:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |           0.0076 |          44.5735 |           0.2709 |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |           0.0030 |          40.8284 |           0.2704 |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |          -0.0061 |          37.0447 |           0.2704 |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |          -0.0083 |          35.9846 |           0.2701 |
[32m[20221213 18:06:14 @agent_ppo2.py:185][0m |          -0.0133 |          35.0487 |           0.2700 |
[32m[20221213 18:06:15 @agent_ppo2.py:185][0m |          -0.0091 |          34.8176 |           0.2700 |
[32m[20221213 18:06:15 @agent_ppo2.py:185][0m |          -0.0159 |          33.9582 |           0.2699 |
[32m[20221213 18:06:15 @agent_ppo2.py:185][0m |          -0.0092 |          33.8403 |           0.2699 |
[32m[20221213 18:06:15 @agent_ppo2.py:185][0m |          -0.0092 |          34.5551 |           0.2698 |
[32m[20221213 18:06:15 @agent_ppo2.py:185][0m |          -0.0114 |          32.6460 |           0.2697 |
[32m[20221213 18:06:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:06:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.94
[32m[20221213 18:06:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.52
[32m[20221213 18:06:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 180.51
[32m[20221213 18:06:15 @agent_ppo2.py:143][0m Total time:       2.09 min
[32m[20221213 18:06:15 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 18:06:15 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 18:06:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:15 @agent_ppo2.py:185][0m |          -0.0043 |          42.1265 |           0.2788 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0048 |          38.7326 |           0.2787 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0063 |          37.4087 |           0.2786 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0108 |          36.5213 |           0.2785 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0151 |          35.7364 |           0.2785 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0109 |          35.2088 |           0.2786 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0051 |          39.4744 |           0.2785 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0132 |          34.3877 |           0.2787 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0128 |          33.9348 |           0.2785 |
[32m[20221213 18:06:16 @agent_ppo2.py:185][0m |          -0.0170 |          33.6228 |           0.2786 |
[32m[20221213 18:06:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:06:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.28
[32m[20221213 18:06:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.51
[32m[20221213 18:06:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.51
[32m[20221213 18:06:16 @agent_ppo2.py:143][0m Total time:       2.11 min
[32m[20221213 18:06:16 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 18:06:16 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 18:06:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0036 |          43.7383 |           0.2791 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0057 |          41.9497 |           0.2786 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0081 |          41.0985 |           0.2782 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0096 |          40.5015 |           0.2781 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0067 |          39.8947 |           0.2781 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0026 |          39.9863 |           0.2781 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0114 |          39.2840 |           0.2782 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |           0.0003 |          40.2895 |           0.2783 |
[32m[20221213 18:06:17 @agent_ppo2.py:185][0m |          -0.0137 |          38.5946 |           0.2784 |
[32m[20221213 18:06:18 @agent_ppo2.py:185][0m |          -0.0122 |          38.2707 |           0.2782 |
[32m[20221213 18:06:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:06:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.18
[32m[20221213 18:06:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.88
[32m[20221213 18:06:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.77
[32m[20221213 18:06:18 @agent_ppo2.py:143][0m Total time:       2.13 min
[32m[20221213 18:06:18 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 18:06:18 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 18:06:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:18 @agent_ppo2.py:185][0m |          -0.0013 |          43.1925 |           0.2832 |
[32m[20221213 18:06:18 @agent_ppo2.py:185][0m |          -0.0087 |          40.0377 |           0.2829 |
[32m[20221213 18:06:18 @agent_ppo2.py:185][0m |          -0.0120 |          38.7619 |           0.2826 |
[32m[20221213 18:06:18 @agent_ppo2.py:185][0m |          -0.0100 |          37.9482 |           0.2825 |
[32m[20221213 18:06:18 @agent_ppo2.py:185][0m |          -0.0097 |          37.4790 |           0.2823 |
[32m[20221213 18:06:19 @agent_ppo2.py:185][0m |          -0.0118 |          36.6632 |           0.2823 |
[32m[20221213 18:06:19 @agent_ppo2.py:185][0m |          -0.0150 |          35.8487 |           0.2822 |
[32m[20221213 18:06:19 @agent_ppo2.py:185][0m |          -0.0143 |          35.2495 |           0.2821 |
[32m[20221213 18:06:19 @agent_ppo2.py:185][0m |          -0.0116 |          34.7456 |           0.2819 |
[32m[20221213 18:06:19 @agent_ppo2.py:185][0m |          -0.0139 |          34.2503 |           0.2818 |
[32m[20221213 18:06:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:06:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.71
[32m[20221213 18:06:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.15
[32m[20221213 18:06:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 203.99
[32m[20221213 18:06:19 @agent_ppo2.py:143][0m Total time:       2.15 min
[32m[20221213 18:06:19 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 18:06:19 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 18:06:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:19 @agent_ppo2.py:185][0m |          -0.0040 |          44.3899 |           0.2799 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |           0.0013 |          42.5507 |           0.2796 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0052 |          39.9093 |           0.2794 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0105 |          38.6090 |           0.2793 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0119 |          37.4537 |           0.2795 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0085 |          36.5806 |           0.2793 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0107 |          35.7137 |           0.2791 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0154 |          34.7521 |           0.2795 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0132 |          34.0511 |           0.2794 |
[32m[20221213 18:06:20 @agent_ppo2.py:185][0m |          -0.0144 |          33.3172 |           0.2794 |
[32m[20221213 18:06:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:06:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.45
[32m[20221213 18:06:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.74
[32m[20221213 18:06:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.25
[32m[20221213 18:06:20 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 18:06:20 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 18:06:20 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 18:06:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0042 |          44.8955 |           0.2843 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |           0.0043 |          44.6596 |           0.2840 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |           0.0145 |          49.8783 |           0.2838 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0061 |          41.8088 |           0.2837 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0094 |          41.1624 |           0.2835 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0135 |          40.7217 |           0.2838 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0061 |          40.4656 |           0.2837 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0140 |          40.1786 |           0.2838 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0081 |          39.8227 |           0.2839 |
[32m[20221213 18:06:21 @agent_ppo2.py:185][0m |          -0.0136 |          39.3814 |           0.2840 |
[32m[20221213 18:06:21 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:06:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.85
[32m[20221213 18:06:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.39
[32m[20221213 18:06:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.62
[32m[20221213 18:06:22 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 18:06:22 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 18:06:22 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 18:06:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:22 @agent_ppo2.py:185][0m |          -0.0006 |          44.9993 |           0.2875 |
[32m[20221213 18:06:22 @agent_ppo2.py:185][0m |          -0.0010 |          43.2872 |           0.2869 |
[32m[20221213 18:06:22 @agent_ppo2.py:185][0m |          -0.0081 |          41.6075 |           0.2865 |
[32m[20221213 18:06:22 @agent_ppo2.py:185][0m |          -0.0085 |          41.0606 |           0.2866 |
[32m[20221213 18:06:22 @agent_ppo2.py:185][0m |          -0.0066 |          39.9997 |           0.2865 |
[32m[20221213 18:06:22 @agent_ppo2.py:185][0m |          -0.0125 |          38.4946 |           0.2865 |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0011 |          40.9244 |           0.2865 |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0117 |          37.0311 |           0.2865 |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0135 |          36.1241 |           0.2866 |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0107 |          35.4721 |           0.2865 |
[32m[20221213 18:06:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:06:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.00
[32m[20221213 18:06:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.39
[32m[20221213 18:06:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.33
[32m[20221213 18:06:23 @agent_ppo2.py:143][0m Total time:       2.22 min
[32m[20221213 18:06:23 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 18:06:23 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 18:06:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0028 |          45.6396 |           0.2845 |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0059 |          42.7684 |           0.2842 |
[32m[20221213 18:06:23 @agent_ppo2.py:185][0m |          -0.0084 |          41.5021 |           0.2840 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0031 |          41.4176 |           0.2837 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0096 |          39.6063 |           0.2837 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0080 |          38.8252 |           0.2839 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0134 |          38.5516 |           0.2838 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0096 |          38.0467 |           0.2838 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0110 |          37.3981 |           0.2838 |
[32m[20221213 18:06:24 @agent_ppo2.py:185][0m |          -0.0155 |          36.8492 |           0.2838 |
[32m[20221213 18:06:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:06:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.74
[32m[20221213 18:06:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.24
[32m[20221213 18:06:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.21
[32m[20221213 18:06:24 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 521.21
[32m[20221213 18:06:24 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 521.21
[32m[20221213 18:06:24 @agent_ppo2.py:143][0m Total time:       2.24 min
[32m[20221213 18:06:24 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 18:06:24 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 18:06:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:06:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0010 |          44.5377 |           0.2732 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0033 |          42.0180 |           0.2730 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0049 |          39.8121 |           0.2728 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0150 |          38.7513 |           0.2727 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0151 |          37.1708 |           0.2726 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0104 |          36.4793 |           0.2725 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0197 |          35.4744 |           0.2724 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0169 |          34.7626 |           0.2723 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0110 |          34.3180 |           0.2724 |
[32m[20221213 18:06:25 @agent_ppo2.py:185][0m |          -0.0008 |          36.5642 |           0.2724 |
[32m[20221213 18:06:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.59
[32m[20221213 18:06:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.57
[32m[20221213 18:06:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.61
[32m[20221213 18:06:26 @agent_ppo2.py:143][0m Total time:       2.26 min
[32m[20221213 18:06:26 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 18:06:26 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 18:06:26 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:06:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |           0.0018 |          47.5724 |           0.2902 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |          -0.0013 |          44.7183 |           0.2903 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |           0.0017 |          44.4282 |           0.2902 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |          -0.0077 |          42.3989 |           0.2901 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |           0.0055 |          43.5498 |           0.2900 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |          -0.0104 |          40.7596 |           0.2899 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |          -0.0112 |          40.0685 |           0.2897 |
[32m[20221213 18:06:26 @agent_ppo2.py:185][0m |          -0.0103 |          39.5178 |           0.2897 |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0115 |          38.9956 |           0.2896 |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0094 |          38.5047 |           0.2895 |
[32m[20221213 18:06:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:06:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.56
[32m[20221213 18:06:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.58
[32m[20221213 18:06:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.92
[32m[20221213 18:06:27 @agent_ppo2.py:143][0m Total time:       2.28 min
[32m[20221213 18:06:27 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 18:06:27 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 18:06:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0017 |          49.3408 |           0.2916 |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0049 |          47.8421 |           0.2917 |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0099 |          47.0935 |           0.2915 |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0024 |          49.0905 |           0.2913 |
[32m[20221213 18:06:27 @agent_ppo2.py:185][0m |          -0.0090 |          45.9517 |           0.2912 |
[32m[20221213 18:06:28 @agent_ppo2.py:185][0m |          -0.0107 |          45.4715 |           0.2911 |
[32m[20221213 18:06:28 @agent_ppo2.py:185][0m |          -0.0092 |          45.0817 |           0.2911 |
[32m[20221213 18:06:28 @agent_ppo2.py:185][0m |           0.0010 |          48.0262 |           0.2911 |
[32m[20221213 18:06:28 @agent_ppo2.py:185][0m |          -0.0031 |          48.3909 |           0.2911 |
[32m[20221213 18:06:28 @agent_ppo2.py:185][0m |          -0.0141 |          44.3097 |           0.2909 |
[32m[20221213 18:06:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:06:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.85
[32m[20221213 18:06:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.57
[32m[20221213 18:06:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.05
[32m[20221213 18:06:28 @agent_ppo2.py:143][0m Total time:       2.31 min
[32m[20221213 18:06:28 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 18:06:28 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 18:06:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:28 @agent_ppo2.py:185][0m |          -0.0001 |          51.0042 |           0.2877 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0070 |          49.5250 |           0.2876 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0066 |          48.7216 |           0.2876 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0118 |          48.1656 |           0.2877 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0117 |          47.7249 |           0.2879 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0115 |          47.2788 |           0.2881 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0149 |          46.9953 |           0.2883 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0118 |          46.6728 |           0.2885 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0116 |          46.4476 |           0.2885 |
[32m[20221213 18:06:29 @agent_ppo2.py:185][0m |          -0.0150 |          46.2716 |           0.2890 |
[32m[20221213 18:06:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.46
[32m[20221213 18:06:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.61
[32m[20221213 18:06:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.41
[32m[20221213 18:06:29 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 537.41
[32m[20221213 18:06:29 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 537.41
[32m[20221213 18:06:29 @agent_ppo2.py:143][0m Total time:       2.33 min
[32m[20221213 18:06:29 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 18:06:29 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 18:06:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:06:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0041 |          48.0174 |           0.2922 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0086 |          46.4604 |           0.2918 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0087 |          45.6357 |           0.2914 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0135 |          44.9617 |           0.2915 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0105 |          44.5313 |           0.2915 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0116 |          43.9828 |           0.2915 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0049 |          44.9424 |           0.2917 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0065 |          44.3025 |           0.2919 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0137 |          42.8362 |           0.2918 |
[32m[20221213 18:06:30 @agent_ppo2.py:185][0m |          -0.0080 |          43.7065 |           0.2920 |
[32m[20221213 18:06:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:06:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.02
[32m[20221213 18:06:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.92
[32m[20221213 18:06:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.57
[32m[20221213 18:06:31 @agent_ppo2.py:143][0m Total time:       2.35 min
[32m[20221213 18:06:31 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 18:06:31 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 18:06:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:31 @agent_ppo2.py:185][0m |          -0.0030 |          46.4739 |           0.2959 |
[32m[20221213 18:06:31 @agent_ppo2.py:185][0m |          -0.0006 |          46.7444 |           0.2958 |
[32m[20221213 18:06:31 @agent_ppo2.py:185][0m |          -0.0067 |          43.9852 |           0.2953 |
[32m[20221213 18:06:31 @agent_ppo2.py:185][0m |          -0.0071 |          42.9053 |           0.2952 |
[32m[20221213 18:06:31 @agent_ppo2.py:185][0m |          -0.0113 |          41.6645 |           0.2954 |
[32m[20221213 18:06:31 @agent_ppo2.py:185][0m |          -0.0084 |          41.0574 |           0.2956 |
[32m[20221213 18:06:32 @agent_ppo2.py:185][0m |          -0.0008 |          42.1316 |           0.2954 |
[32m[20221213 18:06:32 @agent_ppo2.py:185][0m |          -0.0139 |          38.8626 |           0.2955 |
[32m[20221213 18:06:32 @agent_ppo2.py:185][0m |          -0.0099 |          38.1744 |           0.2955 |
[32m[20221213 18:06:32 @agent_ppo2.py:185][0m |          -0.0118 |          37.6365 |           0.2955 |
[32m[20221213 18:06:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:06:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.79
[32m[20221213 18:06:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.10
[32m[20221213 18:06:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.46
[32m[20221213 18:06:32 @agent_ppo2.py:143][0m Total time:       2.37 min
[32m[20221213 18:06:32 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 18:06:32 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 18:06:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:32 @agent_ppo2.py:185][0m |          -0.0028 |          49.1559 |           0.2963 |
[32m[20221213 18:06:32 @agent_ppo2.py:185][0m |          -0.0105 |          46.7032 |           0.2959 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0077 |          45.6082 |           0.2953 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0100 |          45.0251 |           0.2953 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0115 |          44.0201 |           0.2950 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0130 |          43.4505 |           0.2950 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0134 |          42.8302 |           0.2947 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0158 |          42.6065 |           0.2946 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0144 |          41.7318 |           0.2944 |
[32m[20221213 18:06:33 @agent_ppo2.py:185][0m |          -0.0048 |          42.8759 |           0.2943 |
[32m[20221213 18:06:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.44
[32m[20221213 18:06:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.64
[32m[20221213 18:06:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.00
[32m[20221213 18:06:33 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 18:06:33 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 18:06:33 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 18:06:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |           0.0008 |          47.8675 |           0.2915 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0075 |          45.5017 |           0.2910 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0097 |          44.2639 |           0.2911 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0134 |          43.1052 |           0.2912 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0064 |          43.2265 |           0.2907 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0064 |          43.1902 |           0.2907 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0137 |          40.7520 |           0.2905 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0022 |          43.8826 |           0.2903 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0169 |          39.3810 |           0.2903 |
[32m[20221213 18:06:34 @agent_ppo2.py:185][0m |          -0.0168 |          38.7032 |           0.2901 |
[32m[20221213 18:06:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.71
[32m[20221213 18:06:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.15
[32m[20221213 18:06:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.03
[32m[20221213 18:06:35 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 18:06:35 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 18:06:35 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 18:06:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:06:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |           0.0013 |          52.6162 |           0.3014 |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |           0.0017 |          52.2312 |           0.3006 |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |          -0.0060 |          49.7417 |           0.3000 |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |          -0.0081 |          49.2165 |           0.2999 |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |          -0.0100 |          48.6162 |           0.2995 |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |          -0.0116 |          48.1606 |           0.2991 |
[32m[20221213 18:06:35 @agent_ppo2.py:185][0m |          -0.0098 |          47.5920 |           0.2990 |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0075 |          47.1733 |           0.2986 |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0091 |          46.7666 |           0.2986 |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0121 |          45.9901 |           0.2983 |
[32m[20221213 18:06:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:06:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.47
[32m[20221213 18:06:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.35
[32m[20221213 18:06:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.85
[32m[20221213 18:06:36 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 18:06:36 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 18:06:36 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 18:06:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0027 |          49.0197 |           0.2908 |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0011 |          47.2645 |           0.2908 |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0072 |          45.7115 |           0.2909 |
[32m[20221213 18:06:36 @agent_ppo2.py:185][0m |          -0.0095 |          45.0672 |           0.2909 |
[32m[20221213 18:06:37 @agent_ppo2.py:185][0m |          -0.0074 |          44.4858 |           0.2908 |
[32m[20221213 18:06:37 @agent_ppo2.py:185][0m |          -0.0075 |          45.0578 |           0.2910 |
[32m[20221213 18:06:37 @agent_ppo2.py:185][0m |          -0.0132 |          43.7735 |           0.2912 |
[32m[20221213 18:06:37 @agent_ppo2.py:185][0m |          -0.0129 |          43.3892 |           0.2913 |
[32m[20221213 18:06:37 @agent_ppo2.py:185][0m |          -0.0165 |          43.0553 |           0.2916 |
[32m[20221213 18:06:37 @agent_ppo2.py:185][0m |          -0.0145 |          42.8795 |           0.2917 |
[32m[20221213 18:06:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:06:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.39
[32m[20221213 18:06:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.33
[32m[20221213 18:06:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.79
[32m[20221213 18:06:37 @agent_ppo2.py:143][0m Total time:       2.46 min
[32m[20221213 18:06:37 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 18:06:37 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 18:06:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0011 |          49.3758 |           0.2939 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0055 |          46.5957 |           0.2939 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0043 |          45.1509 |           0.2937 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0115 |          42.6685 |           0.2934 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0148 |          42.0119 |           0.2932 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0131 |          40.4431 |           0.2932 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0059 |          39.8890 |           0.2931 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0145 |          38.6337 |           0.2931 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0127 |          37.8737 |           0.2930 |
[32m[20221213 18:06:38 @agent_ppo2.py:185][0m |          -0.0150 |          37.0498 |           0.2930 |
[32m[20221213 18:06:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:06:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.43
[32m[20221213 18:06:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.08
[32m[20221213 18:06:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.27
[32m[20221213 18:06:38 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221213 18:06:38 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 18:06:39 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 18:06:39 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:06:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0034 |          51.4196 |           0.2947 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0040 |          48.7354 |           0.2940 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0110 |          47.2737 |           0.2938 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0050 |          47.1846 |           0.2935 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0115 |          45.4735 |           0.2932 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0099 |          44.9156 |           0.2929 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0138 |          44.4011 |           0.2927 |
[32m[20221213 18:06:39 @agent_ppo2.py:185][0m |          -0.0019 |          48.2621 |           0.2926 |
[32m[20221213 18:06:40 @agent_ppo2.py:185][0m |          -0.0172 |          43.2178 |           0.2920 |
[32m[20221213 18:06:40 @agent_ppo2.py:185][0m |          -0.0148 |          42.5804 |           0.2920 |
[32m[20221213 18:06:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:06:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.31
[32m[20221213 18:06:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.14
[32m[20221213 18:06:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.42
[32m[20221213 18:06:40 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221213 18:06:40 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 18:06:40 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 18:06:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:40 @agent_ppo2.py:185][0m |          -0.0033 |          53.8586 |           0.2833 |
[32m[20221213 18:06:40 @agent_ppo2.py:185][0m |          -0.0100 |          51.1492 |           0.2832 |
[32m[20221213 18:06:40 @agent_ppo2.py:185][0m |          -0.0102 |          50.1097 |           0.2831 |
[32m[20221213 18:06:40 @agent_ppo2.py:185][0m |          -0.0131 |          49.0958 |           0.2831 |
[32m[20221213 18:06:41 @agent_ppo2.py:185][0m |          -0.0024 |          52.1900 |           0.2831 |
[32m[20221213 18:06:41 @agent_ppo2.py:185][0m |          -0.0144 |          47.7825 |           0.2831 |
[32m[20221213 18:06:41 @agent_ppo2.py:185][0m |          -0.0151 |          47.1280 |           0.2832 |
[32m[20221213 18:06:41 @agent_ppo2.py:185][0m |          -0.0137 |          46.6496 |           0.2833 |
[32m[20221213 18:06:41 @agent_ppo2.py:185][0m |          -0.0149 |          46.2974 |           0.2834 |
[32m[20221213 18:06:41 @agent_ppo2.py:185][0m |          -0.0166 |          45.7188 |           0.2835 |
[32m[20221213 18:06:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:06:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.44
[32m[20221213 18:06:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.22
[32m[20221213 18:06:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.88
[32m[20221213 18:06:41 @agent_ppo2.py:143][0m Total time:       2.52 min
[32m[20221213 18:06:41 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 18:06:41 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 18:06:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0003 |          56.2682 |           0.2966 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0062 |          54.0712 |           0.2962 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0053 |          53.0812 |           0.2958 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0062 |          52.2302 |           0.2958 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0059 |          52.9066 |           0.2956 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0107 |          51.1025 |           0.2953 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0129 |          50.8031 |           0.2954 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0117 |          50.5135 |           0.2950 |
[32m[20221213 18:06:42 @agent_ppo2.py:185][0m |          -0.0130 |          50.1231 |           0.2952 |
[32m[20221213 18:06:43 @agent_ppo2.py:185][0m |          -0.0129 |          50.1722 |           0.2949 |
[32m[20221213 18:06:43 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 18:06:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.83
[32m[20221213 18:06:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.62
[32m[20221213 18:06:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.56
[32m[20221213 18:06:43 @agent_ppo2.py:143][0m Total time:       2.55 min
[32m[20221213 18:06:43 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 18:06:43 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 18:06:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:43 @agent_ppo2.py:185][0m |          -0.0024 |          60.6478 |           0.2867 |
[32m[20221213 18:06:43 @agent_ppo2.py:185][0m |          -0.0030 |          58.9442 |           0.2863 |
[32m[20221213 18:06:43 @agent_ppo2.py:185][0m |          -0.0097 |          57.8773 |           0.2862 |
[32m[20221213 18:06:43 @agent_ppo2.py:185][0m |          -0.0143 |          56.9034 |           0.2864 |
[32m[20221213 18:06:43 @agent_ppo2.py:185][0m |          -0.0121 |          56.0188 |           0.2862 |
[32m[20221213 18:06:44 @agent_ppo2.py:185][0m |          -0.0127 |          55.4071 |           0.2865 |
[32m[20221213 18:06:44 @agent_ppo2.py:185][0m |          -0.0127 |          54.5586 |           0.2864 |
[32m[20221213 18:06:44 @agent_ppo2.py:185][0m |          -0.0159 |          54.0711 |           0.2867 |
[32m[20221213 18:06:44 @agent_ppo2.py:185][0m |          -0.0160 |          53.8076 |           0.2868 |
[32m[20221213 18:06:44 @agent_ppo2.py:185][0m |          -0.0183 |          52.9675 |           0.2871 |
[32m[20221213 18:06:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:06:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.42
[32m[20221213 18:06:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.79
[32m[20221213 18:06:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.14
[32m[20221213 18:06:44 @agent_ppo2.py:143][0m Total time:       2.57 min
[32m[20221213 18:06:44 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 18:06:44 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 18:06:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:44 @agent_ppo2.py:185][0m |          -0.0012 |          57.2081 |           0.2882 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0114 |          54.0440 |           0.2876 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0117 |          52.7493 |           0.2874 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0133 |          51.7004 |           0.2876 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0165 |          50.6746 |           0.2875 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0068 |          49.9947 |           0.2874 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0044 |          52.6063 |           0.2875 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0126 |          48.5713 |           0.2874 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0164 |          47.8796 |           0.2875 |
[32m[20221213 18:06:45 @agent_ppo2.py:185][0m |          -0.0178 |          47.4261 |           0.2875 |
[32m[20221213 18:06:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:06:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.73
[32m[20221213 18:06:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.20
[32m[20221213 18:06:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.79
[32m[20221213 18:06:45 @agent_ppo2.py:143][0m Total time:       2.60 min
[32m[20221213 18:06:45 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 18:06:45 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 18:06:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:06:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0047 |          57.2266 |           0.2879 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0051 |          54.7707 |           0.2882 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0088 |          53.4544 |           0.2881 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0060 |          52.6357 |           0.2883 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0123 |          51.7754 |           0.2883 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0114 |          50.9730 |           0.2884 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0013 |          53.2753 |           0.2884 |
[32m[20221213 18:06:46 @agent_ppo2.py:185][0m |          -0.0116 |          49.7752 |           0.2884 |
[32m[20221213 18:06:47 @agent_ppo2.py:185][0m |          -0.0137 |          49.4576 |           0.2883 |
[32m[20221213 18:06:47 @agent_ppo2.py:185][0m |          -0.0109 |          50.2603 |           0.2884 |
[32m[20221213 18:06:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:06:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.02
[32m[20221213 18:06:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.61
[32m[20221213 18:06:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.75
[32m[20221213 18:06:47 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221213 18:06:47 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 18:06:47 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 18:06:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:47 @agent_ppo2.py:185][0m |          -0.0023 |          55.3875 |           0.2923 |
[32m[20221213 18:06:47 @agent_ppo2.py:185][0m |           0.0070 |          54.6899 |           0.2922 |
[32m[20221213 18:06:47 @agent_ppo2.py:185][0m |          -0.0065 |          50.1432 |           0.2923 |
[32m[20221213 18:06:47 @agent_ppo2.py:185][0m |           0.0023 |          53.9839 |           0.2921 |
[32m[20221213 18:06:48 @agent_ppo2.py:185][0m |          -0.0091 |          47.8950 |           0.2915 |
[32m[20221213 18:06:48 @agent_ppo2.py:185][0m |          -0.0068 |          46.3140 |           0.2920 |
[32m[20221213 18:06:48 @agent_ppo2.py:185][0m |          -0.0001 |          50.6775 |           0.2920 |
[32m[20221213 18:06:48 @agent_ppo2.py:185][0m |          -0.0138 |          44.7757 |           0.2920 |
[32m[20221213 18:06:48 @agent_ppo2.py:185][0m |          -0.0137 |          44.0206 |           0.2919 |
[32m[20221213 18:06:48 @agent_ppo2.py:185][0m |          -0.0180 |          43.3968 |           0.2918 |
[32m[20221213 18:06:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 18:06:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.88
[32m[20221213 18:06:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.82
[32m[20221213 18:06:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.74
[32m[20221213 18:06:48 @agent_ppo2.py:143][0m Total time:       2.64 min
[32m[20221213 18:06:48 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 18:06:48 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 18:06:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:06:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0017 |          62.6250 |           0.2934 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0111 |          60.2787 |           0.2930 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0082 |          59.6180 |           0.2924 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0087 |          57.9298 |           0.2919 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0075 |          57.8300 |           0.2919 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0089 |          56.6230 |           0.2918 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0057 |          57.5185 |           0.2917 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0099 |          55.3640 |           0.2912 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0107 |          54.7268 |           0.2915 |
[32m[20221213 18:06:49 @agent_ppo2.py:185][0m |          -0.0145 |          54.5031 |           0.2912 |
[32m[20221213 18:06:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:06:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.09
[32m[20221213 18:06:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.77
[32m[20221213 18:06:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.73
[32m[20221213 18:06:50 @agent_ppo2.py:143][0m Total time:       2.66 min
[32m[20221213 18:06:50 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 18:06:50 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 18:06:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:06:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:50 @agent_ppo2.py:185][0m |           0.0041 |          65.0854 |           0.2878 |
[32m[20221213 18:06:50 @agent_ppo2.py:185][0m |           0.0040 |          63.5334 |           0.2877 |
[32m[20221213 18:06:50 @agent_ppo2.py:185][0m |          -0.0004 |          60.7258 |           0.2876 |
[32m[20221213 18:06:50 @agent_ppo2.py:185][0m |          -0.0109 |          57.0808 |           0.2873 |
[32m[20221213 18:06:50 @agent_ppo2.py:185][0m |          -0.0058 |          55.9805 |           0.2875 |
[32m[20221213 18:06:50 @agent_ppo2.py:185][0m |          -0.0100 |          55.2507 |           0.2875 |
[32m[20221213 18:06:51 @agent_ppo2.py:185][0m |          -0.0172 |          54.2426 |           0.2877 |
[32m[20221213 18:06:51 @agent_ppo2.py:185][0m |          -0.0151 |          53.3637 |           0.2877 |
[32m[20221213 18:06:51 @agent_ppo2.py:185][0m |          -0.0152 |          52.7884 |           0.2875 |
[32m[20221213 18:06:51 @agent_ppo2.py:185][0m |          -0.0149 |          52.0793 |           0.2877 |
[32m[20221213 18:06:51 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 18:06:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.55
[32m[20221213 18:06:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.85
[32m[20221213 18:06:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.10
[32m[20221213 18:06:51 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 570.10
[32m[20221213 18:06:51 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 570.10
[32m[20221213 18:06:51 @agent_ppo2.py:143][0m Total time:       2.69 min
[32m[20221213 18:06:51 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 18:06:51 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 18:06:51 @agent_ppo2.py:127][0m Sampling time: 0.28 s by 5 slaves
[32m[20221213 18:06:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0051 |          62.2601 |           0.2958 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0044 |          57.7608 |           0.2955 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0058 |          55.7618 |           0.2953 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0090 |          54.2510 |           0.2954 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0098 |          52.8178 |           0.2955 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0144 |          52.0238 |           0.2956 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0040 |          54.6282 |           0.2958 |
[32m[20221213 18:06:52 @agent_ppo2.py:185][0m |          -0.0079 |          50.1562 |           0.2958 |
[32m[20221213 18:06:53 @agent_ppo2.py:185][0m |          -0.0123 |          49.0073 |           0.2959 |
[32m[20221213 18:06:53 @agent_ppo2.py:185][0m |          -0.0047 |          51.0800 |           0.2961 |
[32m[20221213 18:06:53 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 18:06:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.21
[32m[20221213 18:06:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.05
[32m[20221213 18:06:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.33
[32m[20221213 18:06:53 @agent_ppo2.py:143][0m Total time:       2.72 min
[32m[20221213 18:06:53 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 18:06:53 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 18:06:53 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:06:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:53 @agent_ppo2.py:185][0m |          -0.0015 |          61.6060 |           0.2991 |
[32m[20221213 18:06:53 @agent_ppo2.py:185][0m |          -0.0071 |          57.4575 |           0.2989 |
[32m[20221213 18:06:53 @agent_ppo2.py:185][0m |          -0.0099 |          55.3819 |           0.2987 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0108 |          54.0520 |           0.2989 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0097 |          52.8699 |           0.2989 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0093 |          51.9647 |           0.2987 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0109 |          51.1109 |           0.2989 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0104 |          51.4351 |           0.2990 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0181 |          49.8085 |           0.2989 |
[32m[20221213 18:06:54 @agent_ppo2.py:185][0m |          -0.0132 |          49.0633 |           0.2991 |
[32m[20221213 18:06:54 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:06:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.87
[32m[20221213 18:06:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.31
[32m[20221213 18:06:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.66
[32m[20221213 18:06:54 @agent_ppo2.py:143][0m Total time:       2.74 min
[32m[20221213 18:06:54 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 18:06:54 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 18:06:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:06:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0015 |          64.7099 |           0.3077 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |           0.0059 |          64.1862 |           0.3071 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0112 |          53.9519 |           0.3069 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0099 |          51.6968 |           0.3067 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0085 |          50.0719 |           0.3068 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0089 |          49.9709 |           0.3066 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0119 |          47.2983 |           0.3068 |
[32m[20221213 18:06:55 @agent_ppo2.py:185][0m |          -0.0125 |          46.3712 |           0.3069 |
[32m[20221213 18:06:56 @agent_ppo2.py:185][0m |          -0.0155 |          45.5082 |           0.3071 |
[32m[20221213 18:06:56 @agent_ppo2.py:185][0m |          -0.0146 |          44.7233 |           0.3071 |
[32m[20221213 18:06:56 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 18:06:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.27
[32m[20221213 18:06:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.58
[32m[20221213 18:06:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.60
[32m[20221213 18:06:56 @agent_ppo2.py:143][0m Total time:       2.77 min
[32m[20221213 18:06:56 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 18:06:56 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 18:06:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:06:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:56 @agent_ppo2.py:185][0m |           0.0003 |          61.8454 |           0.3040 |
[32m[20221213 18:06:56 @agent_ppo2.py:185][0m |          -0.0017 |          56.9114 |           0.3044 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0050 |          54.6110 |           0.3044 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0050 |          53.2804 |           0.3043 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |           0.0055 |          58.3745 |           0.3043 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0083 |          51.3024 |           0.3043 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0081 |          50.4962 |           0.3042 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0094 |          49.9688 |           0.3045 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0096 |          49.3740 |           0.3042 |
[32m[20221213 18:06:57 @agent_ppo2.py:185][0m |          -0.0146 |          48.8810 |           0.3045 |
[32m[20221213 18:06:57 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:06:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.85
[32m[20221213 18:06:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.80
[32m[20221213 18:06:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.36
[32m[20221213 18:06:57 @agent_ppo2.py:143][0m Total time:       2.79 min
[32m[20221213 18:06:57 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 18:06:57 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 18:06:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:06:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |          -0.0013 |          62.5046 |           0.3035 |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |           0.0014 |          61.0707 |           0.3033 |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |          -0.0031 |          58.5914 |           0.3030 |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |          -0.0077 |          56.6934 |           0.3030 |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |          -0.0056 |          57.1479 |           0.3031 |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |          -0.0133 |          55.0924 |           0.3031 |
[32m[20221213 18:06:58 @agent_ppo2.py:185][0m |          -0.0110 |          54.6552 |           0.3030 |
[32m[20221213 18:06:59 @agent_ppo2.py:185][0m |          -0.0134 |          53.9050 |           0.3031 |
[32m[20221213 18:06:59 @agent_ppo2.py:185][0m |          -0.0098 |          54.2855 |           0.3029 |
[32m[20221213 18:06:59 @agent_ppo2.py:185][0m |          -0.0155 |          52.4268 |           0.3029 |
[32m[20221213 18:06:59 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 18:06:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.48
[32m[20221213 18:06:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.17
[32m[20221213 18:06:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.18
[32m[20221213 18:06:59 @agent_ppo2.py:143][0m Total time:       2.82 min
[32m[20221213 18:06:59 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 18:06:59 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 18:06:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:06:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:06:59 @agent_ppo2.py:185][0m |           0.0054 |          60.3343 |           0.3061 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0062 |          52.9457 |           0.3059 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0049 |          50.2117 |           0.3057 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0095 |          48.4005 |           0.3057 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0146 |          47.0256 |           0.3056 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0055 |          46.1110 |           0.3058 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0127 |          44.6769 |           0.3057 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0053 |          44.0791 |           0.3058 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0139 |          42.7519 |           0.3058 |
[32m[20221213 18:07:00 @agent_ppo2.py:185][0m |          -0.0125 |          41.7582 |           0.3058 |
[32m[20221213 18:07:00 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 18:07:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.02
[32m[20221213 18:07:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.93
[32m[20221213 18:07:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.16
[32m[20221213 18:07:01 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221213 18:07:01 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 18:07:01 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 18:07:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:07:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:01 @agent_ppo2.py:185][0m |           0.0004 |          60.6194 |           0.3052 |
[32m[20221213 18:07:01 @agent_ppo2.py:185][0m |          -0.0069 |          57.3311 |           0.3051 |
[32m[20221213 18:07:01 @agent_ppo2.py:185][0m |          -0.0089 |          57.5647 |           0.3048 |
[32m[20221213 18:07:01 @agent_ppo2.py:185][0m |          -0.0102 |          54.9375 |           0.3047 |
[32m[20221213 18:07:01 @agent_ppo2.py:185][0m |          -0.0112 |          54.1854 |           0.3047 |
[32m[20221213 18:07:01 @agent_ppo2.py:185][0m |          -0.0099 |          53.6864 |           0.3048 |
[32m[20221213 18:07:02 @agent_ppo2.py:185][0m |          -0.0083 |          53.4359 |           0.3048 |
[32m[20221213 18:07:02 @agent_ppo2.py:185][0m |          -0.0155 |          52.6443 |           0.3048 |
[32m[20221213 18:07:02 @agent_ppo2.py:185][0m |           0.0067 |          62.0012 |           0.3048 |
[32m[20221213 18:07:02 @agent_ppo2.py:185][0m |          -0.0137 |          51.7689 |           0.3051 |
[32m[20221213 18:07:02 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:07:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.02
[32m[20221213 18:07:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.03
[32m[20221213 18:07:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.70
[32m[20221213 18:07:02 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 699.70
[32m[20221213 18:07:02 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 699.70
[32m[20221213 18:07:02 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221213 18:07:02 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 18:07:02 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 18:07:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:07:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:02 @agent_ppo2.py:185][0m |          -0.0020 |          56.3217 |           0.3119 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0069 |          54.2481 |           0.3116 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0060 |          52.4106 |           0.3111 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0089 |          50.4067 |           0.3109 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0087 |          49.0683 |           0.3108 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0115 |          47.7794 |           0.3106 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0046 |          47.5930 |           0.3107 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0125 |          46.2688 |           0.3104 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0071 |          45.6009 |           0.3103 |
[32m[20221213 18:07:03 @agent_ppo2.py:185][0m |          -0.0103 |          44.6239 |           0.3102 |
[32m[20221213 18:07:03 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:07:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.23
[32m[20221213 18:07:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.04
[32m[20221213 18:07:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.50
[32m[20221213 18:07:04 @agent_ppo2.py:143][0m Total time:       2.90 min
[32m[20221213 18:07:04 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 18:07:04 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 18:07:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:07:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:04 @agent_ppo2.py:185][0m |          -0.0019 |          57.1958 |           0.2970 |
[32m[20221213 18:07:04 @agent_ppo2.py:185][0m |          -0.0065 |          53.9430 |           0.2967 |
[32m[20221213 18:07:04 @agent_ppo2.py:185][0m |          -0.0073 |          52.2759 |           0.2963 |
[32m[20221213 18:07:04 @agent_ppo2.py:185][0m |          -0.0116 |          51.4191 |           0.2963 |
[32m[20221213 18:07:04 @agent_ppo2.py:185][0m |          -0.0093 |          50.6527 |           0.2961 |
[32m[20221213 18:07:05 @agent_ppo2.py:185][0m |          -0.0100 |          50.0429 |           0.2962 |
[32m[20221213 18:07:05 @agent_ppo2.py:185][0m |          -0.0133 |          49.4331 |           0.2962 |
[32m[20221213 18:07:05 @agent_ppo2.py:185][0m |          -0.0143 |          48.9602 |           0.2961 |
[32m[20221213 18:07:05 @agent_ppo2.py:185][0m |          -0.0146 |          48.5894 |           0.2962 |
[32m[20221213 18:07:05 @agent_ppo2.py:185][0m |          -0.0101 |          48.1246 |           0.2961 |
[32m[20221213 18:07:05 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 18:07:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.35
[32m[20221213 18:07:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.35
[32m[20221213 18:07:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.00
[32m[20221213 18:07:05 @agent_ppo2.py:143][0m Total time:       2.92 min
[32m[20221213 18:07:05 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 18:07:05 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 18:07:05 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:07:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0003 |          60.0372 |           0.3013 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0032 |          56.9741 |           0.3010 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0074 |          55.6132 |           0.3010 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0072 |          54.8542 |           0.3008 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0101 |          54.1465 |           0.3009 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0098 |          53.6120 |           0.3008 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0129 |          53.1929 |           0.3008 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0022 |          55.7747 |           0.3006 |
[32m[20221213 18:07:06 @agent_ppo2.py:185][0m |          -0.0122 |          51.9692 |           0.3007 |
[32m[20221213 18:07:07 @agent_ppo2.py:185][0m |          -0.0122 |          51.5207 |           0.3006 |
[32m[20221213 18:07:07 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:07:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.73
[32m[20221213 18:07:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.97
[32m[20221213 18:07:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.13
[32m[20221213 18:07:07 @agent_ppo2.py:143][0m Total time:       2.95 min
[32m[20221213 18:07:07 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 18:07:07 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 18:07:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:07:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:07 @agent_ppo2.py:185][0m |           0.0022 |          56.1797 |           0.3121 |
[32m[20221213 18:07:07 @agent_ppo2.py:185][0m |          -0.0050 |          52.4629 |           0.3116 |
[32m[20221213 18:07:07 @agent_ppo2.py:185][0m |          -0.0098 |          51.1090 |           0.3111 |
[32m[20221213 18:07:07 @agent_ppo2.py:185][0m |          -0.0091 |          50.1865 |           0.3106 |
[32m[20221213 18:07:08 @agent_ppo2.py:185][0m |          -0.0112 |          49.2440 |           0.3102 |
[32m[20221213 18:07:08 @agent_ppo2.py:185][0m |          -0.0121 |          48.7470 |           0.3099 |
[32m[20221213 18:07:08 @agent_ppo2.py:185][0m |          -0.0052 |          50.4159 |           0.3096 |
[32m[20221213 18:07:08 @agent_ppo2.py:185][0m |          -0.0098 |          47.8245 |           0.3092 |
[32m[20221213 18:07:08 @agent_ppo2.py:185][0m |          -0.0124 |          46.7754 |           0.3090 |
[32m[20221213 18:07:08 @agent_ppo2.py:185][0m |          -0.0162 |          46.1949 |           0.3087 |
[32m[20221213 18:07:08 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:07:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.15
[32m[20221213 18:07:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.75
[32m[20221213 18:07:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.38
[32m[20221213 18:07:08 @agent_ppo2.py:143][0m Total time:       2.97 min
[32m[20221213 18:07:08 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 18:07:08 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 18:07:08 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 18:07:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |           0.0019 |          61.9436 |           0.3115 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |          -0.0027 |          58.9560 |           0.3114 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |          -0.0062 |          57.1933 |           0.3112 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |          -0.0072 |          56.2548 |           0.3114 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |           0.0026 |          61.8465 |           0.3114 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |          -0.0018 |          56.8703 |           0.3115 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |          -0.0122 |          53.0649 |           0.3115 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |          -0.0032 |          54.3235 |           0.3114 |
[32m[20221213 18:07:09 @agent_ppo2.py:185][0m |           0.0103 |          56.8643 |           0.3116 |
[32m[20221213 18:07:10 @agent_ppo2.py:185][0m |          -0.0098 |          51.1504 |           0.3116 |
[32m[20221213 18:07:10 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 18:07:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.74
[32m[20221213 18:07:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.98
[32m[20221213 18:07:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.29
[32m[20221213 18:07:10 @agent_ppo2.py:143][0m Total time:       3.00 min
[32m[20221213 18:07:10 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 18:07:10 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 18:07:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:10 @agent_ppo2.py:185][0m |           0.0065 |          65.1920 |           0.2925 |
[32m[20221213 18:07:10 @agent_ppo2.py:185][0m |          -0.0105 |          59.0642 |           0.2923 |
[32m[20221213 18:07:10 @agent_ppo2.py:185][0m |          -0.0042 |          57.4572 |           0.2921 |
[32m[20221213 18:07:10 @agent_ppo2.py:185][0m |          -0.0067 |          56.6565 |           0.2921 |
[32m[20221213 18:07:10 @agent_ppo2.py:185][0m |          -0.0087 |          55.8574 |           0.2920 |
[32m[20221213 18:07:11 @agent_ppo2.py:185][0m |          -0.0101 |          55.2618 |           0.2920 |
[32m[20221213 18:07:11 @agent_ppo2.py:185][0m |          -0.0120 |          54.9129 |           0.2919 |
[32m[20221213 18:07:11 @agent_ppo2.py:185][0m |          -0.0112 |          54.3636 |           0.2918 |
[32m[20221213 18:07:11 @agent_ppo2.py:185][0m |          -0.0113 |          53.8103 |           0.2917 |
[32m[20221213 18:07:11 @agent_ppo2.py:185][0m |          -0.0058 |          55.1440 |           0.2917 |
[32m[20221213 18:07:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:07:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.18
[32m[20221213 18:07:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.48
[32m[20221213 18:07:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.74
[32m[20221213 18:07:11 @agent_ppo2.py:143][0m Total time:       3.02 min
[32m[20221213 18:07:11 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 18:07:11 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 18:07:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:11 @agent_ppo2.py:185][0m |          -0.0022 |          66.0124 |           0.2990 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0071 |          61.2499 |           0.2983 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0075 |          58.6032 |           0.2981 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0080 |          56.3989 |           0.2980 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0144 |          54.9004 |           0.2981 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0127 |          53.6534 |           0.2980 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0119 |          52.5193 |           0.2981 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0074 |          52.9841 |           0.2982 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0121 |          50.8064 |           0.2982 |
[32m[20221213 18:07:12 @agent_ppo2.py:185][0m |          -0.0165 |          50.3232 |           0.2983 |
[32m[20221213 18:07:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:07:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.94
[32m[20221213 18:07:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.57
[32m[20221213 18:07:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.87
[32m[20221213 18:07:12 @agent_ppo2.py:143][0m Total time:       3.04 min
[32m[20221213 18:07:12 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 18:07:12 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 18:07:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:07:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0030 |          67.4477 |           0.2995 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0088 |          63.9539 |           0.2995 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0102 |          62.4965 |           0.2992 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0135 |          61.4803 |           0.2993 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0101 |          60.8422 |           0.2992 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0121 |          60.0983 |           0.2994 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0038 |          62.3944 |           0.2995 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0129 |          58.6348 |           0.2996 |
[32m[20221213 18:07:13 @agent_ppo2.py:185][0m |          -0.0143 |          58.2280 |           0.2998 |
[32m[20221213 18:07:14 @agent_ppo2.py:185][0m |          -0.0115 |          57.5621 |           0.2999 |
[32m[20221213 18:07:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:07:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.01
[32m[20221213 18:07:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.89
[32m[20221213 18:07:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.72
[32m[20221213 18:07:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 955.72
[32m[20221213 18:07:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 955.72
[32m[20221213 18:07:14 @agent_ppo2.py:143][0m Total time:       3.07 min
[32m[20221213 18:07:14 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 18:07:14 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 18:07:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:14 @agent_ppo2.py:185][0m |          -0.0013 |          69.8568 |           0.3058 |
[32m[20221213 18:07:14 @agent_ppo2.py:185][0m |          -0.0064 |          66.2509 |           0.3061 |
[32m[20221213 18:07:14 @agent_ppo2.py:185][0m |          -0.0084 |          64.8051 |           0.3059 |
[32m[20221213 18:07:14 @agent_ppo2.py:185][0m |          -0.0050 |          63.9477 |           0.3060 |
[32m[20221213 18:07:14 @agent_ppo2.py:185][0m |          -0.0102 |          62.9879 |           0.3060 |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |          -0.0112 |          62.2576 |           0.3063 |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |          -0.0094 |          61.5732 |           0.3062 |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |          -0.0109 |          61.1833 |           0.3065 |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |          -0.0121 |          60.2783 |           0.3066 |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |          -0.0066 |          62.4594 |           0.3068 |
[32m[20221213 18:07:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:07:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.61
[32m[20221213 18:07:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.14
[32m[20221213 18:07:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.12
[32m[20221213 18:07:15 @agent_ppo2.py:143][0m Total time:       3.09 min
[32m[20221213 18:07:15 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 18:07:15 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 18:07:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:07:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |           0.0007 |          66.2230 |           0.3065 |
[32m[20221213 18:07:15 @agent_ppo2.py:185][0m |          -0.0062 |          62.9260 |           0.3061 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0106 |          61.1370 |           0.3058 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0071 |          61.6966 |           0.3055 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0067 |          60.2097 |           0.3053 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0026 |          63.2439 |           0.3051 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0153 |          57.6134 |           0.3051 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0070 |          58.8031 |           0.3049 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0100 |          56.9189 |           0.3049 |
[32m[20221213 18:07:16 @agent_ppo2.py:185][0m |          -0.0108 |          56.0400 |           0.3049 |
[32m[20221213 18:07:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:07:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.09
[32m[20221213 18:07:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.38
[32m[20221213 18:07:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.55
[32m[20221213 18:07:16 @agent_ppo2.py:143][0m Total time:       3.11 min
[32m[20221213 18:07:16 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 18:07:16 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 18:07:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0003 |          64.9786 |           0.3073 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0000 |          60.4788 |           0.3071 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0056 |          57.7688 |           0.3068 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0108 |          56.8165 |           0.3064 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0084 |          56.1517 |           0.3063 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0102 |          55.6555 |           0.3063 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0115 |          55.3471 |           0.3060 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0019 |          57.8634 |           0.3058 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0133 |          54.7224 |           0.3058 |
[32m[20221213 18:07:17 @agent_ppo2.py:185][0m |          -0.0148 |          54.3240 |           0.3059 |
[32m[20221213 18:07:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:07:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.08
[32m[20221213 18:07:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.31
[32m[20221213 18:07:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.49
[32m[20221213 18:07:18 @agent_ppo2.py:143][0m Total time:       3.13 min
[32m[20221213 18:07:18 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 18:07:18 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 18:07:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:18 @agent_ppo2.py:185][0m |           0.0028 |          62.8748 |           0.3103 |
[32m[20221213 18:07:18 @agent_ppo2.py:185][0m |          -0.0044 |          59.0425 |           0.3100 |
[32m[20221213 18:07:18 @agent_ppo2.py:185][0m |          -0.0068 |          57.1429 |           0.3098 |
[32m[20221213 18:07:18 @agent_ppo2.py:185][0m |          -0.0094 |          56.1657 |           0.3096 |
[32m[20221213 18:07:18 @agent_ppo2.py:185][0m |          -0.0093 |          55.2395 |           0.3094 |
[32m[20221213 18:07:18 @agent_ppo2.py:185][0m |          -0.0126 |          54.8167 |           0.3093 |
[32m[20221213 18:07:19 @agent_ppo2.py:185][0m |          -0.0132 |          54.1329 |           0.3092 |
[32m[20221213 18:07:19 @agent_ppo2.py:185][0m |          -0.0104 |          53.6951 |           0.3093 |
[32m[20221213 18:07:19 @agent_ppo2.py:185][0m |          -0.0122 |          53.1528 |           0.3091 |
[32m[20221213 18:07:19 @agent_ppo2.py:185][0m |          -0.0127 |          52.8179 |           0.3090 |
[32m[20221213 18:07:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:07:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.56
[32m[20221213 18:07:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.33
[32m[20221213 18:07:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.98
[32m[20221213 18:07:19 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221213 18:07:19 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 18:07:19 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 18:07:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:07:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:19 @agent_ppo2.py:185][0m |          -0.0018 |          62.9489 |           0.3082 |
[32m[20221213 18:07:19 @agent_ppo2.py:185][0m |          -0.0041 |          61.1597 |           0.3079 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0039 |          61.7015 |           0.3076 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0107 |          58.7171 |           0.3071 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0122 |          57.7132 |           0.3071 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0128 |          56.9107 |           0.3071 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0153 |          55.8764 |           0.3070 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0047 |          56.4799 |           0.3067 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0068 |          54.9300 |           0.3066 |
[32m[20221213 18:07:20 @agent_ppo2.py:185][0m |          -0.0145 |          53.3264 |           0.3063 |
[32m[20221213 18:07:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:07:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.80
[32m[20221213 18:07:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.50
[32m[20221213 18:07:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.85
[32m[20221213 18:07:20 @agent_ppo2.py:143][0m Total time:       3.18 min
[32m[20221213 18:07:20 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 18:07:20 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 18:07:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |           0.0003 |          60.3501 |           0.3007 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0034 |          56.9475 |           0.3011 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0051 |          55.2595 |           0.3008 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0131 |          53.7571 |           0.3011 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0040 |          54.0749 |           0.3012 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0107 |          51.1027 |           0.3014 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0093 |          51.0120 |           0.3013 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0055 |          52.9090 |           0.3011 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0126 |          48.9400 |           0.3011 |
[32m[20221213 18:07:21 @agent_ppo2.py:185][0m |          -0.0116 |          48.2128 |           0.3011 |
[32m[20221213 18:07:21 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:07:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.95
[32m[20221213 18:07:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.62
[32m[20221213 18:07:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.48
[32m[20221213 18:07:22 @agent_ppo2.py:143][0m Total time:       3.20 min
[32m[20221213 18:07:22 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 18:07:22 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 18:07:22 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:07:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:22 @agent_ppo2.py:185][0m |          -0.0013 |          69.1030 |           0.3068 |
[32m[20221213 18:07:22 @agent_ppo2.py:185][0m |          -0.0081 |          63.4420 |           0.3072 |
[32m[20221213 18:07:22 @agent_ppo2.py:185][0m |          -0.0085 |          61.5812 |           0.3073 |
[32m[20221213 18:07:22 @agent_ppo2.py:185][0m |          -0.0099 |          60.2788 |           0.3075 |
[32m[20221213 18:07:22 @agent_ppo2.py:185][0m |          -0.0105 |          59.1957 |           0.3077 |
[32m[20221213 18:07:22 @agent_ppo2.py:185][0m |          -0.0125 |          58.2627 |           0.3078 |
[32m[20221213 18:07:23 @agent_ppo2.py:185][0m |          -0.0122 |          57.5065 |           0.3081 |
[32m[20221213 18:07:23 @agent_ppo2.py:185][0m |          -0.0120 |          56.5297 |           0.3083 |
[32m[20221213 18:07:23 @agent_ppo2.py:185][0m |          -0.0100 |          56.1300 |           0.3085 |
[32m[20221213 18:07:23 @agent_ppo2.py:185][0m |          -0.0113 |          55.6893 |           0.3086 |
[32m[20221213 18:07:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:07:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.66
[32m[20221213 18:07:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.83
[32m[20221213 18:07:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.69
[32m[20221213 18:07:23 @agent_ppo2.py:143][0m Total time:       3.22 min
[32m[20221213 18:07:23 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 18:07:23 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 18:07:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:23 @agent_ppo2.py:185][0m |           0.0002 |          69.4964 |           0.3137 |
[32m[20221213 18:07:23 @agent_ppo2.py:185][0m |          -0.0059 |          65.5028 |           0.3131 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0047 |          63.4970 |           0.3131 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0094 |          61.7935 |           0.3130 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0009 |          64.9863 |           0.3128 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0097 |          60.1232 |           0.3128 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0106 |          58.5148 |           0.3127 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0098 |          57.8657 |           0.3126 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0133 |          57.1994 |           0.3126 |
[32m[20221213 18:07:24 @agent_ppo2.py:185][0m |          -0.0136 |          56.5513 |           0.3125 |
[32m[20221213 18:07:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:07:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.96
[32m[20221213 18:07:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.56
[32m[20221213 18:07:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.60
[32m[20221213 18:07:24 @agent_ppo2.py:143][0m Total time:       3.24 min
[32m[20221213 18:07:24 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 18:07:24 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 18:07:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0041 |          62.2427 |           0.3163 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0072 |          59.1324 |           0.3157 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0119 |          57.4738 |           0.3158 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0119 |          56.1325 |           0.3155 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0070 |          55.8444 |           0.3157 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0105 |          54.2273 |           0.3157 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |           0.0004 |          57.3429 |           0.3156 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0166 |          52.9490 |           0.3157 |
[32m[20221213 18:07:25 @agent_ppo2.py:185][0m |          -0.0159 |          52.5354 |           0.3158 |
[32m[20221213 18:07:26 @agent_ppo2.py:185][0m |          -0.0043 |          55.9922 |           0.3156 |
[32m[20221213 18:07:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:07:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.75
[32m[20221213 18:07:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.50
[32m[20221213 18:07:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.74
[32m[20221213 18:07:26 @agent_ppo2.py:143][0m Total time:       3.27 min
[32m[20221213 18:07:26 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 18:07:26 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 18:07:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:26 @agent_ppo2.py:185][0m |           0.0010 |          70.5085 |           0.3142 |
[32m[20221213 18:07:26 @agent_ppo2.py:185][0m |          -0.0105 |          66.9520 |           0.3136 |
[32m[20221213 18:07:26 @agent_ppo2.py:185][0m |          -0.0154 |          65.2566 |           0.3131 |
[32m[20221213 18:07:26 @agent_ppo2.py:185][0m |          -0.0120 |          64.1920 |           0.3127 |
[32m[20221213 18:07:26 @agent_ppo2.py:185][0m |          -0.0109 |          64.2519 |           0.3123 |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |          -0.0116 |          62.4077 |           0.3120 |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |          -0.0148 |          61.8067 |           0.3118 |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |          -0.0140 |          61.4009 |           0.3115 |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |          -0.0147 |          60.7999 |           0.3114 |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |          -0.0087 |          68.6662 |           0.3113 |
[32m[20221213 18:07:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:07:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.49
[32m[20221213 18:07:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.10
[32m[20221213 18:07:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.49
[32m[20221213 18:07:27 @agent_ppo2.py:143][0m Total time:       3.29 min
[32m[20221213 18:07:27 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 18:07:27 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 18:07:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |           0.0047 |          55.9435 |           0.3105 |
[32m[20221213 18:07:27 @agent_ppo2.py:185][0m |          -0.0045 |          51.0746 |           0.3094 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0078 |          50.2225 |           0.3097 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0093 |          49.7528 |           0.3098 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0042 |          50.9938 |           0.3097 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0114 |          48.8055 |           0.3093 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0026 |          51.3515 |           0.3099 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0128 |          48.3388 |           0.3100 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0132 |          47.7448 |           0.3100 |
[32m[20221213 18:07:28 @agent_ppo2.py:185][0m |          -0.0119 |          47.4742 |           0.3099 |
[32m[20221213 18:07:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:07:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.45
[32m[20221213 18:07:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.09
[32m[20221213 18:07:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.83
[32m[20221213 18:07:28 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221213 18:07:28 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 18:07:28 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 18:07:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0006 |          69.6126 |           0.3160 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0089 |          66.5134 |           0.3157 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0045 |          64.9693 |           0.3154 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0106 |          63.4182 |           0.3154 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0142 |          62.1660 |           0.3154 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0124 |          61.1329 |           0.3154 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0157 |          60.3319 |           0.3155 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0129 |          59.7719 |           0.3155 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0141 |          58.6293 |           0.3156 |
[32m[20221213 18:07:29 @agent_ppo2.py:185][0m |          -0.0150 |          57.7900 |           0.3157 |
[32m[20221213 18:07:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:07:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.82
[32m[20221213 18:07:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.19
[32m[20221213 18:07:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.81
[32m[20221213 18:07:30 @agent_ppo2.py:143][0m Total time:       3.33 min
[32m[20221213 18:07:30 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 18:07:30 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 18:07:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:30 @agent_ppo2.py:185][0m |          -0.0005 |          64.8404 |           0.3197 |
[32m[20221213 18:07:30 @agent_ppo2.py:185][0m |          -0.0072 |          61.6281 |           0.3194 |
[32m[20221213 18:07:30 @agent_ppo2.py:185][0m |          -0.0089 |          59.9384 |           0.3190 |
[32m[20221213 18:07:30 @agent_ppo2.py:185][0m |          -0.0123 |          59.0542 |           0.3189 |
[32m[20221213 18:07:30 @agent_ppo2.py:185][0m |          -0.0112 |          57.8408 |           0.3188 |
[32m[20221213 18:07:30 @agent_ppo2.py:185][0m |          -0.0121 |          56.8236 |           0.3189 |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0136 |          55.8771 |           0.3186 |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0172 |          55.1142 |           0.3186 |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0140 |          54.5243 |           0.3187 |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0172 |          53.8882 |           0.3185 |
[32m[20221213 18:07:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:07:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.16
[32m[20221213 18:07:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.33
[32m[20221213 18:07:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.34
[32m[20221213 18:07:31 @agent_ppo2.py:143][0m Total time:       3.35 min
[32m[20221213 18:07:31 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 18:07:31 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 18:07:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0027 |          69.0634 |           0.3178 |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0067 |          66.4808 |           0.3175 |
[32m[20221213 18:07:31 @agent_ppo2.py:185][0m |          -0.0127 |          65.5500 |           0.3173 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0125 |          64.7912 |           0.3172 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0102 |          64.2343 |           0.3171 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0102 |          63.5506 |           0.3168 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0110 |          62.9427 |           0.3169 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0132 |          62.5961 |           0.3170 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0097 |          63.7793 |           0.3168 |
[32m[20221213 18:07:32 @agent_ppo2.py:185][0m |          -0.0132 |          62.1999 |           0.3169 |
[32m[20221213 18:07:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:07:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.06
[32m[20221213 18:07:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.64
[32m[20221213 18:07:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.95
[32m[20221213 18:07:32 @agent_ppo2.py:143][0m Total time:       3.37 min
[32m[20221213 18:07:32 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 18:07:32 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 18:07:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0005 |          62.6917 |           0.3184 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0026 |          60.3490 |           0.3181 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0114 |          58.7574 |           0.3179 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0130 |          58.0151 |           0.3177 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0077 |          57.3056 |           0.3175 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0086 |          56.5702 |           0.3170 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0104 |          56.1755 |           0.3168 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0131 |          55.9235 |           0.3166 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0166 |          55.5797 |           0.3164 |
[32m[20221213 18:07:33 @agent_ppo2.py:185][0m |          -0.0152 |          55.2523 |           0.3161 |
[32m[20221213 18:07:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:07:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.96
[32m[20221213 18:07:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.10
[32m[20221213 18:07:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.53
[32m[20221213 18:07:34 @agent_ppo2.py:143][0m Total time:       3.40 min
[32m[20221213 18:07:34 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 18:07:34 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 18:07:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0004 |          66.5233 |           0.3061 |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0067 |          64.4751 |           0.3058 |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0100 |          63.4280 |           0.3055 |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0107 |          62.2199 |           0.3051 |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0131 |          61.5456 |           0.3048 |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0065 |          62.6014 |           0.3047 |
[32m[20221213 18:07:34 @agent_ppo2.py:185][0m |          -0.0103 |          60.4304 |           0.3045 |
[32m[20221213 18:07:35 @agent_ppo2.py:185][0m |          -0.0133 |          59.9930 |           0.3044 |
[32m[20221213 18:07:35 @agent_ppo2.py:185][0m |          -0.0122 |          59.6206 |           0.3043 |
[32m[20221213 18:07:35 @agent_ppo2.py:185][0m |          -0.0164 |          59.2160 |           0.3041 |
[32m[20221213 18:07:35 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 18:07:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.92
[32m[20221213 18:07:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.37
[32m[20221213 18:07:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.44
[32m[20221213 18:07:35 @agent_ppo2.py:143][0m Total time:       3.43 min
[32m[20221213 18:07:35 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 18:07:35 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 18:07:36 @agent_ppo2.py:127][0m Sampling time: 0.30 s by 5 slaves
[32m[20221213 18:07:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0013 |          72.0410 |           0.3100 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0092 |          69.5692 |           0.3094 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0108 |          67.8748 |           0.3088 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0089 |          66.7345 |           0.3082 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0137 |          65.7218 |           0.3080 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0105 |          65.0366 |           0.3079 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0120 |          64.2999 |           0.3074 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0131 |          63.9738 |           0.3073 |
[32m[20221213 18:07:36 @agent_ppo2.py:185][0m |          -0.0147 |          63.0968 |           0.3071 |
[32m[20221213 18:07:37 @agent_ppo2.py:185][0m |          -0.0118 |          62.3724 |           0.3070 |
[32m[20221213 18:07:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:07:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.49
[32m[20221213 18:07:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.49
[32m[20221213 18:07:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.07
[32m[20221213 18:07:37 @agent_ppo2.py:143][0m Total time:       3.45 min
[32m[20221213 18:07:37 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 18:07:37 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 18:07:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:37 @agent_ppo2.py:185][0m |          -0.0017 |          61.7171 |           0.3032 |
[32m[20221213 18:07:37 @agent_ppo2.py:185][0m |           0.0138 |          67.9905 |           0.3030 |
[32m[20221213 18:07:37 @agent_ppo2.py:185][0m |          -0.0072 |          58.3267 |           0.3029 |
[32m[20221213 18:07:37 @agent_ppo2.py:185][0m |          -0.0071 |          57.5487 |           0.3027 |
[32m[20221213 18:07:37 @agent_ppo2.py:185][0m |          -0.0073 |          56.4044 |           0.3028 |
[32m[20221213 18:07:38 @agent_ppo2.py:185][0m |          -0.0108 |          55.8501 |           0.3028 |
[32m[20221213 18:07:38 @agent_ppo2.py:185][0m |          -0.0152 |          55.1692 |           0.3028 |
[32m[20221213 18:07:38 @agent_ppo2.py:185][0m |          -0.0116 |          54.5989 |           0.3028 |
[32m[20221213 18:07:38 @agent_ppo2.py:185][0m |          -0.0117 |          54.2051 |           0.3028 |
[32m[20221213 18:07:38 @agent_ppo2.py:185][0m |          -0.0167 |          53.7091 |           0.3029 |
[32m[20221213 18:07:38 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 18:07:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.62
[32m[20221213 18:07:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.91
[32m[20221213 18:07:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.27
[32m[20221213 18:07:38 @agent_ppo2.py:143][0m Total time:       3.47 min
[32m[20221213 18:07:38 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 18:07:38 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 18:07:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |           0.0015 |          67.3799 |           0.3104 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0061 |          64.8778 |           0.3102 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0084 |          63.9599 |           0.3094 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0114 |          63.2234 |           0.3092 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0086 |          62.6804 |           0.3090 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0152 |          62.2235 |           0.3088 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0074 |          67.2842 |           0.3087 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0042 |          67.4120 |           0.3084 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0171 |          60.9683 |           0.3085 |
[32m[20221213 18:07:39 @agent_ppo2.py:185][0m |          -0.0180 |          60.6431 |           0.3084 |
[32m[20221213 18:07:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:07:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.99
[32m[20221213 18:07:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.75
[32m[20221213 18:07:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.97
[32m[20221213 18:07:40 @agent_ppo2.py:143][0m Total time:       3.50 min
[32m[20221213 18:07:40 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 18:07:40 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 18:07:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0007 |          65.7845 |           0.2944 |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0077 |          63.1275 |           0.2944 |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0114 |          61.4283 |           0.2943 |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0080 |          60.1997 |           0.2945 |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0085 |          59.2896 |           0.2946 |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0104 |          58.6505 |           0.2945 |
[32m[20221213 18:07:40 @agent_ppo2.py:185][0m |          -0.0118 |          58.1486 |           0.2944 |
[32m[20221213 18:07:41 @agent_ppo2.py:185][0m |          -0.0138 |          57.5500 |           0.2944 |
[32m[20221213 18:07:41 @agent_ppo2.py:185][0m |          -0.0148 |          57.1729 |           0.2943 |
[32m[20221213 18:07:41 @agent_ppo2.py:185][0m |          -0.0101 |          57.2699 |           0.2942 |
[32m[20221213 18:07:41 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 18:07:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.19
[32m[20221213 18:07:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.89
[32m[20221213 18:07:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.34
[32m[20221213 18:07:41 @agent_ppo2.py:143][0m Total time:       3.52 min
[32m[20221213 18:07:41 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 18:07:41 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 18:07:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:41 @agent_ppo2.py:185][0m |           0.0117 |          64.0869 |           0.3109 |
[32m[20221213 18:07:41 @agent_ppo2.py:185][0m |          -0.0080 |          57.1725 |           0.3102 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0075 |          55.5693 |           0.3101 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0091 |          54.5234 |           0.3101 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0106 |          53.8822 |           0.3098 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0136 |          52.7928 |           0.3099 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0130 |          52.2624 |           0.3099 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0142 |          51.6439 |           0.3101 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0133 |          51.1789 |           0.3101 |
[32m[20221213 18:07:42 @agent_ppo2.py:185][0m |          -0.0143 |          50.7944 |           0.3102 |
[32m[20221213 18:07:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:07:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.67
[32m[20221213 18:07:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.36
[32m[20221213 18:07:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.72
[32m[20221213 18:07:42 @agent_ppo2.py:143][0m Total time:       3.54 min
[32m[20221213 18:07:42 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 18:07:42 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 18:07:43 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:07:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0011 |          64.4364 |           0.3088 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0072 |          59.4126 |           0.3085 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0117 |          55.9190 |           0.3083 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |           0.0001 |          61.5214 |           0.3081 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0134 |          53.0022 |           0.3082 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0159 |          51.8400 |           0.3081 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0176 |          50.7149 |           0.3080 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0159 |          49.6995 |           0.3081 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0152 |          48.7050 |           0.3081 |
[32m[20221213 18:07:43 @agent_ppo2.py:185][0m |          -0.0155 |          47.9497 |           0.3081 |
[32m[20221213 18:07:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:07:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.70
[32m[20221213 18:07:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.31
[32m[20221213 18:07:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.29
[32m[20221213 18:07:44 @agent_ppo2.py:143][0m Total time:       3.57 min
[32m[20221213 18:07:44 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 18:07:44 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 18:07:44 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:07:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:44 @agent_ppo2.py:185][0m |           0.0053 |          66.3963 |           0.3203 |
[32m[20221213 18:07:44 @agent_ppo2.py:185][0m |          -0.0098 |          58.9639 |           0.3200 |
[32m[20221213 18:07:44 @agent_ppo2.py:185][0m |          -0.0066 |          56.7027 |           0.3196 |
[32m[20221213 18:07:44 @agent_ppo2.py:185][0m |          -0.0114 |          53.9023 |           0.3198 |
[32m[20221213 18:07:44 @agent_ppo2.py:185][0m |          -0.0082 |          51.6393 |           0.3198 |
[32m[20221213 18:07:44 @agent_ppo2.py:185][0m |          -0.0100 |          49.6103 |           0.3198 |
[32m[20221213 18:07:45 @agent_ppo2.py:185][0m |          -0.0176 |          47.8749 |           0.3200 |
[32m[20221213 18:07:45 @agent_ppo2.py:185][0m |          -0.0129 |          46.5760 |           0.3201 |
[32m[20221213 18:07:45 @agent_ppo2.py:185][0m |          -0.0155 |          45.3437 |           0.3200 |
[32m[20221213 18:07:45 @agent_ppo2.py:185][0m |          -0.0106 |          44.4613 |           0.3201 |
[32m[20221213 18:07:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:07:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.68
[32m[20221213 18:07:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.72
[32m[20221213 18:07:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 877.40
[32m[20221213 18:07:45 @agent_ppo2.py:143][0m Total time:       3.59 min
[32m[20221213 18:07:45 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 18:07:45 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 18:07:45 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:07:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:45 @agent_ppo2.py:185][0m |           0.0094 |          73.6431 |           0.3146 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0031 |          67.6073 |           0.3141 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0090 |          66.2239 |           0.3140 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0055 |          65.4380 |           0.3140 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0096 |          64.2102 |           0.3140 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0114 |          63.3508 |           0.3143 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0137 |          62.7520 |           0.3142 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0154 |          62.2901 |           0.3142 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0159 |          61.7139 |           0.3143 |
[32m[20221213 18:07:46 @agent_ppo2.py:185][0m |          -0.0172 |          61.2190 |           0.3144 |
[32m[20221213 18:07:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:07:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.25
[32m[20221213 18:07:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.38
[32m[20221213 18:07:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.82
[32m[20221213 18:07:46 @agent_ppo2.py:143][0m Total time:       3.61 min
[32m[20221213 18:07:46 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 18:07:46 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 18:07:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0039 |          61.6173 |           0.3220 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0089 |          57.2339 |           0.3213 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0075 |          54.6124 |           0.3213 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0084 |          52.9006 |           0.3212 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0052 |          53.5439 |           0.3211 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0077 |          51.3674 |           0.3212 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |          -0.0116 |          49.4150 |           0.3212 |
[32m[20221213 18:07:47 @agent_ppo2.py:185][0m |           0.0024 |          52.0806 |           0.3210 |
[32m[20221213 18:07:48 @agent_ppo2.py:185][0m |          -0.0153 |          47.6477 |           0.3213 |
[32m[20221213 18:07:48 @agent_ppo2.py:185][0m |          -0.0109 |          46.9883 |           0.3212 |
[32m[20221213 18:07:48 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 18:07:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.00
[32m[20221213 18:07:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.77
[32m[20221213 18:07:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.78
[32m[20221213 18:07:48 @agent_ppo2.py:143][0m Total time:       3.63 min
[32m[20221213 18:07:48 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 18:07:48 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 18:07:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:48 @agent_ppo2.py:185][0m |          -0.0009 |          69.1595 |           0.3124 |
[32m[20221213 18:07:48 @agent_ppo2.py:185][0m |          -0.0069 |          65.3743 |           0.3119 |
[32m[20221213 18:07:48 @agent_ppo2.py:185][0m |           0.0119 |          72.2962 |           0.3116 |
[32m[20221213 18:07:48 @agent_ppo2.py:185][0m |          -0.0084 |          62.7098 |           0.3115 |
[32m[20221213 18:07:49 @agent_ppo2.py:185][0m |          -0.0118 |          61.9131 |           0.3113 |
[32m[20221213 18:07:49 @agent_ppo2.py:185][0m |          -0.0128 |          61.1441 |           0.3113 |
[32m[20221213 18:07:49 @agent_ppo2.py:185][0m |          -0.0136 |          60.5089 |           0.3112 |
[32m[20221213 18:07:49 @agent_ppo2.py:185][0m |          -0.0096 |          62.2334 |           0.3109 |
[32m[20221213 18:07:49 @agent_ppo2.py:185][0m |          -0.0167 |          59.6321 |           0.3108 |
[32m[20221213 18:07:49 @agent_ppo2.py:185][0m |          -0.0186 |          59.3057 |           0.3107 |
[32m[20221213 18:07:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:07:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.61
[32m[20221213 18:07:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.96
[32m[20221213 18:07:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.52
[32m[20221213 18:07:49 @agent_ppo2.py:143][0m Total time:       3.66 min
[32m[20221213 18:07:49 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 18:07:49 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 18:07:49 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:07:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |           0.0016 |          72.9618 |           0.3098 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0059 |          70.0259 |           0.3094 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0062 |          68.5205 |           0.3095 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0093 |          67.4036 |           0.3093 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0097 |          66.6099 |           0.3091 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0111 |          65.9802 |           0.3091 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0104 |          65.3335 |           0.3092 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0133 |          64.7414 |           0.3092 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0184 |          64.6008 |           0.3093 |
[32m[20221213 18:07:50 @agent_ppo2.py:185][0m |          -0.0099 |          64.4060 |           0.3091 |
[32m[20221213 18:07:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:07:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.24
[32m[20221213 18:07:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.93
[32m[20221213 18:07:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.29
[32m[20221213 18:07:50 @agent_ppo2.py:143][0m Total time:       3.68 min
[32m[20221213 18:07:50 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 18:07:50 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 18:07:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0006 |          68.9930 |           0.3126 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0012 |          66.8178 |           0.3125 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0107 |          65.2280 |           0.3126 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0093 |          64.2683 |           0.3127 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0106 |          63.6581 |           0.3129 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0152 |          63.0281 |           0.3129 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0102 |          62.6070 |           0.3129 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0201 |          62.1954 |           0.3131 |
[32m[20221213 18:07:51 @agent_ppo2.py:185][0m |          -0.0126 |          61.6492 |           0.3131 |
[32m[20221213 18:07:52 @agent_ppo2.py:185][0m |          -0.0153 |          60.9773 |           0.3132 |
[32m[20221213 18:07:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:07:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.81
[32m[20221213 18:07:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.50
[32m[20221213 18:07:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.74
[32m[20221213 18:07:52 @agent_ppo2.py:143][0m Total time:       3.70 min
[32m[20221213 18:07:52 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 18:07:52 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 18:07:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:52 @agent_ppo2.py:185][0m |           0.0012 |          65.3941 |           0.3201 |
[32m[20221213 18:07:52 @agent_ppo2.py:185][0m |          -0.0081 |          61.6355 |           0.3199 |
[32m[20221213 18:07:52 @agent_ppo2.py:185][0m |          -0.0099 |          59.4309 |           0.3195 |
[32m[20221213 18:07:52 @agent_ppo2.py:185][0m |          -0.0151 |          58.1741 |           0.3193 |
[32m[20221213 18:07:53 @agent_ppo2.py:185][0m |          -0.0151 |          57.1715 |           0.3190 |
[32m[20221213 18:07:53 @agent_ppo2.py:185][0m |          -0.0121 |          56.2401 |           0.3189 |
[32m[20221213 18:07:53 @agent_ppo2.py:185][0m |          -0.0099 |          55.7042 |           0.3185 |
[32m[20221213 18:07:53 @agent_ppo2.py:185][0m |          -0.0173 |          54.7335 |           0.3187 |
[32m[20221213 18:07:53 @agent_ppo2.py:185][0m |          -0.0199 |          54.1779 |           0.3184 |
[32m[20221213 18:07:53 @agent_ppo2.py:185][0m |          -0.0177 |          53.3397 |           0.3186 |
[32m[20221213 18:07:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 18:07:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.86
[32m[20221213 18:07:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.64
[32m[20221213 18:07:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.94
[32m[20221213 18:07:53 @agent_ppo2.py:143][0m Total time:       3.72 min
[32m[20221213 18:07:53 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 18:07:53 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 18:07:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0023 |          68.1845 |           0.3194 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0027 |          64.1848 |           0.3191 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0073 |          61.9627 |           0.3187 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0125 |          60.2216 |           0.3182 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0089 |          58.0658 |           0.3179 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0153 |          56.5786 |           0.3174 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0094 |          55.4018 |           0.3175 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0070 |          54.7954 |           0.3173 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |           0.0055 |          56.7389 |           0.3172 |
[32m[20221213 18:07:54 @agent_ppo2.py:185][0m |          -0.0114 |          54.1536 |           0.3168 |
[32m[20221213 18:07:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:07:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.84
[32m[20221213 18:07:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.17
[32m[20221213 18:07:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.75
[32m[20221213 18:07:55 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221213 18:07:55 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 18:07:55 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 18:07:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |           0.0055 |          82.6208 |           0.3077 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0045 |          73.3352 |           0.3075 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0064 |          70.8177 |           0.3073 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0110 |          69.0500 |           0.3075 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0021 |          71.7051 |           0.3076 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0148 |          66.6025 |           0.3075 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0058 |          65.7887 |           0.3075 |
[32m[20221213 18:07:55 @agent_ppo2.py:185][0m |          -0.0102 |          64.7391 |           0.3076 |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |          -0.0103 |          63.6200 |           0.3076 |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |          -0.0156 |          62.2870 |           0.3076 |
[32m[20221213 18:07:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:07:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.42
[32m[20221213 18:07:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.86
[32m[20221213 18:07:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.65
[32m[20221213 18:07:56 @agent_ppo2.py:143][0m Total time:       3.77 min
[32m[20221213 18:07:56 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 18:07:56 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 18:07:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |           0.0033 |          74.7107 |           0.3090 |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |          -0.0059 |          67.8666 |           0.3081 |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |          -0.0113 |          65.3680 |           0.3075 |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |          -0.0095 |          64.4684 |           0.3069 |
[32m[20221213 18:07:56 @agent_ppo2.py:185][0m |          -0.0131 |          62.4936 |           0.3065 |
[32m[20221213 18:07:57 @agent_ppo2.py:185][0m |          -0.0141 |          61.0719 |           0.3064 |
[32m[20221213 18:07:57 @agent_ppo2.py:185][0m |          -0.0157 |          60.1365 |           0.3063 |
[32m[20221213 18:07:57 @agent_ppo2.py:185][0m |          -0.0148 |          58.9808 |           0.3062 |
[32m[20221213 18:07:57 @agent_ppo2.py:185][0m |          -0.0179 |          58.1735 |           0.3061 |
[32m[20221213 18:07:57 @agent_ppo2.py:185][0m |          -0.0168 |          57.4466 |           0.3063 |
[32m[20221213 18:07:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:07:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.03
[32m[20221213 18:07:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.73
[32m[20221213 18:07:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.92
[32m[20221213 18:07:57 @agent_ppo2.py:143][0m Total time:       3.79 min
[32m[20221213 18:07:57 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 18:07:57 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 18:07:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:57 @agent_ppo2.py:185][0m |           0.0219 |          81.0628 |           0.3082 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0068 |          65.7629 |           0.3076 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0097 |          63.6704 |           0.3078 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0046 |          63.3364 |           0.3078 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0125 |          61.8345 |           0.3079 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0135 |          61.2141 |           0.3080 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0065 |          62.5419 |           0.3084 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0181 |          60.0680 |           0.3086 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0182 |          59.7083 |           0.3086 |
[32m[20221213 18:07:58 @agent_ppo2.py:185][0m |          -0.0143 |          59.2909 |           0.3086 |
[32m[20221213 18:07:58 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:07:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.53
[32m[20221213 18:07:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.66
[32m[20221213 18:07:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.75
[32m[20221213 18:07:58 @agent_ppo2.py:143][0m Total time:       3.81 min
[32m[20221213 18:07:58 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 18:07:58 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 18:07:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:07:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0037 |          65.5480 |           0.3180 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0047 |          62.8604 |           0.3182 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0098 |          61.0543 |           0.3182 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0083 |          59.9376 |           0.3183 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0124 |          58.7417 |           0.3182 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0127 |          57.7149 |           0.3184 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0159 |          57.0284 |           0.3185 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0169 |          56.1425 |           0.3185 |
[32m[20221213 18:07:59 @agent_ppo2.py:185][0m |          -0.0147 |          55.3419 |           0.3185 |
[32m[20221213 18:08:00 @agent_ppo2.py:185][0m |          -0.0084 |          55.4351 |           0.3187 |
[32m[20221213 18:08:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:08:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.15
[32m[20221213 18:08:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.22
[32m[20221213 18:08:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.78
[32m[20221213 18:08:00 @agent_ppo2.py:143][0m Total time:       3.83 min
[32m[20221213 18:08:00 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 18:08:00 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 18:08:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:08:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:00 @agent_ppo2.py:185][0m |           0.0002 |          69.5505 |           0.3278 |
[32m[20221213 18:08:00 @agent_ppo2.py:185][0m |          -0.0046 |          62.7115 |           0.3273 |
[32m[20221213 18:08:00 @agent_ppo2.py:185][0m |          -0.0015 |          60.4014 |           0.3273 |
[32m[20221213 18:08:00 @agent_ppo2.py:185][0m |          -0.0048 |          56.6167 |           0.3273 |
[32m[20221213 18:08:00 @agent_ppo2.py:185][0m |          -0.0065 |          54.8938 |           0.3271 |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0091 |          53.5513 |           0.3272 |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0103 |          52.6493 |           0.3271 |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0104 |          51.8570 |           0.3272 |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0075 |          50.9573 |           0.3272 |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0120 |          50.4063 |           0.3273 |
[32m[20221213 18:08:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:08:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.31
[32m[20221213 18:08:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.50
[32m[20221213 18:08:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.90
[32m[20221213 18:08:01 @agent_ppo2.py:143][0m Total time:       3.85 min
[32m[20221213 18:08:01 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 18:08:01 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 18:08:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:08:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0030 |          71.4930 |           0.3239 |
[32m[20221213 18:08:01 @agent_ppo2.py:185][0m |          -0.0119 |          67.8782 |           0.3236 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0057 |          66.4612 |           0.3234 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0114 |          65.3598 |           0.3232 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0087 |          64.8761 |           0.3232 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0077 |          65.6951 |           0.3231 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0132 |          63.6226 |           0.3231 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0132 |          63.0129 |           0.3230 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0097 |          62.3919 |           0.3231 |
[32m[20221213 18:08:02 @agent_ppo2.py:185][0m |          -0.0143 |          61.6964 |           0.3231 |
[32m[20221213 18:08:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:08:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.83
[32m[20221213 18:08:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.13
[32m[20221213 18:08:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.06
[32m[20221213 18:08:02 @agent_ppo2.py:143][0m Total time:       3.88 min
[32m[20221213 18:08:02 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 18:08:02 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 18:08:03 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:08:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0000 |          67.5671 |           0.3223 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0044 |          64.7695 |           0.3219 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0098 |          62.9834 |           0.3218 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0103 |          60.9888 |           0.3216 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0108 |          60.1745 |           0.3217 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0102 |          58.1229 |           0.3219 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0106 |          57.3850 |           0.3219 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0145 |          56.0772 |           0.3220 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0161 |          55.3076 |           0.3221 |
[32m[20221213 18:08:03 @agent_ppo2.py:185][0m |          -0.0142 |          54.6497 |           0.3221 |
[32m[20221213 18:08:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:08:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.80
[32m[20221213 18:08:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.96
[32m[20221213 18:08:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.34
[32m[20221213 18:08:04 @agent_ppo2.py:143][0m Total time:       3.90 min
[32m[20221213 18:08:04 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 18:08:04 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 18:08:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:08:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0032 |          73.8285 |           0.3353 |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0049 |          69.7347 |           0.3349 |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0085 |          67.4703 |           0.3343 |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0068 |          66.3597 |           0.3342 |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0111 |          64.6639 |           0.3340 |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0145 |          63.5315 |           0.3338 |
[32m[20221213 18:08:04 @agent_ppo2.py:185][0m |          -0.0141 |          62.6202 |           0.3337 |
[32m[20221213 18:08:05 @agent_ppo2.py:185][0m |          -0.0155 |          61.8297 |           0.3333 |
[32m[20221213 18:08:05 @agent_ppo2.py:185][0m |          -0.0128 |          60.7090 |           0.3334 |
[32m[20221213 18:08:05 @agent_ppo2.py:185][0m |          -0.0165 |          60.1769 |           0.3335 |
[32m[20221213 18:08:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:08:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.74
[32m[20221213 18:08:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.17
[32m[20221213 18:08:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.46
[32m[20221213 18:08:05 @agent_ppo2.py:143][0m Total time:       3.92 min
[32m[20221213 18:08:05 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 18:08:05 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 18:08:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:08:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:05 @agent_ppo2.py:185][0m |          -0.0013 |          59.0813 |           0.3319 |
[32m[20221213 18:08:05 @agent_ppo2.py:185][0m |          -0.0047 |          54.1467 |           0.3316 |
[32m[20221213 18:08:05 @agent_ppo2.py:185][0m |          -0.0049 |          52.9141 |           0.3317 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0121 |          50.0930 |           0.3319 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0107 |          48.7991 |           0.3321 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0146 |          47.5942 |           0.3320 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0152 |          46.5241 |           0.3322 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0148 |          45.9879 |           0.3325 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0163 |          44.9946 |           0.3325 |
[32m[20221213 18:08:06 @agent_ppo2.py:185][0m |          -0.0187 |          44.1131 |           0.3326 |
[32m[20221213 18:08:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:08:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.90
[32m[20221213 18:08:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.78
[32m[20221213 18:08:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.08
[32m[20221213 18:08:06 @agent_ppo2.py:143][0m Total time:       3.94 min
[32m[20221213 18:08:06 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 18:08:06 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 18:08:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:08:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0016 |          79.1027 |           0.3354 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0094 |          74.8670 |           0.3346 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0102 |          72.4407 |           0.3340 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0102 |          70.6319 |           0.3340 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0064 |          70.7810 |           0.3338 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0116 |          67.2901 |           0.3337 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0157 |          65.9125 |           0.3335 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0156 |          64.6451 |           0.3336 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0139 |          63.3094 |           0.3334 |
[32m[20221213 18:08:07 @agent_ppo2.py:185][0m |          -0.0123 |          62.9612 |           0.3333 |
[32m[20221213 18:08:07 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 18:08:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.81
[32m[20221213 18:08:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.82
[32m[20221213 18:08:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.35
[32m[20221213 18:08:08 @agent_ppo2.py:143][0m Total time:       3.97 min
[32m[20221213 18:08:08 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 18:08:08 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 18:08:08 @agent_ppo2.py:127][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 18:08:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:08 @agent_ppo2.py:185][0m |          -0.0014 |          75.2743 |           0.3310 |
[32m[20221213 18:08:08 @agent_ppo2.py:185][0m |           0.0011 |          76.0059 |           0.3304 |
[32m[20221213 18:08:08 @agent_ppo2.py:185][0m |          -0.0073 |          70.5883 |           0.3296 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0084 |          69.4509 |           0.3297 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0100 |          68.5950 |           0.3296 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0082 |          67.7588 |           0.3294 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0089 |          67.3495 |           0.3295 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0151 |          66.7733 |           0.3297 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0141 |          66.1471 |           0.3295 |
[32m[20221213 18:08:09 @agent_ppo2.py:185][0m |          -0.0127 |          65.7311 |           0.3295 |
[32m[20221213 18:08:09 @agent_ppo2.py:130][0m Policy update time: 1.33 s
[32m[20221213 18:08:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.97
[32m[20221213 18:08:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.87
[32m[20221213 18:08:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.83
[32m[20221213 18:08:10 @agent_ppo2.py:143][0m Total time:       4.00 min
[32m[20221213 18:08:10 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 18:08:10 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 18:08:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:10 @agent_ppo2.py:185][0m |          -0.0014 |          71.4496 |           0.3281 |
[32m[20221213 18:08:10 @agent_ppo2.py:185][0m |           0.0129 |          75.7769 |           0.3275 |
[32m[20221213 18:08:10 @agent_ppo2.py:185][0m |          -0.0054 |          64.0906 |           0.3275 |
[32m[20221213 18:08:10 @agent_ppo2.py:185][0m |          -0.0095 |          62.0852 |           0.3276 |
[32m[20221213 18:08:10 @agent_ppo2.py:185][0m |          -0.0118 |          61.2438 |           0.3275 |
[32m[20221213 18:08:10 @agent_ppo2.py:185][0m |          -0.0115 |          61.1913 |           0.3276 |
[32m[20221213 18:08:11 @agent_ppo2.py:185][0m |          -0.0130 |          60.0572 |           0.3272 |
[32m[20221213 18:08:11 @agent_ppo2.py:185][0m |          -0.0152 |          59.3852 |           0.3271 |
[32m[20221213 18:08:11 @agent_ppo2.py:185][0m |          -0.0141 |          58.7686 |           0.3270 |
[32m[20221213 18:08:11 @agent_ppo2.py:185][0m |          -0.0142 |          58.4264 |           0.3270 |
[32m[20221213 18:08:11 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.76
[32m[20221213 18:08:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.21
[32m[20221213 18:08:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.56
[32m[20221213 18:08:11 @agent_ppo2.py:143][0m Total time:       4.02 min
[32m[20221213 18:08:11 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 18:08:11 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 18:08:11 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:08:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:11 @agent_ppo2.py:185][0m |          -0.0040 |          67.1785 |           0.3300 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0048 |          62.7967 |           0.3295 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0120 |          60.8358 |           0.3292 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |           0.0020 |          61.2011 |           0.3293 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0110 |          56.4745 |           0.3286 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0095 |          54.8546 |           0.3288 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0151 |          53.1903 |           0.3287 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0110 |          54.1455 |           0.3286 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0158 |          50.6152 |           0.3285 |
[32m[20221213 18:08:12 @agent_ppo2.py:185][0m |          -0.0041 |          53.4411 |           0.3287 |
[32m[20221213 18:08:12 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.37
[32m[20221213 18:08:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.64
[32m[20221213 18:08:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 620.49
[32m[20221213 18:08:13 @agent_ppo2.py:143][0m Total time:       4.05 min
[32m[20221213 18:08:13 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 18:08:13 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 18:08:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:13 @agent_ppo2.py:185][0m |           0.0003 |          69.7694 |           0.3346 |
[32m[20221213 18:08:13 @agent_ppo2.py:185][0m |          -0.0075 |          64.6784 |           0.3344 |
[32m[20221213 18:08:13 @agent_ppo2.py:185][0m |          -0.0078 |          62.6109 |           0.3342 |
[32m[20221213 18:08:13 @agent_ppo2.py:185][0m |          -0.0044 |          63.4995 |           0.3341 |
[32m[20221213 18:08:13 @agent_ppo2.py:185][0m |          -0.0052 |          62.3828 |           0.3337 |
[32m[20221213 18:08:14 @agent_ppo2.py:185][0m |          -0.0096 |          59.5897 |           0.3338 |
[32m[20221213 18:08:14 @agent_ppo2.py:185][0m |          -0.0142 |          58.2928 |           0.3336 |
[32m[20221213 18:08:14 @agent_ppo2.py:185][0m |          -0.0132 |          57.6208 |           0.3333 |
[32m[20221213 18:08:14 @agent_ppo2.py:185][0m |          -0.0141 |          56.9185 |           0.3332 |
[32m[20221213 18:08:14 @agent_ppo2.py:185][0m |          -0.0156 |          56.3412 |           0.3331 |
[32m[20221213 18:08:14 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.51
[32m[20221213 18:08:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.99
[32m[20221213 18:08:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.19
[32m[20221213 18:08:14 @agent_ppo2.py:143][0m Total time:       4.07 min
[32m[20221213 18:08:14 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 18:08:14 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 18:08:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0040 |          77.3693 |           0.3179 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0056 |          71.2073 |           0.3178 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0087 |          68.6308 |           0.3178 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0140 |          66.2791 |           0.3175 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0137 |          64.6976 |           0.3176 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0151 |          63.5207 |           0.3175 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0150 |          62.5816 |           0.3174 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0146 |          61.7491 |           0.3175 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0166 |          60.9548 |           0.3175 |
[32m[20221213 18:08:15 @agent_ppo2.py:185][0m |          -0.0191 |          60.2243 |           0.3172 |
[32m[20221213 18:08:15 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 18:08:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.54
[32m[20221213 18:08:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.83
[32m[20221213 18:08:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 575.17
[32m[20221213 18:08:16 @agent_ppo2.py:143][0m Total time:       4.10 min
[32m[20221213 18:08:16 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 18:08:16 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 18:08:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:16 @agent_ppo2.py:185][0m |           0.0005 |          69.6016 |           0.3298 |
[32m[20221213 18:08:16 @agent_ppo2.py:185][0m |          -0.0003 |          66.2281 |           0.3300 |
[32m[20221213 18:08:16 @agent_ppo2.py:185][0m |          -0.0099 |          61.8911 |           0.3296 |
[32m[20221213 18:08:16 @agent_ppo2.py:185][0m |          -0.0120 |          59.0973 |           0.3292 |
[32m[20221213 18:08:16 @agent_ppo2.py:185][0m |           0.0007 |          58.4959 |           0.3292 |
[32m[20221213 18:08:17 @agent_ppo2.py:185][0m |          -0.0136 |          55.1572 |           0.3289 |
[32m[20221213 18:08:17 @agent_ppo2.py:185][0m |          -0.0140 |          53.5469 |           0.3289 |
[32m[20221213 18:08:17 @agent_ppo2.py:185][0m |          -0.0127 |          52.1386 |           0.3286 |
[32m[20221213 18:08:17 @agent_ppo2.py:185][0m |          -0.0078 |          51.4430 |           0.3288 |
[32m[20221213 18:08:17 @agent_ppo2.py:185][0m |          -0.0153 |          49.5573 |           0.3285 |
[32m[20221213 18:08:17 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.42
[32m[20221213 18:08:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.47
[32m[20221213 18:08:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.15
[32m[20221213 18:08:17 @agent_ppo2.py:143][0m Total time:       4.12 min
[32m[20221213 18:08:17 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 18:08:17 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 18:08:17 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 18:08:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0046 |          79.5674 |           0.3317 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0065 |          73.8421 |           0.3318 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0089 |          70.4951 |           0.3317 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0137 |          68.1622 |           0.3320 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0133 |          66.6206 |           0.3321 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0147 |          65.6561 |           0.3324 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0143 |          64.4041 |           0.3327 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0151 |          63.7273 |           0.3328 |
[32m[20221213 18:08:18 @agent_ppo2.py:185][0m |          -0.0143 |          62.9225 |           0.3328 |
[32m[20221213 18:08:19 @agent_ppo2.py:185][0m |          -0.0170 |          62.1670 |           0.3330 |
[32m[20221213 18:08:19 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.45
[32m[20221213 18:08:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.61
[32m[20221213 18:08:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.20
[32m[20221213 18:08:19 @agent_ppo2.py:143][0m Total time:       4.15 min
[32m[20221213 18:08:19 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 18:08:19 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 18:08:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:19 @agent_ppo2.py:185][0m |           0.0007 |          82.8766 |           0.3273 |
[32m[20221213 18:08:19 @agent_ppo2.py:185][0m |          -0.0092 |          76.3771 |           0.3265 |
[32m[20221213 18:08:19 @agent_ppo2.py:185][0m |          -0.0100 |          73.4462 |           0.3258 |
[32m[20221213 18:08:19 @agent_ppo2.py:185][0m |          -0.0155 |          71.5869 |           0.3258 |
[32m[20221213 18:08:20 @agent_ppo2.py:185][0m |          -0.0195 |          70.1528 |           0.3256 |
[32m[20221213 18:08:20 @agent_ppo2.py:185][0m |          -0.0164 |          69.0429 |           0.3255 |
[32m[20221213 18:08:20 @agent_ppo2.py:185][0m |          -0.0110 |          67.9358 |           0.3252 |
[32m[20221213 18:08:20 @agent_ppo2.py:185][0m |          -0.0164 |          67.0308 |           0.3253 |
[32m[20221213 18:08:20 @agent_ppo2.py:185][0m |          -0.0148 |          66.0983 |           0.3251 |
[32m[20221213 18:08:20 @agent_ppo2.py:185][0m |          -0.0180 |          64.8130 |           0.3251 |
[32m[20221213 18:08:20 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.61
[32m[20221213 18:08:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.55
[32m[20221213 18:08:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.90
[32m[20221213 18:08:20 @agent_ppo2.py:143][0m Total time:       4.18 min
[32m[20221213 18:08:20 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 18:08:20 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 18:08:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0031 |          72.8787 |           0.3234 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0060 |          69.1328 |           0.3228 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |           0.0002 |          69.7553 |           0.3227 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0095 |          65.7350 |           0.3224 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0076 |          64.2035 |           0.3223 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0108 |          63.2110 |           0.3222 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0130 |          62.9980 |           0.3221 |
[32m[20221213 18:08:21 @agent_ppo2.py:185][0m |          -0.0121 |          61.2580 |           0.3223 |
[32m[20221213 18:08:22 @agent_ppo2.py:185][0m |          -0.0145 |          60.5451 |           0.3220 |
[32m[20221213 18:08:22 @agent_ppo2.py:185][0m |          -0.0109 |          60.2429 |           0.3221 |
[32m[20221213 18:08:22 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.79
[32m[20221213 18:08:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.08
[32m[20221213 18:08:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.16
[32m[20221213 18:08:22 @agent_ppo2.py:143][0m Total time:       4.20 min
[32m[20221213 18:08:22 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 18:08:22 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 18:08:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:22 @agent_ppo2.py:185][0m |           0.0001 |          72.3596 |           0.3321 |
[32m[20221213 18:08:22 @agent_ppo2.py:185][0m |          -0.0053 |          68.2003 |           0.3321 |
[32m[20221213 18:08:22 @agent_ppo2.py:185][0m |          -0.0080 |          66.1183 |           0.3320 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0143 |          65.2746 |           0.3318 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0091 |          63.6352 |           0.3318 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0105 |          63.1656 |           0.3319 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0002 |          65.1627 |           0.3318 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0118 |          60.6129 |           0.3322 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0090 |          60.4483 |           0.3321 |
[32m[20221213 18:08:23 @agent_ppo2.py:185][0m |          -0.0133 |          59.2743 |           0.3321 |
[32m[20221213 18:08:23 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:08:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.04
[32m[20221213 18:08:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.71
[32m[20221213 18:08:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.14
[32m[20221213 18:08:23 @agent_ppo2.py:143][0m Total time:       4.23 min
[32m[20221213 18:08:23 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 18:08:23 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 18:08:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0029 |          82.7781 |           0.3355 |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0014 |          79.2538 |           0.3348 |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0088 |          77.1405 |           0.3340 |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0046 |          76.1858 |           0.3333 |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0119 |          74.9858 |           0.3331 |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0107 |          74.1339 |           0.3329 |
[32m[20221213 18:08:24 @agent_ppo2.py:185][0m |          -0.0127 |          73.8358 |           0.3327 |
[32m[20221213 18:08:25 @agent_ppo2.py:185][0m |          -0.0127 |          73.1403 |           0.3324 |
[32m[20221213 18:08:25 @agent_ppo2.py:185][0m |          -0.0148 |          72.7695 |           0.3325 |
[32m[20221213 18:08:25 @agent_ppo2.py:185][0m |           0.0008 |          79.1375 |           0.3325 |
[32m[20221213 18:08:25 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.07
[32m[20221213 18:08:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.76
[32m[20221213 18:08:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.92
[32m[20221213 18:08:25 @agent_ppo2.py:143][0m Total time:       4.25 min
[32m[20221213 18:08:25 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 18:08:25 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 18:08:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:25 @agent_ppo2.py:185][0m |          -0.0009 |          77.2791 |           0.3280 |
[32m[20221213 18:08:25 @agent_ppo2.py:185][0m |           0.0016 |          74.8489 |           0.3279 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0104 |          70.2017 |           0.3274 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0085 |          68.9417 |           0.3279 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0103 |          68.0554 |           0.3279 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0132 |          67.0360 |           0.3279 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0062 |          67.2601 |           0.3283 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0129 |          65.5562 |           0.3283 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0147 |          64.8061 |           0.3284 |
[32m[20221213 18:08:26 @agent_ppo2.py:185][0m |          -0.0163 |          64.3915 |           0.3287 |
[32m[20221213 18:08:26 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.50
[32m[20221213 18:08:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.25
[32m[20221213 18:08:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.72
[32m[20221213 18:08:26 @agent_ppo2.py:143][0m Total time:       4.28 min
[32m[20221213 18:08:26 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 18:08:26 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 18:08:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0018 |          73.8095 |           0.3323 |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0042 |          70.7079 |           0.3320 |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0074 |          69.2691 |           0.3319 |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0097 |          67.6632 |           0.3318 |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0025 |          68.5826 |           0.3321 |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0103 |          65.4186 |           0.3319 |
[32m[20221213 18:08:27 @agent_ppo2.py:185][0m |          -0.0115 |          64.1928 |           0.3320 |
[32m[20221213 18:08:28 @agent_ppo2.py:185][0m |          -0.0137 |          63.3448 |           0.3321 |
[32m[20221213 18:08:28 @agent_ppo2.py:185][0m |          -0.0145 |          62.3587 |           0.3323 |
[32m[20221213 18:08:28 @agent_ppo2.py:185][0m |          -0.0114 |          61.4198 |           0.3324 |
[32m[20221213 18:08:28 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.01
[32m[20221213 18:08:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.95
[32m[20221213 18:08:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.91
[32m[20221213 18:08:28 @agent_ppo2.py:143][0m Total time:       4.30 min
[32m[20221213 18:08:28 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 18:08:28 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 18:08:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:28 @agent_ppo2.py:185][0m |          -0.0006 |          77.8649 |           0.3430 |
[32m[20221213 18:08:28 @agent_ppo2.py:185][0m |          -0.0033 |          71.7492 |           0.3426 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0102 |          67.8132 |           0.3421 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0065 |          63.8867 |           0.3421 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0024 |          62.7512 |           0.3421 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0054 |          60.5300 |           0.3419 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |           0.0026 |          62.8698 |           0.3417 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0123 |          54.8495 |           0.3417 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0144 |          53.2728 |           0.3415 |
[32m[20221213 18:08:29 @agent_ppo2.py:185][0m |          -0.0142 |          52.0941 |           0.3414 |
[32m[20221213 18:08:29 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:08:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.94
[32m[20221213 18:08:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.63
[32m[20221213 18:08:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.69
[32m[20221213 18:08:30 @agent_ppo2.py:143][0m Total time:       4.33 min
[32m[20221213 18:08:30 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 18:08:30 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 18:08:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:30 @agent_ppo2.py:185][0m |           0.0027 |          77.4177 |           0.3307 |
[32m[20221213 18:08:30 @agent_ppo2.py:185][0m |           0.0033 |          75.0604 |           0.3309 |
[32m[20221213 18:08:30 @agent_ppo2.py:185][0m |          -0.0062 |          71.2171 |           0.3309 |
[32m[20221213 18:08:30 @agent_ppo2.py:185][0m |          -0.0096 |          69.6579 |           0.3309 |
[32m[20221213 18:08:30 @agent_ppo2.py:185][0m |          -0.0107 |          68.2628 |           0.3310 |
[32m[20221213 18:08:30 @agent_ppo2.py:185][0m |          -0.0028 |          70.4141 |           0.3311 |
[32m[20221213 18:08:31 @agent_ppo2.py:185][0m |          -0.0026 |          70.3503 |           0.3311 |
[32m[20221213 18:08:31 @agent_ppo2.py:185][0m |          -0.0104 |          65.2727 |           0.3311 |
[32m[20221213 18:08:31 @agent_ppo2.py:185][0m |          -0.0138 |          64.3195 |           0.3312 |
[32m[20221213 18:08:31 @agent_ppo2.py:185][0m |          -0.0141 |          63.7673 |           0.3314 |
[32m[20221213 18:08:31 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.59
[32m[20221213 18:08:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.15
[32m[20221213 18:08:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.87
[32m[20221213 18:08:31 @agent_ppo2.py:143][0m Total time:       4.35 min
[32m[20221213 18:08:31 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 18:08:31 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 18:08:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:31 @agent_ppo2.py:185][0m |          -0.0019 |          73.3216 |           0.3248 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0058 |          68.9653 |           0.3242 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0070 |          65.1252 |           0.3238 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0028 |          63.7155 |           0.3235 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0097 |          59.8194 |           0.3231 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0112 |          57.7185 |           0.3231 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0116 |          55.8390 |           0.3229 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0096 |          54.6489 |           0.3228 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0171 |          53.4110 |           0.3227 |
[32m[20221213 18:08:32 @agent_ppo2.py:185][0m |          -0.0138 |          52.1507 |           0.3226 |
[32m[20221213 18:08:32 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.34
[32m[20221213 18:08:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.18
[32m[20221213 18:08:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.39
[32m[20221213 18:08:33 @agent_ppo2.py:143][0m Total time:       4.38 min
[32m[20221213 18:08:33 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 18:08:33 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 18:08:33 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:08:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:33 @agent_ppo2.py:185][0m |          -0.0015 |          79.8326 |           0.3313 |
[32m[20221213 18:08:33 @agent_ppo2.py:185][0m |          -0.0046 |          75.3373 |           0.3307 |
[32m[20221213 18:08:33 @agent_ppo2.py:185][0m |          -0.0078 |          72.9925 |           0.3303 |
[32m[20221213 18:08:33 @agent_ppo2.py:185][0m |          -0.0088 |          71.9404 |           0.3300 |
[32m[20221213 18:08:33 @agent_ppo2.py:185][0m |          -0.0024 |          73.0386 |           0.3298 |
[32m[20221213 18:08:34 @agent_ppo2.py:185][0m |          -0.0110 |          69.7811 |           0.3295 |
[32m[20221213 18:08:34 @agent_ppo2.py:185][0m |          -0.0094 |          69.0216 |           0.3294 |
[32m[20221213 18:08:34 @agent_ppo2.py:185][0m |          -0.0113 |          68.3372 |           0.3292 |
[32m[20221213 18:08:34 @agent_ppo2.py:185][0m |          -0.0056 |          69.8459 |           0.3290 |
[32m[20221213 18:08:34 @agent_ppo2.py:185][0m |          -0.0112 |          67.1485 |           0.3289 |
[32m[20221213 18:08:34 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.93
[32m[20221213 18:08:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.20
[32m[20221213 18:08:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.82
[32m[20221213 18:08:34 @agent_ppo2.py:143][0m Total time:       4.41 min
[32m[20221213 18:08:34 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 18:08:34 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 18:08:34 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:08:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |           0.0006 |          84.8016 |           0.3291 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0047 |          80.5683 |           0.3296 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0086 |          78.7144 |           0.3295 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0100 |          77.6869 |           0.3297 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0111 |          76.4729 |           0.3301 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0116 |          75.4906 |           0.3303 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0141 |          74.4384 |           0.3305 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0116 |          73.5195 |           0.3310 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |          -0.0115 |          73.0666 |           0.3309 |
[32m[20221213 18:08:35 @agent_ppo2.py:185][0m |           0.0018 |          78.1810 |           0.3314 |
[32m[20221213 18:08:35 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:08:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.19
[32m[20221213 18:08:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.17
[32m[20221213 18:08:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.53
[32m[20221213 18:08:36 @agent_ppo2.py:143][0m Total time:       4.43 min
[32m[20221213 18:08:36 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 18:08:36 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 18:08:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:36 @agent_ppo2.py:185][0m |           0.0000 |          77.1982 |           0.3397 |
[32m[20221213 18:08:36 @agent_ppo2.py:185][0m |          -0.0068 |          73.5907 |           0.3402 |
[32m[20221213 18:08:36 @agent_ppo2.py:185][0m |          -0.0081 |          72.8423 |           0.3406 |
[32m[20221213 18:08:36 @agent_ppo2.py:185][0m |          -0.0113 |          71.3358 |           0.3406 |
[32m[20221213 18:08:36 @agent_ppo2.py:185][0m |          -0.0124 |          70.4422 |           0.3409 |
[32m[20221213 18:08:37 @agent_ppo2.py:185][0m |          -0.0051 |          72.5261 |           0.3409 |
[32m[20221213 18:08:37 @agent_ppo2.py:185][0m |          -0.0092 |          69.3959 |           0.3412 |
[32m[20221213 18:08:37 @agent_ppo2.py:185][0m |          -0.0147 |          68.0929 |           0.3417 |
[32m[20221213 18:08:37 @agent_ppo2.py:185][0m |          -0.0081 |          71.4091 |           0.3416 |
[32m[20221213 18:08:37 @agent_ppo2.py:185][0m |          -0.0154 |          67.1895 |           0.3420 |
[32m[20221213 18:08:37 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.65
[32m[20221213 18:08:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.51
[32m[20221213 18:08:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.26
[32m[20221213 18:08:37 @agent_ppo2.py:143][0m Total time:       4.46 min
[32m[20221213 18:08:37 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 18:08:37 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 18:08:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0012 |          76.1711 |           0.3546 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0080 |          71.7599 |           0.3547 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0099 |          68.9796 |           0.3544 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0115 |          66.3339 |           0.3541 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0140 |          64.2255 |           0.3541 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0140 |          62.7673 |           0.3540 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0096 |          61.5599 |           0.3538 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0132 |          59.9380 |           0.3539 |
[32m[20221213 18:08:38 @agent_ppo2.py:185][0m |          -0.0172 |          58.9390 |           0.3538 |
[32m[20221213 18:08:39 @agent_ppo2.py:185][0m |          -0.0146 |          57.9148 |           0.3536 |
[32m[20221213 18:08:39 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.85
[32m[20221213 18:08:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.96
[32m[20221213 18:08:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.93
[32m[20221213 18:08:39 @agent_ppo2.py:143][0m Total time:       4.48 min
[32m[20221213 18:08:39 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 18:08:39 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 18:08:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:39 @agent_ppo2.py:185][0m |          -0.0053 |          78.1866 |           0.3470 |
[32m[20221213 18:08:39 @agent_ppo2.py:185][0m |           0.0027 |          75.2401 |           0.3470 |
[32m[20221213 18:08:39 @agent_ppo2.py:185][0m |          -0.0018 |          64.1267 |           0.3470 |
[32m[20221213 18:08:39 @agent_ppo2.py:185][0m |          -0.0101 |          58.1120 |           0.3472 |
[32m[20221213 18:08:40 @agent_ppo2.py:185][0m |          -0.0099 |          55.7604 |           0.3471 |
[32m[20221213 18:08:40 @agent_ppo2.py:185][0m |          -0.0123 |          54.2832 |           0.3471 |
[32m[20221213 18:08:40 @agent_ppo2.py:185][0m |          -0.0139 |          53.0705 |           0.3470 |
[32m[20221213 18:08:40 @agent_ppo2.py:185][0m |          -0.0168 |          52.2749 |           0.3469 |
[32m[20221213 18:08:40 @agent_ppo2.py:185][0m |          -0.0151 |          51.1996 |           0.3469 |
[32m[20221213 18:08:40 @agent_ppo2.py:185][0m |          -0.0163 |          50.5794 |           0.3468 |
[32m[20221213 18:08:40 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.92
[32m[20221213 18:08:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.02
[32m[20221213 18:08:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.15
[32m[20221213 18:08:40 @agent_ppo2.py:143][0m Total time:       4.51 min
[32m[20221213 18:08:40 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 18:08:40 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 18:08:41 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:08:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |           0.0066 |          83.2851 |           0.3398 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0078 |          74.2892 |           0.3395 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0076 |          70.7833 |           0.3390 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0084 |          69.5253 |           0.3389 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0089 |          67.6439 |           0.3386 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0138 |          66.6108 |           0.3387 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0122 |          65.5350 |           0.3384 |
[32m[20221213 18:08:41 @agent_ppo2.py:185][0m |          -0.0109 |          64.4803 |           0.3383 |
[32m[20221213 18:08:42 @agent_ppo2.py:185][0m |          -0.0076 |          65.4888 |           0.3383 |
[32m[20221213 18:08:42 @agent_ppo2.py:185][0m |          -0.0125 |          63.1154 |           0.3380 |
[32m[20221213 18:08:42 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.28
[32m[20221213 18:08:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 611.62
[32m[20221213 18:08:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 623.63
[32m[20221213 18:08:42 @agent_ppo2.py:143][0m Total time:       4.53 min
[32m[20221213 18:08:42 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 18:08:42 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 18:08:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:42 @agent_ppo2.py:185][0m |           0.0003 |          77.8758 |           0.3469 |
[32m[20221213 18:08:42 @agent_ppo2.py:185][0m |           0.0021 |          74.3480 |           0.3472 |
[32m[20221213 18:08:42 @agent_ppo2.py:185][0m |          -0.0084 |          68.2285 |           0.3472 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0109 |          64.7192 |           0.3476 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0048 |          63.7510 |           0.3474 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0018 |          65.5012 |           0.3477 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0065 |          59.4035 |           0.3477 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0137 |          57.9421 |           0.3478 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0147 |          56.9463 |           0.3479 |
[32m[20221213 18:08:43 @agent_ppo2.py:185][0m |          -0.0119 |          56.4235 |           0.3481 |
[32m[20221213 18:08:43 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.55
[32m[20221213 18:08:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.43
[32m[20221213 18:08:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.91
[32m[20221213 18:08:43 @agent_ppo2.py:143][0m Total time:       4.56 min
[32m[20221213 18:08:43 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 18:08:43 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 18:08:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |           0.0034 |          81.6255 |           0.3385 |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |           0.0020 |          79.3188 |           0.3383 |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |          -0.0079 |          74.1395 |           0.3383 |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |          -0.0099 |          73.4016 |           0.3383 |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |          -0.0117 |          71.8949 |           0.3386 |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |          -0.0116 |          70.9446 |           0.3388 |
[32m[20221213 18:08:44 @agent_ppo2.py:185][0m |          -0.0132 |          70.0252 |           0.3392 |
[32m[20221213 18:08:45 @agent_ppo2.py:185][0m |          -0.0123 |          69.1401 |           0.3393 |
[32m[20221213 18:08:45 @agent_ppo2.py:185][0m |          -0.0134 |          68.2590 |           0.3397 |
[32m[20221213 18:08:45 @agent_ppo2.py:185][0m |          -0.0124 |          67.7158 |           0.3396 |
[32m[20221213 18:08:45 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:08:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.91
[32m[20221213 18:08:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.32
[32m[20221213 18:08:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.77
[32m[20221213 18:08:45 @agent_ppo2.py:143][0m Total time:       4.59 min
[32m[20221213 18:08:45 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 18:08:45 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 18:08:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:45 @agent_ppo2.py:185][0m |          -0.0007 |          85.3349 |           0.3532 |
[32m[20221213 18:08:45 @agent_ppo2.py:185][0m |          -0.0079 |          78.8325 |           0.3531 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0026 |          77.2073 |           0.3528 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0119 |          74.1893 |           0.3527 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0156 |          72.5955 |           0.3528 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0109 |          71.2522 |           0.3526 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0124 |          69.7545 |           0.3525 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0129 |          68.7763 |           0.3526 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0085 |          67.7644 |           0.3527 |
[32m[20221213 18:08:46 @agent_ppo2.py:185][0m |          -0.0130 |          66.0719 |           0.3526 |
[32m[20221213 18:08:46 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:08:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.30
[32m[20221213 18:08:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.80
[32m[20221213 18:08:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 595.45
[32m[20221213 18:08:46 @agent_ppo2.py:143][0m Total time:       4.61 min
[32m[20221213 18:08:46 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 18:08:46 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 18:08:47 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:08:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:47 @agent_ppo2.py:185][0m |          -0.0023 |          83.1114 |           0.3458 |
[32m[20221213 18:08:47 @agent_ppo2.py:185][0m |          -0.0030 |          75.5947 |           0.3455 |
[32m[20221213 18:08:47 @agent_ppo2.py:185][0m |          -0.0100 |          73.2072 |           0.3453 |
[32m[20221213 18:08:47 @agent_ppo2.py:185][0m |          -0.0072 |          71.6003 |           0.3451 |
[32m[20221213 18:08:47 @agent_ppo2.py:185][0m |          -0.0050 |          71.9207 |           0.3453 |
[32m[20221213 18:08:47 @agent_ppo2.py:185][0m |          -0.0112 |          69.3144 |           0.3451 |
[32m[20221213 18:08:48 @agent_ppo2.py:185][0m |          -0.0133 |          67.7870 |           0.3448 |
[32m[20221213 18:08:48 @agent_ppo2.py:185][0m |          -0.0094 |          67.4739 |           0.3448 |
[32m[20221213 18:08:48 @agent_ppo2.py:185][0m |          -0.0140 |          65.8750 |           0.3446 |
[32m[20221213 18:08:48 @agent_ppo2.py:185][0m |          -0.0168 |          64.7022 |           0.3446 |
[32m[20221213 18:08:48 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:08:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.31
[32m[20221213 18:08:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.58
[32m[20221213 18:08:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.84
[32m[20221213 18:08:48 @agent_ppo2.py:143][0m Total time:       4.64 min
[32m[20221213 18:08:48 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 18:08:48 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 18:08:48 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:08:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:48 @agent_ppo2.py:185][0m |           0.0100 |          98.2031 |           0.3508 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0085 |          82.5718 |           0.3504 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0046 |          79.7559 |           0.3501 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0125 |          77.8515 |           0.3498 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0103 |          76.2889 |           0.3497 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0112 |          75.0195 |           0.3496 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0133 |          74.1821 |           0.3495 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0141 |          73.1514 |           0.3498 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0156 |          72.2027 |           0.3492 |
[32m[20221213 18:08:49 @agent_ppo2.py:185][0m |          -0.0142 |          71.8970 |           0.3495 |
[32m[20221213 18:08:49 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:08:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.98
[32m[20221213 18:08:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.76
[32m[20221213 18:08:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.53
[32m[20221213 18:08:49 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 18:08:49 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 18:08:49 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 18:08:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |           0.0005 |          79.8723 |           0.3516 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0081 |          76.0668 |           0.3513 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0105 |          73.9646 |           0.3512 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0143 |          72.5683 |           0.3512 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0166 |          71.4183 |           0.3511 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0163 |          70.2305 |           0.3513 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0142 |          69.6081 |           0.3512 |
[32m[20221213 18:08:50 @agent_ppo2.py:185][0m |          -0.0151 |          69.4820 |           0.3512 |
[32m[20221213 18:08:51 @agent_ppo2.py:185][0m |          -0.0199 |          68.3457 |           0.3511 |
[32m[20221213 18:08:51 @agent_ppo2.py:185][0m |          -0.0196 |          67.7484 |           0.3513 |
[32m[20221213 18:08:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:08:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.95
[32m[20221213 18:08:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.42
[32m[20221213 18:08:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.37
[32m[20221213 18:08:51 @agent_ppo2.py:143][0m Total time:       4.68 min
[32m[20221213 18:08:51 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 18:08:51 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 18:08:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:51 @agent_ppo2.py:185][0m |           0.0011 |          75.6153 |           0.3504 |
[32m[20221213 18:08:51 @agent_ppo2.py:185][0m |          -0.0071 |          72.6899 |           0.3496 |
[32m[20221213 18:08:51 @agent_ppo2.py:185][0m |           0.0030 |          74.8535 |           0.3495 |
[32m[20221213 18:08:51 @agent_ppo2.py:185][0m |          -0.0066 |          69.7236 |           0.3494 |
[32m[20221213 18:08:52 @agent_ppo2.py:185][0m |          -0.0113 |          68.1625 |           0.3491 |
[32m[20221213 18:08:52 @agent_ppo2.py:185][0m |          -0.0156 |          66.9721 |           0.3491 |
[32m[20221213 18:08:52 @agent_ppo2.py:185][0m |          -0.0136 |          66.0876 |           0.3489 |
[32m[20221213 18:08:52 @agent_ppo2.py:185][0m |          -0.0118 |          65.3795 |           0.3487 |
[32m[20221213 18:08:52 @agent_ppo2.py:185][0m |          -0.0162 |          64.2191 |           0.3487 |
[32m[20221213 18:08:52 @agent_ppo2.py:185][0m |          -0.0150 |          63.4081 |           0.3485 |
[32m[20221213 18:08:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:08:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.82
[32m[20221213 18:08:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.52
[32m[20221213 18:08:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.48
[32m[20221213 18:08:52 @agent_ppo2.py:143][0m Total time:       4.71 min
[32m[20221213 18:08:52 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 18:08:52 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 18:08:52 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:08:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0035 |          82.1660 |           0.3555 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0075 |          77.1873 |           0.3550 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0077 |          74.4466 |           0.3547 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0099 |          72.3456 |           0.3543 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0107 |          70.7497 |           0.3542 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0156 |          69.6835 |           0.3538 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0168 |          68.5931 |           0.3538 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0163 |          67.9189 |           0.3538 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0133 |          67.0272 |           0.3535 |
[32m[20221213 18:08:53 @agent_ppo2.py:185][0m |          -0.0160 |          65.9532 |           0.3535 |
[32m[20221213 18:08:53 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:08:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.00
[32m[20221213 18:08:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.24
[32m[20221213 18:08:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.72
[32m[20221213 18:08:54 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 960.72
[32m[20221213 18:08:54 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 960.72
[32m[20221213 18:08:54 @agent_ppo2.py:143][0m Total time:       4.73 min
[32m[20221213 18:08:54 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 18:08:54 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 18:08:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:08:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:54 @agent_ppo2.py:185][0m |           0.0027 |          78.5411 |           0.3488 |
[32m[20221213 18:08:54 @agent_ppo2.py:185][0m |          -0.0046 |          74.3077 |           0.3483 |
[32m[20221213 18:08:54 @agent_ppo2.py:185][0m |          -0.0072 |          72.2474 |           0.3477 |
[32m[20221213 18:08:54 @agent_ppo2.py:185][0m |          -0.0026 |          73.2093 |           0.3474 |
[32m[20221213 18:08:54 @agent_ppo2.py:185][0m |          -0.0092 |          69.8339 |           0.3474 |
[32m[20221213 18:08:55 @agent_ppo2.py:185][0m |          -0.0092 |          68.8874 |           0.3474 |
[32m[20221213 18:08:55 @agent_ppo2.py:185][0m |          -0.0160 |          68.2954 |           0.3473 |
[32m[20221213 18:08:55 @agent_ppo2.py:185][0m |          -0.0164 |          67.6091 |           0.3472 |
[32m[20221213 18:08:55 @agent_ppo2.py:185][0m |          -0.0034 |          73.6210 |           0.3471 |
[32m[20221213 18:08:55 @agent_ppo2.py:185][0m |          -0.0123 |          66.5055 |           0.3470 |
[32m[20221213 18:08:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:08:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.23
[32m[20221213 18:08:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.43
[32m[20221213 18:08:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.59
[32m[20221213 18:08:55 @agent_ppo2.py:143][0m Total time:       4.76 min
[32m[20221213 18:08:55 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 18:08:55 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 18:08:55 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:08:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:55 @agent_ppo2.py:185][0m |          -0.0007 |          82.2059 |           0.3457 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0053 |          78.9442 |           0.3455 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0069 |          77.3936 |           0.3455 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0082 |          76.3289 |           0.3454 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0097 |          75.5780 |           0.3456 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0110 |          74.5045 |           0.3455 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0114 |          73.8931 |           0.3455 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0135 |          73.4108 |           0.3454 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0133 |          72.5892 |           0.3454 |
[32m[20221213 18:08:56 @agent_ppo2.py:185][0m |          -0.0167 |          71.9285 |           0.3456 |
[32m[20221213 18:08:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:08:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.34
[32m[20221213 18:08:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.97
[32m[20221213 18:08:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.21
[32m[20221213 18:08:56 @agent_ppo2.py:143][0m Total time:       4.78 min
[32m[20221213 18:08:56 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 18:08:56 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 18:08:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:08:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |           0.0027 |          82.4400 |           0.3478 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0090 |          79.1973 |           0.3474 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0090 |          77.7774 |           0.3468 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0074 |          77.8113 |           0.3466 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0108 |          75.8779 |           0.3465 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0131 |          75.2341 |           0.3464 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0150 |          74.8971 |           0.3464 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0162 |          74.4178 |           0.3462 |
[32m[20221213 18:08:57 @agent_ppo2.py:185][0m |          -0.0116 |          75.5838 |           0.3461 |
[32m[20221213 18:08:58 @agent_ppo2.py:185][0m |          -0.0049 |          77.9810 |           0.3461 |
[32m[20221213 18:08:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:08:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.45
[32m[20221213 18:08:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.34
[32m[20221213 18:08:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.16
[32m[20221213 18:08:58 @agent_ppo2.py:143][0m Total time:       4.80 min
[32m[20221213 18:08:58 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 18:08:58 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 18:08:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:08:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:58 @agent_ppo2.py:185][0m |          -0.0007 |          82.7109 |           0.3496 |
[32m[20221213 18:08:58 @agent_ppo2.py:185][0m |          -0.0059 |          77.8865 |           0.3490 |
[32m[20221213 18:08:58 @agent_ppo2.py:185][0m |          -0.0126 |          75.1275 |           0.3486 |
[32m[20221213 18:08:58 @agent_ppo2.py:185][0m |          -0.0123 |          72.9176 |           0.3484 |
[32m[20221213 18:08:58 @agent_ppo2.py:185][0m |          -0.0089 |          71.4133 |           0.3482 |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |          -0.0141 |          69.5624 |           0.3485 |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |          -0.0136 |          68.4578 |           0.3486 |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |          -0.0161 |          67.2625 |           0.3486 |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |          -0.0143 |          66.1455 |           0.3483 |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |          -0.0068 |          68.4408 |           0.3485 |
[32m[20221213 18:08:59 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:08:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.99
[32m[20221213 18:08:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.07
[32m[20221213 18:08:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.71
[32m[20221213 18:08:59 @agent_ppo2.py:143][0m Total time:       4.82 min
[32m[20221213 18:08:59 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 18:08:59 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 18:08:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:08:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |           0.0030 |          83.6870 |           0.3516 |
[32m[20221213 18:08:59 @agent_ppo2.py:185][0m |          -0.0014 |          79.7188 |           0.3516 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0099 |          76.9172 |           0.3514 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0106 |          75.3973 |           0.3512 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0136 |          74.1875 |           0.3513 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0158 |          73.0965 |           0.3512 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0108 |          72.1429 |           0.3511 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0131 |          71.2464 |           0.3511 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0167 |          70.5717 |           0.3514 |
[32m[20221213 18:09:00 @agent_ppo2.py:185][0m |          -0.0122 |          69.8491 |           0.3514 |
[32m[20221213 18:09:00 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:09:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.01
[32m[20221213 18:09:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.90
[32m[20221213 18:09:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.16
[32m[20221213 18:09:00 @agent_ppo2.py:143][0m Total time:       4.84 min
[32m[20221213 18:09:00 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 18:09:00 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 18:09:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |           0.0008 |          89.3800 |           0.3586 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |           0.0016 |          90.2903 |           0.3578 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0082 |          84.0469 |           0.3575 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0110 |          82.1822 |           0.3576 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0006 |          91.1956 |           0.3577 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0127 |          80.4443 |           0.3576 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0122 |          80.0308 |           0.3579 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0151 |          79.4823 |           0.3579 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0161 |          78.8290 |           0.3582 |
[32m[20221213 18:09:01 @agent_ppo2.py:185][0m |          -0.0147 |          78.3925 |           0.3583 |
[32m[20221213 18:09:01 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.31
[32m[20221213 18:09:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.56
[32m[20221213 18:09:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.83
[32m[20221213 18:09:01 @agent_ppo2.py:143][0m Total time:       4.86 min
[32m[20221213 18:09:01 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 18:09:01 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 18:09:02 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:09:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0046 |          87.6168 |           0.3588 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0048 |          84.3817 |           0.3577 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0071 |          82.7908 |           0.3570 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0137 |          80.9246 |           0.3564 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0091 |          79.8914 |           0.3561 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0146 |          79.2589 |           0.3557 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |          -0.0168 |          78.1244 |           0.3555 |
[32m[20221213 18:09:02 @agent_ppo2.py:185][0m |           0.0042 |          89.2955 |           0.3551 |
[32m[20221213 18:09:03 @agent_ppo2.py:185][0m |          -0.0174 |          77.0325 |           0.3545 |
[32m[20221213 18:09:03 @agent_ppo2.py:185][0m |          -0.0139 |          76.3713 |           0.3542 |
[32m[20221213 18:09:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:09:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.82
[32m[20221213 18:09:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.89
[32m[20221213 18:09:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.47
[32m[20221213 18:09:03 @agent_ppo2.py:143][0m Total time:       4.88 min
[32m[20221213 18:09:03 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 18:09:03 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 18:09:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:03 @agent_ppo2.py:185][0m |           0.0047 |          85.1750 |           0.3552 |
[32m[20221213 18:09:03 @agent_ppo2.py:185][0m |           0.0022 |          86.1652 |           0.3551 |
[32m[20221213 18:09:03 @agent_ppo2.py:185][0m |          -0.0049 |          78.4608 |           0.3548 |
[32m[20221213 18:09:03 @agent_ppo2.py:185][0m |          -0.0113 |          74.7579 |           0.3546 |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |          -0.0147 |          72.4296 |           0.3547 |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |          -0.0154 |          70.4091 |           0.3552 |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |          -0.0136 |          68.8856 |           0.3550 |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |          -0.0172 |          68.1399 |           0.3549 |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |          -0.0154 |          66.9271 |           0.3549 |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |          -0.0201 |          66.0826 |           0.3549 |
[32m[20221213 18:09:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:09:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.64
[32m[20221213 18:09:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.56
[32m[20221213 18:09:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.69
[32m[20221213 18:09:04 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221213 18:09:04 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 18:09:04 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 18:09:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:04 @agent_ppo2.py:185][0m |           0.0141 |          90.8808 |           0.3575 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0038 |          78.6571 |           0.3565 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0058 |          76.4757 |           0.3567 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0093 |          74.7681 |           0.3565 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0110 |          73.1478 |           0.3564 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0110 |          71.9658 |           0.3564 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0101 |          71.2749 |           0.3563 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0067 |          72.5645 |           0.3563 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0124 |          69.8960 |           0.3563 |
[32m[20221213 18:09:05 @agent_ppo2.py:185][0m |          -0.0179 |          69.1043 |           0.3561 |
[32m[20221213 18:09:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.90
[32m[20221213 18:09:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.57
[32m[20221213 18:09:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.26
[32m[20221213 18:09:05 @agent_ppo2.py:143][0m Total time:       4.93 min
[32m[20221213 18:09:05 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 18:09:05 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 18:09:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0003 |          86.0306 |           0.3587 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0088 |          81.1369 |           0.3582 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0098 |          78.0241 |           0.3580 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0107 |          75.8727 |           0.3580 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0106 |          73.9461 |           0.3579 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0085 |          72.4514 |           0.3579 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0121 |          70.9995 |           0.3579 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0153 |          70.6121 |           0.3579 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0150 |          68.9931 |           0.3577 |
[32m[20221213 18:09:06 @agent_ppo2.py:185][0m |          -0.0164 |          68.1008 |           0.3578 |
[32m[20221213 18:09:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:09:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.17
[32m[20221213 18:09:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.55
[32m[20221213 18:09:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.85
[32m[20221213 18:09:07 @agent_ppo2.py:143][0m Total time:       4.95 min
[32m[20221213 18:09:07 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 18:09:07 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 18:09:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:07 @agent_ppo2.py:185][0m |          -0.0042 |          88.8728 |           0.3636 |
[32m[20221213 18:09:07 @agent_ppo2.py:185][0m |          -0.0106 |          82.7688 |           0.3631 |
[32m[20221213 18:09:07 @agent_ppo2.py:185][0m |          -0.0094 |          79.6554 |           0.3630 |
[32m[20221213 18:09:07 @agent_ppo2.py:185][0m |          -0.0111 |          77.6732 |           0.3634 |
[32m[20221213 18:09:07 @agent_ppo2.py:185][0m |          -0.0109 |          76.2970 |           0.3632 |
[32m[20221213 18:09:07 @agent_ppo2.py:185][0m |          -0.0136 |          74.9825 |           0.3633 |
[32m[20221213 18:09:08 @agent_ppo2.py:185][0m |          -0.0109 |          74.2600 |           0.3631 |
[32m[20221213 18:09:08 @agent_ppo2.py:185][0m |          -0.0113 |          74.7700 |           0.3634 |
[32m[20221213 18:09:08 @agent_ppo2.py:185][0m |          -0.0160 |          72.9638 |           0.3632 |
[32m[20221213 18:09:08 @agent_ppo2.py:185][0m |          -0.0194 |          72.2748 |           0.3634 |
[32m[20221213 18:09:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:09:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.09
[32m[20221213 18:09:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 604.59
[32m[20221213 18:09:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.48
[32m[20221213 18:09:08 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 971.48
[32m[20221213 18:09:08 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 971.48
[32m[20221213 18:09:08 @agent_ppo2.py:143][0m Total time:       4.97 min
[32m[20221213 18:09:08 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 18:09:08 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 18:09:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:09:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:08 @agent_ppo2.py:185][0m |           0.0069 |          86.9901 |           0.3562 |
[32m[20221213 18:09:08 @agent_ppo2.py:185][0m |          -0.0059 |          80.5692 |           0.3554 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0077 |          78.5980 |           0.3549 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0067 |          78.8076 |           0.3547 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0083 |          75.6869 |           0.3543 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0033 |          82.5594 |           0.3538 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0133 |          71.8047 |           0.3534 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0153 |          70.0735 |           0.3533 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0150 |          68.9024 |           0.3533 |
[32m[20221213 18:09:09 @agent_ppo2.py:185][0m |          -0.0149 |          67.8489 |           0.3528 |
[32m[20221213 18:09:09 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 18:09:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.90
[32m[20221213 18:09:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.14
[32m[20221213 18:09:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.67
[32m[20221213 18:09:09 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221213 18:09:09 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 18:09:09 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 18:09:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0022 |          86.7617 |           0.3582 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0101 |          81.3174 |           0.3579 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0108 |          78.9569 |           0.3576 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0118 |          77.1875 |           0.3575 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0153 |          75.4326 |           0.3576 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0165 |          74.0093 |           0.3575 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0161 |          72.7735 |           0.3573 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0165 |          71.5568 |           0.3573 |
[32m[20221213 18:09:10 @agent_ppo2.py:185][0m |          -0.0076 |          75.2869 |           0.3574 |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |          -0.0171 |          69.9548 |           0.3571 |
[32m[20221213 18:09:11 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.45
[32m[20221213 18:09:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.57
[32m[20221213 18:09:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.00
[32m[20221213 18:09:11 @agent_ppo2.py:143][0m Total time:       5.02 min
[32m[20221213 18:09:11 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 18:09:11 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 18:09:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |           0.0028 |          86.3736 |           0.3568 |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |          -0.0078 |          82.9624 |           0.3567 |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |          -0.0026 |          83.7852 |           0.3566 |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |          -0.0053 |          81.6275 |           0.3569 |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |          -0.0129 |          79.6279 |           0.3568 |
[32m[20221213 18:09:11 @agent_ppo2.py:185][0m |          -0.0108 |          80.2232 |           0.3570 |
[32m[20221213 18:09:12 @agent_ppo2.py:185][0m |          -0.0154 |          78.2022 |           0.3571 |
[32m[20221213 18:09:12 @agent_ppo2.py:185][0m |          -0.0148 |          77.5811 |           0.3569 |
[32m[20221213 18:09:12 @agent_ppo2.py:185][0m |          -0.0143 |          77.1405 |           0.3572 |
[32m[20221213 18:09:12 @agent_ppo2.py:185][0m |          -0.0153 |          76.6686 |           0.3574 |
[32m[20221213 18:09:12 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.22
[32m[20221213 18:09:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.67
[32m[20221213 18:09:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.41
[32m[20221213 18:09:12 @agent_ppo2.py:143][0m Total time:       5.04 min
[32m[20221213 18:09:12 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 18:09:12 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 18:09:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:12 @agent_ppo2.py:185][0m |          -0.0008 |          84.2012 |           0.3538 |
[32m[20221213 18:09:12 @agent_ppo2.py:185][0m |          -0.0063 |          79.4946 |           0.3530 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0080 |          77.9676 |           0.3527 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0099 |          77.9077 |           0.3526 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0127 |          75.6387 |           0.3522 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0056 |          83.3206 |           0.3522 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0140 |          74.4949 |           0.3522 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0154 |          73.7708 |           0.3520 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0124 |          73.1334 |           0.3520 |
[32m[20221213 18:09:13 @agent_ppo2.py:185][0m |          -0.0158 |          72.8621 |           0.3520 |
[32m[20221213 18:09:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:09:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.62
[32m[20221213 18:09:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.24
[32m[20221213 18:09:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.50
[32m[20221213 18:09:13 @agent_ppo2.py:143][0m Total time:       5.06 min
[32m[20221213 18:09:13 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 18:09:13 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 18:09:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0005 |          87.6377 |           0.3499 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0052 |          80.8056 |           0.3501 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0091 |          77.9771 |           0.3503 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0004 |          81.8033 |           0.3509 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0105 |          75.3056 |           0.3508 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0120 |          74.3211 |           0.3510 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0088 |          73.5700 |           0.3512 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0128 |          72.4355 |           0.3513 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0121 |          71.5445 |           0.3515 |
[32m[20221213 18:09:14 @agent_ppo2.py:185][0m |          -0.0116 |          71.1389 |           0.3515 |
[32m[20221213 18:09:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:09:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.71
[32m[20221213 18:09:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.59
[32m[20221213 18:09:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.53
[32m[20221213 18:09:15 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 978.53
[32m[20221213 18:09:15 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 978.53
[32m[20221213 18:09:15 @agent_ppo2.py:143][0m Total time:       5.08 min
[32m[20221213 18:09:15 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 18:09:15 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 18:09:15 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:09:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:15 @agent_ppo2.py:185][0m |           0.0062 |          87.0610 |           0.3608 |
[32m[20221213 18:09:15 @agent_ppo2.py:185][0m |          -0.0071 |          79.7929 |           0.3607 |
[32m[20221213 18:09:15 @agent_ppo2.py:185][0m |          -0.0013 |          79.0806 |           0.3606 |
[32m[20221213 18:09:15 @agent_ppo2.py:185][0m |          -0.0071 |          76.0027 |           0.3605 |
[32m[20221213 18:09:15 @agent_ppo2.py:185][0m |          -0.0058 |          80.2190 |           0.3606 |
[32m[20221213 18:09:15 @agent_ppo2.py:185][0m |          -0.0088 |          72.4313 |           0.3606 |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |           0.0119 |          89.0624 |           0.3606 |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |          -0.0136 |          70.0309 |           0.3609 |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |          -0.0071 |          72.3704 |           0.3610 |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |          -0.0128 |          67.5132 |           0.3612 |
[32m[20221213 18:09:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:09:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.52
[32m[20221213 18:09:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 618.55
[32m[20221213 18:09:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.65
[32m[20221213 18:09:16 @agent_ppo2.py:143][0m Total time:       5.10 min
[32m[20221213 18:09:16 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 18:09:16 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 18:09:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |          -0.0045 |          93.8843 |           0.3619 |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |          -0.0038 |          87.0881 |           0.3617 |
[32m[20221213 18:09:16 @agent_ppo2.py:185][0m |          -0.0089 |          84.3739 |           0.3615 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0115 |          82.9460 |           0.3610 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0091 |          82.2170 |           0.3609 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0094 |          80.7708 |           0.3611 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0064 |          81.1314 |           0.3608 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0099 |          79.7718 |           0.3607 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0143 |          78.6964 |           0.3607 |
[32m[20221213 18:09:17 @agent_ppo2.py:185][0m |          -0.0136 |          78.1362 |           0.3603 |
[32m[20221213 18:09:17 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:09:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.88
[32m[20221213 18:09:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.80
[32m[20221213 18:09:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.97
[32m[20221213 18:09:17 @agent_ppo2.py:143][0m Total time:       5.12 min
[32m[20221213 18:09:17 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 18:09:17 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 18:09:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0010 |          92.2449 |           0.3632 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0072 |          84.7238 |           0.3628 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0015 |          82.7119 |           0.3626 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0124 |          78.7926 |           0.3625 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0118 |          76.4374 |           0.3625 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0127 |          74.3501 |           0.3627 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0146 |          72.9724 |           0.3629 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0130 |          71.5128 |           0.3627 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0134 |          70.1907 |           0.3628 |
[32m[20221213 18:09:18 @agent_ppo2.py:185][0m |          -0.0161 |          69.0895 |           0.3627 |
[32m[20221213 18:09:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:09:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.93
[32m[20221213 18:09:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.14
[32m[20221213 18:09:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.00
[32m[20221213 18:09:18 @agent_ppo2.py:143][0m Total time:       5.15 min
[32m[20221213 18:09:18 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 18:09:18 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 18:09:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0010 |          89.8802 |           0.3661 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0081 |          85.9557 |           0.3654 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0103 |          83.7816 |           0.3651 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0117 |          82.3662 |           0.3648 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0103 |          81.4286 |           0.3647 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0119 |          80.6194 |           0.3644 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0026 |          87.8755 |           0.3646 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0151 |          79.2888 |           0.3641 |
[32m[20221213 18:09:19 @agent_ppo2.py:185][0m |          -0.0088 |          79.4349 |           0.3642 |
[32m[20221213 18:09:20 @agent_ppo2.py:185][0m |          -0.0161 |          78.0756 |           0.3642 |
[32m[20221213 18:09:20 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.44
[32m[20221213 18:09:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.20
[32m[20221213 18:09:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.61
[32m[20221213 18:09:20 @agent_ppo2.py:143][0m Total time:       5.17 min
[32m[20221213 18:09:20 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 18:09:20 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 18:09:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:20 @agent_ppo2.py:185][0m |          -0.0007 |          98.0333 |           0.3622 |
[32m[20221213 18:09:20 @agent_ppo2.py:185][0m |          -0.0023 |          92.1912 |           0.3612 |
[32m[20221213 18:09:20 @agent_ppo2.py:185][0m |          -0.0081 |          89.8017 |           0.3601 |
[32m[20221213 18:09:20 @agent_ppo2.py:185][0m |          -0.0088 |          88.3939 |           0.3599 |
[32m[20221213 18:09:20 @agent_ppo2.py:185][0m |          -0.0075 |          87.4784 |           0.3594 |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0001 |          91.8186 |           0.3589 |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0027 |          86.5782 |           0.3585 |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0100 |          85.0828 |           0.3584 |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0096 |          84.7905 |           0.3580 |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0139 |          83.8493 |           0.3579 |
[32m[20221213 18:09:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.46
[32m[20221213 18:09:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.90
[32m[20221213 18:09:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.70
[32m[20221213 18:09:21 @agent_ppo2.py:143][0m Total time:       5.19 min
[32m[20221213 18:09:21 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 18:09:21 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 18:09:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0032 |          92.4444 |           0.3633 |
[32m[20221213 18:09:21 @agent_ppo2.py:185][0m |          -0.0094 |          88.8356 |           0.3627 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0110 |          86.9136 |           0.3623 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0099 |          85.3811 |           0.3622 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0045 |          86.6309 |           0.3620 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0117 |          83.4683 |           0.3617 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0157 |          82.5970 |           0.3616 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0140 |          81.9036 |           0.3613 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0147 |          81.1779 |           0.3612 |
[32m[20221213 18:09:22 @agent_ppo2.py:185][0m |          -0.0130 |          80.7376 |           0.3610 |
[32m[20221213 18:09:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:09:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.37
[32m[20221213 18:09:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.50
[32m[20221213 18:09:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 883.19
[32m[20221213 18:09:22 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221213 18:09:22 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 18:09:22 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 18:09:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0026 |          96.9522 |           0.3723 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0072 |          92.8829 |           0.3713 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0063 |          90.2452 |           0.3712 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0120 |          88.3646 |           0.3706 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0111 |          86.7874 |           0.3705 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0035 |          90.6379 |           0.3704 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0154 |          84.5492 |           0.3703 |
[32m[20221213 18:09:23 @agent_ppo2.py:185][0m |          -0.0152 |          83.4886 |           0.3702 |
[32m[20221213 18:09:24 @agent_ppo2.py:185][0m |          -0.0105 |          88.7540 |           0.3701 |
[32m[20221213 18:09:24 @agent_ppo2.py:185][0m |          -0.0105 |          82.1396 |           0.3698 |
[32m[20221213 18:09:24 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 18:09:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.76
[32m[20221213 18:09:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.88
[32m[20221213 18:09:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.15
[32m[20221213 18:09:24 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221213 18:09:24 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 18:09:24 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 18:09:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:24 @agent_ppo2.py:185][0m |           0.0013 |          90.3730 |           0.3517 |
[32m[20221213 18:09:24 @agent_ppo2.py:185][0m |          -0.0050 |          81.9918 |           0.3515 |
[32m[20221213 18:09:24 @agent_ppo2.py:185][0m |           0.0041 |          83.9206 |           0.3518 |
[32m[20221213 18:09:24 @agent_ppo2.py:185][0m |          -0.0079 |          76.4658 |           0.3516 |
[32m[20221213 18:09:25 @agent_ppo2.py:185][0m |          -0.0117 |          74.9386 |           0.3517 |
[32m[20221213 18:09:25 @agent_ppo2.py:185][0m |          -0.0073 |          73.0827 |           0.3522 |
[32m[20221213 18:09:25 @agent_ppo2.py:185][0m |          -0.0116 |          71.7141 |           0.3523 |
[32m[20221213 18:09:25 @agent_ppo2.py:185][0m |          -0.0079 |          70.7178 |           0.3524 |
[32m[20221213 18:09:25 @agent_ppo2.py:185][0m |          -0.0119 |          69.3949 |           0.3526 |
[32m[20221213 18:09:25 @agent_ppo2.py:185][0m |          -0.0134 |          68.1869 |           0.3527 |
[32m[20221213 18:09:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:09:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.03
[32m[20221213 18:09:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.39
[32m[20221213 18:09:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.55
[32m[20221213 18:09:25 @agent_ppo2.py:143][0m Total time:       5.26 min
[32m[20221213 18:09:25 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 18:09:25 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 18:09:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |           0.0009 |          90.3583 |           0.3662 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0062 |          78.7240 |           0.3658 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0052 |          72.7572 |           0.3657 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0123 |          69.8332 |           0.3654 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0079 |          67.1976 |           0.3653 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0106 |          65.4110 |           0.3653 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0094 |          63.4711 |           0.3651 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0132 |          61.7583 |           0.3650 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0142 |          60.5864 |           0.3649 |
[32m[20221213 18:09:26 @agent_ppo2.py:185][0m |          -0.0144 |          59.4384 |           0.3650 |
[32m[20221213 18:09:26 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 18:09:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.78
[32m[20221213 18:09:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.70
[32m[20221213 18:09:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.67
[32m[20221213 18:09:27 @agent_ppo2.py:143][0m Total time:       5.28 min
[32m[20221213 18:09:27 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 18:09:27 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 18:09:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0015 |          93.6230 |           0.3622 |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0037 |          83.0695 |           0.3619 |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0072 |          79.8591 |           0.3615 |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0084 |          77.6616 |           0.3614 |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0102 |          76.2079 |           0.3615 |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0109 |          74.9231 |           0.3614 |
[32m[20221213 18:09:27 @agent_ppo2.py:185][0m |          -0.0090 |          76.0037 |           0.3613 |
[32m[20221213 18:09:28 @agent_ppo2.py:185][0m |          -0.0099 |          72.7659 |           0.3614 |
[32m[20221213 18:09:28 @agent_ppo2.py:185][0m |          -0.0216 |          72.3349 |           0.3615 |
[32m[20221213 18:09:28 @agent_ppo2.py:185][0m |          -0.0107 |          71.5032 |           0.3615 |
[32m[20221213 18:09:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:09:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.78
[32m[20221213 18:09:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.98
[32m[20221213 18:09:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.03
[32m[20221213 18:09:28 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221213 18:09:28 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 18:09:28 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 18:09:28 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:09:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:28 @agent_ppo2.py:185][0m |          -0.0036 |          94.4589 |           0.3653 |
[32m[20221213 18:09:28 @agent_ppo2.py:185][0m |          -0.0087 |          88.0897 |           0.3649 |
[32m[20221213 18:09:28 @agent_ppo2.py:185][0m |          -0.0152 |          85.3625 |           0.3651 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0038 |          89.3427 |           0.3650 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0168 |          81.6398 |           0.3648 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0147 |          79.9427 |           0.3653 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0138 |          79.2079 |           0.3655 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0161 |          78.0980 |           0.3654 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0179 |          77.2888 |           0.3657 |
[32m[20221213 18:09:29 @agent_ppo2.py:185][0m |          -0.0156 |          76.5488 |           0.3656 |
[32m[20221213 18:09:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:09:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.74
[32m[20221213 18:09:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.50
[32m[20221213 18:09:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.51
[32m[20221213 18:09:29 @agent_ppo2.py:143][0m Total time:       5.32 min
[32m[20221213 18:09:29 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 18:09:29 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 18:09:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |           0.0002 |         102.5507 |           0.3596 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0008 |          94.2559 |           0.3594 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0056 |          89.0331 |           0.3593 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0079 |          86.5887 |           0.3593 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0064 |          83.6346 |           0.3591 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0122 |          81.7218 |           0.3590 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0119 |          80.3540 |           0.3590 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0075 |          79.5662 |           0.3590 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0070 |          82.1718 |           0.3588 |
[32m[20221213 18:09:30 @agent_ppo2.py:185][0m |          -0.0115 |          77.8767 |           0.3591 |
[32m[20221213 18:09:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:09:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.29
[32m[20221213 18:09:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.91
[32m[20221213 18:09:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.44
[32m[20221213 18:09:31 @agent_ppo2.py:143][0m Total time:       5.35 min
[32m[20221213 18:09:31 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 18:09:31 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 18:09:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0023 |          92.1621 |           0.3661 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0100 |          86.3947 |           0.3655 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0137 |          83.2987 |           0.3652 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0117 |          81.9024 |           0.3648 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0116 |          79.8893 |           0.3646 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0152 |          79.3819 |           0.3644 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0156 |          78.1207 |           0.3642 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0135 |          77.4013 |           0.3641 |
[32m[20221213 18:09:31 @agent_ppo2.py:185][0m |          -0.0156 |          76.7656 |           0.3641 |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0155 |          76.0100 |           0.3640 |
[32m[20221213 18:09:32 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.02
[32m[20221213 18:09:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.52
[32m[20221213 18:09:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.09
[32m[20221213 18:09:32 @agent_ppo2.py:143][0m Total time:       5.37 min
[32m[20221213 18:09:32 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 18:09:32 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 18:09:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0030 |         110.4775 |           0.3586 |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0066 |         104.6760 |           0.3582 |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0051 |         101.6941 |           0.3579 |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0114 |         100.0799 |           0.3577 |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0108 |          98.4778 |           0.3577 |
[32m[20221213 18:09:32 @agent_ppo2.py:185][0m |          -0.0139 |          97.4548 |           0.3574 |
[32m[20221213 18:09:33 @agent_ppo2.py:185][0m |          -0.0146 |          96.0251 |           0.3573 |
[32m[20221213 18:09:33 @agent_ppo2.py:185][0m |          -0.0094 |          95.4516 |           0.3571 |
[32m[20221213 18:09:33 @agent_ppo2.py:185][0m |          -0.0146 |          94.2679 |           0.3571 |
[32m[20221213 18:09:33 @agent_ppo2.py:185][0m |          -0.0118 |          93.0632 |           0.3570 |
[32m[20221213 18:09:33 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.13
[32m[20221213 18:09:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.70
[32m[20221213 18:09:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.95
[32m[20221213 18:09:33 @agent_ppo2.py:143][0m Total time:       5.39 min
[32m[20221213 18:09:33 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 18:09:33 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 18:09:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:33 @agent_ppo2.py:185][0m |           0.0043 |         111.7310 |           0.3630 |
[32m[20221213 18:09:33 @agent_ppo2.py:185][0m |          -0.0119 |         106.5869 |           0.3624 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0093 |         105.3692 |           0.3617 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0124 |         102.9573 |           0.3614 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0165 |         101.9740 |           0.3611 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0135 |         101.2508 |           0.3607 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0087 |         105.2326 |           0.3603 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0120 |          99.2051 |           0.3602 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0159 |          98.3683 |           0.3601 |
[32m[20221213 18:09:34 @agent_ppo2.py:185][0m |          -0.0161 |          97.2995 |           0.3599 |
[32m[20221213 18:09:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:09:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.03
[32m[20221213 18:09:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.11
[32m[20221213 18:09:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.51
[32m[20221213 18:09:34 @agent_ppo2.py:143][0m Total time:       5.41 min
[32m[20221213 18:09:34 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 18:09:34 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 18:09:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0017 |         100.2898 |           0.3641 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0056 |          94.7432 |           0.3635 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |           0.0027 |          95.9913 |           0.3636 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0053 |          90.3908 |           0.3635 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0113 |          89.2433 |           0.3638 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0112 |          87.6594 |           0.3635 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0109 |          86.1678 |           0.3635 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0118 |          84.9113 |           0.3637 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0114 |          83.4320 |           0.3634 |
[32m[20221213 18:09:35 @agent_ppo2.py:185][0m |          -0.0129 |          82.4923 |           0.3637 |
[32m[20221213 18:09:35 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 599.67
[32m[20221213 18:09:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.75
[32m[20221213 18:09:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.77
[32m[20221213 18:09:35 @agent_ppo2.py:143][0m Total time:       5.43 min
[32m[20221213 18:09:35 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 18:09:35 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 18:09:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |           0.0001 |          93.3794 |           0.3761 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |           0.0013 |          93.8729 |           0.3757 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |           0.0066 |          98.6038 |           0.3755 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |          -0.0094 |          86.9689 |           0.3748 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |          -0.0136 |          86.2018 |           0.3747 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |          -0.0069 |          84.9674 |           0.3746 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |          -0.0148 |          84.1109 |           0.3742 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |          -0.0113 |          83.9875 |           0.3742 |
[32m[20221213 18:09:36 @agent_ppo2.py:185][0m |          -0.0155 |          82.9496 |           0.3742 |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |          -0.0114 |          82.4887 |           0.3740 |
[32m[20221213 18:09:37 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:09:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.04
[32m[20221213 18:09:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.65
[32m[20221213 18:09:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.05
[32m[20221213 18:09:37 @agent_ppo2.py:143][0m Total time:       5.45 min
[32m[20221213 18:09:37 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 18:09:37 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 18:09:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |          -0.0007 |         101.6690 |           0.3654 |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |          -0.0013 |          95.4875 |           0.3651 |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |           0.0052 |          99.4995 |           0.3652 |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |          -0.0114 |          92.0833 |           0.3647 |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |          -0.0147 |          90.9023 |           0.3647 |
[32m[20221213 18:09:37 @agent_ppo2.py:185][0m |          -0.0145 |          89.8640 |           0.3643 |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |          -0.0052 |          90.4006 |           0.3644 |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |          -0.0132 |          87.9155 |           0.3643 |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |          -0.0136 |          86.9317 |           0.3641 |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |          -0.0141 |          86.2666 |           0.3640 |
[32m[20221213 18:09:38 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.77
[32m[20221213 18:09:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.02
[32m[20221213 18:09:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 967.72
[32m[20221213 18:09:38 @agent_ppo2.py:143][0m Total time:       5.47 min
[32m[20221213 18:09:38 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 18:09:38 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 18:09:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |           0.0023 |         110.6121 |           0.3613 |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |          -0.0044 |         106.2638 |           0.3612 |
[32m[20221213 18:09:38 @agent_ppo2.py:185][0m |          -0.0060 |         103.9241 |           0.3615 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |          -0.0117 |         102.7863 |           0.3617 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |           0.0008 |         110.7154 |           0.3621 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |          -0.0101 |         100.3567 |           0.3621 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |          -0.0104 |         101.7565 |           0.3626 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |          -0.0139 |          98.4715 |           0.3630 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |          -0.0130 |          97.8155 |           0.3632 |
[32m[20221213 18:09:39 @agent_ppo2.py:185][0m |          -0.0154 |          97.1062 |           0.3636 |
[32m[20221213 18:09:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.46
[32m[20221213 18:09:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.08
[32m[20221213 18:09:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.00
[32m[20221213 18:09:39 @agent_ppo2.py:143][0m Total time:       5.49 min
[32m[20221213 18:09:39 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 18:09:39 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 18:09:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0000 |          96.6007 |           0.3684 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0011 |          90.4661 |           0.3684 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0051 |          87.0844 |           0.3685 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0095 |          85.1159 |           0.3687 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0056 |          84.1882 |           0.3687 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0113 |          81.7538 |           0.3686 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0099 |          80.2879 |           0.3687 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0133 |          79.0348 |           0.3687 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0145 |          77.8764 |           0.3689 |
[32m[20221213 18:09:40 @agent_ppo2.py:185][0m |          -0.0121 |          76.8726 |           0.3689 |
[32m[20221213 18:09:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:09:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.34
[32m[20221213 18:09:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.98
[32m[20221213 18:09:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.25
[32m[20221213 18:09:40 @agent_ppo2.py:143][0m Total time:       5.51 min
[32m[20221213 18:09:40 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 18:09:40 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 18:09:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:09:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0038 |          95.8200 |           0.3693 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |           0.0048 |          92.9682 |           0.3692 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0075 |          87.9020 |           0.3690 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0081 |          86.4741 |           0.3689 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0125 |          85.5334 |           0.3689 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0035 |          85.9823 |           0.3687 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0110 |          84.2997 |           0.3688 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0086 |          83.9097 |           0.3687 |
[32m[20221213 18:09:41 @agent_ppo2.py:185][0m |          -0.0112 |          83.2533 |           0.3685 |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |          -0.0128 |          82.8134 |           0.3687 |
[32m[20221213 18:09:42 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.54
[32m[20221213 18:09:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 667.67
[32m[20221213 18:09:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.84
[32m[20221213 18:09:42 @agent_ppo2.py:143][0m Total time:       5.53 min
[32m[20221213 18:09:42 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 18:09:42 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 18:09:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |          -0.0002 |         103.2101 |           0.3745 |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |          -0.0063 |          95.9637 |           0.3740 |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |          -0.0107 |          92.1610 |           0.3735 |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |          -0.0046 |          91.7422 |           0.3734 |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |           0.0072 |         100.5512 |           0.3731 |
[32m[20221213 18:09:42 @agent_ppo2.py:185][0m |          -0.0014 |          98.7834 |           0.3721 |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0156 |          84.9919 |           0.3721 |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0132 |          83.5028 |           0.3722 |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0091 |          83.4444 |           0.3720 |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0136 |          80.8928 |           0.3721 |
[32m[20221213 18:09:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.70
[32m[20221213 18:09:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.45
[32m[20221213 18:09:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 918.15
[32m[20221213 18:09:43 @agent_ppo2.py:143][0m Total time:       5.55 min
[32m[20221213 18:09:43 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 18:09:43 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 18:09:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0009 |          98.5081 |           0.3700 |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0082 |          93.8928 |           0.3692 |
[32m[20221213 18:09:43 @agent_ppo2.py:185][0m |          -0.0125 |          91.6941 |           0.3689 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0104 |          90.0157 |           0.3688 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0026 |          94.6554 |           0.3686 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0125 |          87.8952 |           0.3681 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0159 |          86.4192 |           0.3683 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0167 |          85.5385 |           0.3683 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0175 |          84.4709 |           0.3683 |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0167 |          83.7110 |           0.3682 |
[32m[20221213 18:09:44 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.62
[32m[20221213 18:09:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 634.14
[32m[20221213 18:09:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 901.56
[32m[20221213 18:09:44 @agent_ppo2.py:143][0m Total time:       5.57 min
[32m[20221213 18:09:44 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 18:09:44 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 18:09:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:44 @agent_ppo2.py:185][0m |          -0.0035 |         104.1303 |           0.3766 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0076 |          98.7895 |           0.3763 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0048 |          96.5601 |           0.3758 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0072 |          94.7619 |           0.3757 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0131 |          92.7945 |           0.3758 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0103 |          91.6859 |           0.3754 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0068 |          93.5185 |           0.3756 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0164 |          88.4720 |           0.3756 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0155 |          87.4221 |           0.3756 |
[32m[20221213 18:09:45 @agent_ppo2.py:185][0m |          -0.0168 |          86.1279 |           0.3755 |
[32m[20221213 18:09:45 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:09:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.77
[32m[20221213 18:09:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.70
[32m[20221213 18:09:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 876.25
[32m[20221213 18:09:45 @agent_ppo2.py:143][0m Total time:       5.59 min
[32m[20221213 18:09:45 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 18:09:45 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 18:09:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |           0.0011 |         112.4013 |           0.3845 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0076 |         107.5494 |           0.3843 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0105 |         105.3359 |           0.3836 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0052 |         112.4227 |           0.3833 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0140 |         101.6673 |           0.3830 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0112 |         100.1271 |           0.3827 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0127 |          99.1906 |           0.3826 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0169 |          98.4155 |           0.3827 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0158 |          97.4499 |           0.3824 |
[32m[20221213 18:09:46 @agent_ppo2.py:185][0m |          -0.0139 |          96.5091 |           0.3822 |
[32m[20221213 18:09:46 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:09:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.57
[32m[20221213 18:09:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 652.15
[32m[20221213 18:09:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.13
[32m[20221213 18:09:47 @agent_ppo2.py:143][0m Total time:       5.62 min
[32m[20221213 18:09:47 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 18:09:47 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 18:09:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |          -0.0027 |         102.1119 |           0.3676 |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |           0.0028 |         100.3900 |           0.3676 |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |          -0.0025 |          94.6987 |           0.3673 |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |          -0.0046 |          92.0281 |           0.3674 |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |          -0.0125 |          89.2425 |           0.3675 |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |          -0.0120 |          87.6702 |           0.3676 |
[32m[20221213 18:09:47 @agent_ppo2.py:185][0m |          -0.0122 |          86.4358 |           0.3677 |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0122 |          85.1588 |           0.3678 |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0153 |          83.5823 |           0.3679 |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0142 |          82.4569 |           0.3679 |
[32m[20221213 18:09:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:09:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.98
[32m[20221213 18:09:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.40
[32m[20221213 18:09:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.42
[32m[20221213 18:09:48 @agent_ppo2.py:143][0m Total time:       5.64 min
[32m[20221213 18:09:48 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 18:09:48 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 18:09:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0032 |         107.0365 |           0.3721 |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0022 |         100.7535 |           0.3718 |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0078 |          98.0148 |           0.3717 |
[32m[20221213 18:09:48 @agent_ppo2.py:185][0m |          -0.0081 |          96.0184 |           0.3714 |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |          -0.0008 |          96.9591 |           0.3711 |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |          -0.0129 |          93.5672 |           0.3710 |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |          -0.0124 |          92.5906 |           0.3712 |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |          -0.0120 |          91.5442 |           0.3709 |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |          -0.0139 |          90.5353 |           0.3710 |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |          -0.0131 |          89.8139 |           0.3708 |
[32m[20221213 18:09:49 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:09:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.44
[32m[20221213 18:09:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 679.05
[32m[20221213 18:09:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.65
[32m[20221213 18:09:49 @agent_ppo2.py:143][0m Total time:       5.66 min
[32m[20221213 18:09:49 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 18:09:49 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 18:09:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:49 @agent_ppo2.py:185][0m |           0.0026 |          96.5614 |           0.3777 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0087 |          88.5173 |           0.3766 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0124 |          85.9189 |           0.3766 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0112 |          83.3640 |           0.3767 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0144 |          81.5961 |           0.3768 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0204 |          80.6928 |           0.3768 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0161 |          79.6029 |           0.3769 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0179 |          78.5317 |           0.3770 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0108 |          78.9628 |           0.3769 |
[32m[20221213 18:09:50 @agent_ppo2.py:185][0m |          -0.0202 |          76.8695 |           0.3768 |
[32m[20221213 18:09:50 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:09:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.23
[32m[20221213 18:09:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.59
[32m[20221213 18:09:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.52
[32m[20221213 18:09:50 @agent_ppo2.py:143][0m Total time:       5.68 min
[32m[20221213 18:09:50 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 18:09:50 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 18:09:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:09:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |           0.0054 |          96.1707 |           0.3775 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0044 |          82.7795 |           0.3769 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0078 |          77.2779 |           0.3769 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0038 |          75.2451 |           0.3768 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0154 |          70.8603 |           0.3766 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0070 |          69.5366 |           0.3766 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0123 |          66.5703 |           0.3769 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0140 |          65.1353 |           0.3767 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0072 |          64.9555 |           0.3766 |
[32m[20221213 18:09:51 @agent_ppo2.py:185][0m |          -0.0135 |          61.3174 |           0.3769 |
[32m[20221213 18:09:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:09:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.23
[32m[20221213 18:09:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.32
[32m[20221213 18:09:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.08
[32m[20221213 18:09:52 @agent_ppo2.py:143][0m Total time:       5.70 min
[32m[20221213 18:09:52 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 18:09:52 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 18:09:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:09:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:52 @agent_ppo2.py:185][0m |          -0.0020 |         105.4464 |           0.3662 |
[32m[20221213 18:09:52 @agent_ppo2.py:185][0m |           0.0105 |         112.6203 |           0.3666 |
[32m[20221213 18:09:52 @agent_ppo2.py:185][0m |          -0.0108 |          90.5033 |           0.3663 |
[32m[20221213 18:09:52 @agent_ppo2.py:185][0m |          -0.0067 |          88.2026 |           0.3665 |
[32m[20221213 18:09:52 @agent_ppo2.py:185][0m |          -0.0145 |          85.0172 |           0.3666 |
[32m[20221213 18:09:52 @agent_ppo2.py:185][0m |          -0.0095 |          82.9797 |           0.3667 |
[32m[20221213 18:09:53 @agent_ppo2.py:185][0m |          -0.0121 |          81.3905 |           0.3669 |
[32m[20221213 18:09:53 @agent_ppo2.py:185][0m |          -0.0168 |          79.6908 |           0.3672 |
[32m[20221213 18:09:53 @agent_ppo2.py:185][0m |          -0.0020 |          84.8422 |           0.3669 |
[32m[20221213 18:09:53 @agent_ppo2.py:185][0m |          -0.0056 |          77.8923 |           0.3667 |
[32m[20221213 18:09:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:09:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.66
[32m[20221213 18:09:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.03
[32m[20221213 18:09:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.31
[32m[20221213 18:09:53 @agent_ppo2.py:143][0m Total time:       5.72 min
[32m[20221213 18:09:53 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 18:09:53 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 18:09:53 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:09:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:53 @agent_ppo2.py:185][0m |          -0.0056 |         114.7475 |           0.3761 |
[32m[20221213 18:09:53 @agent_ppo2.py:185][0m |          -0.0074 |         107.5832 |           0.3768 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0069 |         105.2210 |           0.3770 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0092 |         102.2629 |           0.3772 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0110 |         101.5770 |           0.3771 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0118 |          99.1888 |           0.3769 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0147 |          97.9852 |           0.3772 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0143 |          97.1448 |           0.3776 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0115 |          97.1239 |           0.3776 |
[32m[20221213 18:09:54 @agent_ppo2.py:185][0m |          -0.0163 |          95.6133 |           0.3777 |
[32m[20221213 18:09:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:09:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.40
[32m[20221213 18:09:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 913.89
[32m[20221213 18:09:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.00
[32m[20221213 18:09:54 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 18:09:54 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 18:09:54 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 18:09:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0050 |         123.4067 |           0.3793 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0078 |         116.2017 |           0.3792 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0083 |         112.4969 |           0.3785 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0076 |         109.6489 |           0.3784 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |           0.0046 |         112.1435 |           0.3784 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |           0.0001 |         113.8937 |           0.3780 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0134 |         105.2614 |           0.3780 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0133 |         104.0558 |           0.3786 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0083 |         103.3927 |           0.3787 |
[32m[20221213 18:09:55 @agent_ppo2.py:185][0m |          -0.0106 |         102.2628 |           0.3785 |
[32m[20221213 18:09:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:09:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.01
[32m[20221213 18:09:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.64
[32m[20221213 18:09:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.23
[32m[20221213 18:09:56 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221213 18:09:56 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 18:09:56 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 18:09:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:56 @agent_ppo2.py:185][0m |          -0.0024 |         106.1110 |           0.3934 |
[32m[20221213 18:09:56 @agent_ppo2.py:185][0m |          -0.0043 |         100.0647 |           0.3927 |
[32m[20221213 18:09:56 @agent_ppo2.py:185][0m |          -0.0087 |          96.3264 |           0.3923 |
[32m[20221213 18:09:56 @agent_ppo2.py:185][0m |          -0.0087 |          94.1149 |           0.3922 |
[32m[20221213 18:09:56 @agent_ppo2.py:185][0m |          -0.0125 |          92.1763 |           0.3920 |
[32m[20221213 18:09:56 @agent_ppo2.py:185][0m |          -0.0119 |          90.6667 |           0.3919 |
[32m[20221213 18:09:57 @agent_ppo2.py:185][0m |          -0.0120 |          89.0527 |           0.3919 |
[32m[20221213 18:09:57 @agent_ppo2.py:185][0m |          -0.0150 |          87.6498 |           0.3916 |
[32m[20221213 18:09:57 @agent_ppo2.py:185][0m |          -0.0138 |          86.5345 |           0.3914 |
[32m[20221213 18:09:57 @agent_ppo2.py:185][0m |          -0.0168 |          85.5085 |           0.3913 |
[32m[20221213 18:09:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:09:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.20
[32m[20221213 18:09:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.67
[32m[20221213 18:09:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 910.01
[32m[20221213 18:09:57 @agent_ppo2.py:143][0m Total time:       5.79 min
[32m[20221213 18:09:57 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 18:09:57 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 18:09:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:57 @agent_ppo2.py:185][0m |          -0.0006 |         117.5978 |           0.3922 |
[32m[20221213 18:09:57 @agent_ppo2.py:185][0m |          -0.0103 |         110.6478 |           0.3917 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |           0.0014 |         113.6868 |           0.3918 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0091 |         102.8162 |           0.3918 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0103 |          99.8170 |           0.3919 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0145 |          97.6732 |           0.3916 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0127 |          95.8735 |           0.3916 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0128 |          94.2570 |           0.3921 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0196 |          93.3204 |           0.3917 |
[32m[20221213 18:09:58 @agent_ppo2.py:185][0m |          -0.0126 |          91.3407 |           0.3921 |
[32m[20221213 18:09:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:09:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.81
[32m[20221213 18:09:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.84
[32m[20221213 18:09:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.28
[32m[20221213 18:09:58 @agent_ppo2.py:143][0m Total time:       5.81 min
[32m[20221213 18:09:58 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 18:09:58 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 18:09:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:09:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |           0.0011 |         125.5416 |           0.3879 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0078 |         118.9472 |           0.3874 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0022 |         120.0451 |           0.3868 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0073 |         114.0888 |           0.3862 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0123 |         112.8803 |           0.3860 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0084 |         113.1504 |           0.3854 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0120 |         110.6225 |           0.3855 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0146 |         109.4989 |           0.3850 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0088 |         109.9391 |           0.3850 |
[32m[20221213 18:09:59 @agent_ppo2.py:185][0m |          -0.0114 |         108.1732 |           0.3849 |
[32m[20221213 18:09:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 647.74
[32m[20221213 18:10:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.87
[32m[20221213 18:10:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.28
[32m[20221213 18:10:00 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221213 18:10:00 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 18:10:00 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 18:10:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |           0.0000 |         129.2657 |           0.3822 |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |          -0.0046 |         125.7582 |           0.3820 |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |          -0.0076 |         122.9826 |           0.3820 |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |           0.0040 |         129.0814 |           0.3817 |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |          -0.0079 |         120.1316 |           0.3817 |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |          -0.0137 |         119.4376 |           0.3810 |
[32m[20221213 18:10:00 @agent_ppo2.py:185][0m |          -0.0095 |         118.1306 |           0.3813 |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |          -0.0133 |         117.2337 |           0.3811 |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |          -0.0131 |         116.4604 |           0.3810 |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |          -0.0129 |         115.8619 |           0.3812 |
[32m[20221213 18:10:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:10:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 667.73
[32m[20221213 18:10:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 873.83
[32m[20221213 18:10:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 906.02
[32m[20221213 18:10:01 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221213 18:10:01 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 18:10:01 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 18:10:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |           0.0002 |         149.5929 |           0.3912 |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |          -0.0019 |         143.7458 |           0.3908 |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |          -0.0066 |         140.6790 |           0.3906 |
[32m[20221213 18:10:01 @agent_ppo2.py:185][0m |          -0.0038 |         138.7793 |           0.3905 |
[32m[20221213 18:10:02 @agent_ppo2.py:185][0m |          -0.0069 |         136.7237 |           0.3902 |
[32m[20221213 18:10:02 @agent_ppo2.py:185][0m |           0.0030 |         143.4467 |           0.3901 |
[32m[20221213 18:10:02 @agent_ppo2.py:185][0m |          -0.0058 |         134.1865 |           0.3905 |
[32m[20221213 18:10:02 @agent_ppo2.py:185][0m |           0.0045 |         151.1843 |           0.3906 |
[32m[20221213 18:10:02 @agent_ppo2.py:185][0m |          -0.0114 |         131.7506 |           0.3900 |
[32m[20221213 18:10:02 @agent_ppo2.py:185][0m |          -0.0045 |         134.6121 |           0.3903 |
[32m[20221213 18:10:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.08
[32m[20221213 18:10:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 895.99
[32m[20221213 18:10:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.17
[32m[20221213 18:10:02 @agent_ppo2.py:143][0m Total time:       5.87 min
[32m[20221213 18:10:02 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 18:10:02 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 18:10:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0046 |         181.2766 |           0.3825 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0044 |         174.1911 |           0.3821 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0061 |         172.1258 |           0.3824 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |           0.0037 |         182.8609 |           0.3823 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0061 |         170.3060 |           0.3826 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0074 |         168.8592 |           0.3828 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0086 |         168.1139 |           0.3833 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0130 |         167.2734 |           0.3832 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0132 |         166.4644 |           0.3837 |
[32m[20221213 18:10:03 @agent_ppo2.py:185][0m |          -0.0099 |         166.0279 |           0.3838 |
[32m[20221213 18:10:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.89
[32m[20221213 18:10:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 882.76
[32m[20221213 18:10:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.13
[32m[20221213 18:10:03 @agent_ppo2.py:143][0m Total time:       5.89 min
[32m[20221213 18:10:03 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 18:10:03 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 18:10:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |           0.0007 |         164.1181 |           0.3945 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0017 |         158.0210 |           0.3941 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |           0.0028 |         165.8010 |           0.3939 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0114 |         152.8816 |           0.3936 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0110 |         151.3746 |           0.3938 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0098 |         150.3819 |           0.3936 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0081 |         149.8768 |           0.3938 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0125 |         148.0549 |           0.3939 |
[32m[20221213 18:10:04 @agent_ppo2.py:185][0m |          -0.0136 |         147.2271 |           0.3936 |
[32m[20221213 18:10:05 @agent_ppo2.py:185][0m |          -0.0040 |         158.0784 |           0.3937 |
[32m[20221213 18:10:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:10:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.12
[32m[20221213 18:10:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.85
[32m[20221213 18:10:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.94
[32m[20221213 18:10:05 @agent_ppo2.py:143][0m Total time:       5.92 min
[32m[20221213 18:10:05 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 18:10:05 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 18:10:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:05 @agent_ppo2.py:185][0m |          -0.0027 |         146.0988 |           0.3991 |
[32m[20221213 18:10:05 @agent_ppo2.py:185][0m |           0.0035 |         155.4564 |           0.3989 |
[32m[20221213 18:10:05 @agent_ppo2.py:185][0m |          -0.0109 |         133.5019 |           0.3988 |
[32m[20221213 18:10:05 @agent_ppo2.py:185][0m |          -0.0087 |         129.8790 |           0.3988 |
[32m[20221213 18:10:05 @agent_ppo2.py:185][0m |          -0.0107 |         127.7338 |           0.3989 |
[32m[20221213 18:10:06 @agent_ppo2.py:185][0m |          -0.0143 |         124.5438 |           0.3987 |
[32m[20221213 18:10:06 @agent_ppo2.py:185][0m |          -0.0152 |         122.8107 |           0.3988 |
[32m[20221213 18:10:06 @agent_ppo2.py:185][0m |          -0.0082 |         133.1807 |           0.3986 |
[32m[20221213 18:10:06 @agent_ppo2.py:185][0m |          -0.0137 |         118.4895 |           0.3991 |
[32m[20221213 18:10:06 @agent_ppo2.py:185][0m |          -0.0189 |         116.6491 |           0.3993 |
[32m[20221213 18:10:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.17
[32m[20221213 18:10:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.23
[32m[20221213 18:10:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 926.32
[32m[20221213 18:10:06 @agent_ppo2.py:143][0m Total time:       5.94 min
[32m[20221213 18:10:06 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 18:10:06 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 18:10:06 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:10:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:06 @agent_ppo2.py:185][0m |          -0.0007 |         180.6940 |           0.3876 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0021 |         174.7474 |           0.3877 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0036 |         170.8189 |           0.3878 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0080 |         168.4432 |           0.3878 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0028 |         167.4002 |           0.3878 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0026 |         171.2012 |           0.3880 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0109 |         164.6192 |           0.3879 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0105 |         163.7238 |           0.3881 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0061 |         163.4976 |           0.3882 |
[32m[20221213 18:10:07 @agent_ppo2.py:185][0m |          -0.0109 |         161.6452 |           0.3883 |
[32m[20221213 18:10:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.46
[32m[20221213 18:10:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.90
[32m[20221213 18:10:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.63
[32m[20221213 18:10:07 @agent_ppo2.py:143][0m Total time:       5.96 min
[32m[20221213 18:10:07 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 18:10:07 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 18:10:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |           0.0001 |         196.4875 |           0.4009 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |          -0.0026 |         191.2691 |           0.4009 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |           0.0002 |         189.1030 |           0.4009 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |          -0.0026 |         187.5879 |           0.4010 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |          -0.0047 |         186.3816 |           0.4007 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |          -0.0038 |         184.8233 |           0.4009 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |           0.0014 |         187.7497 |           0.4011 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |          -0.0043 |         182.6679 |           0.4012 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |          -0.0030 |         182.2368 |           0.4009 |
[32m[20221213 18:10:08 @agent_ppo2.py:185][0m |           0.0016 |         184.2888 |           0.4011 |
[32m[20221213 18:10:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.83
[32m[20221213 18:10:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.43
[32m[20221213 18:10:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.30
[32m[20221213 18:10:09 @agent_ppo2.py:143][0m Total time:       5.98 min
[32m[20221213 18:10:09 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 18:10:09 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 18:10:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |           0.0010 |         184.8539 |           0.4053 |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |          -0.0022 |         180.5317 |           0.4056 |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |          -0.0062 |         178.6144 |           0.4056 |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |          -0.0060 |         177.3674 |           0.4055 |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |          -0.0074 |         176.6809 |           0.4058 |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |          -0.0061 |         176.1109 |           0.4057 |
[32m[20221213 18:10:09 @agent_ppo2.py:185][0m |          -0.0101 |         175.7017 |           0.4060 |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0078 |         175.4416 |           0.4059 |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0117 |         174.7504 |           0.4062 |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0066 |         174.2013 |           0.4058 |
[32m[20221213 18:10:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 864.03
[32m[20221213 18:10:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.34
[32m[20221213 18:10:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 918.71
[32m[20221213 18:10:10 @agent_ppo2.py:143][0m Total time:       6.00 min
[32m[20221213 18:10:10 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 18:10:10 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 18:10:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0044 |         186.3953 |           0.3953 |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0051 |         183.0057 |           0.3943 |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0043 |         180.2565 |           0.3937 |
[32m[20221213 18:10:10 @agent_ppo2.py:185][0m |          -0.0054 |         176.7748 |           0.3935 |
[32m[20221213 18:10:11 @agent_ppo2.py:185][0m |          -0.0073 |         175.9056 |           0.3933 |
[32m[20221213 18:10:11 @agent_ppo2.py:185][0m |          -0.0085 |         174.1478 |           0.3936 |
[32m[20221213 18:10:11 @agent_ppo2.py:185][0m |          -0.0110 |         172.4343 |           0.3931 |
[32m[20221213 18:10:11 @agent_ppo2.py:185][0m |          -0.0090 |         171.6770 |           0.3930 |
[32m[20221213 18:10:11 @agent_ppo2.py:185][0m |          -0.0105 |         170.5113 |           0.3931 |
[32m[20221213 18:10:11 @agent_ppo2.py:185][0m |          -0.0123 |         169.6825 |           0.3928 |
[32m[20221213 18:10:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.51
[32m[20221213 18:10:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 994.98
[32m[20221213 18:10:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.18
[32m[20221213 18:10:11 @agent_ppo2.py:143][0m Total time:       6.02 min
[32m[20221213 18:10:11 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 18:10:11 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 18:10:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |           0.0042 |         194.1507 |           0.3906 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |           0.0072 |         193.0330 |           0.3898 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |          -0.0008 |         189.6075 |           0.3904 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |          -0.0068 |         184.9556 |           0.3892 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |          -0.0035 |         186.6039 |           0.3889 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |           0.0016 |         205.0552 |           0.3893 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |          -0.0038 |         184.2142 |           0.3882 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |          -0.0065 |         183.7793 |           0.3887 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |          -0.0081 |         183.1553 |           0.3886 |
[32m[20221213 18:10:12 @agent_ppo2.py:185][0m |           0.0028 |         201.6166 |           0.3884 |
[32m[20221213 18:10:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:10:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.67
[32m[20221213 18:10:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.19
[32m[20221213 18:10:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.58
[32m[20221213 18:10:12 @agent_ppo2.py:143][0m Total time:       6.04 min
[32m[20221213 18:10:12 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 18:10:12 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 18:10:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0003 |         185.6134 |           0.3893 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |           0.0075 |         192.7894 |           0.3885 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0069 |         178.7761 |           0.3892 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0023 |         176.7401 |           0.3890 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0045 |         175.1932 |           0.3894 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0060 |         174.5029 |           0.3898 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0065 |         173.3307 |           0.3900 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0059 |         172.8131 |           0.3899 |
[32m[20221213 18:10:13 @agent_ppo2.py:185][0m |          -0.0057 |         172.1640 |           0.3903 |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |          -0.0076 |         171.5282 |           0.3903 |
[32m[20221213 18:10:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.07
[32m[20221213 18:10:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 995.71
[32m[20221213 18:10:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.76
[32m[20221213 18:10:14 @agent_ppo2.py:143][0m Total time:       6.07 min
[32m[20221213 18:10:14 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 18:10:14 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 18:10:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |          -0.0019 |         198.3003 |           0.3997 |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |          -0.0042 |         195.2761 |           0.3986 |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |          -0.0062 |         194.1495 |           0.3980 |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |          -0.0067 |         193.4194 |           0.3977 |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |          -0.0076 |         192.7337 |           0.3977 |
[32m[20221213 18:10:14 @agent_ppo2.py:185][0m |           0.0037 |         216.7928 |           0.3971 |
[32m[20221213 18:10:15 @agent_ppo2.py:185][0m |          -0.0104 |         192.0244 |           0.3964 |
[32m[20221213 18:10:15 @agent_ppo2.py:185][0m |          -0.0090 |         191.5681 |           0.3961 |
[32m[20221213 18:10:15 @agent_ppo2.py:185][0m |          -0.0088 |         192.0637 |           0.3961 |
[32m[20221213 18:10:15 @agent_ppo2.py:185][0m |          -0.0095 |         191.0983 |           0.3957 |
[32m[20221213 18:10:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:10:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 909.08
[32m[20221213 18:10:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.29
[32m[20221213 18:10:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.77
[32m[20221213 18:10:15 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221213 18:10:15 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 18:10:15 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 18:10:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:15 @agent_ppo2.py:185][0m |           0.0009 |         193.0005 |           0.3945 |
[32m[20221213 18:10:15 @agent_ppo2.py:185][0m |          -0.0069 |         187.0513 |           0.3935 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0072 |         185.3985 |           0.3931 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0079 |         183.9564 |           0.3928 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0104 |         183.3533 |           0.3926 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0054 |         183.5956 |           0.3924 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0089 |         182.3822 |           0.3922 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0110 |         181.6307 |           0.3924 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0094 |         181.2382 |           0.3920 |
[32m[20221213 18:10:16 @agent_ppo2.py:185][0m |          -0.0114 |         180.9656 |           0.3921 |
[32m[20221213 18:10:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.42
[32m[20221213 18:10:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.93
[32m[20221213 18:10:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.15
[32m[20221213 18:10:16 @agent_ppo2.py:143][0m Total time:       6.11 min
[32m[20221213 18:10:16 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 18:10:16 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 18:10:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |           0.0002 |         197.1983 |           0.3947 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0036 |         193.6297 |           0.3943 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |           0.0127 |         218.9009 |           0.3939 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0071 |         188.7758 |           0.3927 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0012 |         188.4590 |           0.3928 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0086 |         185.8426 |           0.3926 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0096 |         185.2179 |           0.3924 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0078 |         184.6618 |           0.3922 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |          -0.0097 |         182.9334 |           0.3920 |
[32m[20221213 18:10:17 @agent_ppo2.py:185][0m |           0.0001 |         190.7732 |           0.3917 |
[32m[20221213 18:10:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.21
[32m[20221213 18:10:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.54
[32m[20221213 18:10:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 595.94
[32m[20221213 18:10:18 @agent_ppo2.py:143][0m Total time:       6.13 min
[32m[20221213 18:10:18 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 18:10:18 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 18:10:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0002 |         203.8936 |           0.3825 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0028 |         200.0247 |           0.3818 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0034 |         198.1419 |           0.3819 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0056 |         196.9820 |           0.3819 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0063 |         195.6051 |           0.3818 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0012 |         195.4711 |           0.3815 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |          -0.0070 |         193.6681 |           0.3820 |
[32m[20221213 18:10:18 @agent_ppo2.py:185][0m |           0.0013 |         198.1143 |           0.3812 |
[32m[20221213 18:10:19 @agent_ppo2.py:185][0m |          -0.0068 |         193.0415 |           0.3816 |
[32m[20221213 18:10:19 @agent_ppo2.py:185][0m |          -0.0015 |         193.6409 |           0.3812 |
[32m[20221213 18:10:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 866.71
[32m[20221213 18:10:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 995.61
[32m[20221213 18:10:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.56
[32m[20221213 18:10:19 @agent_ppo2.py:143][0m Total time:       6.15 min
[32m[20221213 18:10:19 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 18:10:19 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 18:10:19 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:10:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:19 @agent_ppo2.py:185][0m |           0.0063 |         198.0375 |           0.3882 |
[32m[20221213 18:10:19 @agent_ppo2.py:185][0m |          -0.0072 |         179.6682 |           0.3870 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0077 |         177.2320 |           0.3866 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0115 |         175.3281 |           0.3860 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0109 |         174.1125 |           0.3859 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0076 |         172.4172 |           0.3852 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0137 |         171.6317 |           0.3852 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0092 |         172.0817 |           0.3848 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0093 |         168.6347 |           0.3847 |
[32m[20221213 18:10:20 @agent_ppo2.py:185][0m |          -0.0123 |         167.5661 |           0.3845 |
[32m[20221213 18:10:20 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 18:10:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.84
[32m[20221213 18:10:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.74
[32m[20221213 18:10:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.81
[32m[20221213 18:10:20 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221213 18:10:20 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 18:10:20 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 18:10:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0005 |         177.8359 |           0.3845 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0086 |         172.3993 |           0.3844 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0089 |         169.6970 |           0.3844 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0107 |         168.5382 |           0.3845 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |           0.0017 |         182.9208 |           0.3844 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0079 |         165.6461 |           0.3847 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |           0.0083 |         183.8994 |           0.3852 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0079 |         164.7548 |           0.3853 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0112 |         162.7340 |           0.3855 |
[32m[20221213 18:10:21 @agent_ppo2.py:185][0m |          -0.0094 |         162.3438 |           0.3857 |
[32m[20221213 18:10:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:10:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.00
[32m[20221213 18:10:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.80
[32m[20221213 18:10:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 899.15
[32m[20221213 18:10:22 @agent_ppo2.py:143][0m Total time:       6.20 min
[32m[20221213 18:10:22 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 18:10:22 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 18:10:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |           0.0016 |         210.4042 |           0.3872 |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |          -0.0043 |         207.2305 |           0.3875 |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |          -0.0027 |         207.2656 |           0.3873 |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |          -0.0077 |         205.3657 |           0.3874 |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |          -0.0036 |         205.2833 |           0.3871 |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |          -0.0077 |         204.2052 |           0.3872 |
[32m[20221213 18:10:22 @agent_ppo2.py:185][0m |           0.0086 |         217.6755 |           0.3872 |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0009 |         211.8853 |           0.3871 |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0014 |         209.0644 |           0.3872 |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0077 |         202.8318 |           0.3872 |
[32m[20221213 18:10:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 865.91
[32m[20221213 18:10:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.09
[32m[20221213 18:10:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 957.43
[32m[20221213 18:10:23 @agent_ppo2.py:143][0m Total time:       6.22 min
[32m[20221213 18:10:23 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 18:10:23 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 18:10:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0042 |         225.3429 |           0.3880 |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0042 |         220.7412 |           0.3878 |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0061 |         218.9998 |           0.3877 |
[32m[20221213 18:10:23 @agent_ppo2.py:185][0m |          -0.0053 |         217.5843 |           0.3883 |
[32m[20221213 18:10:24 @agent_ppo2.py:185][0m |          -0.0042 |         217.1370 |           0.3886 |
[32m[20221213 18:10:24 @agent_ppo2.py:185][0m |          -0.0082 |         216.3556 |           0.3888 |
[32m[20221213 18:10:24 @agent_ppo2.py:185][0m |          -0.0076 |         215.8805 |           0.3891 |
[32m[20221213 18:10:24 @agent_ppo2.py:185][0m |          -0.0068 |         215.6755 |           0.3887 |
[32m[20221213 18:10:24 @agent_ppo2.py:185][0m |          -0.0088 |         215.2943 |           0.3891 |
[32m[20221213 18:10:24 @agent_ppo2.py:185][0m |          -0.0064 |         214.8397 |           0.3897 |
[32m[20221213 18:10:24 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:10:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.93
[32m[20221213 18:10:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.04
[32m[20221213 18:10:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 910.81
[32m[20221213 18:10:24 @agent_ppo2.py:143][0m Total time:       6.24 min
[32m[20221213 18:10:24 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 18:10:24 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 18:10:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0017 |         220.8585 |           0.3808 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0052 |         218.4805 |           0.3799 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0054 |         217.6187 |           0.3801 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0062 |         217.0983 |           0.3805 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0045 |         216.7900 |           0.3800 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0075 |         216.4556 |           0.3805 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0066 |         216.0646 |           0.3803 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0079 |         215.7321 |           0.3806 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0075 |         215.5154 |           0.3804 |
[32m[20221213 18:10:25 @agent_ppo2.py:185][0m |          -0.0088 |         215.2408 |           0.3808 |
[32m[20221213 18:10:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 856.14
[32m[20221213 18:10:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.42
[32m[20221213 18:10:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.99
[32m[20221213 18:10:25 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221213 18:10:25 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 18:10:25 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 18:10:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |           0.0024 |         204.9398 |           0.3871 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0089 |         198.9319 |           0.3868 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0090 |         197.5454 |           0.3869 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0122 |         195.7898 |           0.3864 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0109 |         194.9038 |           0.3864 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0101 |         194.3390 |           0.3860 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0120 |         193.5568 |           0.3860 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0118 |         193.1856 |           0.3855 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0123 |         192.5947 |           0.3855 |
[32m[20221213 18:10:26 @agent_ppo2.py:185][0m |          -0.0123 |         192.3853 |           0.3852 |
[32m[20221213 18:10:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:10:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.94
[32m[20221213 18:10:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 905.27
[32m[20221213 18:10:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.83
[32m[20221213 18:10:27 @agent_ppo2.py:143][0m Total time:       6.28 min
[32m[20221213 18:10:27 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 18:10:27 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 18:10:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:27 @agent_ppo2.py:185][0m |           0.0016 |         203.5572 |           0.3915 |
[32m[20221213 18:10:27 @agent_ppo2.py:185][0m |          -0.0085 |         193.2347 |           0.3918 |
[32m[20221213 18:10:27 @agent_ppo2.py:185][0m |           0.0043 |         203.1279 |           0.3918 |
[32m[20221213 18:10:27 @agent_ppo2.py:185][0m |          -0.0099 |         190.3359 |           0.3912 |
[32m[20221213 18:10:27 @agent_ppo2.py:185][0m |          -0.0079 |         188.8119 |           0.3913 |
[32m[20221213 18:10:27 @agent_ppo2.py:185][0m |          -0.0109 |         187.7767 |           0.3908 |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0056 |         189.1798 |           0.3908 |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0130 |         185.7435 |           0.3902 |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0142 |         185.0560 |           0.3905 |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0138 |         183.8056 |           0.3907 |
[32m[20221213 18:10:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.33
[32m[20221213 18:10:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 893.55
[32m[20221213 18:10:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 911.72
[32m[20221213 18:10:28 @agent_ppo2.py:143][0m Total time:       6.30 min
[32m[20221213 18:10:28 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 18:10:28 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 18:10:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0020 |         205.2169 |           0.3929 |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0051 |         202.4238 |           0.3937 |
[32m[20221213 18:10:28 @agent_ppo2.py:185][0m |          -0.0062 |         201.3586 |           0.3926 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |          -0.0068 |         200.7348 |           0.3935 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |           0.0009 |         213.8760 |           0.3931 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |          -0.0067 |         200.6423 |           0.3933 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |          -0.0070 |         200.0995 |           0.3933 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |          -0.0075 |         200.4249 |           0.3935 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |          -0.0077 |         200.0985 |           0.3933 |
[32m[20221213 18:10:29 @agent_ppo2.py:185][0m |          -0.0063 |         200.2868 |           0.3932 |
[32m[20221213 18:10:29 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:10:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 975.42
[32m[20221213 18:10:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 990.41
[32m[20221213 18:10:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.68
[32m[20221213 18:10:29 @agent_ppo2.py:143][0m Total time:       6.32 min
[32m[20221213 18:10:29 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 18:10:29 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 18:10:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |           0.0031 |         217.6700 |           0.3885 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0020 |         213.4620 |           0.3888 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0060 |         211.1183 |           0.3886 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0055 |         210.3508 |           0.3889 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0083 |         209.7129 |           0.3884 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0092 |         209.1993 |           0.3890 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0090 |         209.0053 |           0.3885 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0102 |         208.6491 |           0.3886 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0080 |         208.4540 |           0.3889 |
[32m[20221213 18:10:30 @agent_ppo2.py:185][0m |          -0.0109 |         208.1479 |           0.3888 |
[32m[20221213 18:10:30 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:10:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.65
[32m[20221213 18:10:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 918.93
[32m[20221213 18:10:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.90
[32m[20221213 18:10:30 @agent_ppo2.py:143][0m Total time:       6.35 min
[32m[20221213 18:10:30 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 18:10:30 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 18:10:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0034 |         216.7637 |           0.3994 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0082 |         210.1990 |           0.3983 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0078 |         206.7675 |           0.3984 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0076 |         203.7726 |           0.3980 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0089 |         201.8966 |           0.3978 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0082 |         200.1860 |           0.3978 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0075 |         198.5002 |           0.3979 |
[32m[20221213 18:10:31 @agent_ppo2.py:185][0m |          -0.0076 |         197.3506 |           0.3978 |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0121 |         195.7371 |           0.3975 |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0086 |         195.4831 |           0.3980 |
[32m[20221213 18:10:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:10:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.09
[32m[20221213 18:10:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.41
[32m[20221213 18:10:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 881.71
[32m[20221213 18:10:32 @agent_ppo2.py:143][0m Total time:       6.37 min
[32m[20221213 18:10:32 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 18:10:32 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 18:10:32 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:10:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0023 |         199.3680 |           0.3901 |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0022 |         192.3523 |           0.3900 |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0054 |         185.6241 |           0.3901 |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0058 |         183.4284 |           0.3904 |
[32m[20221213 18:10:32 @agent_ppo2.py:185][0m |          -0.0067 |         181.7434 |           0.3902 |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0051 |         181.1047 |           0.3906 |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0083 |         181.0636 |           0.3911 |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0082 |         179.4471 |           0.3916 |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0079 |         179.6699 |           0.3919 |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0102 |         178.9150 |           0.3922 |
[32m[20221213 18:10:33 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:10:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.75
[32m[20221213 18:10:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 931.18
[32m[20221213 18:10:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 912.88
[32m[20221213 18:10:33 @agent_ppo2.py:143][0m Total time:       6.39 min
[32m[20221213 18:10:33 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 18:10:33 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 18:10:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0058 |         229.7536 |           0.3947 |
[32m[20221213 18:10:33 @agent_ppo2.py:185][0m |          -0.0074 |         225.3436 |           0.3941 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0073 |         223.1070 |           0.3943 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0067 |         223.3924 |           0.3949 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0071 |         223.8379 |           0.3949 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0083 |         220.8508 |           0.3947 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0056 |         223.0971 |           0.3951 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0084 |         219.8780 |           0.3952 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0109 |         219.3071 |           0.3954 |
[32m[20221213 18:10:34 @agent_ppo2.py:185][0m |          -0.0114 |         219.1269 |           0.3957 |
[32m[20221213 18:10:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.45
[32m[20221213 18:10:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 924.14
[32m[20221213 18:10:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 876.81
[32m[20221213 18:10:34 @agent_ppo2.py:143][0m Total time:       6.41 min
[32m[20221213 18:10:34 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 18:10:34 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 18:10:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0029 |         226.3437 |           0.4090 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0060 |         222.5857 |           0.4081 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0050 |         221.1192 |           0.4082 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0041 |         220.1613 |           0.4088 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0066 |         219.4498 |           0.4085 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0064 |         219.4954 |           0.4088 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0001 |         238.0102 |           0.4089 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0076 |         218.1433 |           0.4092 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0069 |         217.5133 |           0.4094 |
[32m[20221213 18:10:35 @agent_ppo2.py:185][0m |          -0.0114 |         217.7456 |           0.4094 |
[32m[20221213 18:10:35 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:10:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.84
[32m[20221213 18:10:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.19
[32m[20221213 18:10:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.67
[32m[20221213 18:10:36 @agent_ppo2.py:143][0m Total time:       6.43 min
[32m[20221213 18:10:36 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 18:10:36 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 18:10:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |          -0.0029 |         219.7333 |           0.4299 |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |          -0.0033 |         218.4070 |           0.4290 |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |           0.0028 |         220.2399 |           0.4284 |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |          -0.0074 |         213.7173 |           0.4282 |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |          -0.0068 |         213.2579 |           0.4282 |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |          -0.0015 |         217.4193 |           0.4279 |
[32m[20221213 18:10:36 @agent_ppo2.py:185][0m |           0.0041 |         240.9829 |           0.4274 |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0031 |         211.8383 |           0.4276 |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0085 |         211.8842 |           0.4273 |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0097 |         211.3892 |           0.4268 |
[32m[20221213 18:10:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:10:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 850.65
[32m[20221213 18:10:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 929.57
[32m[20221213 18:10:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.72
[32m[20221213 18:10:37 @agent_ppo2.py:143][0m Total time:       6.45 min
[32m[20221213 18:10:37 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 18:10:37 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 18:10:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0007 |         221.2139 |           0.4068 |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0027 |         218.1274 |           0.4059 |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0059 |         216.5791 |           0.4056 |
[32m[20221213 18:10:37 @agent_ppo2.py:185][0m |          -0.0054 |         215.4812 |           0.4053 |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |          -0.0063 |         215.1321 |           0.4055 |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |          -0.0060 |         216.6860 |           0.4055 |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |          -0.0089 |         213.8747 |           0.4051 |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |          -0.0095 |         213.8010 |           0.4053 |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |          -0.0093 |         213.2946 |           0.4046 |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |           0.0104 |         253.0630 |           0.4052 |
[32m[20221213 18:10:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.91
[32m[20221213 18:10:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.36
[32m[20221213 18:10:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.54
[32m[20221213 18:10:38 @agent_ppo2.py:143][0m Total time:       6.47 min
[32m[20221213 18:10:38 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 18:10:38 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 18:10:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:38 @agent_ppo2.py:185][0m |          -0.0016 |         207.3217 |           0.4091 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |           0.0077 |         224.8717 |           0.4091 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0063 |         200.9114 |           0.4090 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0079 |         199.7630 |           0.4095 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0092 |         198.9916 |           0.4097 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0090 |         198.3996 |           0.4096 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0066 |         198.5100 |           0.4096 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0099 |         197.8673 |           0.4097 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0112 |         197.5226 |           0.4097 |
[32m[20221213 18:10:39 @agent_ppo2.py:185][0m |          -0.0102 |         197.2152 |           0.4099 |
[32m[20221213 18:10:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:10:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 888.79
[32m[20221213 18:10:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.45
[32m[20221213 18:10:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.88
[32m[20221213 18:10:39 @agent_ppo2.py:143][0m Total time:       6.49 min
[32m[20221213 18:10:39 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 18:10:39 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 18:10:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:10:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0030 |         217.0234 |           0.4078 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |           0.0000 |         210.4143 |           0.4076 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0010 |         216.2590 |           0.4076 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |           0.0049 |         227.3476 |           0.4081 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0094 |         201.5869 |           0.4080 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0051 |         204.2674 |           0.4074 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0105 |         199.0768 |           0.4078 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0117 |         197.8625 |           0.4082 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0118 |         196.9869 |           0.4083 |
[32m[20221213 18:10:40 @agent_ppo2.py:185][0m |          -0.0121 |         196.1966 |           0.4088 |
[32m[20221213 18:10:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.61
[32m[20221213 18:10:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 858.62
[32m[20221213 18:10:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 930.78
[32m[20221213 18:10:41 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221213 18:10:41 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 18:10:41 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 18:10:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:41 @agent_ppo2.py:185][0m |           0.0007 |         224.3467 |           0.4171 |
[32m[20221213 18:10:41 @agent_ppo2.py:185][0m |          -0.0048 |         220.2057 |           0.4169 |
[32m[20221213 18:10:41 @agent_ppo2.py:185][0m |          -0.0064 |         218.2511 |           0.4163 |
[32m[20221213 18:10:41 @agent_ppo2.py:185][0m |          -0.0041 |         219.5408 |           0.4161 |
[32m[20221213 18:10:41 @agent_ppo2.py:185][0m |          -0.0078 |         216.1972 |           0.4160 |
[32m[20221213 18:10:41 @agent_ppo2.py:185][0m |          -0.0092 |         215.5006 |           0.4159 |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |          -0.0079 |         214.9422 |           0.4160 |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |          -0.0082 |         214.5119 |           0.4162 |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |          -0.0055 |         215.7448 |           0.4162 |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |          -0.0093 |         213.7320 |           0.4158 |
[32m[20221213 18:10:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:10:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.11
[32m[20221213 18:10:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.46
[32m[20221213 18:10:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.28
[32m[20221213 18:10:42 @agent_ppo2.py:143][0m Total time:       6.54 min
[32m[20221213 18:10:42 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 18:10:42 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 18:10:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |           0.0004 |         217.7585 |           0.4058 |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |          -0.0037 |         214.9369 |           0.4049 |
[32m[20221213 18:10:42 @agent_ppo2.py:185][0m |           0.0267 |         263.9854 |           0.4046 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |          -0.0065 |         213.3070 |           0.4033 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |          -0.0069 |         212.8131 |           0.4041 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |          -0.0066 |         212.2786 |           0.4036 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |          -0.0075 |         211.9139 |           0.4034 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |          -0.0076 |         211.5913 |           0.4031 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |           0.0010 |         220.5325 |           0.4030 |
[32m[20221213 18:10:43 @agent_ppo2.py:185][0m |          -0.0084 |         211.0901 |           0.4024 |
[32m[20221213 18:10:43 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:10:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.11
[32m[20221213 18:10:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 925.98
[32m[20221213 18:10:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.22
[32m[20221213 18:10:43 @agent_ppo2.py:143][0m Total time:       6.56 min
[32m[20221213 18:10:43 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 18:10:43 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 18:10:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0035 |         220.3428 |           0.4064 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0030 |         216.7051 |           0.4065 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0053 |         214.4523 |           0.4063 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0063 |         213.3458 |           0.4067 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |           0.0042 |         228.3131 |           0.4068 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0072 |         210.8952 |           0.4069 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0051 |         210.5708 |           0.4074 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0086 |         209.4473 |           0.4074 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0064 |         208.9777 |           0.4075 |
[32m[20221213 18:10:44 @agent_ppo2.py:185][0m |          -0.0087 |         208.4219 |           0.4076 |
[32m[20221213 18:10:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:10:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.18
[32m[20221213 18:10:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 910.86
[32m[20221213 18:10:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.91
[32m[20221213 18:10:44 @agent_ppo2.py:143][0m Total time:       6.58 min
[32m[20221213 18:10:44 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 18:10:44 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 18:10:45 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:10:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |           0.0008 |         209.5271 |           0.4035 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0050 |         204.7109 |           0.4032 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0036 |         202.5542 |           0.4031 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0078 |         199.9890 |           0.4031 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0049 |         198.6330 |           0.4032 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0088 |         197.0651 |           0.4033 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0080 |         195.9559 |           0.4034 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0058 |         195.5000 |           0.4037 |
[32m[20221213 18:10:45 @agent_ppo2.py:185][0m |          -0.0095 |         193.4166 |           0.4036 |
[32m[20221213 18:10:46 @agent_ppo2.py:185][0m |          -0.0090 |         192.4091 |           0.4036 |
[32m[20221213 18:10:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:10:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.32
[32m[20221213 18:10:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.28
[32m[20221213 18:10:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 934.04
[32m[20221213 18:10:46 @agent_ppo2.py:143][0m Total time:       6.60 min
[32m[20221213 18:10:46 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 18:10:46 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 18:10:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:46 @agent_ppo2.py:185][0m |          -0.0032 |         171.3062 |           0.4180 |
[32m[20221213 18:10:46 @agent_ppo2.py:185][0m |          -0.0094 |         161.0208 |           0.4176 |
[32m[20221213 18:10:46 @agent_ppo2.py:185][0m |          -0.0122 |         156.2234 |           0.4172 |
[32m[20221213 18:10:46 @agent_ppo2.py:185][0m |          -0.0124 |         153.0889 |           0.4172 |
[32m[20221213 18:10:46 @agent_ppo2.py:185][0m |          -0.0179 |         150.7977 |           0.4172 |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |          -0.0137 |         148.7413 |           0.4175 |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |          -0.0170 |         147.2035 |           0.4177 |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |          -0.0158 |         146.0456 |           0.4179 |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |          -0.0147 |         145.7075 |           0.4183 |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |          -0.0169 |         143.9359 |           0.4182 |
[32m[20221213 18:10:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:10:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.04
[32m[20221213 18:10:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 909.75
[32m[20221213 18:10:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.41
[32m[20221213 18:10:47 @agent_ppo2.py:143][0m Total time:       6.62 min
[32m[20221213 18:10:47 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 18:10:47 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 18:10:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |           0.0006 |         232.0711 |           0.4262 |
[32m[20221213 18:10:47 @agent_ppo2.py:185][0m |          -0.0034 |         226.4357 |           0.4261 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0069 |         224.7317 |           0.4262 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |           0.0019 |         233.7050 |           0.4261 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0086 |         222.4196 |           0.4257 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0087 |         221.6028 |           0.4259 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0070 |         221.0413 |           0.4256 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0050 |         223.9470 |           0.4256 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0098 |         220.4057 |           0.4256 |
[32m[20221213 18:10:48 @agent_ppo2.py:185][0m |          -0.0071 |         219.7703 |           0.4258 |
[32m[20221213 18:10:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:10:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 854.00
[32m[20221213 18:10:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 932.52
[32m[20221213 18:10:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.75
[32m[20221213 18:10:48 @agent_ppo2.py:143][0m Total time:       6.64 min
[32m[20221213 18:10:48 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 18:10:48 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 18:10:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |           0.0043 |         238.6327 |           0.4331 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0051 |         228.9099 |           0.4324 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0067 |         227.0391 |           0.4327 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0051 |         225.6049 |           0.4325 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0062 |         224.2061 |           0.4327 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0102 |         224.1749 |           0.4326 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0076 |         222.7714 |           0.4326 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0039 |         224.1560 |           0.4329 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0073 |         222.6668 |           0.4327 |
[32m[20221213 18:10:49 @agent_ppo2.py:185][0m |          -0.0048 |         221.8962 |           0.4332 |
[32m[20221213 18:10:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:10:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 893.25
[32m[20221213 18:10:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.16
[32m[20221213 18:10:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.20
[32m[20221213 18:10:50 @agent_ppo2.py:143][0m Total time:       6.67 min
[32m[20221213 18:10:50 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 18:10:50 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 18:10:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:50 @agent_ppo2.py:185][0m |           0.0003 |         225.8541 |           0.4324 |
[32m[20221213 18:10:50 @agent_ppo2.py:185][0m |           0.0016 |         227.4636 |           0.4322 |
[32m[20221213 18:10:50 @agent_ppo2.py:185][0m |          -0.0060 |         210.7313 |           0.4315 |
[32m[20221213 18:10:50 @agent_ppo2.py:185][0m |          -0.0031 |         204.0808 |           0.4319 |
[32m[20221213 18:10:50 @agent_ppo2.py:185][0m |          -0.0082 |         200.1582 |           0.4315 |
[32m[20221213 18:10:50 @agent_ppo2.py:185][0m |          -0.0086 |         197.4269 |           0.4322 |
[32m[20221213 18:10:51 @agent_ppo2.py:185][0m |          -0.0099 |         195.9106 |           0.4322 |
[32m[20221213 18:10:51 @agent_ppo2.py:185][0m |          -0.0100 |         193.7509 |           0.4316 |
[32m[20221213 18:10:51 @agent_ppo2.py:185][0m |          -0.0044 |         192.9023 |           0.4317 |
[32m[20221213 18:10:51 @agent_ppo2.py:185][0m |          -0.0088 |         192.2324 |           0.4319 |
[32m[20221213 18:10:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:10:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.97
[32m[20221213 18:10:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.59
[32m[20221213 18:10:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.91
[32m[20221213 18:10:51 @agent_ppo2.py:143][0m Total time:       6.69 min
[32m[20221213 18:10:51 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 18:10:51 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 18:10:51 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:10:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:51 @agent_ppo2.py:185][0m |          -0.0035 |         233.2866 |           0.4338 |
[32m[20221213 18:10:51 @agent_ppo2.py:185][0m |          -0.0038 |         228.2002 |           0.4336 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0063 |         224.8425 |           0.4334 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0091 |         223.7315 |           0.4332 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0037 |         223.9895 |           0.4330 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0102 |         222.0654 |           0.4326 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0017 |         227.6327 |           0.4326 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0054 |         221.1031 |           0.4325 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |           0.0035 |         237.9912 |           0.4327 |
[32m[20221213 18:10:52 @agent_ppo2.py:185][0m |          -0.0042 |         221.3240 |           0.4327 |
[32m[20221213 18:10:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:10:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.05
[32m[20221213 18:10:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 894.70
[32m[20221213 18:10:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.88
[32m[20221213 18:10:52 @agent_ppo2.py:143][0m Total time:       6.71 min
[32m[20221213 18:10:52 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 18:10:52 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 18:10:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |           0.0006 |         190.5786 |           0.4316 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0045 |         179.7807 |           0.4312 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0016 |         175.6759 |           0.4307 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0086 |         165.3799 |           0.4305 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0099 |         161.4083 |           0.4303 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0096 |         157.4966 |           0.4304 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0055 |         155.4732 |           0.4304 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0028 |         164.9006 |           0.4305 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0163 |         150.9247 |           0.4307 |
[32m[20221213 18:10:53 @agent_ppo2.py:185][0m |          -0.0177 |         147.9692 |           0.4308 |
[32m[20221213 18:10:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.57
[32m[20221213 18:10:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.29
[32m[20221213 18:10:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.30
[32m[20221213 18:10:54 @agent_ppo2.py:143][0m Total time:       6.73 min
[32m[20221213 18:10:54 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 18:10:54 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 18:10:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0013 |         189.7040 |           0.4273 |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0043 |         180.7593 |           0.4276 |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0067 |         177.9741 |           0.4279 |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0080 |         176.1561 |           0.4282 |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0022 |         183.9020 |           0.4284 |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0096 |         174.4222 |           0.4283 |
[32m[20221213 18:10:54 @agent_ppo2.py:185][0m |          -0.0045 |         175.1957 |           0.4284 |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |          -0.0105 |         170.2605 |           0.4287 |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |          -0.0128 |         170.2804 |           0.4289 |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |          -0.0101 |         168.7357 |           0.4293 |
[32m[20221213 18:10:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 586.29
[32m[20221213 18:10:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.09
[32m[20221213 18:10:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 903.82
[32m[20221213 18:10:55 @agent_ppo2.py:143][0m Total time:       6.75 min
[32m[20221213 18:10:55 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 18:10:55 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 18:10:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |           0.0009 |         211.2438 |           0.4419 |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |           0.0003 |         204.8984 |           0.4411 |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |          -0.0022 |         203.5416 |           0.4409 |
[32m[20221213 18:10:55 @agent_ppo2.py:185][0m |          -0.0086 |         200.8212 |           0.4409 |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0064 |         199.0149 |           0.4409 |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0070 |         197.6825 |           0.4411 |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0050 |         196.1783 |           0.4412 |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0071 |         195.9883 |           0.4410 |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0048 |         194.5270 |           0.4412 |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0098 |         195.1454 |           0.4413 |
[32m[20221213 18:10:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:10:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.52
[32m[20221213 18:10:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.75
[32m[20221213 18:10:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.78
[32m[20221213 18:10:56 @agent_ppo2.py:143][0m Total time:       6.77 min
[32m[20221213 18:10:56 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 18:10:56 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 18:10:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:10:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:56 @agent_ppo2.py:185][0m |          -0.0006 |         210.6638 |           0.4295 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0066 |         207.5340 |           0.4299 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0068 |         206.7823 |           0.4295 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0077 |         206.4264 |           0.4301 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0059 |         206.8445 |           0.4301 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0030 |         209.9507 |           0.4305 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |           0.0019 |         213.2516 |           0.4308 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0087 |         205.5849 |           0.4312 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0098 |         205.2977 |           0.4312 |
[32m[20221213 18:10:57 @agent_ppo2.py:185][0m |          -0.0082 |         205.2586 |           0.4316 |
[32m[20221213 18:10:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:10:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.58
[32m[20221213 18:10:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.72
[32m[20221213 18:10:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.87
[32m[20221213 18:10:57 @agent_ppo2.py:143][0m Total time:       6.79 min
[32m[20221213 18:10:57 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 18:10:57 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 18:10:58 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:10:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0010 |         217.2514 |           0.4470 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0019 |         212.9249 |           0.4467 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0049 |         210.8786 |           0.4468 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0051 |         210.0705 |           0.4468 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0083 |         208.3275 |           0.4474 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0046 |         207.5193 |           0.4474 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0086 |         206.5925 |           0.4477 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0060 |         206.4908 |           0.4480 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0011 |         208.6067 |           0.4482 |
[32m[20221213 18:10:58 @agent_ppo2.py:185][0m |          -0.0043 |         206.0407 |           0.4482 |
[32m[20221213 18:10:58 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:10:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.26
[32m[20221213 18:10:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 932.46
[32m[20221213 18:10:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.01
[32m[20221213 18:10:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 980.01
[32m[20221213 18:10:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 980.01
[32m[20221213 18:10:59 @agent_ppo2.py:143][0m Total time:       6.81 min
[32m[20221213 18:10:59 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 18:10:59 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 18:10:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:10:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0018 |         205.3016 |           0.4586 |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0045 |         200.1498 |           0.4575 |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0071 |         198.1922 |           0.4574 |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0050 |         197.1636 |           0.4570 |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0067 |         195.6331 |           0.4571 |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0071 |         194.9956 |           0.4569 |
[32m[20221213 18:10:59 @agent_ppo2.py:185][0m |          -0.0069 |         193.9123 |           0.4570 |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0020 |         197.9149 |           0.4570 |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0077 |         192.8233 |           0.4562 |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0102 |         192.3721 |           0.4568 |
[32m[20221213 18:11:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:11:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.17
[32m[20221213 18:11:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.94
[32m[20221213 18:11:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.89
[32m[20221213 18:11:00 @agent_ppo2.py:143][0m Total time:       6.84 min
[32m[20221213 18:11:00 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 18:11:00 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 18:11:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0020 |         202.0007 |           0.4657 |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0024 |         194.2970 |           0.4657 |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0052 |         189.1345 |           0.4656 |
[32m[20221213 18:11:00 @agent_ppo2.py:185][0m |          -0.0058 |         185.3042 |           0.4656 |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |           0.0067 |         197.9647 |           0.4656 |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |          -0.0016 |         185.0241 |           0.4657 |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |           0.0081 |         203.9822 |           0.4655 |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |          -0.0078 |         174.6459 |           0.4653 |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |          -0.0094 |         172.6021 |           0.4659 |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |          -0.0084 |         170.8757 |           0.4660 |
[32m[20221213 18:11:01 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:11:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.57
[32m[20221213 18:11:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.45
[32m[20221213 18:11:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 969.84
[32m[20221213 18:11:01 @agent_ppo2.py:143][0m Total time:       6.86 min
[32m[20221213 18:11:01 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 18:11:01 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 18:11:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:01 @agent_ppo2.py:185][0m |          -0.0022 |         215.4344 |           0.4736 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |           0.0006 |         217.0621 |           0.4733 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0060 |         211.8342 |           0.4729 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0049 |         211.0713 |           0.4728 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0062 |         210.5835 |           0.4734 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0054 |         210.1425 |           0.4736 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0062 |         210.1213 |           0.4730 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0075 |         209.7485 |           0.4730 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0075 |         209.7248 |           0.4739 |
[32m[20221213 18:11:02 @agent_ppo2.py:185][0m |          -0.0073 |         209.3620 |           0.4735 |
[32m[20221213 18:11:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:11:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 956.41
[32m[20221213 18:11:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.42
[32m[20221213 18:11:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.91
[32m[20221213 18:11:02 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 999.91
[32m[20221213 18:11:02 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 999.91
[32m[20221213 18:11:02 @agent_ppo2.py:143][0m Total time:       6.88 min
[32m[20221213 18:11:02 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 18:11:02 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 18:11:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0009 |         223.3155 |           0.4488 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0024 |         219.1612 |           0.4485 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0039 |         217.7953 |           0.4481 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |           0.0070 |         238.7816 |           0.4482 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0007 |         216.3576 |           0.4476 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0083 |         215.0865 |           0.4477 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0012 |         218.4174 |           0.4478 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0020 |         214.1574 |           0.4478 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0083 |         213.1161 |           0.4476 |
[32m[20221213 18:11:03 @agent_ppo2.py:185][0m |          -0.0015 |         223.8936 |           0.4478 |
[32m[20221213 18:11:03 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:11:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 862.13
[32m[20221213 18:11:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.91
[32m[20221213 18:11:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.42
[32m[20221213 18:11:04 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221213 18:11:04 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221213 18:11:04 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 18:11:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |           0.0085 |         231.5716 |           0.4649 |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |          -0.0038 |         214.9958 |           0.4646 |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |          -0.0036 |         214.1465 |           0.4645 |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |           0.0003 |         216.2779 |           0.4649 |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |          -0.0044 |         213.6446 |           0.4646 |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |           0.0076 |         230.5193 |           0.4647 |
[32m[20221213 18:11:04 @agent_ppo2.py:185][0m |          -0.0045 |         213.4878 |           0.4646 |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |          -0.0054 |         213.4710 |           0.4647 |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |          -0.0008 |         216.0501 |           0.4649 |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |           0.0019 |         221.3604 |           0.4654 |
[32m[20221213 18:11:05 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:11:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.41
[32m[20221213 18:11:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.75
[32m[20221213 18:11:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.19
[32m[20221213 18:11:05 @agent_ppo2.py:143][0m Total time:       6.92 min
[32m[20221213 18:11:05 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 18:11:05 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 18:11:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |          -0.0019 |         222.4647 |           0.4707 |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |          -0.0044 |         220.4495 |           0.4686 |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |          -0.0019 |         220.8709 |           0.4693 |
[32m[20221213 18:11:05 @agent_ppo2.py:185][0m |          -0.0075 |         219.1809 |           0.4688 |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |          -0.0080 |         218.7034 |           0.4682 |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |          -0.0066 |         219.1378 |           0.4681 |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |          -0.0087 |         216.9562 |           0.4678 |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |          -0.0064 |         216.0198 |           0.4688 |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |          -0.0059 |         217.7519 |           0.4681 |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |          -0.0098 |         213.0451 |           0.4679 |
[32m[20221213 18:11:06 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:11:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 913.96
[32m[20221213 18:11:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 983.12
[32m[20221213 18:11:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.33
[32m[20221213 18:11:06 @agent_ppo2.py:143][0m Total time:       6.94 min
[32m[20221213 18:11:06 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221213 18:11:06 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 18:11:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:06 @agent_ppo2.py:185][0m |           0.0060 |         241.2540 |           0.4595 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0056 |         220.2466 |           0.4586 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0066 |         218.0105 |           0.4582 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0064 |         215.9527 |           0.4583 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |           0.0060 |         241.9669 |           0.4584 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0090 |         212.4437 |           0.4585 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0060 |         210.7843 |           0.4583 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0092 |         209.1105 |           0.4581 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0088 |         208.3192 |           0.4582 |
[32m[20221213 18:11:07 @agent_ppo2.py:185][0m |          -0.0008 |         224.5692 |           0.4582 |
[32m[20221213 18:11:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:11:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.53
[32m[20221213 18:11:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 884.60
[32m[20221213 18:11:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 883.56
[32m[20221213 18:11:07 @agent_ppo2.py:143][0m Total time:       6.96 min
[32m[20221213 18:11:07 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 18:11:07 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 18:11:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0031 |         222.5474 |           0.4546 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0057 |         218.6618 |           0.4542 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0065 |         217.1545 |           0.4536 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |           0.0014 |         230.4254 |           0.4535 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0071 |         215.2779 |           0.4535 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0052 |         215.8309 |           0.4533 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0080 |         213.9331 |           0.4534 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0088 |         213.6978 |           0.4532 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0091 |         213.3940 |           0.4530 |
[32m[20221213 18:11:08 @agent_ppo2.py:185][0m |          -0.0098 |         213.1115 |           0.4532 |
[32m[20221213 18:11:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:11:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 928.27
[32m[20221213 18:11:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.85
[32m[20221213 18:11:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.41
[32m[20221213 18:11:09 @agent_ppo2.py:143][0m Total time:       6.98 min
[32m[20221213 18:11:09 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221213 18:11:09 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 18:11:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |           0.0015 |         222.3319 |           0.4699 |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |          -0.0042 |         219.2135 |           0.4689 |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |          -0.0041 |         218.1552 |           0.4687 |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |          -0.0057 |         217.1737 |           0.4684 |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |          -0.0087 |         216.5741 |           0.4689 |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |          -0.0067 |         215.8949 |           0.4690 |
[32m[20221213 18:11:09 @agent_ppo2.py:185][0m |          -0.0068 |         215.8280 |           0.4685 |
[32m[20221213 18:11:10 @agent_ppo2.py:185][0m |          -0.0065 |         215.1274 |           0.4689 |
[32m[20221213 18:11:10 @agent_ppo2.py:185][0m |          -0.0044 |         214.8180 |           0.4691 |
[32m[20221213 18:11:10 @agent_ppo2.py:185][0m |          -0.0105 |         214.5096 |           0.4687 |
[32m[20221213 18:11:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.56
[32m[20221213 18:11:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.45
[32m[20221213 18:11:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.92
[32m[20221213 18:11:10 @agent_ppo2.py:143][0m Total time:       7.00 min
[32m[20221213 18:11:10 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 18:11:10 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 18:11:10 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:11:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:10 @agent_ppo2.py:185][0m |          -0.0018 |         219.3687 |           0.4680 |
[32m[20221213 18:11:10 @agent_ppo2.py:185][0m |          -0.0043 |         217.3322 |           0.4666 |
[32m[20221213 18:11:10 @agent_ppo2.py:185][0m |          -0.0053 |         216.6521 |           0.4660 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0054 |         215.7330 |           0.4658 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0063 |         215.3731 |           0.4655 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0057 |         215.1269 |           0.4651 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0068 |         214.5931 |           0.4650 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0074 |         214.3757 |           0.4646 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0083 |         214.0531 |           0.4644 |
[32m[20221213 18:11:11 @agent_ppo2.py:185][0m |          -0.0083 |         213.8888 |           0.4645 |
[32m[20221213 18:11:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:11:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.09
[32m[20221213 18:11:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 910.45
[32m[20221213 18:11:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 913.09
[32m[20221213 18:11:11 @agent_ppo2.py:143][0m Total time:       7.02 min
[32m[20221213 18:11:11 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221213 18:11:11 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 18:11:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0022 |         210.9622 |           0.4670 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0055 |         205.2424 |           0.4653 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |           0.0012 |         230.3437 |           0.4644 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0056 |         202.3610 |           0.4641 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0106 |         200.8273 |           0.4639 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0045 |         200.9548 |           0.4640 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0096 |         199.0286 |           0.4635 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0098 |         198.6663 |           0.4634 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0048 |         202.8860 |           0.4637 |
[32m[20221213 18:11:12 @agent_ppo2.py:185][0m |          -0.0106 |         197.2275 |           0.4629 |
[32m[20221213 18:11:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:11:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.94
[32m[20221213 18:11:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.49
[32m[20221213 18:11:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.67
[32m[20221213 18:11:12 @agent_ppo2.py:143][0m Total time:       7.04 min
[32m[20221213 18:11:12 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 18:11:12 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 18:11:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0006 |         223.3114 |           0.4527 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0015 |         214.8155 |           0.4516 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0025 |         210.9297 |           0.4517 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0062 |         208.5263 |           0.4519 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0064 |         207.6932 |           0.4519 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0070 |         207.8157 |           0.4518 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0082 |         205.9285 |           0.4516 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0042 |         205.6856 |           0.4517 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0087 |         205.3658 |           0.4514 |
[32m[20221213 18:11:13 @agent_ppo2.py:185][0m |          -0.0089 |         204.5668 |           0.4510 |
[32m[20221213 18:11:13 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:11:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.18
[32m[20221213 18:11:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 931.02
[32m[20221213 18:11:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.67
[32m[20221213 18:11:14 @agent_ppo2.py:143][0m Total time:       7.06 min
[32m[20221213 18:11:14 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221213 18:11:14 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 18:11:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |          -0.0015 |         219.6491 |           0.4526 |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |          -0.0057 |         217.5377 |           0.4514 |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |          -0.0064 |         215.2314 |           0.4517 |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |          -0.0055 |         215.0219 |           0.4516 |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |          -0.0027 |         214.5517 |           0.4515 |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |          -0.0038 |         214.1610 |           0.4514 |
[32m[20221213 18:11:14 @agent_ppo2.py:185][0m |           0.0131 |         245.0240 |           0.4520 |
[32m[20221213 18:11:15 @agent_ppo2.py:185][0m |          -0.0074 |         213.5412 |           0.4514 |
[32m[20221213 18:11:15 @agent_ppo2.py:185][0m |          -0.0075 |         213.3245 |           0.4513 |
[32m[20221213 18:11:15 @agent_ppo2.py:185][0m |          -0.0078 |         212.8565 |           0.4509 |
[32m[20221213 18:11:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:11:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 905.41
[32m[20221213 18:11:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.21
[32m[20221213 18:11:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.27
[32m[20221213 18:11:15 @agent_ppo2.py:143][0m Total time:       7.09 min
[32m[20221213 18:11:15 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 18:11:15 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 18:11:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:15 @agent_ppo2.py:185][0m |          -0.0000 |         208.5230 |           0.4625 |
[32m[20221213 18:11:15 @agent_ppo2.py:185][0m |          -0.0038 |         196.7383 |           0.4631 |
[32m[20221213 18:11:15 @agent_ppo2.py:185][0m |          -0.0024 |         192.4063 |           0.4630 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0039 |         189.0942 |           0.4634 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0070 |         187.8328 |           0.4634 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0071 |         186.8687 |           0.4635 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0060 |         185.9104 |           0.4638 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0093 |         185.9446 |           0.4633 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0098 |         184.5679 |           0.4641 |
[32m[20221213 18:11:16 @agent_ppo2.py:185][0m |          -0.0090 |         184.3718 |           0.4646 |
[32m[20221213 18:11:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:11:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.23
[32m[20221213 18:11:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.39
[32m[20221213 18:11:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 903.58
[32m[20221213 18:11:16 @agent_ppo2.py:143][0m Total time:       7.11 min
[32m[20221213 18:11:16 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221213 18:11:16 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 18:11:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |           0.0002 |         227.6680 |           0.4693 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0012 |         226.3767 |           0.4692 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0019 |         223.8604 |           0.4686 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0084 |         222.2773 |           0.4685 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0086 |         221.5940 |           0.4681 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0090 |         221.0273 |           0.4681 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0065 |         221.0858 |           0.4678 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0096 |         220.2701 |           0.4674 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0085 |         219.7896 |           0.4682 |
[32m[20221213 18:11:17 @agent_ppo2.py:185][0m |          -0.0103 |         219.4269 |           0.4679 |
[32m[20221213 18:11:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.23
[32m[20221213 18:11:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.77
[32m[20221213 18:11:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.65
[32m[20221213 18:11:18 @agent_ppo2.py:143][0m Total time:       7.13 min
[32m[20221213 18:11:18 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 18:11:18 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 18:11:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |           0.0008 |         225.8814 |           0.4725 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |          -0.0022 |         220.3977 |           0.4721 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |          -0.0023 |         217.3838 |           0.4715 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |           0.0116 |         245.0486 |           0.4709 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |          -0.0029 |         215.8191 |           0.4701 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |          -0.0062 |         214.4613 |           0.4710 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |          -0.0059 |         214.3139 |           0.4702 |
[32m[20221213 18:11:18 @agent_ppo2.py:185][0m |          -0.0050 |         214.0194 |           0.4708 |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |          -0.0085 |         213.5535 |           0.4705 |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |          -0.0087 |         213.0674 |           0.4703 |
[32m[20221213 18:11:19 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:11:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 914.55
[32m[20221213 18:11:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.61
[32m[20221213 18:11:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.26
[32m[20221213 18:11:19 @agent_ppo2.py:143][0m Total time:       7.15 min
[32m[20221213 18:11:19 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221213 18:11:19 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 18:11:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |          -0.0024 |         225.9677 |           0.4643 |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |          -0.0053 |         223.4792 |           0.4641 |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |          -0.0061 |         221.4952 |           0.4637 |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |          -0.0052 |         220.3280 |           0.4640 |
[32m[20221213 18:11:19 @agent_ppo2.py:185][0m |           0.0037 |         241.2137 |           0.4637 |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |          -0.0041 |         220.5578 |           0.4633 |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |          -0.0043 |         220.8672 |           0.4636 |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |          -0.0075 |         218.8794 |           0.4636 |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |          -0.0087 |         218.3385 |           0.4636 |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |           0.0029 |         225.9033 |           0.4636 |
[32m[20221213 18:11:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:11:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 866.56
[32m[20221213 18:11:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.43
[32m[20221213 18:11:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 898.66
[32m[20221213 18:11:20 @agent_ppo2.py:143][0m Total time:       7.17 min
[32m[20221213 18:11:20 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 18:11:20 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 18:11:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:11:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |          -0.0036 |         220.2621 |           0.4593 |
[32m[20221213 18:11:20 @agent_ppo2.py:185][0m |          -0.0054 |         217.4367 |           0.4589 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0015 |         220.4727 |           0.4583 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0061 |         215.6896 |           0.4585 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |           0.0033 |         231.7821 |           0.4580 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0063 |         214.1706 |           0.4578 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0063 |         213.5779 |           0.4575 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0101 |         213.0623 |           0.4574 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0106 |         212.6313 |           0.4570 |
[32m[20221213 18:11:21 @agent_ppo2.py:185][0m |          -0.0104 |         212.2149 |           0.4572 |
[32m[20221213 18:11:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:11:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 871.59
[32m[20221213 18:11:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.50
[32m[20221213 18:11:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 973.12
[32m[20221213 18:11:21 @agent_ppo2.py:143][0m Total time:       7.19 min
[32m[20221213 18:11:21 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221213 18:11:21 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 18:11:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:11:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0006 |         227.9174 |           0.4702 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0044 |         224.7375 |           0.4690 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0060 |         223.8992 |           0.4682 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0058 |         223.5480 |           0.4677 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0066 |         222.6624 |           0.4674 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0079 |         222.3719 |           0.4676 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0006 |         225.3361 |           0.4676 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0056 |         222.1054 |           0.4663 |
[32m[20221213 18:11:22 @agent_ppo2.py:185][0m |          -0.0039 |         223.7731 |           0.4662 |
[32m[20221213 18:11:23 @agent_ppo2.py:185][0m |           0.0015 |         235.4902 |           0.4658 |
[32m[20221213 18:11:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:11:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 878.57
[32m[20221213 18:11:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 925.33
[32m[20221213 18:11:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.34
[32m[20221213 18:11:23 @agent_ppo2.py:143][0m Total time:       7.22 min
[32m[20221213 18:11:23 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 18:11:23 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 18:11:23 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:11:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:23 @agent_ppo2.py:185][0m |           0.0016 |         226.6108 |           0.4443 |
[32m[20221213 18:11:23 @agent_ppo2.py:185][0m |          -0.0024 |         224.7964 |           0.4434 |
[32m[20221213 18:11:23 @agent_ppo2.py:185][0m |          -0.0004 |         226.9432 |           0.4447 |
[32m[20221213 18:11:23 @agent_ppo2.py:185][0m |          -0.0045 |         223.7274 |           0.4451 |
[32m[20221213 18:11:23 @agent_ppo2.py:185][0m |          -0.0073 |         223.2640 |           0.4448 |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0078 |         223.0870 |           0.4445 |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0048 |         223.3003 |           0.4445 |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0048 |         223.5775 |           0.4446 |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0037 |         227.5065 |           0.4446 |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0019 |         224.8945 |           0.4448 |
[32m[20221213 18:11:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:11:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 909.14
[32m[20221213 18:11:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.46
[32m[20221213 18:11:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.95
[32m[20221213 18:11:24 @agent_ppo2.py:143][0m Total time:       7.24 min
[32m[20221213 18:11:24 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221213 18:11:24 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 18:11:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0012 |         233.5306 |           0.4509 |
[32m[20221213 18:11:24 @agent_ppo2.py:185][0m |          -0.0055 |         226.6851 |           0.4504 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |           0.0026 |         234.1216 |           0.4501 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0047 |         225.2168 |           0.4496 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0065 |         224.6577 |           0.4505 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0042 |         225.9689 |           0.4500 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0056 |         224.1413 |           0.4503 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0058 |         223.6617 |           0.4503 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0081 |         223.3561 |           0.4505 |
[32m[20221213 18:11:25 @agent_ppo2.py:185][0m |          -0.0085 |         223.1969 |           0.4506 |
[32m[20221213 18:11:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.66
[32m[20221213 18:11:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 935.65
[32m[20221213 18:11:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.78
[32m[20221213 18:11:25 @agent_ppo2.py:143][0m Total time:       7.26 min
[32m[20221213 18:11:25 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 18:11:25 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 18:11:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |           0.0009 |         221.2867 |           0.4520 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |           0.0067 |         241.5606 |           0.4512 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |          -0.0037 |         215.4193 |           0.4506 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |          -0.0064 |         215.0503 |           0.4502 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |           0.0007 |         226.5652 |           0.4501 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |          -0.0083 |         213.9914 |           0.4498 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |           0.0035 |         230.7577 |           0.4498 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |          -0.0093 |         213.5832 |           0.4495 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |           0.0006 |         223.6554 |           0.4497 |
[32m[20221213 18:11:26 @agent_ppo2.py:185][0m |          -0.0063 |         212.7756 |           0.4495 |
[32m[20221213 18:11:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:11:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 707.87
[32m[20221213 18:11:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.08
[32m[20221213 18:11:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 959.63
[32m[20221213 18:11:27 @agent_ppo2.py:143][0m Total time:       7.28 min
[32m[20221213 18:11:27 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221213 18:11:27 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 18:11:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:27 @agent_ppo2.py:185][0m |          -0.0042 |         220.9040 |           0.4644 |
[32m[20221213 18:11:27 @agent_ppo2.py:185][0m |          -0.0046 |         219.0970 |           0.4633 |
[32m[20221213 18:11:27 @agent_ppo2.py:185][0m |          -0.0082 |         218.6204 |           0.4627 |
[32m[20221213 18:11:27 @agent_ppo2.py:185][0m |          -0.0074 |         217.8237 |           0.4627 |
[32m[20221213 18:11:27 @agent_ppo2.py:185][0m |           0.0043 |         241.6877 |           0.4622 |
[32m[20221213 18:11:27 @agent_ppo2.py:185][0m |          -0.0028 |         219.9611 |           0.4623 |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |          -0.0002 |         231.0186 |           0.4622 |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |          -0.0093 |         217.1170 |           0.4621 |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |          -0.0106 |         216.9100 |           0.4620 |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |           0.0007 |         232.2290 |           0.4621 |
[32m[20221213 18:11:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:11:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.48
[32m[20221213 18:11:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.71
[32m[20221213 18:11:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.67
[32m[20221213 18:11:28 @agent_ppo2.py:143][0m Total time:       7.30 min
[32m[20221213 18:11:28 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 18:11:28 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 18:11:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |          -0.0043 |         232.0906 |           0.4604 |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |          -0.0035 |         226.6629 |           0.4608 |
[32m[20221213 18:11:28 @agent_ppo2.py:185][0m |          -0.0055 |         223.0185 |           0.4611 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0064 |         221.6776 |           0.4606 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0062 |         220.4606 |           0.4612 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0092 |         219.3858 |           0.4612 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0063 |         218.7956 |           0.4610 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0079 |         217.8467 |           0.4609 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0088 |         216.2591 |           0.4609 |
[32m[20221213 18:11:29 @agent_ppo2.py:185][0m |          -0.0089 |         215.2963 |           0.4611 |
[32m[20221213 18:11:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:11:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.66
[32m[20221213 18:11:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.67
[32m[20221213 18:11:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 910.10
[32m[20221213 18:11:29 @agent_ppo2.py:143][0m Total time:       7.33 min
[32m[20221213 18:11:29 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221213 18:11:29 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 18:11:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0036 |         222.3875 |           0.4610 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0046 |         220.3762 |           0.4607 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0009 |         221.4185 |           0.4606 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0070 |         218.0959 |           0.4607 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0063 |         217.0495 |           0.4611 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0043 |         218.0425 |           0.4608 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0080 |         215.5570 |           0.4610 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0095 |         215.5752 |           0.4609 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0091 |         215.0272 |           0.4611 |
[32m[20221213 18:11:30 @agent_ppo2.py:185][0m |          -0.0078 |         214.8687 |           0.4611 |
[32m[20221213 18:11:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:11:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 903.92
[32m[20221213 18:11:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.73
[32m[20221213 18:11:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.58
[32m[20221213 18:11:31 @agent_ppo2.py:143][0m Total time:       7.35 min
[32m[20221213 18:11:31 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 18:11:31 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 18:11:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |           0.0000 |         229.3249 |           0.4578 |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |          -0.0037 |         225.5678 |           0.4574 |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |          -0.0055 |         224.4055 |           0.4573 |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |          -0.0042 |         223.6953 |           0.4577 |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |          -0.0055 |         223.4342 |           0.4577 |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |          -0.0073 |         222.8276 |           0.4573 |
[32m[20221213 18:11:31 @agent_ppo2.py:185][0m |          -0.0052 |         224.0677 |           0.4575 |
[32m[20221213 18:11:32 @agent_ppo2.py:185][0m |          -0.0073 |         222.2132 |           0.4575 |
[32m[20221213 18:11:32 @agent_ppo2.py:185][0m |          -0.0076 |         221.8347 |           0.4575 |
[32m[20221213 18:11:32 @agent_ppo2.py:185][0m |          -0.0083 |         221.5047 |           0.4578 |
[32m[20221213 18:11:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:11:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.45
[32m[20221213 18:11:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.80
[32m[20221213 18:11:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.77
[32m[20221213 18:11:32 @agent_ppo2.py:143][0m Total time:       7.37 min
[32m[20221213 18:11:32 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221213 18:11:32 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 18:11:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:32 @agent_ppo2.py:185][0m |           0.0020 |         223.4219 |           0.4610 |
[32m[20221213 18:11:32 @agent_ppo2.py:185][0m |          -0.0004 |         219.8826 |           0.4609 |
[32m[20221213 18:11:32 @agent_ppo2.py:185][0m |           0.0049 |         227.8499 |           0.4611 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |          -0.0056 |         217.7057 |           0.4618 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |          -0.0056 |         217.0585 |           0.4618 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |           0.0023 |         224.3670 |           0.4626 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |          -0.0056 |         216.1238 |           0.4637 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |          -0.0059 |         215.8483 |           0.4639 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |          -0.0062 |         215.3511 |           0.4647 |
[32m[20221213 18:11:33 @agent_ppo2.py:185][0m |          -0.0059 |         214.9464 |           0.4646 |
[32m[20221213 18:11:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:11:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.10
[32m[20221213 18:11:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.57
[32m[20221213 18:11:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.82
[32m[20221213 18:11:33 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221213 18:11:33 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 18:11:33 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 18:11:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0033 |         219.0760 |           0.4690 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0028 |         215.1368 |           0.4691 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0072 |         212.4155 |           0.4685 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0084 |         211.7566 |           0.4683 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |           0.0032 |         222.3309 |           0.4686 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0100 |         209.8041 |           0.4685 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0051 |         210.4779 |           0.4683 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0100 |         209.7047 |           0.4686 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0066 |         209.1756 |           0.4686 |
[32m[20221213 18:11:34 @agent_ppo2.py:185][0m |          -0.0073 |         209.0052 |           0.4686 |
[32m[20221213 18:11:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:11:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.83
[32m[20221213 18:11:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 905.35
[32m[20221213 18:11:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 996.46
[32m[20221213 18:11:34 @agent_ppo2.py:143][0m Total time:       7.41 min
[32m[20221213 18:11:34 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221213 18:11:34 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 18:11:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0031 |         239.0546 |           0.4707 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0047 |         234.4288 |           0.4691 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0058 |         232.2895 |           0.4699 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0021 |         233.0123 |           0.4694 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0072 |         230.1404 |           0.4691 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0031 |         232.1432 |           0.4692 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0061 |         228.8803 |           0.4691 |
[32m[20221213 18:11:35 @agent_ppo2.py:185][0m |          -0.0067 |         228.3827 |           0.4689 |
[32m[20221213 18:11:36 @agent_ppo2.py:185][0m |          -0.0030 |         231.8220 |           0.4691 |
[32m[20221213 18:11:36 @agent_ppo2.py:185][0m |          -0.0062 |         228.0619 |           0.4685 |
[32m[20221213 18:11:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:11:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 912.45
[32m[20221213 18:11:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.50
[32m[20221213 18:11:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 906.66
[32m[20221213 18:11:36 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221213 18:11:36 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 18:11:36 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 18:11:36 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:11:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:36 @agent_ppo2.py:185][0m |           0.0012 |         234.4771 |           0.4718 |
[32m[20221213 18:11:36 @agent_ppo2.py:185][0m |          -0.0023 |         231.6974 |           0.4713 |
[32m[20221213 18:11:36 @agent_ppo2.py:185][0m |          -0.0035 |         230.4634 |           0.4715 |
[32m[20221213 18:11:36 @agent_ppo2.py:185][0m |          -0.0044 |         229.3445 |           0.4706 |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |          -0.0038 |         228.8555 |           0.4704 |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |          -0.0072 |         228.1472 |           0.4707 |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |          -0.0058 |         228.0041 |           0.4706 |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |          -0.0077 |         227.0467 |           0.4704 |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |          -0.0061 |         226.6831 |           0.4709 |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |           0.0063 |         261.2155 |           0.4705 |
[32m[20221213 18:11:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 869.60
[32m[20221213 18:11:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.92
[32m[20221213 18:11:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.58
[32m[20221213 18:11:37 @agent_ppo2.py:143][0m Total time:       7.46 min
[32m[20221213 18:11:37 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221213 18:11:37 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 18:11:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:37 @agent_ppo2.py:185][0m |           0.0058 |         240.2125 |           0.4834 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0026 |         231.5856 |           0.4833 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0021 |         230.0576 |           0.4831 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0047 |         228.8657 |           0.4826 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |           0.0000 |         232.8393 |           0.4832 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0054 |         228.1105 |           0.4827 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0055 |         228.9603 |           0.4827 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0049 |         227.7294 |           0.4831 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0047 |         227.5717 |           0.4835 |
[32m[20221213 18:11:38 @agent_ppo2.py:185][0m |          -0.0070 |         227.2961 |           0.4829 |
[32m[20221213 18:11:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:11:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.33
[32m[20221213 18:11:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.31
[32m[20221213 18:11:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 868.46
[32m[20221213 18:11:38 @agent_ppo2.py:143][0m Total time:       7.48 min
[32m[20221213 18:11:38 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 18:11:38 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 18:11:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0013 |         226.6878 |           0.4809 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0033 |         224.8458 |           0.4796 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0048 |         224.1708 |           0.4794 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0048 |         224.0276 |           0.4790 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0005 |         226.6376 |           0.4784 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0052 |         223.5049 |           0.4790 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |           0.0022 |         235.7910 |           0.4781 |
[32m[20221213 18:11:39 @agent_ppo2.py:185][0m |          -0.0056 |         223.0303 |           0.4773 |
[32m[20221213 18:11:40 @agent_ppo2.py:185][0m |          -0.0063 |         222.6362 |           0.4777 |
[32m[20221213 18:11:40 @agent_ppo2.py:185][0m |          -0.0067 |         222.5790 |           0.4780 |
[32m[20221213 18:11:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:11:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 947.46
[32m[20221213 18:11:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.95
[32m[20221213 18:11:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.43
[32m[20221213 18:11:40 @agent_ppo2.py:143][0m Total time:       7.50 min
[32m[20221213 18:11:40 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221213 18:11:40 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 18:11:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:11:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:40 @agent_ppo2.py:185][0m |           0.0007 |         231.0805 |           0.4770 |
[32m[20221213 18:11:40 @agent_ppo2.py:185][0m |          -0.0014 |         227.4176 |           0.4759 |
[32m[20221213 18:11:40 @agent_ppo2.py:185][0m |          -0.0056 |         226.5224 |           0.4755 |
[32m[20221213 18:11:40 @agent_ppo2.py:185][0m |          -0.0044 |         225.5721 |           0.4744 |
[32m[20221213 18:11:41 @agent_ppo2.py:185][0m |          -0.0033 |         226.4005 |           0.4749 |
[32m[20221213 18:11:41 @agent_ppo2.py:185][0m |          -0.0070 |         224.8990 |           0.4737 |
[32m[20221213 18:11:41 @agent_ppo2.py:185][0m |          -0.0053 |         224.6154 |           0.4736 |
[32m[20221213 18:11:41 @agent_ppo2.py:185][0m |          -0.0042 |         224.5885 |           0.4738 |
[32m[20221213 18:11:41 @agent_ppo2.py:185][0m |          -0.0020 |         226.0974 |           0.4728 |
[32m[20221213 18:11:41 @agent_ppo2.py:185][0m |          -0.0038 |         226.0901 |           0.4729 |
[32m[20221213 18:11:41 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:11:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 865.19
[32m[20221213 18:11:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.29
[32m[20221213 18:11:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.12
[32m[20221213 18:11:41 @agent_ppo2.py:143][0m Total time:       7.53 min
[32m[20221213 18:11:41 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 18:11:41 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 18:11:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |           0.0000 |         229.9005 |           0.4568 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |          -0.0043 |         227.5952 |           0.4562 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |          -0.0069 |         226.4258 |           0.4560 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |          -0.0024 |         229.3109 |           0.4557 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |           0.0011 |         236.2607 |           0.4558 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |          -0.0066 |         224.4796 |           0.4549 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |          -0.0035 |         226.1667 |           0.4551 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |           0.0048 |         243.9840 |           0.4551 |
[32m[20221213 18:11:42 @agent_ppo2.py:185][0m |          -0.0025 |         228.5985 |           0.4546 |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |          -0.0088 |         222.8598 |           0.4550 |
[32m[20221213 18:11:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:11:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 869.71
[32m[20221213 18:11:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 904.88
[32m[20221213 18:11:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 998.40
[32m[20221213 18:11:43 @agent_ppo2.py:143][0m Total time:       7.55 min
[32m[20221213 18:11:43 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221213 18:11:43 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 18:11:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |           0.0103 |         249.8919 |           0.4801 |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |          -0.0018 |         228.8019 |           0.4789 |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |          -0.0037 |         227.9334 |           0.4796 |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |          -0.0035 |         227.2841 |           0.4794 |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |          -0.0062 |         226.9030 |           0.4794 |
[32m[20221213 18:11:43 @agent_ppo2.py:185][0m |          -0.0055 |         226.4767 |           0.4798 |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |           0.0079 |         260.2028 |           0.4801 |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |          -0.0072 |         225.9306 |           0.4793 |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |          -0.0072 |         225.2907 |           0.4800 |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |          -0.0067 |         224.9261 |           0.4802 |
[32m[20221213 18:11:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.01
[32m[20221213 18:11:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.62
[32m[20221213 18:11:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.16
[32m[20221213 18:11:44 @agent_ppo2.py:143][0m Total time:       7.57 min
[32m[20221213 18:11:44 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 18:11:44 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 18:11:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |          -0.0011 |         234.5080 |           0.4759 |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |          -0.0049 |         226.9569 |           0.4754 |
[32m[20221213 18:11:44 @agent_ppo2.py:185][0m |           0.0096 |         239.9595 |           0.4754 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |          -0.0054 |         224.3661 |           0.4745 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |          -0.0076 |         223.4600 |           0.4746 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |           0.0010 |         234.6109 |           0.4741 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |          -0.0009 |         227.1957 |           0.4742 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |          -0.0069 |         222.5392 |           0.4745 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |           0.0048 |         238.8784 |           0.4741 |
[32m[20221213 18:11:45 @agent_ppo2.py:185][0m |          -0.0083 |         221.7989 |           0.4738 |
[32m[20221213 18:11:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:11:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.75
[32m[20221213 18:11:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 906.09
[32m[20221213 18:11:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.50
[32m[20221213 18:11:45 @agent_ppo2.py:143][0m Total time:       7.59 min
[32m[20221213 18:11:45 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221213 18:11:45 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 18:11:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0005 |         230.9553 |           0.4730 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0052 |         226.1340 |           0.4723 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0052 |         223.2600 |           0.4722 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0046 |         220.9278 |           0.4725 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0033 |         220.5873 |           0.4719 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0033 |         220.7554 |           0.4721 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0059 |         217.1359 |           0.4723 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0074 |         216.3326 |           0.4727 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0069 |         215.1585 |           0.4724 |
[32m[20221213 18:11:46 @agent_ppo2.py:185][0m |          -0.0077 |         214.6259 |           0.4723 |
[32m[20221213 18:11:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:11:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.79
[32m[20221213 18:11:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 879.09
[32m[20221213 18:11:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.05
[32m[20221213 18:11:47 @agent_ppo2.py:143][0m Total time:       7.61 min
[32m[20221213 18:11:47 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 18:11:47 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 18:11:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0035 |         218.5880 |           0.4815 |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0019 |         218.4140 |           0.4804 |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0059 |         216.9217 |           0.4797 |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0070 |         216.4220 |           0.4786 |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0070 |         216.2572 |           0.4784 |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0074 |         215.9313 |           0.4776 |
[32m[20221213 18:11:47 @agent_ppo2.py:185][0m |          -0.0044 |         219.9135 |           0.4773 |
[32m[20221213 18:11:48 @agent_ppo2.py:185][0m |          -0.0075 |         215.5242 |           0.4768 |
[32m[20221213 18:11:48 @agent_ppo2.py:185][0m |          -0.0097 |         215.5168 |           0.4761 |
[32m[20221213 18:11:48 @agent_ppo2.py:185][0m |          -0.0106 |         215.3611 |           0.4754 |
[32m[20221213 18:11:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:11:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.80
[32m[20221213 18:11:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.88
[32m[20221213 18:11:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.30
[32m[20221213 18:11:48 @agent_ppo2.py:143][0m Total time:       7.64 min
[32m[20221213 18:11:48 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221213 18:11:48 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 18:11:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:48 @agent_ppo2.py:185][0m |          -0.0009 |         223.9794 |           0.4583 |
[32m[20221213 18:11:48 @agent_ppo2.py:185][0m |          -0.0042 |         220.1727 |           0.4582 |
[32m[20221213 18:11:48 @agent_ppo2.py:185][0m |          -0.0065 |         219.0354 |           0.4581 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |           0.0062 |         238.1435 |           0.4587 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |          -0.0053 |         218.5194 |           0.4583 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |           0.0062 |         231.6416 |           0.4583 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |          -0.0053 |         217.1674 |           0.4586 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |          -0.0018 |         223.2508 |           0.4587 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |          -0.0083 |         216.7142 |           0.4585 |
[32m[20221213 18:11:49 @agent_ppo2.py:185][0m |          -0.0081 |         216.5751 |           0.4590 |
[32m[20221213 18:11:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.12
[32m[20221213 18:11:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.99
[32m[20221213 18:11:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.68
[32m[20221213 18:11:49 @agent_ppo2.py:143][0m Total time:       7.66 min
[32m[20221213 18:11:49 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 18:11:49 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 18:11:49 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:11:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |           0.0001 |         202.0963 |           0.4621 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0007 |         198.7236 |           0.4616 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |           0.0008 |         202.2750 |           0.4609 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |           0.0076 |         205.1144 |           0.4606 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0069 |         194.7171 |           0.4594 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0025 |         193.5969 |           0.4602 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0060 |         193.3068 |           0.4601 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0078 |         192.8239 |           0.4597 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0029 |         199.4865 |           0.4597 |
[32m[20221213 18:11:50 @agent_ppo2.py:185][0m |          -0.0054 |         192.6930 |           0.4590 |
[32m[20221213 18:11:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:11:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.17
[32m[20221213 18:11:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.64
[32m[20221213 18:11:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.67
[32m[20221213 18:11:51 @agent_ppo2.py:143][0m Total time:       7.68 min
[32m[20221213 18:11:51 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221213 18:11:51 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 18:11:51 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:11:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:51 @agent_ppo2.py:185][0m |           0.0006 |         224.4728 |           0.4722 |
[32m[20221213 18:11:51 @agent_ppo2.py:185][0m |          -0.0029 |         221.4325 |           0.4714 |
[32m[20221213 18:11:51 @agent_ppo2.py:185][0m |          -0.0033 |         220.0066 |           0.4710 |
[32m[20221213 18:11:51 @agent_ppo2.py:185][0m |          -0.0043 |         219.7116 |           0.4708 |
[32m[20221213 18:11:51 @agent_ppo2.py:185][0m |          -0.0058 |         219.2471 |           0.4707 |
[32m[20221213 18:11:51 @agent_ppo2.py:185][0m |          -0.0014 |         220.6560 |           0.4710 |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |          -0.0052 |         218.7706 |           0.4702 |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |           0.0023 |         234.7381 |           0.4705 |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |          -0.0015 |         219.8858 |           0.4703 |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |          -0.0057 |         218.3774 |           0.4708 |
[32m[20221213 18:11:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:11:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.44
[32m[20221213 18:11:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.82
[32m[20221213 18:11:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.85
[32m[20221213 18:11:52 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221213 18:11:52 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 18:11:52 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 18:11:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |           0.0046 |         228.5429 |           0.4556 |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |          -0.0026 |         222.7714 |           0.4550 |
[32m[20221213 18:11:52 @agent_ppo2.py:185][0m |          -0.0057 |         221.2300 |           0.4552 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0074 |         220.9130 |           0.4546 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0071 |         220.6481 |           0.4547 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0075 |         220.2694 |           0.4546 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0012 |         227.0195 |           0.4543 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0041 |         221.3355 |           0.4542 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0083 |         219.9318 |           0.4541 |
[32m[20221213 18:11:53 @agent_ppo2.py:185][0m |          -0.0021 |         221.2583 |           0.4545 |
[32m[20221213 18:11:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:11:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 892.71
[32m[20221213 18:11:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.30
[32m[20221213 18:11:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.61
[32m[20221213 18:11:53 @agent_ppo2.py:143][0m Total time:       7.72 min
[32m[20221213 18:11:53 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221213 18:11:53 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 18:11:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0035 |         225.0970 |           0.4646 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0042 |         224.3277 |           0.4641 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |           0.0004 |         231.3419 |           0.4637 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0074 |         222.6449 |           0.4646 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0072 |         223.0640 |           0.4642 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0101 |         221.4094 |           0.4640 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0092 |         221.6535 |           0.4641 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |           0.0092 |         254.6489 |           0.4644 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0119 |         221.8141 |           0.4641 |
[32m[20221213 18:11:54 @agent_ppo2.py:185][0m |          -0.0114 |         220.9182 |           0.4640 |
[32m[20221213 18:11:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:11:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.58
[32m[20221213 18:11:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.96
[32m[20221213 18:11:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 962.89
[32m[20221213 18:11:55 @agent_ppo2.py:143][0m Total time:       7.75 min
[32m[20221213 18:11:55 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 18:11:55 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 18:11:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |          -0.0028 |         226.1965 |           0.4629 |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |           0.0039 |         231.3616 |           0.4627 |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |          -0.0014 |         222.9415 |           0.4618 |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |          -0.0057 |         222.2347 |           0.4617 |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |          -0.0068 |         222.0137 |           0.4614 |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |          -0.0066 |         221.8456 |           0.4614 |
[32m[20221213 18:11:55 @agent_ppo2.py:185][0m |           0.0018 |         235.7053 |           0.4611 |
[32m[20221213 18:11:56 @agent_ppo2.py:185][0m |          -0.0076 |         221.6080 |           0.4609 |
[32m[20221213 18:11:56 @agent_ppo2.py:185][0m |           0.0001 |         229.3097 |           0.4611 |
[32m[20221213 18:11:56 @agent_ppo2.py:185][0m |          -0.0042 |         221.5360 |           0.4606 |
[32m[20221213 18:11:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:11:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 917.10
[32m[20221213 18:11:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.46
[32m[20221213 18:11:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.78
[32m[20221213 18:11:56 @agent_ppo2.py:143][0m Total time:       7.77 min
[32m[20221213 18:11:56 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221213 18:11:56 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 18:11:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:56 @agent_ppo2.py:185][0m |          -0.0012 |         227.6857 |           0.4606 |
[32m[20221213 18:11:56 @agent_ppo2.py:185][0m |           0.0022 |         228.3093 |           0.4604 |
[32m[20221213 18:11:56 @agent_ppo2.py:185][0m |          -0.0029 |         225.4003 |           0.4605 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |          -0.0018 |         225.2738 |           0.4601 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |          -0.0020 |         226.2683 |           0.4603 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |          -0.0047 |         224.6084 |           0.4605 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |           0.0064 |         234.1127 |           0.4599 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |          -0.0044 |         224.1152 |           0.4606 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |          -0.0053 |         223.8325 |           0.4604 |
[32m[20221213 18:11:57 @agent_ppo2.py:185][0m |          -0.0048 |         223.6341 |           0.4602 |
[32m[20221213 18:11:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:11:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 985.09
[32m[20221213 18:11:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.90
[32m[20221213 18:11:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 982.85
[32m[20221213 18:11:57 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221213 18:11:57 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 18:11:57 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 18:11:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0016 |         227.6404 |           0.4618 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0075 |         222.7833 |           0.4615 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0059 |         221.8917 |           0.4618 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0001 |         234.6913 |           0.4609 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0079 |         219.8505 |           0.4613 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0088 |         218.9397 |           0.4614 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0006 |         230.8090 |           0.4616 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0100 |         217.6711 |           0.4607 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0090 |         217.5127 |           0.4613 |
[32m[20221213 18:11:58 @agent_ppo2.py:185][0m |          -0.0096 |         216.8425 |           0.4616 |
[32m[20221213 18:11:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:11:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.17
[32m[20221213 18:11:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.97
[32m[20221213 18:11:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 969.39
[32m[20221213 18:11:59 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221213 18:11:59 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221213 18:11:59 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 18:11:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:11:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |           0.0025 |         235.2140 |           0.4551 |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |          -0.0031 |         231.8658 |           0.4548 |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |          -0.0058 |         231.1252 |           0.4549 |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |          -0.0056 |         230.1709 |           0.4548 |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |           0.0059 |         248.9882 |           0.4547 |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |          -0.0069 |         229.8690 |           0.4546 |
[32m[20221213 18:11:59 @agent_ppo2.py:185][0m |          -0.0070 |         229.4414 |           0.4543 |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0065 |         229.4317 |           0.4548 |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0075 |         229.4109 |           0.4550 |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0078 |         228.9619 |           0.4549 |
[32m[20221213 18:12:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:12:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 923.79
[32m[20221213 18:12:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.94
[32m[20221213 18:12:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.14
[32m[20221213 18:12:00 @agent_ppo2.py:143][0m Total time:       7.84 min
[32m[20221213 18:12:00 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 18:12:00 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 18:12:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0006 |         239.6770 |           0.4556 |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0050 |         235.0530 |           0.4557 |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0001 |         231.6719 |           0.4549 |
[32m[20221213 18:12:00 @agent_ppo2.py:185][0m |          -0.0048 |         228.4565 |           0.4554 |
[32m[20221213 18:12:01 @agent_ppo2.py:185][0m |          -0.0049 |         226.9370 |           0.4549 |
[32m[20221213 18:12:01 @agent_ppo2.py:185][0m |          -0.0065 |         225.1848 |           0.4555 |
[32m[20221213 18:12:01 @agent_ppo2.py:185][0m |          -0.0074 |         224.3877 |           0.4549 |
[32m[20221213 18:12:01 @agent_ppo2.py:185][0m |          -0.0064 |         222.8655 |           0.4549 |
[32m[20221213 18:12:01 @agent_ppo2.py:185][0m |          -0.0074 |         222.6834 |           0.4545 |
[32m[20221213 18:12:01 @agent_ppo2.py:185][0m |          -0.0070 |         222.0675 |           0.4546 |
[32m[20221213 18:12:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 884.90
[32m[20221213 18:12:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.07
[32m[20221213 18:12:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.94
[32m[20221213 18:12:01 @agent_ppo2.py:143][0m Total time:       7.86 min
[32m[20221213 18:12:01 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221213 18:12:01 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 18:12:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |           0.0100 |         269.0318 |           0.4553 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0068 |         231.8501 |           0.4539 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0079 |         230.9381 |           0.4545 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0076 |         229.9047 |           0.4546 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0069 |         230.2790 |           0.4549 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0086 |         228.7791 |           0.4548 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0112 |         228.1730 |           0.4549 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0080 |         228.0890 |           0.4548 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0102 |         227.3254 |           0.4549 |
[32m[20221213 18:12:02 @agent_ppo2.py:185][0m |          -0.0044 |         237.5048 |           0.4547 |
[32m[20221213 18:12:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 908.61
[32m[20221213 18:12:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.07
[32m[20221213 18:12:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 901.26
[32m[20221213 18:12:02 @agent_ppo2.py:143][0m Total time:       7.88 min
[32m[20221213 18:12:02 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 18:12:02 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 18:12:03 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:12:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0030 |         235.8940 |           0.4639 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |           0.0010 |         231.9246 |           0.4633 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0001 |         232.3023 |           0.4630 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0052 |         228.4976 |           0.4631 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0060 |         227.5207 |           0.4632 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0078 |         226.8365 |           0.4632 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0077 |         226.2024 |           0.4627 |
[32m[20221213 18:12:03 @agent_ppo2.py:185][0m |          -0.0060 |         225.9269 |           0.4625 |
[32m[20221213 18:12:04 @agent_ppo2.py:185][0m |           0.0050 |         237.0218 |           0.4623 |
[32m[20221213 18:12:04 @agent_ppo2.py:185][0m |          -0.0059 |         225.5518 |           0.4625 |
[32m[20221213 18:12:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:12:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 855.48
[32m[20221213 18:12:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.29
[32m[20221213 18:12:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.25
[32m[20221213 18:12:04 @agent_ppo2.py:143][0m Total time:       7.90 min
[32m[20221213 18:12:04 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221213 18:12:04 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 18:12:04 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:12:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:04 @agent_ppo2.py:185][0m |          -0.0022 |         227.6864 |           0.4627 |
[32m[20221213 18:12:04 @agent_ppo2.py:185][0m |           0.0038 |         229.3861 |           0.4611 |
[32m[20221213 18:12:04 @agent_ppo2.py:185][0m |          -0.0055 |         224.9509 |           0.4610 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0070 |         224.4073 |           0.4611 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0059 |         224.1218 |           0.4604 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0059 |         223.9355 |           0.4605 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0059 |         223.6101 |           0.4607 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0052 |         223.6067 |           0.4607 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0027 |         225.5297 |           0.4605 |
[32m[20221213 18:12:05 @agent_ppo2.py:185][0m |          -0.0086 |         223.3574 |           0.4606 |
[32m[20221213 18:12:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:12:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.83
[32m[20221213 18:12:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 986.11
[32m[20221213 18:12:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.82
[32m[20221213 18:12:05 @agent_ppo2.py:143][0m Total time:       7.92 min
[32m[20221213 18:12:05 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 18:12:05 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 18:12:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0019 |         227.5980 |           0.4639 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0048 |         226.1206 |           0.4632 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0063 |         225.6976 |           0.4633 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0055 |         225.8564 |           0.4629 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0071 |         224.8213 |           0.4630 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0075 |         224.6457 |           0.4631 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0088 |         224.3972 |           0.4625 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0082 |         224.1145 |           0.4628 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |           0.0027 |         235.5679 |           0.4626 |
[32m[20221213 18:12:06 @agent_ppo2.py:185][0m |          -0.0067 |         224.2654 |           0.4629 |
[32m[20221213 18:12:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.65
[32m[20221213 18:12:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 928.67
[32m[20221213 18:12:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.80
[32m[20221213 18:12:06 @agent_ppo2.py:143][0m Total time:       7.95 min
[32m[20221213 18:12:06 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221213 18:12:07 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 18:12:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0016 |         223.5325 |           0.4685 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0042 |         222.5574 |           0.4679 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0026 |         222.1651 |           0.4679 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0054 |         221.9345 |           0.4675 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0045 |         221.4900 |           0.4673 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0040 |         221.4367 |           0.4669 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0062 |         221.3563 |           0.4680 |
[32m[20221213 18:12:07 @agent_ppo2.py:185][0m |          -0.0041 |         220.9752 |           0.4670 |
[32m[20221213 18:12:08 @agent_ppo2.py:185][0m |          -0.0063 |         220.9553 |           0.4670 |
[32m[20221213 18:12:08 @agent_ppo2.py:185][0m |          -0.0036 |         220.8059 |           0.4670 |
[32m[20221213 18:12:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.36
[32m[20221213 18:12:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.29
[32m[20221213 18:12:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 970.16
[32m[20221213 18:12:08 @agent_ppo2.py:143][0m Total time:       7.97 min
[32m[20221213 18:12:08 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 18:12:08 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 18:12:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:12:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:08 @agent_ppo2.py:185][0m |          -0.0021 |         224.6709 |           0.4713 |
[32m[20221213 18:12:08 @agent_ppo2.py:185][0m |           0.0019 |         228.3233 |           0.4700 |
[32m[20221213 18:12:08 @agent_ppo2.py:185][0m |          -0.0040 |         222.1886 |           0.4693 |
[32m[20221213 18:12:08 @agent_ppo2.py:185][0m |          -0.0059 |         221.9626 |           0.4696 |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |          -0.0061 |         221.5788 |           0.4701 |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |           0.0067 |         250.2728 |           0.4697 |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |          -0.0049 |         221.4245 |           0.4698 |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |          -0.0072 |         220.8746 |           0.4705 |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |          -0.0084 |         220.7344 |           0.4707 |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |          -0.0090 |         220.5602 |           0.4710 |
[32m[20221213 18:12:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 900.85
[32m[20221213 18:12:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.21
[32m[20221213 18:12:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 868.70
[32m[20221213 18:12:09 @agent_ppo2.py:143][0m Total time:       7.99 min
[32m[20221213 18:12:09 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221213 18:12:09 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 18:12:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:09 @agent_ppo2.py:185][0m |          -0.0018 |         228.0189 |           0.4744 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0026 |         225.1671 |           0.4728 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0059 |         224.7602 |           0.4729 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0049 |         224.1300 |           0.4729 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0061 |         223.9686 |           0.4722 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0054 |         223.7061 |           0.4721 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0064 |         223.5522 |           0.4719 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0076 |         223.4217 |           0.4716 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0075 |         223.2826 |           0.4717 |
[32m[20221213 18:12:10 @agent_ppo2.py:185][0m |          -0.0073 |         223.1772 |           0.4711 |
[32m[20221213 18:12:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:12:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 907.19
[32m[20221213 18:12:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.39
[32m[20221213 18:12:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 885.57
[32m[20221213 18:12:10 @agent_ppo2.py:143][0m Total time:       8.01 min
[32m[20221213 18:12:10 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 18:12:10 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 18:12:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0046 |         225.1711 |           0.4695 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0071 |         223.8149 |           0.4692 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0076 |         223.3191 |           0.4691 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0079 |         223.0421 |           0.4695 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0079 |         222.7674 |           0.4694 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0074 |         222.7006 |           0.4694 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0089 |         222.6919 |           0.4695 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0075 |         222.6652 |           0.4694 |
[32m[20221213 18:12:11 @agent_ppo2.py:185][0m |          -0.0096 |         222.5597 |           0.4697 |
[32m[20221213 18:12:12 @agent_ppo2.py:185][0m |          -0.0090 |         222.3347 |           0.4693 |
[32m[20221213 18:12:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.39
[32m[20221213 18:12:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.44
[32m[20221213 18:12:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 895.42
[32m[20221213 18:12:12 @agent_ppo2.py:143][0m Total time:       8.03 min
[32m[20221213 18:12:12 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221213 18:12:12 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 18:12:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:12 @agent_ppo2.py:185][0m |          -0.0004 |         228.2864 |           0.4547 |
[32m[20221213 18:12:12 @agent_ppo2.py:185][0m |          -0.0027 |         226.8117 |           0.4544 |
[32m[20221213 18:12:12 @agent_ppo2.py:185][0m |          -0.0022 |         228.1689 |           0.4542 |
[32m[20221213 18:12:12 @agent_ppo2.py:185][0m |          -0.0055 |         226.2823 |           0.4540 |
[32m[20221213 18:12:12 @agent_ppo2.py:185][0m |          -0.0058 |         226.4838 |           0.4543 |
[32m[20221213 18:12:13 @agent_ppo2.py:185][0m |          -0.0068 |         225.8350 |           0.4545 |
[32m[20221213 18:12:13 @agent_ppo2.py:185][0m |          -0.0061 |         225.8890 |           0.4547 |
[32m[20221213 18:12:13 @agent_ppo2.py:185][0m |          -0.0003 |         232.1499 |           0.4549 |
[32m[20221213 18:12:13 @agent_ppo2.py:185][0m |          -0.0076 |         225.6579 |           0.4551 |
[32m[20221213 18:12:13 @agent_ppo2.py:185][0m |           0.0006 |         236.9092 |           0.4552 |
[32m[20221213 18:12:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 969.40
[32m[20221213 18:12:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 991.81
[32m[20221213 18:12:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.19
[32m[20221213 18:12:13 @agent_ppo2.py:143][0m Total time:       8.06 min
[32m[20221213 18:12:13 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 18:12:13 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 18:12:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:13 @agent_ppo2.py:185][0m |           0.0027 |         242.9122 |           0.4622 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0088 |         233.4432 |           0.4616 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0090 |         231.9740 |           0.4613 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0100 |         231.1400 |           0.4612 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0076 |         230.5130 |           0.4611 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0099 |         230.2284 |           0.4606 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |           0.0009 |         244.6285 |           0.4610 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0045 |         230.4067 |           0.4600 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0108 |         229.5347 |           0.4601 |
[32m[20221213 18:12:14 @agent_ppo2.py:185][0m |          -0.0088 |         229.6231 |           0.4600 |
[32m[20221213 18:12:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 871.58
[32m[20221213 18:12:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.65
[32m[20221213 18:12:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.21
[32m[20221213 18:12:14 @agent_ppo2.py:143][0m Total time:       8.08 min
[32m[20221213 18:12:14 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221213 18:12:14 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 18:12:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0005 |         225.0826 |           0.4651 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0046 |         219.7085 |           0.4646 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |           0.0004 |         234.3536 |           0.4641 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0079 |         213.2400 |           0.4629 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0080 |         210.7656 |           0.4630 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0026 |         211.1045 |           0.4631 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0104 |         207.0824 |           0.4626 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0112 |         205.1517 |           0.4624 |
[32m[20221213 18:12:15 @agent_ppo2.py:185][0m |          -0.0126 |         204.3429 |           0.4621 |
[32m[20221213 18:12:16 @agent_ppo2.py:185][0m |          -0.0087 |         204.0218 |           0.4615 |
[32m[20221213 18:12:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:12:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.90
[32m[20221213 18:12:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 930.71
[32m[20221213 18:12:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 906.87
[32m[20221213 18:12:16 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221213 18:12:16 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 18:12:16 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 18:12:16 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:12:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:16 @agent_ppo2.py:185][0m |          -0.0010 |         211.5805 |           0.4747 |
[32m[20221213 18:12:16 @agent_ppo2.py:185][0m |           0.0049 |         211.0642 |           0.4746 |
[32m[20221213 18:12:16 @agent_ppo2.py:185][0m |          -0.0038 |         205.7376 |           0.4728 |
[32m[20221213 18:12:16 @agent_ppo2.py:185][0m |           0.0016 |         207.7975 |           0.4734 |
[32m[20221213 18:12:16 @agent_ppo2.py:185][0m |          -0.0003 |         209.6932 |           0.4719 |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |          -0.0056 |         204.6559 |           0.4725 |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |          -0.0079 |         204.3577 |           0.4716 |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |          -0.0044 |         204.2635 |           0.4719 |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |          -0.0038 |         203.9417 |           0.4713 |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |          -0.0058 |         204.2420 |           0.4711 |
[32m[20221213 18:12:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.43
[32m[20221213 18:12:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.90
[32m[20221213 18:12:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.87
[32m[20221213 18:12:17 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221213 18:12:17 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221213 18:12:17 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 18:12:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |           0.0065 |         235.8002 |           0.4635 |
[32m[20221213 18:12:17 @agent_ppo2.py:185][0m |          -0.0059 |         225.0991 |           0.4634 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0002 |         229.3451 |           0.4632 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0077 |         224.6046 |           0.4631 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |           0.0012 |         239.7802 |           0.4631 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0097 |         224.2650 |           0.4635 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0101 |         224.2910 |           0.4642 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0087 |         224.1786 |           0.4640 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0102 |         224.0811 |           0.4646 |
[32m[20221213 18:12:18 @agent_ppo2.py:185][0m |          -0.0108 |         224.0704 |           0.4646 |
[32m[20221213 18:12:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:12:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.78
[32m[20221213 18:12:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.08
[32m[20221213 18:12:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 928.11
[32m[20221213 18:12:18 @agent_ppo2.py:143][0m Total time:       8.14 min
[32m[20221213 18:12:18 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 18:12:18 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 18:12:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0016 |         237.8954 |           0.4583 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0067 |         235.0366 |           0.4582 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0073 |         234.0362 |           0.4582 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0066 |         233.4307 |           0.4585 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0078 |         233.0988 |           0.4582 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0092 |         232.6681 |           0.4582 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0086 |         232.5059 |           0.4591 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0069 |         232.3663 |           0.4590 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0082 |         232.1215 |           0.4589 |
[32m[20221213 18:12:19 @agent_ppo2.py:185][0m |          -0.0097 |         231.6501 |           0.4590 |
[32m[20221213 18:12:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:12:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.93
[32m[20221213 18:12:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.26
[32m[20221213 18:12:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.39
[32m[20221213 18:12:20 @agent_ppo2.py:143][0m Total time:       8.17 min
[32m[20221213 18:12:20 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221213 18:12:20 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 18:12:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:20 @agent_ppo2.py:185][0m |          -0.0005 |         237.3156 |           0.4690 |
[32m[20221213 18:12:20 @agent_ppo2.py:185][0m |          -0.0070 |         234.8397 |           0.4687 |
[32m[20221213 18:12:20 @agent_ppo2.py:185][0m |           0.0023 |         244.8020 |           0.4677 |
[32m[20221213 18:12:20 @agent_ppo2.py:185][0m |          -0.0065 |         235.5162 |           0.4678 |
[32m[20221213 18:12:20 @agent_ppo2.py:185][0m |          -0.0081 |         233.2676 |           0.4674 |
[32m[20221213 18:12:20 @agent_ppo2.py:185][0m |          -0.0071 |         232.8558 |           0.4672 |
[32m[20221213 18:12:21 @agent_ppo2.py:185][0m |           0.0126 |         283.2829 |           0.4672 |
[32m[20221213 18:12:21 @agent_ppo2.py:185][0m |          -0.0087 |         233.0087 |           0.4662 |
[32m[20221213 18:12:21 @agent_ppo2.py:185][0m |          -0.0106 |         232.3926 |           0.4666 |
[32m[20221213 18:12:21 @agent_ppo2.py:185][0m |           0.0007 |         251.5028 |           0.4670 |
[32m[20221213 18:12:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.36
[32m[20221213 18:12:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.16
[32m[20221213 18:12:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 917.08
[32m[20221213 18:12:21 @agent_ppo2.py:143][0m Total time:       8.19 min
[32m[20221213 18:12:21 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 18:12:21 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 18:12:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:21 @agent_ppo2.py:185][0m |          -0.0048 |         233.1845 |           0.4593 |
[32m[20221213 18:12:21 @agent_ppo2.py:185][0m |          -0.0067 |         228.6564 |           0.4585 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0041 |         227.9334 |           0.4591 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0080 |         226.9357 |           0.4593 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0111 |         227.0107 |           0.4594 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0043 |         228.6221 |           0.4595 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0088 |         225.7636 |           0.4596 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0063 |         225.9054 |           0.4599 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0060 |         225.6687 |           0.4593 |
[32m[20221213 18:12:22 @agent_ppo2.py:185][0m |          -0.0091 |         225.5351 |           0.4601 |
[32m[20221213 18:12:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:12:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 860.48
[32m[20221213 18:12:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.22
[32m[20221213 18:12:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 905.67
[32m[20221213 18:12:22 @agent_ppo2.py:143][0m Total time:       8.21 min
[32m[20221213 18:12:22 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221213 18:12:22 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 18:12:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0029 |         232.0392 |           0.4788 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0050 |         231.2618 |           0.4784 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0047 |         231.1095 |           0.4782 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |           0.0004 |         233.4800 |           0.4781 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0060 |         231.3217 |           0.4784 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0067 |         231.1103 |           0.4783 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0051 |         230.8821 |           0.4784 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0063 |         231.0099 |           0.4785 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |           0.0011 |         234.5358 |           0.4786 |
[32m[20221213 18:12:23 @agent_ppo2.py:185][0m |          -0.0078 |         231.0031 |           0.4783 |
[32m[20221213 18:12:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 995.53
[32m[20221213 18:12:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.57
[32m[20221213 18:12:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.06
[32m[20221213 18:12:24 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221213 18:12:24 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 18:12:24 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 18:12:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:24 @agent_ppo2.py:185][0m |          -0.0029 |         236.3868 |           0.4735 |
[32m[20221213 18:12:24 @agent_ppo2.py:185][0m |          -0.0015 |         238.7927 |           0.4733 |
[32m[20221213 18:12:24 @agent_ppo2.py:185][0m |          -0.0009 |         235.1040 |           0.4729 |
[32m[20221213 18:12:24 @agent_ppo2.py:185][0m |          -0.0037 |         234.8577 |           0.4726 |
[32m[20221213 18:12:24 @agent_ppo2.py:185][0m |          -0.0076 |         232.6852 |           0.4733 |
[32m[20221213 18:12:24 @agent_ppo2.py:185][0m |          -0.0088 |         232.5590 |           0.4733 |
[32m[20221213 18:12:25 @agent_ppo2.py:185][0m |           0.0023 |         250.4876 |           0.4734 |
[32m[20221213 18:12:25 @agent_ppo2.py:185][0m |          -0.0082 |         232.2375 |           0.4731 |
[32m[20221213 18:12:25 @agent_ppo2.py:185][0m |          -0.0095 |         231.9547 |           0.4739 |
[32m[20221213 18:12:25 @agent_ppo2.py:185][0m |          -0.0031 |         239.6461 |           0.4736 |
[32m[20221213 18:12:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:12:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.54
[32m[20221213 18:12:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.74
[32m[20221213 18:12:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.72
[32m[20221213 18:12:25 @agent_ppo2.py:143][0m Total time:       8.25 min
[32m[20221213 18:12:25 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221213 18:12:25 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 18:12:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:25 @agent_ppo2.py:185][0m |          -0.0040 |         236.3971 |           0.4774 |
[32m[20221213 18:12:25 @agent_ppo2.py:185][0m |           0.0001 |         243.8409 |           0.4772 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0084 |         231.3073 |           0.4774 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0076 |         230.9513 |           0.4778 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0099 |         230.3548 |           0.4779 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0100 |         230.1601 |           0.4778 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0138 |         230.0986 |           0.4782 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0126 |         229.8129 |           0.4784 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0108 |         229.3921 |           0.4783 |
[32m[20221213 18:12:26 @agent_ppo2.py:185][0m |          -0.0093 |         229.2486 |           0.4785 |
[32m[20221213 18:12:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.17
[32m[20221213 18:12:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 905.25
[32m[20221213 18:12:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.73
[32m[20221213 18:12:26 @agent_ppo2.py:143][0m Total time:       8.28 min
[32m[20221213 18:12:26 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 18:12:26 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 18:12:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:12:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0016 |         255.3442 |           0.4760 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0058 |         251.4549 |           0.4754 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0069 |         250.0127 |           0.4754 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0052 |         249.5068 |           0.4751 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0080 |         248.9533 |           0.4747 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0064 |         248.6781 |           0.4748 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |           0.0013 |         273.2072 |           0.4748 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0085 |         248.2314 |           0.4737 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |          -0.0091 |         248.0014 |           0.4741 |
[32m[20221213 18:12:27 @agent_ppo2.py:185][0m |           0.0017 |         268.9381 |           0.4745 |
[32m[20221213 18:12:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:12:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.52
[32m[20221213 18:12:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.30
[32m[20221213 18:12:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.30
[32m[20221213 18:12:28 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221213 18:12:28 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221213 18:12:28 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 18:12:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:28 @agent_ppo2.py:185][0m |          -0.0014 |         253.5290 |           0.4796 |
[32m[20221213 18:12:28 @agent_ppo2.py:185][0m |          -0.0028 |         251.6744 |           0.4779 |
[32m[20221213 18:12:28 @agent_ppo2.py:185][0m |          -0.0045 |         250.7516 |           0.4775 |
[32m[20221213 18:12:28 @agent_ppo2.py:185][0m |          -0.0015 |         250.7400 |           0.4779 |
[32m[20221213 18:12:28 @agent_ppo2.py:185][0m |          -0.0030 |         250.9238 |           0.4776 |
[32m[20221213 18:12:28 @agent_ppo2.py:185][0m |          -0.0065 |         249.4274 |           0.4772 |
[32m[20221213 18:12:29 @agent_ppo2.py:185][0m |          -0.0049 |         249.1760 |           0.4774 |
[32m[20221213 18:12:29 @agent_ppo2.py:185][0m |          -0.0081 |         249.2034 |           0.4774 |
[32m[20221213 18:12:29 @agent_ppo2.py:185][0m |          -0.0053 |         248.6590 |           0.4774 |
[32m[20221213 18:12:29 @agent_ppo2.py:185][0m |          -0.0081 |         248.6314 |           0.4776 |
[32m[20221213 18:12:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.66
[32m[20221213 18:12:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 907.75
[32m[20221213 18:12:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.20
[32m[20221213 18:12:29 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221213 18:12:29 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 18:12:29 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 18:12:29 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:12:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:29 @agent_ppo2.py:185][0m |          -0.0006 |         254.1497 |           0.4830 |
[32m[20221213 18:12:29 @agent_ppo2.py:185][0m |          -0.0022 |         250.5711 |           0.4823 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0049 |         249.4532 |           0.4821 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0049 |         248.1097 |           0.4808 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0032 |         246.3337 |           0.4815 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0069 |         244.5457 |           0.4810 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0055 |         243.9685 |           0.4807 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0063 |         243.6919 |           0.4801 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0032 |         243.9969 |           0.4807 |
[32m[20221213 18:12:30 @agent_ppo2.py:185][0m |          -0.0066 |         243.2568 |           0.4805 |
[32m[20221213 18:12:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:12:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.95
[32m[20221213 18:12:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.86
[32m[20221213 18:12:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 937.37
[32m[20221213 18:12:30 @agent_ppo2.py:143][0m Total time:       8.34 min
[32m[20221213 18:12:30 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221213 18:12:30 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 18:12:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0002 |         249.5048 |           0.4896 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0023 |         248.5939 |           0.4890 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |           0.0116 |         274.2193 |           0.4890 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0033 |         248.0440 |           0.4893 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |           0.0060 |         270.7730 |           0.4897 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |           0.0001 |         251.9628 |           0.4881 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0058 |         247.4693 |           0.4893 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0071 |         247.3844 |           0.4887 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0070 |         247.1596 |           0.4891 |
[32m[20221213 18:12:31 @agent_ppo2.py:185][0m |          -0.0071 |         247.0434 |           0.4890 |
[32m[20221213 18:12:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:12:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 973.68
[32m[20221213 18:12:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.80
[32m[20221213 18:12:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.56
[32m[20221213 18:12:32 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221213 18:12:32 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 18:12:32 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 18:12:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |          -0.0022 |         254.1987 |           0.4877 |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |           0.0027 |         263.7915 |           0.4866 |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |           0.0044 |         275.9316 |           0.4861 |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |          -0.0067 |         249.8289 |           0.4847 |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |           0.0017 |         265.2089 |           0.4852 |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |          -0.0087 |         248.8676 |           0.4853 |
[32m[20221213 18:12:32 @agent_ppo2.py:185][0m |          -0.0089 |         248.4733 |           0.4852 |
[32m[20221213 18:12:33 @agent_ppo2.py:185][0m |          -0.0093 |         248.2695 |           0.4859 |
[32m[20221213 18:12:33 @agent_ppo2.py:185][0m |          -0.0089 |         248.1490 |           0.4843 |
[32m[20221213 18:12:33 @agent_ppo2.py:185][0m |          -0.0098 |         247.7404 |           0.4849 |
[32m[20221213 18:12:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.42
[32m[20221213 18:12:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.81
[32m[20221213 18:12:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.42
[32m[20221213 18:12:33 @agent_ppo2.py:143][0m Total time:       8.39 min
[32m[20221213 18:12:33 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221213 18:12:33 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 18:12:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:33 @agent_ppo2.py:185][0m |           0.0145 |         289.0261 |           0.4783 |
[32m[20221213 18:12:33 @agent_ppo2.py:185][0m |           0.0055 |         253.6109 |           0.4766 |
[32m[20221213 18:12:33 @agent_ppo2.py:185][0m |          -0.0056 |         246.8953 |           0.4777 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |          -0.0059 |         246.0211 |           0.4780 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |          -0.0066 |         245.5720 |           0.4778 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |           0.0028 |         257.6559 |           0.4773 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |           0.0089 |         281.1795 |           0.4775 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |          -0.0083 |         244.5257 |           0.4779 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |          -0.0088 |         244.4258 |           0.4779 |
[32m[20221213 18:12:34 @agent_ppo2.py:185][0m |          -0.0005 |         255.4082 |           0.4776 |
[32m[20221213 18:12:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 906.73
[32m[20221213 18:12:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.83
[32m[20221213 18:12:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.86
[32m[20221213 18:12:34 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221213 18:12:34 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 18:12:34 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 18:12:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0026 |         248.4612 |           0.4776 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0045 |         248.0261 |           0.4775 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0057 |         248.2457 |           0.4777 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0066 |         247.8904 |           0.4768 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0058 |         247.7992 |           0.4773 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0064 |         247.8542 |           0.4772 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0070 |         247.7257 |           0.4774 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |           0.0169 |         295.3932 |           0.4772 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0060 |         248.7021 |           0.4760 |
[32m[20221213 18:12:35 @agent_ppo2.py:185][0m |          -0.0027 |         249.9404 |           0.4772 |
[32m[20221213 18:12:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.57
[32m[20221213 18:12:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.71
[32m[20221213 18:12:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 963.99
[32m[20221213 18:12:36 @agent_ppo2.py:143][0m Total time:       8.43 min
[32m[20221213 18:12:36 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221213 18:12:36 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 18:12:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |           0.0008 |         253.4609 |           0.4794 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0046 |         250.7542 |           0.4792 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0071 |         249.5993 |           0.4795 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0046 |         250.1323 |           0.4792 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0075 |         248.9511 |           0.4799 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0067 |         249.1565 |           0.4800 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0084 |         248.2204 |           0.4803 |
[32m[20221213 18:12:36 @agent_ppo2.py:185][0m |          -0.0077 |         248.2323 |           0.4804 |
[32m[20221213 18:12:37 @agent_ppo2.py:185][0m |          -0.0088 |         247.9268 |           0.4800 |
[32m[20221213 18:12:37 @agent_ppo2.py:185][0m |          -0.0101 |         247.6251 |           0.4805 |
[32m[20221213 18:12:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 867.13
[32m[20221213 18:12:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 912.96
[32m[20221213 18:12:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 905.19
[32m[20221213 18:12:37 @agent_ppo2.py:143][0m Total time:       8.45 min
[32m[20221213 18:12:37 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 18:12:37 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 18:12:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:37 @agent_ppo2.py:185][0m |          -0.0010 |         257.0042 |           0.4784 |
[32m[20221213 18:12:37 @agent_ppo2.py:185][0m |          -0.0056 |         254.9151 |           0.4771 |
[32m[20221213 18:12:37 @agent_ppo2.py:185][0m |          -0.0074 |         254.0561 |           0.4775 |
[32m[20221213 18:12:37 @agent_ppo2.py:185][0m |          -0.0085 |         253.4859 |           0.4763 |
[32m[20221213 18:12:38 @agent_ppo2.py:185][0m |          -0.0056 |         252.9658 |           0.4767 |
[32m[20221213 18:12:38 @agent_ppo2.py:185][0m |          -0.0078 |         252.5619 |           0.4763 |
[32m[20221213 18:12:38 @agent_ppo2.py:185][0m |          -0.0079 |         252.2855 |           0.4766 |
[32m[20221213 18:12:38 @agent_ppo2.py:185][0m |          -0.0072 |         252.7188 |           0.4762 |
[32m[20221213 18:12:38 @agent_ppo2.py:185][0m |          -0.0091 |         251.8624 |           0.4765 |
[32m[20221213 18:12:38 @agent_ppo2.py:185][0m |          -0.0081 |         251.7923 |           0.4766 |
[32m[20221213 18:12:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 901.46
[32m[20221213 18:12:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.47
[32m[20221213 18:12:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.72
[32m[20221213 18:12:38 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221213 18:12:38 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221213 18:12:38 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 18:12:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |           0.0057 |         260.8288 |           0.5064 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |          -0.0009 |         252.7392 |           0.5057 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |           0.0022 |         264.9599 |           0.5055 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |          -0.0033 |         251.0900 |           0.5054 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |          -0.0060 |         249.8932 |           0.5055 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |           0.0070 |         285.9733 |           0.5055 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |           0.0014 |         254.5850 |           0.5055 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |          -0.0061 |         248.9537 |           0.5058 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |          -0.0008 |         252.7234 |           0.5062 |
[32m[20221213 18:12:39 @agent_ppo2.py:185][0m |          -0.0025 |         251.7286 |           0.5062 |
[32m[20221213 18:12:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 905.06
[32m[20221213 18:12:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.37
[32m[20221213 18:12:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.16
[32m[20221213 18:12:39 @agent_ppo2.py:143][0m Total time:       8.50 min
[32m[20221213 18:12:39 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 18:12:39 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 18:12:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |           0.0003 |         253.0070 |           0.4970 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0052 |         249.8177 |           0.4945 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0066 |         249.5181 |           0.4946 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0060 |         248.8403 |           0.4937 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0086 |         248.6836 |           0.4935 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0068 |         248.2998 |           0.4924 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0087 |         248.2056 |           0.4930 |
[32m[20221213 18:12:40 @agent_ppo2.py:185][0m |          -0.0082 |         247.8231 |           0.4922 |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |          -0.0086 |         247.8042 |           0.4922 |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |          -0.0074 |         248.7331 |           0.4914 |
[32m[20221213 18:12:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 948.71
[32m[20221213 18:12:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.93
[32m[20221213 18:12:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.37
[32m[20221213 18:12:41 @agent_ppo2.py:143][0m Total time:       8.52 min
[32m[20221213 18:12:41 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221213 18:12:41 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 18:12:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |          -0.0002 |         251.2639 |           0.4921 |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |          -0.0037 |         249.6492 |           0.4920 |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |          -0.0052 |         249.0966 |           0.4921 |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |          -0.0015 |         249.0225 |           0.4909 |
[32m[20221213 18:12:41 @agent_ppo2.py:185][0m |           0.0033 |         258.2793 |           0.4911 |
[32m[20221213 18:12:42 @agent_ppo2.py:185][0m |           0.0074 |         283.9857 |           0.4914 |
[32m[20221213 18:12:42 @agent_ppo2.py:185][0m |          -0.0067 |         248.2897 |           0.4908 |
[32m[20221213 18:12:42 @agent_ppo2.py:185][0m |          -0.0009 |         252.0750 |           0.4905 |
[32m[20221213 18:12:42 @agent_ppo2.py:185][0m |           0.0009 |         261.8625 |           0.4904 |
[32m[20221213 18:12:42 @agent_ppo2.py:185][0m |          -0.0072 |         247.5180 |           0.4905 |
[32m[20221213 18:12:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:12:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 905.87
[32m[20221213 18:12:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 980.72
[32m[20221213 18:12:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.45
[32m[20221213 18:12:42 @agent_ppo2.py:143][0m Total time:       8.54 min
[32m[20221213 18:12:42 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 18:12:42 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 18:12:42 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:12:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:42 @agent_ppo2.py:185][0m |           0.0001 |         250.5392 |           0.4945 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0053 |         247.5877 |           0.4937 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0058 |         246.8388 |           0.4933 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0070 |         245.8448 |           0.4930 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0077 |         245.1681 |           0.4924 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0086 |         245.4735 |           0.4927 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0073 |         244.9414 |           0.4925 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |           0.0005 |         269.8913 |           0.4923 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0099 |         244.4538 |           0.4915 |
[32m[20221213 18:12:43 @agent_ppo2.py:185][0m |          -0.0086 |         244.2433 |           0.4916 |
[32m[20221213 18:12:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:12:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.27
[32m[20221213 18:12:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.96
[32m[20221213 18:12:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.73
[32m[20221213 18:12:43 @agent_ppo2.py:143][0m Total time:       8.56 min
[32m[20221213 18:12:43 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221213 18:12:43 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 18:12:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0014 |         235.4722 |           0.4969 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0026 |         229.2021 |           0.4970 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0051 |         225.6930 |           0.4973 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0098 |         223.7375 |           0.4973 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |           0.0003 |         225.8485 |           0.4974 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0067 |         220.7101 |           0.4976 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0079 |         220.1518 |           0.4972 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0066 |         219.4486 |           0.4978 |
[32m[20221213 18:12:44 @agent_ppo2.py:185][0m |          -0.0110 |         218.8912 |           0.4978 |
[32m[20221213 18:12:45 @agent_ppo2.py:185][0m |          -0.0094 |         217.9540 |           0.4979 |
[32m[20221213 18:12:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.55
[32m[20221213 18:12:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 952.79
[32m[20221213 18:12:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.24
[32m[20221213 18:12:45 @agent_ppo2.py:143][0m Total time:       8.58 min
[32m[20221213 18:12:45 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 18:12:45 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 18:12:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:45 @agent_ppo2.py:185][0m |           0.0000 |         250.9507 |           0.4951 |
[32m[20221213 18:12:45 @agent_ppo2.py:185][0m |          -0.0052 |         245.9517 |           0.4940 |
[32m[20221213 18:12:45 @agent_ppo2.py:185][0m |          -0.0069 |         243.5945 |           0.4932 |
[32m[20221213 18:12:45 @agent_ppo2.py:185][0m |          -0.0023 |         242.8627 |           0.4930 |
[32m[20221213 18:12:45 @agent_ppo2.py:185][0m |          -0.0071 |         241.5087 |           0.4928 |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |           0.0019 |         255.3921 |           0.4925 |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |          -0.0095 |         240.3540 |           0.4921 |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |          -0.0100 |         239.8142 |           0.4916 |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |          -0.0093 |         239.6944 |           0.4912 |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |          -0.0080 |         238.9976 |           0.4917 |
[32m[20221213 18:12:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:12:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 884.77
[32m[20221213 18:12:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 935.87
[32m[20221213 18:12:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.93
[32m[20221213 18:12:46 @agent_ppo2.py:143][0m Total time:       8.60 min
[32m[20221213 18:12:46 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221213 18:12:46 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 18:12:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |           0.0086 |         266.1684 |           0.4788 |
[32m[20221213 18:12:46 @agent_ppo2.py:185][0m |          -0.0011 |         250.1262 |           0.4776 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |          -0.0053 |         248.2357 |           0.4779 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |          -0.0068 |         247.7292 |           0.4777 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |           0.0020 |         253.0502 |           0.4777 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |          -0.0005 |         249.8094 |           0.4767 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |          -0.0047 |         246.7171 |           0.4771 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |           0.0032 |         255.2968 |           0.4773 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |          -0.0071 |         246.3034 |           0.4769 |
[32m[20221213 18:12:47 @agent_ppo2.py:185][0m |          -0.0040 |         247.2426 |           0.4761 |
[32m[20221213 18:12:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:12:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.86
[32m[20221213 18:12:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.60
[32m[20221213 18:12:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.84
[32m[20221213 18:12:47 @agent_ppo2.py:143][0m Total time:       8.63 min
[32m[20221213 18:12:47 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 18:12:47 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 18:12:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:12:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0008 |         244.8973 |           0.4839 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0066 |         243.3635 |           0.4834 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0041 |         242.3698 |           0.4832 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |           0.0002 |         247.8141 |           0.4829 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0043 |         241.5913 |           0.4830 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0076 |         240.8200 |           0.4830 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0008 |         249.2725 |           0.4830 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0086 |         240.3495 |           0.4823 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0041 |         243.0403 |           0.4829 |
[32m[20221213 18:12:48 @agent_ppo2.py:185][0m |          -0.0081 |         239.9167 |           0.4828 |
[32m[20221213 18:12:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:12:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.21
[32m[20221213 18:12:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.33
[32m[20221213 18:12:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.34
[32m[20221213 18:12:49 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 18:12:49 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221213 18:12:49 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 18:12:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:49 @agent_ppo2.py:185][0m |           0.0116 |         252.0998 |           0.4733 |
[32m[20221213 18:12:49 @agent_ppo2.py:185][0m |          -0.0067 |         224.6356 |           0.4729 |
[32m[20221213 18:12:49 @agent_ppo2.py:185][0m |          -0.0043 |         223.5187 |           0.4730 |
[32m[20221213 18:12:49 @agent_ppo2.py:185][0m |          -0.0022 |         229.2154 |           0.4732 |
[32m[20221213 18:12:49 @agent_ppo2.py:185][0m |          -0.0078 |         222.5881 |           0.4728 |
[32m[20221213 18:12:49 @agent_ppo2.py:185][0m |          -0.0053 |         224.2301 |           0.4731 |
[32m[20221213 18:12:50 @agent_ppo2.py:185][0m |           0.0010 |         234.0493 |           0.4728 |
[32m[20221213 18:12:50 @agent_ppo2.py:185][0m |          -0.0106 |         221.7226 |           0.4728 |
[32m[20221213 18:12:50 @agent_ppo2.py:185][0m |          -0.0074 |         221.5866 |           0.4733 |
[32m[20221213 18:12:50 @agent_ppo2.py:185][0m |          -0.0057 |         221.2950 |           0.4733 |
[32m[20221213 18:12:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:12:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.91
[32m[20221213 18:12:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.86
[32m[20221213 18:12:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 902.47
[32m[20221213 18:12:50 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221213 18:12:50 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 18:12:50 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 18:12:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:12:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:50 @agent_ppo2.py:185][0m |          -0.0014 |         246.7017 |           0.4805 |
[32m[20221213 18:12:50 @agent_ppo2.py:185][0m |          -0.0039 |         245.7137 |           0.4794 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0046 |         245.1575 |           0.4787 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0025 |         245.1562 |           0.4788 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0011 |         250.9302 |           0.4794 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0061 |         244.6918 |           0.4780 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0045 |         245.5085 |           0.4784 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0045 |         244.6274 |           0.4784 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |          -0.0077 |         244.7932 |           0.4784 |
[32m[20221213 18:12:51 @agent_ppo2.py:185][0m |           0.0039 |         262.2078 |           0.4786 |
[32m[20221213 18:12:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:12:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 983.73
[32m[20221213 18:12:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.39
[32m[20221213 18:12:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 933.96
[32m[20221213 18:12:51 @agent_ppo2.py:143][0m Total time:       8.69 min
[32m[20221213 18:12:51 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221213 18:12:51 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 18:12:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0000 |         248.6026 |           0.4861 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0032 |         246.5545 |           0.4858 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0040 |         245.6401 |           0.4848 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0050 |         245.0159 |           0.4848 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0070 |         244.3066 |           0.4842 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0056 |         243.7359 |           0.4841 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0051 |         243.7317 |           0.4838 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0037 |         242.6521 |           0.4834 |
[32m[20221213 18:12:52 @agent_ppo2.py:185][0m |          -0.0074 |         242.7993 |           0.4833 |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0070 |         242.4974 |           0.4833 |
[32m[20221213 18:12:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 856.55
[32m[20221213 18:12:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.09
[32m[20221213 18:12:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 918.76
[32m[20221213 18:12:53 @agent_ppo2.py:143][0m Total time:       8.72 min
[32m[20221213 18:12:53 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 18:12:53 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 18:12:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0007 |         244.4488 |           0.4719 |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0032 |         242.5879 |           0.4709 |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0027 |         242.0088 |           0.4711 |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0051 |         241.4462 |           0.4707 |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0068 |         240.9518 |           0.4704 |
[32m[20221213 18:12:53 @agent_ppo2.py:185][0m |          -0.0020 |         244.7205 |           0.4706 |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0062 |         240.5201 |           0.4702 |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0072 |         240.1574 |           0.4700 |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0074 |         240.0381 |           0.4701 |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0083 |         239.9020 |           0.4699 |
[32m[20221213 18:12:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:12:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.90
[32m[20221213 18:12:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.61
[32m[20221213 18:12:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.53
[32m[20221213 18:12:54 @agent_ppo2.py:143][0m Total time:       8.74 min
[32m[20221213 18:12:54 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221213 18:12:54 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 18:12:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0019 |         237.8598 |           0.4714 |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0050 |         234.0830 |           0.4701 |
[32m[20221213 18:12:54 @agent_ppo2.py:185][0m |          -0.0057 |         232.0566 |           0.4712 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0019 |         233.4516 |           0.4702 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0029 |         232.9054 |           0.4703 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0075 |         229.8226 |           0.4705 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0080 |         229.9070 |           0.4704 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0076 |         229.1205 |           0.4703 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0086 |         229.1617 |           0.4701 |
[32m[20221213 18:12:55 @agent_ppo2.py:185][0m |          -0.0084 |         229.1158 |           0.4697 |
[32m[20221213 18:12:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 943.32
[32m[20221213 18:12:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.47
[32m[20221213 18:12:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.66
[32m[20221213 18:12:55 @agent_ppo2.py:143][0m Total time:       8.76 min
[32m[20221213 18:12:55 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 18:12:55 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 18:12:55 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:12:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0009 |         251.9133 |           0.4832 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0026 |         250.3254 |           0.4830 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0002 |         251.1008 |           0.4838 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0034 |         249.4740 |           0.4836 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0030 |         249.4219 |           0.4845 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0038 |         249.0515 |           0.4847 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |           0.0002 |         249.9572 |           0.4850 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0059 |         248.6182 |           0.4855 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0067 |         248.2495 |           0.4851 |
[32m[20221213 18:12:56 @agent_ppo2.py:185][0m |          -0.0055 |         248.1560 |           0.4865 |
[32m[20221213 18:12:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:12:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 998.44
[32m[20221213 18:12:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.60
[32m[20221213 18:12:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.05
[32m[20221213 18:12:57 @agent_ppo2.py:143][0m Total time:       8.78 min
[32m[20221213 18:12:57 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221213 18:12:57 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 18:12:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |          -0.0006 |         260.1858 |           0.4903 |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |          -0.0045 |         255.5217 |           0.4894 |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |          -0.0060 |         253.6048 |           0.4889 |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |          -0.0001 |         254.2872 |           0.4891 |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |          -0.0049 |         251.3743 |           0.4885 |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |          -0.0064 |         250.6658 |           0.4886 |
[32m[20221213 18:12:57 @agent_ppo2.py:185][0m |           0.0128 |         268.5906 |           0.4883 |
[32m[20221213 18:12:58 @agent_ppo2.py:185][0m |          -0.0011 |         254.0497 |           0.4882 |
[32m[20221213 18:12:58 @agent_ppo2.py:185][0m |          -0.0059 |         249.8665 |           0.4885 |
[32m[20221213 18:12:58 @agent_ppo2.py:185][0m |          -0.0052 |         250.1713 |           0.4885 |
[32m[20221213 18:12:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:12:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 864.76
[32m[20221213 18:12:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 894.67
[32m[20221213 18:12:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.60
[32m[20221213 18:12:58 @agent_ppo2.py:143][0m Total time:       8.80 min
[32m[20221213 18:12:58 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 18:12:58 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 18:12:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:12:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:12:58 @agent_ppo2.py:185][0m |          -0.0012 |         249.4622 |           0.4755 |
[32m[20221213 18:12:58 @agent_ppo2.py:185][0m |          -0.0053 |         247.7825 |           0.4756 |
[32m[20221213 18:12:58 @agent_ppo2.py:185][0m |          -0.0062 |         246.7501 |           0.4756 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0043 |         246.7349 |           0.4756 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0079 |         245.8752 |           0.4756 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0055 |         245.2747 |           0.4759 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0088 |         245.2757 |           0.4759 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0068 |         244.8452 |           0.4764 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0078 |         244.7989 |           0.4763 |
[32m[20221213 18:12:59 @agent_ppo2.py:185][0m |          -0.0079 |         244.8998 |           0.4766 |
[32m[20221213 18:12:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:12:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.31
[32m[20221213 18:12:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.57
[32m[20221213 18:12:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.07
[32m[20221213 18:12:59 @agent_ppo2.py:143][0m Total time:       8.82 min
[32m[20221213 18:12:59 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221213 18:12:59 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 18:12:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0015 |         246.3293 |           0.4892 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0051 |         244.4572 |           0.4894 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0051 |         243.6165 |           0.4886 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0054 |         242.9452 |           0.4889 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0031 |         244.0655 |           0.4884 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0070 |         242.4988 |           0.4888 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0066 |         242.5421 |           0.4890 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0084 |         241.4083 |           0.4891 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0083 |         241.4783 |           0.4893 |
[32m[20221213 18:13:00 @agent_ppo2.py:185][0m |          -0.0078 |         240.8107 |           0.4895 |
[32m[20221213 18:13:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:13:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.93
[32m[20221213 18:13:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.17
[32m[20221213 18:13:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.64
[32m[20221213 18:13:01 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221213 18:13:01 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 18:13:01 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 18:13:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |          -0.0030 |         240.2714 |           0.4853 |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |          -0.0059 |         237.8632 |           0.4845 |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |          -0.0058 |         236.8703 |           0.4834 |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |           0.0076 |         262.4263 |           0.4830 |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |          -0.0066 |         236.1059 |           0.4823 |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |          -0.0068 |         236.3762 |           0.4822 |
[32m[20221213 18:13:01 @agent_ppo2.py:185][0m |          -0.0073 |         235.8530 |           0.4814 |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |          -0.0069 |         235.8646 |           0.4810 |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |           0.0040 |         248.6434 |           0.4808 |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |           0.0004 |         250.2653 |           0.4797 |
[32m[20221213 18:13:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:13:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 908.39
[32m[20221213 18:13:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.14
[32m[20221213 18:13:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.51
[32m[20221213 18:13:02 @agent_ppo2.py:143][0m Total time:       8.87 min
[32m[20221213 18:13:02 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221213 18:13:02 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 18:13:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |          -0.0024 |         241.1928 |           0.4712 |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |          -0.0041 |         238.5852 |           0.4710 |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |          -0.0057 |         236.9354 |           0.4716 |
[32m[20221213 18:13:02 @agent_ppo2.py:185][0m |          -0.0076 |         235.3203 |           0.4717 |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |          -0.0066 |         233.4510 |           0.4713 |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |          -0.0059 |         232.5427 |           0.4716 |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |          -0.0029 |         232.8483 |           0.4715 |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |          -0.0026 |         235.9983 |           0.4715 |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |          -0.0094 |         230.7522 |           0.4715 |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |          -0.0088 |         230.4681 |           0.4716 |
[32m[20221213 18:13:03 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 858.26
[32m[20221213 18:13:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.47
[32m[20221213 18:13:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 932.70
[32m[20221213 18:13:03 @agent_ppo2.py:143][0m Total time:       8.89 min
[32m[20221213 18:13:03 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 18:13:03 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 18:13:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:03 @agent_ppo2.py:185][0m |           0.0050 |         222.5613 |           0.4873 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |           0.0096 |         235.7823 |           0.4857 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0020 |         209.8647 |           0.4857 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0039 |         209.3176 |           0.4851 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0050 |         208.8071 |           0.4848 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0048 |         208.5434 |           0.4843 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0006 |         215.9738 |           0.4838 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0029 |         207.9915 |           0.4835 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |           0.0046 |         237.2962 |           0.4832 |
[32m[20221213 18:13:04 @agent_ppo2.py:185][0m |          -0.0040 |         208.9044 |           0.4827 |
[32m[20221213 18:13:04 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.61
[32m[20221213 18:13:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.48
[32m[20221213 18:13:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.98
[32m[20221213 18:13:04 @agent_ppo2.py:143][0m Total time:       8.91 min
[32m[20221213 18:13:04 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221213 18:13:04 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 18:13:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0025 |         233.0018 |           0.4891 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0042 |         232.1105 |           0.4887 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0057 |         231.3867 |           0.4886 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0044 |         232.2729 |           0.4896 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0037 |         232.0611 |           0.4896 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0071 |         231.2446 |           0.4902 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0064 |         230.5442 |           0.4904 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0074 |         230.5434 |           0.4910 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0071 |         230.1948 |           0.4916 |
[32m[20221213 18:13:05 @agent_ppo2.py:185][0m |          -0.0057 |         230.0513 |           0.4916 |
[32m[20221213 18:13:05 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.55
[32m[20221213 18:13:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.30
[32m[20221213 18:13:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.90
[32m[20221213 18:13:06 @agent_ppo2.py:143][0m Total time:       8.93 min
[32m[20221213 18:13:06 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 18:13:06 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 18:13:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0045 |         219.5218 |           0.4812 |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0044 |         214.2774 |           0.4804 |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0052 |         211.3130 |           0.4800 |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0074 |         210.5593 |           0.4800 |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0055 |         209.7666 |           0.4795 |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0095 |         209.6283 |           0.4799 |
[32m[20221213 18:13:06 @agent_ppo2.py:185][0m |          -0.0020 |         218.7476 |           0.4798 |
[32m[20221213 18:13:07 @agent_ppo2.py:185][0m |          -0.0083 |         209.2467 |           0.4801 |
[32m[20221213 18:13:07 @agent_ppo2.py:185][0m |          -0.0111 |         209.4100 |           0.4801 |
[32m[20221213 18:13:07 @agent_ppo2.py:185][0m |          -0.0089 |         208.6899 |           0.4801 |
[32m[20221213 18:13:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:13:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 726.21
[32m[20221213 18:13:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 930.26
[32m[20221213 18:13:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 945.92
[32m[20221213 18:13:07 @agent_ppo2.py:143][0m Total time:       8.95 min
[32m[20221213 18:13:07 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221213 18:13:07 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 18:13:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:07 @agent_ppo2.py:185][0m |          -0.0015 |         244.0005 |           0.4890 |
[32m[20221213 18:13:07 @agent_ppo2.py:185][0m |          -0.0041 |         239.9019 |           0.4891 |
[32m[20221213 18:13:07 @agent_ppo2.py:185][0m |          -0.0050 |         238.4589 |           0.4884 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |          -0.0033 |         237.9900 |           0.4884 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |          -0.0043 |         238.0467 |           0.4883 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |          -0.0072 |         236.7842 |           0.4880 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |          -0.0081 |         236.6056 |           0.4877 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |          -0.0068 |         236.4099 |           0.4875 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |           0.0053 |         257.7689 |           0.4871 |
[32m[20221213 18:13:08 @agent_ppo2.py:185][0m |          -0.0065 |         235.5882 |           0.4858 |
[32m[20221213 18:13:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:13:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.84
[32m[20221213 18:13:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.48
[32m[20221213 18:13:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.86
[32m[20221213 18:13:08 @agent_ppo2.py:143][0m Total time:       8.97 min
[32m[20221213 18:13:08 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 18:13:08 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 18:13:08 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:13:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |           0.0001 |         212.3977 |           0.4836 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0044 |         209.2962 |           0.4832 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |           0.0049 |         221.1780 |           0.4827 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0062 |         209.1084 |           0.4832 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0079 |         208.1081 |           0.4837 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0083 |         207.9502 |           0.4839 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0041 |         210.6783 |           0.4839 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0076 |         207.5319 |           0.4845 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0080 |         207.4594 |           0.4848 |
[32m[20221213 18:13:09 @agent_ppo2.py:185][0m |          -0.0077 |         207.2596 |           0.4847 |
[32m[20221213 18:13:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:13:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.82
[32m[20221213 18:13:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.21
[32m[20221213 18:13:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.16
[32m[20221213 18:13:10 @agent_ppo2.py:143][0m Total time:       9.00 min
[32m[20221213 18:13:10 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221213 18:13:10 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 18:13:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |           0.0110 |         255.7462 |           0.4950 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0032 |         237.8005 |           0.4949 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0036 |         236.9396 |           0.4948 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0010 |         237.6175 |           0.4946 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0057 |         236.0988 |           0.4946 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0052 |         235.8718 |           0.4951 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0073 |         235.6081 |           0.4949 |
[32m[20221213 18:13:10 @agent_ppo2.py:185][0m |          -0.0049 |         235.6650 |           0.4947 |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0063 |         235.4642 |           0.4947 |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0074 |         235.2443 |           0.4950 |
[32m[20221213 18:13:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:13:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.52
[32m[20221213 18:13:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.63
[32m[20221213 18:13:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.94
[32m[20221213 18:13:11 @agent_ppo2.py:143][0m Total time:       9.02 min
[32m[20221213 18:13:11 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 18:13:11 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 18:13:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0018 |         241.6442 |           0.4768 |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0037 |         239.8632 |           0.4758 |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0041 |         238.9850 |           0.4760 |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0050 |         238.4524 |           0.4757 |
[32m[20221213 18:13:11 @agent_ppo2.py:185][0m |          -0.0044 |         237.8786 |           0.4755 |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |          -0.0044 |         238.0721 |           0.4750 |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |          -0.0072 |         237.1677 |           0.4756 |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |          -0.0068 |         237.1739 |           0.4748 |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |          -0.0033 |         239.3645 |           0.4756 |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |          -0.0069 |         236.5610 |           0.4754 |
[32m[20221213 18:13:12 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:13:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.39
[32m[20221213 18:13:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.92
[32m[20221213 18:13:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.29
[32m[20221213 18:13:12 @agent_ppo2.py:143][0m Total time:       9.04 min
[32m[20221213 18:13:12 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221213 18:13:12 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 18:13:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |          -0.0003 |         241.8408 |           0.5018 |
[32m[20221213 18:13:12 @agent_ppo2.py:185][0m |           0.0009 |         243.4619 |           0.5014 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0028 |         238.3401 |           0.5008 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0063 |         237.7940 |           0.5003 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0050 |         237.2465 |           0.5006 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0043 |         237.1321 |           0.5001 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0041 |         236.9779 |           0.5002 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0052 |         236.5308 |           0.4998 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0056 |         236.1908 |           0.4994 |
[32m[20221213 18:13:13 @agent_ppo2.py:185][0m |          -0.0037 |         238.8941 |           0.4996 |
[32m[20221213 18:13:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:13:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.44
[32m[20221213 18:13:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.26
[32m[20221213 18:13:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 969.07
[32m[20221213 18:13:13 @agent_ppo2.py:143][0m Total time:       9.06 min
[32m[20221213 18:13:13 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 18:13:13 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 18:13:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0007 |         241.4876 |           0.4892 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0048 |         238.9676 |           0.4882 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0067 |         237.5687 |           0.4881 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0084 |         236.9055 |           0.4880 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0047 |         238.0909 |           0.4881 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0007 |         247.4812 |           0.4880 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0085 |         235.5979 |           0.4874 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0005 |         241.6530 |           0.4883 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |          -0.0095 |         234.8617 |           0.4878 |
[32m[20221213 18:13:14 @agent_ppo2.py:185][0m |           0.0048 |         253.4633 |           0.4880 |
[32m[20221213 18:13:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:13:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 872.63
[32m[20221213 18:13:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.20
[32m[20221213 18:13:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 922.25
[32m[20221213 18:13:15 @agent_ppo2.py:143][0m Total time:       9.08 min
[32m[20221213 18:13:15 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221213 18:13:15 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 18:13:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |          -0.0003 |         220.3571 |           0.4875 |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |           0.0031 |         213.6399 |           0.4871 |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |          -0.0045 |         211.6549 |           0.4865 |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |           0.0043 |         220.6720 |           0.4859 |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |           0.0042 |         226.3061 |           0.4860 |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |          -0.0035 |         211.5732 |           0.4859 |
[32m[20221213 18:13:15 @agent_ppo2.py:185][0m |          -0.0020 |         211.0910 |           0.4861 |
[32m[20221213 18:13:16 @agent_ppo2.py:185][0m |          -0.0041 |         210.1627 |           0.4854 |
[32m[20221213 18:13:16 @agent_ppo2.py:185][0m |          -0.0074 |         208.3347 |           0.4859 |
[32m[20221213 18:13:16 @agent_ppo2.py:185][0m |          -0.0019 |         211.3213 |           0.4855 |
[32m[20221213 18:13:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:13:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.70
[32m[20221213 18:13:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.05
[32m[20221213 18:13:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 931.31
[32m[20221213 18:13:16 @agent_ppo2.py:143][0m Total time:       9.10 min
[32m[20221213 18:13:16 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 18:13:16 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 18:13:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:16 @agent_ppo2.py:185][0m |          -0.0019 |         233.4323 |           0.4981 |
[32m[20221213 18:13:16 @agent_ppo2.py:185][0m |          -0.0043 |         231.6727 |           0.4967 |
[32m[20221213 18:13:16 @agent_ppo2.py:185][0m |          -0.0036 |         231.2368 |           0.4965 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |           0.0050 |         245.0083 |           0.4972 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |          -0.0046 |         229.3676 |           0.4963 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |           0.0058 |         263.4781 |           0.4966 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |          -0.0047 |         228.8170 |           0.4955 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |          -0.0067 |         228.3687 |           0.4966 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |          -0.0074 |         228.2169 |           0.4967 |
[32m[20221213 18:13:17 @agent_ppo2.py:185][0m |          -0.0078 |         228.0239 |           0.4963 |
[32m[20221213 18:13:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:13:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 970.83
[32m[20221213 18:13:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.32
[32m[20221213 18:13:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 879.73
[32m[20221213 18:13:17 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221213 18:13:17 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221213 18:13:17 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 18:13:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |           0.0037 |         242.9632 |           0.4898 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0050 |         231.7367 |           0.4891 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0056 |         229.7071 |           0.4887 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0060 |         229.0161 |           0.4887 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |           0.0017 |         244.0133 |           0.4886 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0082 |         226.8427 |           0.4884 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0049 |         227.2272 |           0.4888 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0085 |         226.1345 |           0.4888 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0089 |         226.1811 |           0.4891 |
[32m[20221213 18:13:18 @agent_ppo2.py:185][0m |          -0.0067 |         226.9617 |           0.4890 |
[32m[20221213 18:13:18 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 895.39
[32m[20221213 18:13:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.41
[32m[20221213 18:13:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 880.18
[32m[20221213 18:13:18 @agent_ppo2.py:143][0m Total time:       9.14 min
[32m[20221213 18:13:18 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 18:13:18 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 18:13:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |          -0.0012 |         237.4359 |           0.4908 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |           0.0042 |         238.8873 |           0.4912 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |          -0.0026 |         235.8304 |           0.4913 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |          -0.0027 |         235.0033 |           0.4921 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |          -0.0054 |         235.0842 |           0.4922 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |           0.0066 |         254.3712 |           0.4924 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |          -0.0055 |         234.1014 |           0.4919 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |          -0.0028 |         235.0754 |           0.4928 |
[32m[20221213 18:13:19 @agent_ppo2.py:185][0m |           0.0010 |         237.3037 |           0.4931 |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |          -0.0053 |         233.5806 |           0.4934 |
[32m[20221213 18:13:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:13:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 957.06
[32m[20221213 18:13:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 993.33
[32m[20221213 18:13:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 957.84
[32m[20221213 18:13:20 @agent_ppo2.py:143][0m Total time:       9.17 min
[32m[20221213 18:13:20 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221213 18:13:20 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 18:13:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |          -0.0013 |         245.8917 |           0.5041 |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |          -0.0063 |         241.2866 |           0.5025 |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |          -0.0027 |         243.4898 |           0.5012 |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |          -0.0098 |         239.0600 |           0.5021 |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |          -0.0069 |         238.5128 |           0.5024 |
[32m[20221213 18:13:20 @agent_ppo2.py:185][0m |           0.0027 |         266.1726 |           0.5016 |
[32m[20221213 18:13:21 @agent_ppo2.py:185][0m |          -0.0094 |         238.0638 |           0.5007 |
[32m[20221213 18:13:21 @agent_ppo2.py:185][0m |          -0.0098 |         237.3787 |           0.5027 |
[32m[20221213 18:13:21 @agent_ppo2.py:185][0m |          -0.0102 |         236.9873 |           0.5027 |
[32m[20221213 18:13:21 @agent_ppo2.py:185][0m |          -0.0058 |         239.7264 |           0.5024 |
[32m[20221213 18:13:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:13:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 915.63
[32m[20221213 18:13:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.49
[32m[20221213 18:13:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 908.46
[32m[20221213 18:13:21 @agent_ppo2.py:143][0m Total time:       9.19 min
[32m[20221213 18:13:21 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 18:13:21 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 18:13:21 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:13:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:21 @agent_ppo2.py:185][0m |          -0.0002 |         241.1306 |           0.5084 |
[32m[20221213 18:13:21 @agent_ppo2.py:185][0m |          -0.0013 |         236.7375 |           0.5076 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0051 |         234.4191 |           0.5071 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0063 |         233.0642 |           0.5067 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0056 |         232.6628 |           0.5073 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0057 |         231.3965 |           0.5070 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |           0.0005 |         239.2706 |           0.5072 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0003 |         235.8373 |           0.5074 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0066 |         230.6932 |           0.5071 |
[32m[20221213 18:13:22 @agent_ppo2.py:185][0m |          -0.0047 |         230.9230 |           0.5071 |
[32m[20221213 18:13:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.76
[32m[20221213 18:13:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 903.45
[32m[20221213 18:13:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.69
[32m[20221213 18:13:22 @agent_ppo2.py:143][0m Total time:       9.21 min
[32m[20221213 18:13:22 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221213 18:13:22 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 18:13:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0025 |         240.4665 |           0.5103 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0045 |         237.6424 |           0.5097 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0056 |         237.0691 |           0.5100 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0072 |         236.2979 |           0.5100 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0070 |         235.8673 |           0.5101 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0086 |         235.8487 |           0.5100 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0089 |         235.5207 |           0.5103 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0073 |         235.2668 |           0.5106 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0085 |         234.9209 |           0.5104 |
[32m[20221213 18:13:23 @agent_ppo2.py:185][0m |          -0.0064 |         235.2039 |           0.5106 |
[32m[20221213 18:13:23 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 909.61
[32m[20221213 18:13:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.43
[32m[20221213 18:13:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 925.98
[32m[20221213 18:13:23 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221213 18:13:23 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 18:13:23 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 18:13:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0019 |         251.7540 |           0.5168 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0023 |         246.4814 |           0.5161 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0046 |         244.5924 |           0.5155 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0052 |         243.9981 |           0.5154 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0050 |         243.0387 |           0.5153 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0023 |         245.7865 |           0.5147 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0081 |         241.9742 |           0.5147 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0069 |         241.3499 |           0.5143 |
[32m[20221213 18:13:24 @agent_ppo2.py:185][0m |          -0.0097 |         241.2047 |           0.5143 |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |          -0.0077 |         241.0106 |           0.5143 |
[32m[20221213 18:13:25 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.05
[32m[20221213 18:13:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.19
[32m[20221213 18:13:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 874.21
[32m[20221213 18:13:25 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221213 18:13:25 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221213 18:13:25 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 18:13:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |           0.0144 |         282.9272 |           0.5037 |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |          -0.0018 |         246.8065 |           0.5017 |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |          -0.0054 |         244.4612 |           0.5041 |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |          -0.0066 |         243.4813 |           0.5038 |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |           0.0093 |         277.3141 |           0.5046 |
[32m[20221213 18:13:25 @agent_ppo2.py:185][0m |          -0.0068 |         242.4831 |           0.5050 |
[32m[20221213 18:13:26 @agent_ppo2.py:185][0m |          -0.0089 |         241.9647 |           0.5052 |
[32m[20221213 18:13:26 @agent_ppo2.py:185][0m |          -0.0082 |         241.0594 |           0.5056 |
[32m[20221213 18:13:26 @agent_ppo2.py:185][0m |          -0.0092 |         241.2706 |           0.5056 |
[32m[20221213 18:13:26 @agent_ppo2.py:185][0m |          -0.0060 |         240.5914 |           0.5064 |
[32m[20221213 18:13:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.09
[32m[20221213 18:13:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.96
[32m[20221213 18:13:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 914.74
[32m[20221213 18:13:26 @agent_ppo2.py:143][0m Total time:       9.27 min
[32m[20221213 18:13:26 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 18:13:26 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 18:13:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:26 @agent_ppo2.py:185][0m |           0.0023 |         243.4590 |           0.5154 |
[32m[20221213 18:13:26 @agent_ppo2.py:185][0m |          -0.0027 |         237.2761 |           0.5140 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |           0.0000 |         238.6565 |           0.5131 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0065 |         234.5624 |           0.5131 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0052 |         234.1883 |           0.5127 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0014 |         237.2986 |           0.5123 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0071 |         233.0749 |           0.5119 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0069 |         233.0693 |           0.5113 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0066 |         232.8088 |           0.5111 |
[32m[20221213 18:13:27 @agent_ppo2.py:185][0m |          -0.0045 |         234.0415 |           0.5110 |
[32m[20221213 18:13:27 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:13:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 906.00
[32m[20221213 18:13:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.23
[32m[20221213 18:13:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 957.29
[32m[20221213 18:13:27 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221213 18:13:27 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221213 18:13:27 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 18:13:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0005 |         239.6323 |           0.5245 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0030 |         237.4540 |           0.5241 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0033 |         237.2345 |           0.5242 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |           0.0065 |         256.4165 |           0.5242 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0039 |         236.9793 |           0.5241 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0025 |         237.3629 |           0.5240 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0040 |         236.3965 |           0.5246 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0059 |         236.3608 |           0.5241 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0018 |         236.5997 |           0.5246 |
[32m[20221213 18:13:28 @agent_ppo2.py:185][0m |          -0.0084 |         236.1448 |           0.5247 |
[32m[20221213 18:13:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 998.82
[32m[20221213 18:13:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.44
[32m[20221213 18:13:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 996.87
[32m[20221213 18:13:29 @agent_ppo2.py:143][0m Total time:       9.31 min
[32m[20221213 18:13:29 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 18:13:29 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 18:13:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |           0.0013 |         248.0133 |           0.5198 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |          -0.0036 |         245.1327 |           0.5199 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |          -0.0037 |         243.3957 |           0.5193 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |           0.0106 |         278.4394 |           0.5204 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |          -0.0052 |         242.0135 |           0.5198 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |          -0.0056 |         241.2309 |           0.5199 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |          -0.0062 |         240.8782 |           0.5196 |
[32m[20221213 18:13:29 @agent_ppo2.py:185][0m |          -0.0068 |         240.6073 |           0.5198 |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |          -0.0085 |         240.5110 |           0.5204 |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |          -0.0075 |         240.3139 |           0.5201 |
[32m[20221213 18:13:30 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.16
[32m[20221213 18:13:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.34
[32m[20221213 18:13:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.77
[32m[20221213 18:13:30 @agent_ppo2.py:143][0m Total time:       9.33 min
[32m[20221213 18:13:30 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221213 18:13:30 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 18:13:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |           0.0001 |         246.1064 |           0.5077 |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |           0.0037 |         246.0840 |           0.5075 |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |          -0.0059 |         240.0314 |           0.5074 |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |          -0.0026 |         240.2077 |           0.5068 |
[32m[20221213 18:13:30 @agent_ppo2.py:185][0m |          -0.0077 |         238.6423 |           0.5065 |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |           0.0014 |         247.2945 |           0.5068 |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |          -0.0083 |         238.1842 |           0.5066 |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |          -0.0071 |         237.6572 |           0.5062 |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |          -0.0085 |         237.6545 |           0.5066 |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |          -0.0048 |         239.0224 |           0.5065 |
[32m[20221213 18:13:31 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:13:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.91
[32m[20221213 18:13:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.43
[32m[20221213 18:13:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.51
[32m[20221213 18:13:31 @agent_ppo2.py:143][0m Total time:       9.35 min
[32m[20221213 18:13:31 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 18:13:31 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 18:13:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |          -0.0024 |         243.6550 |           0.5175 |
[32m[20221213 18:13:31 @agent_ppo2.py:185][0m |          -0.0041 |         240.4573 |           0.5167 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0032 |         239.5036 |           0.5166 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0048 |         238.5868 |           0.5164 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0053 |         238.3286 |           0.5161 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0044 |         237.8746 |           0.5158 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0062 |         237.5518 |           0.5158 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0070 |         237.5928 |           0.5153 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |          -0.0017 |         242.9907 |           0.5153 |
[32m[20221213 18:13:32 @agent_ppo2.py:185][0m |           0.0017 |         258.7439 |           0.5145 |
[32m[20221213 18:13:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 893.11
[32m[20221213 18:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.38
[32m[20221213 18:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.46
[32m[20221213 18:13:32 @agent_ppo2.py:143][0m Total time:       9.38 min
[32m[20221213 18:13:32 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221213 18:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 18:13:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |           0.0016 |         246.0830 |           0.5219 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0030 |         241.6163 |           0.5202 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |           0.0062 |         274.6955 |           0.5195 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0055 |         239.1013 |           0.5187 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0017 |         240.5223 |           0.5185 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |           0.0057 |         247.6966 |           0.5188 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0073 |         235.0910 |           0.5177 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0079 |         233.7413 |           0.5174 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0071 |         233.0504 |           0.5171 |
[32m[20221213 18:13:33 @agent_ppo2.py:185][0m |          -0.0084 |         232.8125 |           0.5169 |
[32m[20221213 18:13:33 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.50
[32m[20221213 18:13:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.50
[32m[20221213 18:13:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 914.47
[32m[20221213 18:13:34 @agent_ppo2.py:143][0m Total time:       9.40 min
[32m[20221213 18:13:34 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 18:13:34 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 18:13:34 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:13:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0040 |         238.7607 |           0.5049 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0052 |         236.8473 |           0.5052 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0051 |         236.0448 |           0.5055 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0076 |         235.6441 |           0.5058 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0058 |         235.5172 |           0.5056 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0082 |         235.0137 |           0.5059 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0085 |         235.1076 |           0.5060 |
[32m[20221213 18:13:34 @agent_ppo2.py:185][0m |          -0.0097 |         234.6357 |           0.5059 |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0021 |         237.6704 |           0.5059 |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0091 |         234.9257 |           0.5061 |
[32m[20221213 18:13:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 952.28
[32m[20221213 18:13:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 981.04
[32m[20221213 18:13:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.75
[32m[20221213 18:13:35 @agent_ppo2.py:143][0m Total time:       9.42 min
[32m[20221213 18:13:35 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221213 18:13:35 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 18:13:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0021 |         240.4986 |           0.5107 |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0050 |         237.6979 |           0.5085 |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0039 |         237.0353 |           0.5078 |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0050 |         235.8274 |           0.5080 |
[32m[20221213 18:13:35 @agent_ppo2.py:185][0m |          -0.0067 |         235.0458 |           0.5072 |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |          -0.0079 |         234.5677 |           0.5067 |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |           0.0065 |         264.2724 |           0.5063 |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |          -0.0063 |         234.0118 |           0.5049 |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |          -0.0073 |         234.0326 |           0.5052 |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |          -0.0041 |         236.0670 |           0.5053 |
[32m[20221213 18:13:36 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 912.18
[32m[20221213 18:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.34
[32m[20221213 18:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 928.30
[32m[20221213 18:13:36 @agent_ppo2.py:143][0m Total time:       9.44 min
[32m[20221213 18:13:36 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 18:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 18:13:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |          -0.0034 |         239.5137 |           0.4972 |
[32m[20221213 18:13:36 @agent_ppo2.py:185][0m |           0.0044 |         244.9223 |           0.4964 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |          -0.0060 |         237.1149 |           0.4958 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |           0.0001 |         242.0938 |           0.4959 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |          -0.0055 |         235.7857 |           0.4950 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |          -0.0059 |         235.3228 |           0.4955 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |           0.0127 |         284.3109 |           0.4956 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |           0.0067 |         265.4653 |           0.4932 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |          -0.0076 |         234.9212 |           0.4940 |
[32m[20221213 18:13:37 @agent_ppo2.py:185][0m |          -0.0040 |         235.9740 |           0.4948 |
[32m[20221213 18:13:37 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.69
[32m[20221213 18:13:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.79
[32m[20221213 18:13:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.55
[32m[20221213 18:13:37 @agent_ppo2.py:143][0m Total time:       9.46 min
[32m[20221213 18:13:37 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221213 18:13:37 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 18:13:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0006 |         235.4323 |           0.4945 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0037 |         234.6166 |           0.4944 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0023 |         234.2065 |           0.4945 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0023 |         234.5731 |           0.4950 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0052 |         234.0426 |           0.4952 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0043 |         233.7386 |           0.4954 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0053 |         233.5112 |           0.4956 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0047 |         233.6989 |           0.4957 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |          -0.0065 |         233.6021 |           0.4956 |
[32m[20221213 18:13:38 @agent_ppo2.py:185][0m |           0.0008 |         238.6518 |           0.4964 |
[32m[20221213 18:13:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:13:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 968.18
[32m[20221213 18:13:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.00
[32m[20221213 18:13:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 920.75
[32m[20221213 18:13:39 @agent_ppo2.py:143][0m Total time:       9.48 min
[32m[20221213 18:13:39 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 18:13:39 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 18:13:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |          -0.0042 |         241.1326 |           0.4970 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |          -0.0055 |         237.5659 |           0.4962 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |           0.0005 |         244.2163 |           0.4964 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |          -0.0076 |         233.3194 |           0.4969 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |          -0.0070 |         231.5759 |           0.4969 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |           0.0040 |         252.1808 |           0.4973 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |          -0.0093 |         226.2093 |           0.4970 |
[32m[20221213 18:13:39 @agent_ppo2.py:185][0m |          -0.0059 |         227.6583 |           0.4976 |
[32m[20221213 18:13:40 @agent_ppo2.py:185][0m |          -0.0043 |         222.8124 |           0.4973 |
[32m[20221213 18:13:40 @agent_ppo2.py:185][0m |           0.0033 |         253.7950 |           0.4969 |
[32m[20221213 18:13:40 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 870.05
[32m[20221213 18:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 924.04
[32m[20221213 18:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.73
[32m[20221213 18:13:40 @agent_ppo2.py:143][0m Total time:       9.50 min
[32m[20221213 18:13:40 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221213 18:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 18:13:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:40 @agent_ppo2.py:185][0m |           0.0035 |         249.6502 |           0.5092 |
[32m[20221213 18:13:40 @agent_ppo2.py:185][0m |           0.0014 |         237.1304 |           0.5083 |
[32m[20221213 18:13:40 @agent_ppo2.py:185][0m |          -0.0025 |         230.2462 |           0.5080 |
[32m[20221213 18:13:40 @agent_ppo2.py:185][0m |          -0.0042 |         228.3528 |           0.5074 |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |          -0.0077 |         225.2555 |           0.5075 |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |          -0.0070 |         224.0394 |           0.5070 |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |          -0.0095 |         223.0565 |           0.5072 |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |          -0.0051 |         224.4161 |           0.5071 |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |          -0.0117 |         221.8855 |           0.5069 |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |          -0.0111 |         221.4463 |           0.5067 |
[32m[20221213 18:13:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:13:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.04
[32m[20221213 18:13:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.69
[32m[20221213 18:13:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.43
[32m[20221213 18:13:41 @agent_ppo2.py:143][0m Total time:       9.52 min
[32m[20221213 18:13:41 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 18:13:41 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 18:13:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:41 @agent_ppo2.py:185][0m |           0.0037 |         220.2673 |           0.5073 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0045 |         211.8755 |           0.5062 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |           0.0066 |         227.3368 |           0.5056 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0097 |         205.7822 |           0.5052 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0087 |         204.7576 |           0.5052 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0105 |         203.4606 |           0.5050 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |           0.0029 |         220.8488 |           0.5049 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0095 |         202.3076 |           0.5046 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0100 |         201.5702 |           0.5044 |
[32m[20221213 18:13:42 @agent_ppo2.py:185][0m |          -0.0082 |         200.6686 |           0.5041 |
[32m[20221213 18:13:42 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 897.90
[32m[20221213 18:13:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.77
[32m[20221213 18:13:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.87
[32m[20221213 18:13:42 @agent_ppo2.py:143][0m Total time:       9.54 min
[32m[20221213 18:13:42 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221213 18:13:42 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 18:13:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0002 |         237.3748 |           0.5044 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |           0.0016 |         235.6355 |           0.5039 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0010 |         235.0427 |           0.5028 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0073 |         231.7486 |           0.5032 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0062 |         230.8630 |           0.5027 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0032 |         233.9960 |           0.5023 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0081 |         230.6780 |           0.5019 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0068 |         230.3259 |           0.5015 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0068 |         230.1871 |           0.5008 |
[32m[20221213 18:13:43 @agent_ppo2.py:185][0m |          -0.0097 |         230.2259 |           0.5007 |
[32m[20221213 18:13:43 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 948.87
[32m[20221213 18:13:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.10
[32m[20221213 18:13:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 917.92
[32m[20221213 18:13:44 @agent_ppo2.py:143][0m Total time:       9.56 min
[32m[20221213 18:13:44 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 18:13:44 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 18:13:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0032 |         228.7305 |           0.4934 |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0065 |         218.8909 |           0.4921 |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0073 |         216.5264 |           0.4918 |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0100 |         214.3618 |           0.4914 |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0089 |         212.6339 |           0.4915 |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0097 |         211.6930 |           0.4911 |
[32m[20221213 18:13:44 @agent_ppo2.py:185][0m |          -0.0091 |         210.2059 |           0.4907 |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0082 |         209.0345 |           0.4908 |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0114 |         208.1552 |           0.4907 |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0114 |         206.9537 |           0.4903 |
[32m[20221213 18:13:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:13:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.94
[32m[20221213 18:13:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.29
[32m[20221213 18:13:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 974.49
[32m[20221213 18:13:45 @agent_ppo2.py:143][0m Total time:       9.58 min
[32m[20221213 18:13:45 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221213 18:13:45 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 18:13:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0014 |         238.5915 |           0.4959 |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0036 |         232.9637 |           0.4951 |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0045 |         231.4499 |           0.4948 |
[32m[20221213 18:13:45 @agent_ppo2.py:185][0m |          -0.0067 |         229.9788 |           0.4948 |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0062 |         229.1083 |           0.4944 |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0074 |         228.7047 |           0.4945 |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0039 |         228.6540 |           0.4944 |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0108 |         227.0979 |           0.4940 |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0029 |         234.9249 |           0.4943 |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0115 |         226.2159 |           0.4940 |
[32m[20221213 18:13:46 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.53
[32m[20221213 18:13:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.51
[32m[20221213 18:13:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.56
[32m[20221213 18:13:46 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 18:13:46 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 18:13:46 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 18:13:46 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:13:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:46 @agent_ppo2.py:185][0m |          -0.0051 |         230.0171 |           0.5017 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |           0.0015 |         236.8671 |           0.5014 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |           0.0038 |         245.2096 |           0.5013 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0037 |         229.1520 |           0.5013 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0039 |         226.9584 |           0.5012 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0101 |         223.9898 |           0.5013 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0108 |         223.4784 |           0.5009 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0131 |         223.0348 |           0.5009 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0123 |         222.6932 |           0.5009 |
[32m[20221213 18:13:47 @agent_ppo2.py:185][0m |          -0.0125 |         222.3398 |           0.5008 |
[32m[20221213 18:13:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 906.41
[32m[20221213 18:13:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.90
[32m[20221213 18:13:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.67
[32m[20221213 18:13:47 @agent_ppo2.py:143][0m Total time:       9.63 min
[32m[20221213 18:13:47 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221213 18:13:47 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 18:13:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0016 |         232.8538 |           0.4971 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0013 |         229.5700 |           0.4969 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0063 |         227.7136 |           0.4965 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0070 |         226.6902 |           0.4974 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |           0.0010 |         234.3012 |           0.4966 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0076 |         225.4144 |           0.4979 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0099 |         225.0101 |           0.4978 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0089 |         224.6125 |           0.4981 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0081 |         224.4461 |           0.4978 |
[32m[20221213 18:13:48 @agent_ppo2.py:185][0m |          -0.0076 |         224.4217 |           0.4983 |
[32m[20221213 18:13:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.09
[32m[20221213 18:13:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.37
[32m[20221213 18:13:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.37
[32m[20221213 18:13:49 @agent_ppo2.py:143][0m Total time:       9.65 min
[32m[20221213 18:13:49 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 18:13:49 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 18:13:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |          -0.0000 |         232.8440 |           0.4947 |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |          -0.0005 |         230.2810 |           0.4949 |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |          -0.0035 |         229.0145 |           0.4953 |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |           0.0017 |         232.9695 |           0.4953 |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |          -0.0044 |         226.6477 |           0.4952 |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |          -0.0051 |         225.8463 |           0.4951 |
[32m[20221213 18:13:49 @agent_ppo2.py:185][0m |           0.0010 |         231.1322 |           0.4948 |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |          -0.0059 |         225.0402 |           0.4947 |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |           0.0071 |         250.4311 |           0.4953 |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |          -0.0065 |         225.3355 |           0.4938 |
[32m[20221213 18:13:50 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 943.84
[32m[20221213 18:13:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.87
[32m[20221213 18:13:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.73
[32m[20221213 18:13:50 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221213 18:13:50 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221213 18:13:50 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 18:13:50 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:13:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |           0.0098 |         243.7094 |           0.5093 |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |          -0.0014 |         224.7416 |           0.5084 |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |          -0.0063 |         223.0387 |           0.5074 |
[32m[20221213 18:13:50 @agent_ppo2.py:185][0m |          -0.0081 |         222.0494 |           0.5075 |
[32m[20221213 18:13:51 @agent_ppo2.py:185][0m |          -0.0077 |         221.9527 |           0.5073 |
[32m[20221213 18:13:51 @agent_ppo2.py:185][0m |          -0.0024 |         223.5456 |           0.5077 |
[32m[20221213 18:13:51 @agent_ppo2.py:185][0m |          -0.0064 |         220.3777 |           0.5076 |
[32m[20221213 18:13:51 @agent_ppo2.py:185][0m |          -0.0023 |         226.5033 |           0.5079 |
[32m[20221213 18:13:51 @agent_ppo2.py:185][0m |          -0.0075 |         219.4560 |           0.5076 |
[32m[20221213 18:13:51 @agent_ppo2.py:185][0m |          -0.0095 |         219.1985 |           0.5078 |
[32m[20221213 18:13:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:13:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 852.16
[32m[20221213 18:13:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 882.70
[32m[20221213 18:13:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.00
[32m[20221213 18:13:51 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 18:13:51 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 18:13:51 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 18:13:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |           0.0004 |         227.7639 |           0.5081 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0049 |         225.3571 |           0.5077 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0078 |         224.6420 |           0.5075 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0044 |         224.1515 |           0.5079 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0082 |         223.8739 |           0.5074 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0091 |         223.3398 |           0.5082 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0090 |         223.0099 |           0.5080 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0075 |         222.8672 |           0.5080 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |           0.0020 |         251.0547 |           0.5081 |
[32m[20221213 18:13:52 @agent_ppo2.py:185][0m |          -0.0102 |         222.8243 |           0.5075 |
[32m[20221213 18:13:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 908.61
[32m[20221213 18:13:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.99
[32m[20221213 18:13:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.92
[32m[20221213 18:13:52 @agent_ppo2.py:143][0m Total time:       9.71 min
[32m[20221213 18:13:52 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221213 18:13:52 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 18:13:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |           0.0010 |         233.6606 |           0.5369 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0053 |         222.8886 |           0.5362 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0073 |         218.0795 |           0.5358 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0087 |         215.8229 |           0.5360 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0071 |         214.5724 |           0.5360 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0070 |         214.8858 |           0.5356 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0106 |         211.5961 |           0.5359 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0121 |         210.7756 |           0.5359 |
[32m[20221213 18:13:53 @agent_ppo2.py:185][0m |          -0.0071 |         209.7248 |           0.5361 |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |          -0.0088 |         211.4410 |           0.5359 |
[32m[20221213 18:13:54 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 902.52
[32m[20221213 18:13:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.98
[32m[20221213 18:13:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.19
[32m[20221213 18:13:54 @agent_ppo2.py:143][0m Total time:       9.73 min
[32m[20221213 18:13:54 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 18:13:54 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 18:13:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |          -0.0026 |         249.3605 |           0.5149 |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |          -0.0024 |         237.5951 |           0.5137 |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |          -0.0035 |         234.3656 |           0.5133 |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |           0.0028 |         251.0419 |           0.5129 |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |          -0.0130 |         232.1095 |           0.5129 |
[32m[20221213 18:13:54 @agent_ppo2.py:185][0m |          -0.0024 |         236.6776 |           0.5121 |
[32m[20221213 18:13:55 @agent_ppo2.py:185][0m |          -0.0086 |         230.5191 |           0.5122 |
[32m[20221213 18:13:55 @agent_ppo2.py:185][0m |          -0.0089 |         230.0071 |           0.5118 |
[32m[20221213 18:13:55 @agent_ppo2.py:185][0m |          -0.0097 |         229.3993 |           0.5123 |
[32m[20221213 18:13:55 @agent_ppo2.py:185][0m |          -0.0061 |         232.1387 |           0.5117 |
[32m[20221213 18:13:55 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.97
[32m[20221213 18:13:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.05
[32m[20221213 18:13:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.36
[32m[20221213 18:13:55 @agent_ppo2.py:143][0m Total time:       9.75 min
[32m[20221213 18:13:55 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221213 18:13:55 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 18:13:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:55 @agent_ppo2.py:185][0m |          -0.0015 |         239.1548 |           0.5171 |
[32m[20221213 18:13:55 @agent_ppo2.py:185][0m |          -0.0061 |         235.4124 |           0.5169 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0049 |         234.3011 |           0.5166 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0081 |         233.0427 |           0.5168 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0071 |         232.3451 |           0.5173 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0058 |         231.8069 |           0.5177 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0064 |         231.3646 |           0.5175 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0087 |         231.5254 |           0.5178 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0047 |         235.2978 |           0.5181 |
[32m[20221213 18:13:56 @agent_ppo2.py:185][0m |          -0.0066 |         230.9520 |           0.5179 |
[32m[20221213 18:13:56 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.80
[32m[20221213 18:13:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.43
[32m[20221213 18:13:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.91
[32m[20221213 18:13:56 @agent_ppo2.py:143][0m Total time:       9.77 min
[32m[20221213 18:13:56 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 18:13:56 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 18:13:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:13:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0013 |         236.5251 |           0.5135 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0036 |         233.9117 |           0.5125 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0058 |         232.9298 |           0.5126 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0064 |         232.3275 |           0.5119 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0053 |         231.6915 |           0.5123 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0072 |         231.6894 |           0.5119 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0031 |         232.2570 |           0.5119 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0073 |         231.2126 |           0.5116 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0008 |         234.9707 |           0.5121 |
[32m[20221213 18:13:57 @agent_ppo2.py:185][0m |          -0.0065 |         230.5298 |           0.5118 |
[32m[20221213 18:13:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:13:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.33
[32m[20221213 18:13:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.44
[32m[20221213 18:13:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 926.54
[32m[20221213 18:13:57 @agent_ppo2.py:143][0m Total time:       9.80 min
[32m[20221213 18:13:57 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221213 18:13:58 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 18:13:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:13:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0022 |         231.9901 |           0.5194 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0021 |         231.1815 |           0.5190 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0041 |         227.9906 |           0.5182 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0043 |         227.1870 |           0.5184 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0036 |         227.4317 |           0.5176 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0069 |         225.8794 |           0.5173 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0051 |         225.8194 |           0.5171 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0051 |         224.4727 |           0.5165 |
[32m[20221213 18:13:58 @agent_ppo2.py:185][0m |          -0.0068 |         224.2283 |           0.5164 |
[32m[20221213 18:13:59 @agent_ppo2.py:185][0m |          -0.0081 |         223.8230 |           0.5161 |
[32m[20221213 18:13:59 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:13:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 857.27
[32m[20221213 18:13:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 935.04
[32m[20221213 18:13:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.40
[32m[20221213 18:13:59 @agent_ppo2.py:143][0m Total time:       9.82 min
[32m[20221213 18:13:59 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 18:13:59 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 18:13:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:13:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:13:59 @agent_ppo2.py:185][0m |          -0.0016 |         213.4454 |           0.5178 |
[32m[20221213 18:13:59 @agent_ppo2.py:185][0m |          -0.0015 |         206.0355 |           0.5180 |
[32m[20221213 18:13:59 @agent_ppo2.py:185][0m |          -0.0053 |         204.3265 |           0.5179 |
[32m[20221213 18:13:59 @agent_ppo2.py:185][0m |          -0.0047 |         203.5544 |           0.5179 |
[32m[20221213 18:13:59 @agent_ppo2.py:185][0m |          -0.0062 |         202.4640 |           0.5183 |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |          -0.0082 |         201.4043 |           0.5180 |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |          -0.0118 |         201.3084 |           0.5185 |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |          -0.0066 |         201.2902 |           0.5181 |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |           0.0063 |         232.9701 |           0.5188 |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |          -0.0072 |         199.7404 |           0.5184 |
[32m[20221213 18:14:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:14:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.45
[32m[20221213 18:14:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.54
[32m[20221213 18:14:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.89
[32m[20221213 18:14:00 @agent_ppo2.py:143][0m Total time:       9.84 min
[32m[20221213 18:14:00 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221213 18:14:00 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 18:14:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |           0.0120 |         276.8970 |           0.5225 |
[32m[20221213 18:14:00 @agent_ppo2.py:185][0m |          -0.0069 |         238.4021 |           0.5221 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |           0.0031 |         246.9935 |           0.5222 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |           0.0040 |         258.8986 |           0.5230 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |          -0.0070 |         235.9240 |           0.5219 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |          -0.0076 |         234.9651 |           0.5225 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |          -0.0076 |         234.7313 |           0.5229 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |          -0.0079 |         234.3089 |           0.5226 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |          -0.0088 |         234.0475 |           0.5227 |
[32m[20221213 18:14:01 @agent_ppo2.py:185][0m |          -0.0047 |         236.4810 |           0.5227 |
[32m[20221213 18:14:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:14:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 948.10
[32m[20221213 18:14:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.21
[32m[20221213 18:14:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 945.67
[32m[20221213 18:14:01 @agent_ppo2.py:143][0m Total time:       9.86 min
[32m[20221213 18:14:01 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 18:14:01 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 18:14:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0006 |         239.3828 |           0.5203 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0044 |         235.6385 |           0.5191 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0043 |         234.9649 |           0.5187 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |           0.0002 |         243.4721 |           0.5181 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0047 |         233.7794 |           0.5175 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0049 |         233.2278 |           0.5171 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0050 |         232.9419 |           0.5171 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0064 |         232.7516 |           0.5164 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0074 |         232.2113 |           0.5162 |
[32m[20221213 18:14:02 @agent_ppo2.py:185][0m |          -0.0065 |         232.2343 |           0.5159 |
[32m[20221213 18:14:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:14:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.48
[32m[20221213 18:14:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.15
[32m[20221213 18:14:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.28
[32m[20221213 18:14:03 @agent_ppo2.py:143][0m Total time:       9.88 min
[32m[20221213 18:14:03 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221213 18:14:03 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 18:14:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |          -0.0022 |         236.6578 |           0.5158 |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |          -0.0041 |         232.6063 |           0.5154 |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |          -0.0049 |         229.1748 |           0.5150 |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |          -0.0063 |         224.8138 |           0.5151 |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |           0.0023 |         236.6109 |           0.5146 |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |          -0.0035 |         223.3970 |           0.5144 |
[32m[20221213 18:14:03 @agent_ppo2.py:185][0m |          -0.0072 |         220.8303 |           0.5143 |
[32m[20221213 18:14:04 @agent_ppo2.py:185][0m |           0.0018 |         239.0807 |           0.5147 |
[32m[20221213 18:14:04 @agent_ppo2.py:185][0m |          -0.0075 |         220.5646 |           0.5137 |
[32m[20221213 18:14:04 @agent_ppo2.py:185][0m |          -0.0065 |         219.3773 |           0.5141 |
[32m[20221213 18:14:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:14:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 884.13
[32m[20221213 18:14:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.11
[32m[20221213 18:14:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.52
[32m[20221213 18:14:04 @agent_ppo2.py:143][0m Total time:       9.90 min
[32m[20221213 18:14:04 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 18:14:04 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 18:14:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:04 @agent_ppo2.py:185][0m |          -0.0037 |         220.1255 |           0.5088 |
[32m[20221213 18:14:04 @agent_ppo2.py:185][0m |          -0.0001 |         220.3884 |           0.5082 |
[32m[20221213 18:14:04 @agent_ppo2.py:185][0m |          -0.0034 |         216.5856 |           0.5087 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |          -0.0051 |         215.9584 |           0.5100 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |          -0.0048 |         215.7847 |           0.5094 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |          -0.0081 |         214.1886 |           0.5097 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |          -0.0071 |         213.9349 |           0.5096 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |           0.0049 |         247.4517 |           0.5102 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |          -0.0075 |         213.2468 |           0.5099 |
[32m[20221213 18:14:05 @agent_ppo2.py:185][0m |          -0.0083 |         213.0023 |           0.5104 |
[32m[20221213 18:14:05 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:14:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 884.29
[32m[20221213 18:14:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.94
[32m[20221213 18:14:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.88
[32m[20221213 18:14:06 @agent_ppo2.py:143][0m Total time:       9.93 min
[32m[20221213 18:14:06 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221213 18:14:06 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 18:14:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:14:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |           0.0023 |         242.1633 |           0.5039 |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |          -0.0013 |         239.2245 |           0.5029 |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |           0.0012 |         239.4855 |           0.5031 |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |          -0.0024 |         238.7631 |           0.5032 |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |          -0.0034 |         238.0983 |           0.5027 |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |          -0.0033 |         238.0075 |           0.5027 |
[32m[20221213 18:14:06 @agent_ppo2.py:185][0m |           0.0105 |         259.7187 |           0.5022 |
[32m[20221213 18:14:07 @agent_ppo2.py:185][0m |          -0.0038 |         237.7628 |           0.5023 |
[32m[20221213 18:14:07 @agent_ppo2.py:185][0m |          -0.0059 |         237.5954 |           0.5012 |
[32m[20221213 18:14:07 @agent_ppo2.py:185][0m |          -0.0045 |         237.3838 |           0.5019 |
[32m[20221213 18:14:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:14:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 971.76
[32m[20221213 18:14:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.53
[32m[20221213 18:14:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.13
[32m[20221213 18:14:07 @agent_ppo2.py:143][0m Total time:       9.95 min
[32m[20221213 18:14:07 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 18:14:07 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 18:14:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:07 @agent_ppo2.py:185][0m |           0.0087 |         262.9925 |           0.5196 |
[32m[20221213 18:14:07 @agent_ppo2.py:185][0m |          -0.0047 |         244.0824 |           0.5186 |
[32m[20221213 18:14:07 @agent_ppo2.py:185][0m |          -0.0047 |         242.5780 |           0.5182 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |          -0.0068 |         241.3654 |           0.5179 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |           0.0027 |         260.7478 |           0.5181 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |          -0.0090 |         240.4573 |           0.5180 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |          -0.0073 |         239.9182 |           0.5184 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |          -0.0075 |         239.2172 |           0.5189 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |          -0.0043 |         239.3739 |           0.5186 |
[32m[20221213 18:14:08 @agent_ppo2.py:185][0m |          -0.0073 |         238.2285 |           0.5186 |
[32m[20221213 18:14:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:14:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.89
[32m[20221213 18:14:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.18
[32m[20221213 18:14:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.48
[32m[20221213 18:14:08 @agent_ppo2.py:143][0m Total time:       9.97 min
[32m[20221213 18:14:08 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221213 18:14:08 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 18:14:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:14:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |           0.0141 |         277.4975 |           0.5182 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0039 |         240.2408 |           0.5183 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0046 |         239.7299 |           0.5178 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0051 |         237.5917 |           0.5186 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |           0.0006 |         245.6823 |           0.5181 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0053 |         236.0007 |           0.5179 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0057 |         235.6381 |           0.5184 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0045 |         235.2236 |           0.5188 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0065 |         234.8758 |           0.5185 |
[32m[20221213 18:14:09 @agent_ppo2.py:185][0m |          -0.0067 |         234.7431 |           0.5188 |
[32m[20221213 18:14:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:14:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 964.07
[32m[20221213 18:14:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.53
[32m[20221213 18:14:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.64
[32m[20221213 18:14:10 @agent_ppo2.py:143][0m Total time:      10.00 min
[32m[20221213 18:14:10 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 18:14:10 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 18:14:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |          -0.0003 |         248.2978 |           0.5279 |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |          -0.0035 |         243.6491 |           0.5280 |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |          -0.0023 |         243.3567 |           0.5281 |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |          -0.0056 |         241.1591 |           0.5284 |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |           0.0011 |         249.2203 |           0.5281 |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |          -0.0073 |         239.9351 |           0.5278 |
[32m[20221213 18:14:10 @agent_ppo2.py:185][0m |           0.0045 |         258.0277 |           0.5285 |
[32m[20221213 18:14:11 @agent_ppo2.py:185][0m |          -0.0083 |         238.7409 |           0.5283 |
[32m[20221213 18:14:11 @agent_ppo2.py:185][0m |          -0.0000 |         249.4306 |           0.5286 |
[32m[20221213 18:14:11 @agent_ppo2.py:185][0m |          -0.0074 |         238.3440 |           0.5287 |
[32m[20221213 18:14:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:14:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.14
[32m[20221213 18:14:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 929.35
[32m[20221213 18:14:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.09
[32m[20221213 18:14:11 @agent_ppo2.py:143][0m Total time:      10.02 min
[32m[20221213 18:14:11 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221213 18:14:11 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 18:14:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:11 @agent_ppo2.py:185][0m |          -0.0039 |         232.9085 |           0.5170 |
[32m[20221213 18:14:11 @agent_ppo2.py:185][0m |          -0.0052 |         227.1968 |           0.5168 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0047 |         225.3629 |           0.5166 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0026 |         224.4412 |           0.5165 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |           0.0100 |         257.3371 |           0.5163 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0057 |         221.5143 |           0.5171 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0081 |         221.2267 |           0.5171 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0048 |         222.5628 |           0.5165 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0033 |         224.4516 |           0.5163 |
[32m[20221213 18:14:12 @agent_ppo2.py:185][0m |          -0.0090 |         220.6454 |           0.5165 |
[32m[20221213 18:14:12 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:14:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 961.26
[32m[20221213 18:14:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.29
[32m[20221213 18:14:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 880.27
[32m[20221213 18:14:12 @agent_ppo2.py:143][0m Total time:      10.04 min
[32m[20221213 18:14:12 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 18:14:12 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 18:14:13 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:14:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0025 |         243.4576 |           0.5131 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0018 |         240.3256 |           0.5134 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0074 |         237.4587 |           0.5136 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0074 |         235.5538 |           0.5139 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0098 |         234.2889 |           0.5142 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0036 |         238.8212 |           0.5142 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0095 |         233.2904 |           0.5152 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0088 |         233.4828 |           0.5151 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0097 |         232.6742 |           0.5158 |
[32m[20221213 18:14:13 @agent_ppo2.py:185][0m |          -0.0097 |         232.1593 |           0.5157 |
[32m[20221213 18:14:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:14:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.98
[32m[20221213 18:14:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.54
[32m[20221213 18:14:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.62
[32m[20221213 18:14:14 @agent_ppo2.py:143][0m Total time:      10.06 min
[32m[20221213 18:14:14 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221213 18:14:14 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 18:14:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:14 @agent_ppo2.py:185][0m |           0.0059 |         251.7551 |           0.5390 |
[32m[20221213 18:14:14 @agent_ppo2.py:185][0m |          -0.0051 |         224.7590 |           0.5364 |
[32m[20221213 18:14:14 @agent_ppo2.py:185][0m |          -0.0079 |         213.0882 |           0.5375 |
[32m[20221213 18:14:14 @agent_ppo2.py:185][0m |          -0.0077 |         206.7466 |           0.5376 |
[32m[20221213 18:14:14 @agent_ppo2.py:185][0m |          -0.0099 |         203.8128 |           0.5367 |
[32m[20221213 18:14:14 @agent_ppo2.py:185][0m |          -0.0067 |         204.7763 |           0.5369 |
[32m[20221213 18:14:15 @agent_ppo2.py:185][0m |          -0.0116 |         201.6285 |           0.5362 |
[32m[20221213 18:14:15 @agent_ppo2.py:185][0m |          -0.0097 |         199.8231 |           0.5363 |
[32m[20221213 18:14:15 @agent_ppo2.py:185][0m |          -0.0123 |         199.9839 |           0.5359 |
[32m[20221213 18:14:15 @agent_ppo2.py:185][0m |          -0.0126 |         198.8237 |           0.5363 |
[32m[20221213 18:14:15 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 18:14:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 853.05
[32m[20221213 18:14:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 924.50
[32m[20221213 18:14:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 908.43
[32m[20221213 18:14:15 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221213 18:14:15 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 18:14:15 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 18:14:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:15 @agent_ppo2.py:185][0m |          -0.0041 |         263.1884 |           0.5266 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0078 |         256.8936 |           0.5256 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0092 |         254.8482 |           0.5254 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0076 |         253.3669 |           0.5262 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0074 |         252.7214 |           0.5257 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0099 |         251.9479 |           0.5259 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0107 |         251.3141 |           0.5261 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0096 |         250.7350 |           0.5263 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0094 |         250.3192 |           0.5261 |
[32m[20221213 18:14:16 @agent_ppo2.py:185][0m |          -0.0104 |         250.1130 |           0.5261 |
[32m[20221213 18:14:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:14:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.14
[32m[20221213 18:14:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.75
[32m[20221213 18:14:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.34
[32m[20221213 18:14:16 @agent_ppo2.py:143][0m Total time:      10.11 min
[32m[20221213 18:14:16 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221213 18:14:16 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 18:14:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0011 |         238.1029 |           0.5222 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0046 |         232.9654 |           0.5220 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0073 |         232.4516 |           0.5216 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0082 |         231.1700 |           0.5218 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0082 |         230.6348 |           0.5217 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0075 |         231.0435 |           0.5217 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0092 |         230.0368 |           0.5216 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0050 |         230.9064 |           0.5217 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0082 |         229.5319 |           0.5214 |
[32m[20221213 18:14:17 @agent_ppo2.py:185][0m |          -0.0099 |         230.1792 |           0.5223 |
[32m[20221213 18:14:17 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 933.18
[32m[20221213 18:14:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.42
[32m[20221213 18:14:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.86
[32m[20221213 18:14:18 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221213 18:14:18 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 18:14:18 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 18:14:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |           0.0103 |         258.0296 |           0.5249 |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |          -0.0043 |         245.5878 |           0.5244 |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |          -0.0063 |         243.5143 |           0.5242 |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |          -0.0063 |         242.3298 |           0.5243 |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |          -0.0062 |         241.2459 |           0.5239 |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |          -0.0049 |         243.8207 |           0.5247 |
[32m[20221213 18:14:18 @agent_ppo2.py:185][0m |          -0.0079 |         239.6444 |           0.5244 |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |          -0.0058 |         239.1582 |           0.5247 |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |          -0.0133 |         239.0524 |           0.5242 |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |          -0.0095 |         238.2028 |           0.5243 |
[32m[20221213 18:14:19 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:14:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.92
[32m[20221213 18:14:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.73
[32m[20221213 18:14:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.01
[32m[20221213 18:14:19 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221213 18:14:19 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221213 18:14:19 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 18:14:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |           0.0033 |         255.2722 |           0.5510 |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |          -0.0024 |         248.2280 |           0.5505 |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |          -0.0068 |         246.4336 |           0.5493 |
[32m[20221213 18:14:19 @agent_ppo2.py:185][0m |          -0.0068 |         244.0519 |           0.5494 |
[32m[20221213 18:14:20 @agent_ppo2.py:185][0m |          -0.0057 |         242.5242 |           0.5483 |
[32m[20221213 18:14:20 @agent_ppo2.py:185][0m |          -0.0057 |         241.4109 |           0.5474 |
[32m[20221213 18:14:20 @agent_ppo2.py:185][0m |          -0.0091 |         240.0288 |           0.5474 |
[32m[20221213 18:14:20 @agent_ppo2.py:185][0m |          -0.0094 |         239.5833 |           0.5476 |
[32m[20221213 18:14:20 @agent_ppo2.py:185][0m |          -0.0101 |         238.5085 |           0.5472 |
[32m[20221213 18:14:20 @agent_ppo2.py:185][0m |          -0.0112 |         237.3037 |           0.5466 |
[32m[20221213 18:14:20 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 958.31
[32m[20221213 18:14:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.10
[32m[20221213 18:14:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.78
[32m[20221213 18:14:20 @agent_ppo2.py:143][0m Total time:      10.17 min
[32m[20221213 18:14:20 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 18:14:20 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 18:14:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0051 |         207.4397 |           0.5248 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0060 |         197.9040 |           0.5241 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0101 |         191.6953 |           0.5236 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0126 |         188.5413 |           0.5230 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0159 |         186.3385 |           0.5231 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0135 |         184.4997 |           0.5228 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0129 |         183.0350 |           0.5229 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0162 |         181.8153 |           0.5230 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0166 |         180.4401 |           0.5235 |
[32m[20221213 18:14:21 @agent_ppo2.py:185][0m |          -0.0167 |         179.8667 |           0.5231 |
[32m[20221213 18:14:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.22
[32m[20221213 18:14:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.22
[32m[20221213 18:14:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.14
[32m[20221213 18:14:21 @agent_ppo2.py:143][0m Total time:      10.19 min
[32m[20221213 18:14:21 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221213 18:14:21 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 18:14:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |           0.0055 |         259.9232 |           0.5337 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0021 |         248.2928 |           0.5330 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |           0.0040 |         256.7210 |           0.5330 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0039 |         244.6067 |           0.5326 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0041 |         243.6456 |           0.5326 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0077 |         242.9072 |           0.5324 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0092 |         242.2980 |           0.5326 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0097 |         241.9615 |           0.5327 |
[32m[20221213 18:14:22 @agent_ppo2.py:185][0m |          -0.0091 |         241.6054 |           0.5328 |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0082 |         240.5965 |           0.5325 |
[32m[20221213 18:14:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:14:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.41
[32m[20221213 18:14:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.23
[32m[20221213 18:14:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 910.68
[32m[20221213 18:14:23 @agent_ppo2.py:143][0m Total time:      10.22 min
[32m[20221213 18:14:23 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 18:14:23 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 18:14:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0010 |         251.4737 |           0.5433 |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0010 |         248.9171 |           0.5420 |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0066 |         243.2818 |           0.5408 |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0065 |         241.2481 |           0.5409 |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0079 |         240.6525 |           0.5408 |
[32m[20221213 18:14:23 @agent_ppo2.py:185][0m |          -0.0055 |         238.7702 |           0.5410 |
[32m[20221213 18:14:24 @agent_ppo2.py:185][0m |          -0.0066 |         237.9857 |           0.5405 |
[32m[20221213 18:14:24 @agent_ppo2.py:185][0m |           0.0018 |         243.8674 |           0.5406 |
[32m[20221213 18:14:24 @agent_ppo2.py:185][0m |          -0.0119 |         234.7621 |           0.5399 |
[32m[20221213 18:14:24 @agent_ppo2.py:185][0m |          -0.0131 |         233.8850 |           0.5404 |
[32m[20221213 18:14:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:14:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.81
[32m[20221213 18:14:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.01
[32m[20221213 18:14:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.90
[32m[20221213 18:14:24 @agent_ppo2.py:143][0m Total time:      10.24 min
[32m[20221213 18:14:24 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221213 18:14:24 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 18:14:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:24 @agent_ppo2.py:185][0m |           0.0093 |         271.5175 |           0.5201 |
[32m[20221213 18:14:24 @agent_ppo2.py:185][0m |          -0.0055 |         237.4149 |           0.5187 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |           0.0003 |         244.3964 |           0.5190 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |          -0.0067 |         234.9199 |           0.5190 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |           0.0020 |         251.1962 |           0.5188 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |          -0.0075 |         234.0436 |           0.5186 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |          -0.0081 |         233.4358 |           0.5181 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |          -0.0053 |         233.5948 |           0.5183 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |          -0.0088 |         232.4672 |           0.5177 |
[32m[20221213 18:14:25 @agent_ppo2.py:185][0m |          -0.0102 |         232.1259 |           0.5180 |
[32m[20221213 18:14:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:14:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 909.70
[32m[20221213 18:14:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.69
[32m[20221213 18:14:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.19
[32m[20221213 18:14:25 @agent_ppo2.py:143][0m Total time:      10.26 min
[32m[20221213 18:14:25 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 18:14:25 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 18:14:25 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:14:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0028 |         241.5033 |           0.5249 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0046 |         238.6892 |           0.5244 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |           0.0047 |         271.9932 |           0.5243 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0061 |         237.5669 |           0.5242 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0095 |         235.0487 |           0.5237 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0114 |         234.3037 |           0.5235 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0080 |         234.5823 |           0.5235 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0106 |         233.2053 |           0.5234 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |          -0.0112 |         232.9314 |           0.5231 |
[32m[20221213 18:14:26 @agent_ppo2.py:185][0m |           0.0124 |         278.3453 |           0.5232 |
[32m[20221213 18:14:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:14:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 906.40
[32m[20221213 18:14:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.80
[32m[20221213 18:14:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 973.59
[32m[20221213 18:14:27 @agent_ppo2.py:143][0m Total time:      10.28 min
[32m[20221213 18:14:27 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221213 18:14:27 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 18:14:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0053 |         236.6431 |           0.5149 |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0073 |         231.9054 |           0.5141 |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0064 |         230.3800 |           0.5145 |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0070 |         230.1359 |           0.5144 |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0103 |         228.3357 |           0.5147 |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0069 |         227.8539 |           0.5147 |
[32m[20221213 18:14:27 @agent_ppo2.py:185][0m |          -0.0098 |         227.1958 |           0.5153 |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0108 |         226.3551 |           0.5153 |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0106 |         225.9160 |           0.5161 |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0112 |         225.4949 |           0.5161 |
[32m[20221213 18:14:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:14:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 876.75
[32m[20221213 18:14:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.18
[32m[20221213 18:14:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 906.93
[32m[20221213 18:14:28 @agent_ppo2.py:143][0m Total time:      10.30 min
[32m[20221213 18:14:28 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 18:14:28 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 18:14:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0013 |         235.7608 |           0.5440 |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0035 |         233.5366 |           0.5448 |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0047 |         232.7580 |           0.5446 |
[32m[20221213 18:14:28 @agent_ppo2.py:185][0m |          -0.0060 |         232.0387 |           0.5441 |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |          -0.0063 |         231.8139 |           0.5436 |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |          -0.0045 |         231.3469 |           0.5437 |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |          -0.0058 |         231.5573 |           0.5439 |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |          -0.0073 |         230.9620 |           0.5440 |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |          -0.0083 |         231.1336 |           0.5436 |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |          -0.0041 |         233.0706 |           0.5435 |
[32m[20221213 18:14:29 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 915.53
[32m[20221213 18:14:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.26
[32m[20221213 18:14:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.31
[32m[20221213 18:14:29 @agent_ppo2.py:143][0m Total time:      10.32 min
[32m[20221213 18:14:29 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221213 18:14:29 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 18:14:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:29 @agent_ppo2.py:185][0m |           0.0027 |         250.3689 |           0.5385 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0043 |         245.1925 |           0.5383 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0036 |         243.9489 |           0.5384 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0060 |         242.9810 |           0.5375 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0052 |         242.0473 |           0.5383 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0022 |         244.0539 |           0.5380 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0083 |         241.0257 |           0.5378 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0093 |         240.5538 |           0.5376 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0095 |         239.9961 |           0.5375 |
[32m[20221213 18:14:30 @agent_ppo2.py:185][0m |          -0.0086 |         240.0269 |           0.5372 |
[32m[20221213 18:14:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 884.78
[32m[20221213 18:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.34
[32m[20221213 18:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 953.11
[32m[20221213 18:14:30 @agent_ppo2.py:143][0m Total time:      10.34 min
[32m[20221213 18:14:30 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 18:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 18:14:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |          -0.0007 |         246.9791 |           0.5350 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |           0.0094 |         271.0507 |           0.5341 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |          -0.0058 |         239.1940 |           0.5335 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |          -0.0031 |         239.3897 |           0.5336 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |          -0.0073 |         237.3246 |           0.5329 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |           0.0015 |         247.4917 |           0.5328 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |          -0.0084 |         236.7644 |           0.5326 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |           0.0011 |         250.2151 |           0.5327 |
[32m[20221213 18:14:31 @agent_ppo2.py:185][0m |          -0.0099 |         235.5484 |           0.5321 |
[32m[20221213 18:14:32 @agent_ppo2.py:185][0m |          -0.0096 |         235.9033 |           0.5322 |
[32m[20221213 18:14:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:14:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 879.74
[32m[20221213 18:14:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 933.02
[32m[20221213 18:14:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.22
[32m[20221213 18:14:32 @agent_ppo2.py:143][0m Total time:      10.37 min
[32m[20221213 18:14:32 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221213 18:14:32 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 18:14:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:32 @agent_ppo2.py:185][0m |          -0.0025 |         248.3853 |           0.5324 |
[32m[20221213 18:14:32 @agent_ppo2.py:185][0m |          -0.0101 |         241.0032 |           0.5317 |
[32m[20221213 18:14:32 @agent_ppo2.py:185][0m |          -0.0067 |         238.2709 |           0.5305 |
[32m[20221213 18:14:32 @agent_ppo2.py:185][0m |          -0.0084 |         237.5072 |           0.5302 |
[32m[20221213 18:14:32 @agent_ppo2.py:185][0m |          -0.0090 |         236.2797 |           0.5308 |
[32m[20221213 18:14:33 @agent_ppo2.py:185][0m |          -0.0103 |         235.5509 |           0.5307 |
[32m[20221213 18:14:33 @agent_ppo2.py:185][0m |          -0.0094 |         234.9587 |           0.5303 |
[32m[20221213 18:14:33 @agent_ppo2.py:185][0m |           0.0011 |         247.9571 |           0.5302 |
[32m[20221213 18:14:33 @agent_ppo2.py:185][0m |          -0.0103 |         234.0575 |           0.5303 |
[32m[20221213 18:14:33 @agent_ppo2.py:185][0m |          -0.0118 |         233.5752 |           0.5303 |
[32m[20221213 18:14:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:14:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 906.28
[32m[20221213 18:14:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.27
[32m[20221213 18:14:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 997.20
[32m[20221213 18:14:33 @agent_ppo2.py:143][0m Total time:      10.39 min
[32m[20221213 18:14:33 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 18:14:33 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 18:14:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:33 @agent_ppo2.py:185][0m |          -0.0035 |         261.7021 |           0.5357 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |           0.0005 |         262.6095 |           0.5341 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0076 |         252.8033 |           0.5341 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0053 |         251.6629 |           0.5338 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0098 |         250.8357 |           0.5328 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0070 |         250.7224 |           0.5330 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0099 |         249.6744 |           0.5326 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0067 |         250.2869 |           0.5324 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0092 |         248.5566 |           0.5317 |
[32m[20221213 18:14:34 @agent_ppo2.py:185][0m |          -0.0062 |         248.7220 |           0.5317 |
[32m[20221213 18:14:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:14:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.29
[32m[20221213 18:14:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.51
[32m[20221213 18:14:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 912.96
[32m[20221213 18:14:34 @agent_ppo2.py:143][0m Total time:      10.41 min
[32m[20221213 18:14:34 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221213 18:14:34 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 18:14:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |           0.0030 |         256.1138 |           0.5377 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |           0.0024 |         257.6293 |           0.5373 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0049 |         248.7117 |           0.5376 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0035 |         247.8007 |           0.5371 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0057 |         246.9130 |           0.5369 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |           0.0054 |         274.0919 |           0.5371 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0067 |         246.1271 |           0.5363 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0071 |         245.5550 |           0.5368 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0073 |         245.2493 |           0.5368 |
[32m[20221213 18:14:35 @agent_ppo2.py:185][0m |          -0.0039 |         247.4285 |           0.5366 |
[32m[20221213 18:14:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 944.56
[32m[20221213 18:14:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.20
[32m[20221213 18:14:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.81
[32m[20221213 18:14:36 @agent_ppo2.py:143][0m Total time:      10.43 min
[32m[20221213 18:14:36 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 18:14:36 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 18:14:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |           0.0027 |         256.0392 |           0.5364 |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |          -0.0047 |         249.8835 |           0.5348 |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |          -0.0051 |         248.2278 |           0.5351 |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |          -0.0053 |         247.7976 |           0.5349 |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |          -0.0027 |         247.7277 |           0.5344 |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |          -0.0041 |         247.1169 |           0.5343 |
[32m[20221213 18:14:36 @agent_ppo2.py:185][0m |          -0.0076 |         246.6372 |           0.5347 |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |          -0.0067 |         246.6415 |           0.5343 |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |          -0.0077 |         246.0936 |           0.5350 |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |           0.0088 |         270.8732 |           0.5350 |
[32m[20221213 18:14:37 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.09
[32m[20221213 18:14:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.55
[32m[20221213 18:14:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 885.71
[32m[20221213 18:14:37 @agent_ppo2.py:143][0m Total time:      10.45 min
[32m[20221213 18:14:37 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221213 18:14:37 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 18:14:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |          -0.0028 |         244.8017 |           0.5204 |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |          -0.0052 |         244.4314 |           0.5197 |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |          -0.0054 |         243.6910 |           0.5199 |
[32m[20221213 18:14:37 @agent_ppo2.py:185][0m |          -0.0059 |         243.1172 |           0.5199 |
[32m[20221213 18:14:38 @agent_ppo2.py:185][0m |          -0.0073 |         243.0340 |           0.5198 |
[32m[20221213 18:14:38 @agent_ppo2.py:185][0m |          -0.0070 |         242.5615 |           0.5204 |
[32m[20221213 18:14:38 @agent_ppo2.py:185][0m |           0.0062 |         262.7885 |           0.5212 |
[32m[20221213 18:14:38 @agent_ppo2.py:185][0m |          -0.0062 |         242.7546 |           0.5206 |
[32m[20221213 18:14:38 @agent_ppo2.py:185][0m |          -0.0066 |         242.3849 |           0.5209 |
[32m[20221213 18:14:38 @agent_ppo2.py:185][0m |          -0.0070 |         242.2535 |           0.5209 |
[32m[20221213 18:14:38 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:14:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 979.24
[32m[20221213 18:14:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.16
[32m[20221213 18:14:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 973.18
[32m[20221213 18:14:38 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221213 18:14:38 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 18:14:38 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 18:14:38 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:14:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0014 |         250.2940 |           0.5156 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0035 |         246.4469 |           0.5153 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0052 |         245.3697 |           0.5143 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0047 |         244.8861 |           0.5146 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0068 |         244.2455 |           0.5144 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0070 |         244.0161 |           0.5141 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0085 |         243.8551 |           0.5143 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |           0.0023 |         257.2663 |           0.5143 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0085 |         243.3089 |           0.5148 |
[32m[20221213 18:14:39 @agent_ppo2.py:185][0m |          -0.0019 |         248.1218 |           0.5140 |
[32m[20221213 18:14:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 896.36
[32m[20221213 18:14:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.28
[32m[20221213 18:14:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.11
[32m[20221213 18:14:39 @agent_ppo2.py:143][0m Total time:      10.49 min
[32m[20221213 18:14:39 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221213 18:14:39 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 18:14:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0005 |         245.2427 |           0.5327 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0050 |         241.9753 |           0.5319 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |           0.0046 |         253.2800 |           0.5317 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0072 |         238.3118 |           0.5309 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0076 |         237.6065 |           0.5298 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0066 |         236.8822 |           0.5299 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0080 |         236.6381 |           0.5294 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0082 |         236.1126 |           0.5288 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0083 |         235.9712 |           0.5284 |
[32m[20221213 18:14:40 @agent_ppo2.py:185][0m |          -0.0087 |         235.9856 |           0.5279 |
[32m[20221213 18:14:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 907.30
[32m[20221213 18:14:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.90
[32m[20221213 18:14:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 998.87
[32m[20221213 18:14:41 @agent_ppo2.py:143][0m Total time:      10.52 min
[32m[20221213 18:14:41 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 18:14:41 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 18:14:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |          -0.0018 |         244.6232 |           0.5463 |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |          -0.0051 |         242.2948 |           0.5456 |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |           0.0076 |         276.7940 |           0.5450 |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |          -0.0057 |         241.0117 |           0.5441 |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |          -0.0070 |         240.5329 |           0.5437 |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |          -0.0084 |         240.6004 |           0.5433 |
[32m[20221213 18:14:41 @agent_ppo2.py:185][0m |          -0.0075 |         240.1983 |           0.5426 |
[32m[20221213 18:14:42 @agent_ppo2.py:185][0m |          -0.0103 |         239.9908 |           0.5421 |
[32m[20221213 18:14:42 @agent_ppo2.py:185][0m |          -0.0088 |         239.5703 |           0.5417 |
[32m[20221213 18:14:42 @agent_ppo2.py:185][0m |           0.0013 |         257.1093 |           0.5413 |
[32m[20221213 18:14:42 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:14:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 971.30
[32m[20221213 18:14:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 994.97
[32m[20221213 18:14:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.95
[32m[20221213 18:14:42 @agent_ppo2.py:143][0m Total time:      10.54 min
[32m[20221213 18:14:42 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221213 18:14:42 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 18:14:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:42 @agent_ppo2.py:185][0m |          -0.0033 |         242.6894 |           0.5041 |
[32m[20221213 18:14:42 @agent_ppo2.py:185][0m |          -0.0067 |         238.7923 |           0.5014 |
[32m[20221213 18:14:42 @agent_ppo2.py:185][0m |          -0.0089 |         237.1281 |           0.5024 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0082 |         236.4939 |           0.5015 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0075 |         236.2590 |           0.5024 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0083 |         236.4737 |           0.5014 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0096 |         235.9034 |           0.5017 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0096 |         235.6084 |           0.5018 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0021 |         247.9630 |           0.5014 |
[32m[20221213 18:14:43 @agent_ppo2.py:185][0m |          -0.0090 |         236.0248 |           0.5011 |
[32m[20221213 18:14:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:14:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.78
[32m[20221213 18:14:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.63
[32m[20221213 18:14:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.72
[32m[20221213 18:14:43 @agent_ppo2.py:143][0m Total time:      10.56 min
[32m[20221213 18:14:43 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 18:14:43 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 18:14:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0031 |         236.9178 |           0.5190 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |           0.0064 |         236.6827 |           0.5192 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0047 |         228.1544 |           0.5194 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0063 |         227.9675 |           0.5192 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0060 |         227.2436 |           0.5196 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0046 |         227.3510 |           0.5198 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0075 |         226.9327 |           0.5200 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0076 |         226.3510 |           0.5203 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0057 |         226.3221 |           0.5202 |
[32m[20221213 18:14:44 @agent_ppo2.py:185][0m |          -0.0078 |         226.2009 |           0.5210 |
[32m[20221213 18:14:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:14:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 964.23
[32m[20221213 18:14:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.33
[32m[20221213 18:14:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.69
[32m[20221213 18:14:44 @agent_ppo2.py:143][0m Total time:      10.58 min
[32m[20221213 18:14:44 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221213 18:14:44 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 18:14:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0039 |         257.0359 |           0.5197 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0036 |         253.9529 |           0.5186 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0078 |         252.4827 |           0.5199 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0081 |         251.9423 |           0.5189 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0070 |         251.4415 |           0.5192 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |           0.0069 |         281.4346 |           0.5199 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |           0.0019 |         260.4462 |           0.5194 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0089 |         251.1096 |           0.5195 |
[32m[20221213 18:14:45 @agent_ppo2.py:185][0m |          -0.0089 |         250.2338 |           0.5201 |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0114 |         249.6472 |           0.5191 |
[32m[20221213 18:14:46 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:14:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.12
[32m[20221213 18:14:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.22
[32m[20221213 18:14:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.88
[32m[20221213 18:14:46 @agent_ppo2.py:143][0m Total time:      10.60 min
[32m[20221213 18:14:46 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 18:14:46 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 18:14:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0026 |         260.9942 |           0.5192 |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0010 |         258.1335 |           0.5188 |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0032 |         259.1283 |           0.5194 |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0078 |         254.7074 |           0.5195 |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0073 |         254.3987 |           0.5202 |
[32m[20221213 18:14:46 @agent_ppo2.py:185][0m |          -0.0083 |         254.0944 |           0.5202 |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |           0.0010 |         270.0256 |           0.5197 |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |          -0.0061 |         253.6054 |           0.5197 |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |          -0.0068 |         253.3286 |           0.5210 |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |          -0.0073 |         253.0942 |           0.5205 |
[32m[20221213 18:14:47 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:14:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 902.72
[32m[20221213 18:14:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 951.46
[32m[20221213 18:14:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.85
[32m[20221213 18:14:47 @agent_ppo2.py:143][0m Total time:      10.62 min
[32m[20221213 18:14:47 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221213 18:14:47 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 18:14:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:14:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |           0.0081 |         264.4345 |           0.5177 |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |          -0.0041 |         250.6406 |           0.5167 |
[32m[20221213 18:14:47 @agent_ppo2.py:185][0m |          -0.0049 |         250.1066 |           0.5161 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |          -0.0054 |         249.7976 |           0.5155 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |          -0.0022 |         251.2456 |           0.5156 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |          -0.0055 |         249.3852 |           0.5154 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |          -0.0067 |         249.3100 |           0.5148 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |           0.0054 |         272.4795 |           0.5144 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |          -0.0074 |         249.1257 |           0.5136 |
[32m[20221213 18:14:48 @agent_ppo2.py:185][0m |          -0.0076 |         249.1469 |           0.5138 |
[32m[20221213 18:14:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:14:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.30
[32m[20221213 18:14:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.03
[32m[20221213 18:14:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.92
[32m[20221213 18:14:48 @agent_ppo2.py:143][0m Total time:      10.64 min
[32m[20221213 18:14:48 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 18:14:48 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 18:14:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:14:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0018 |         245.9669 |           0.5195 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |           0.0002 |         237.9219 |           0.5185 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0082 |         236.2843 |           0.5174 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0074 |         233.8245 |           0.5171 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0080 |         232.0970 |           0.5163 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0010 |         244.5172 |           0.5157 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0099 |         230.4328 |           0.5147 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0112 |         229.9074 |           0.5152 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0118 |         229.0558 |           0.5147 |
[32m[20221213 18:14:49 @agent_ppo2.py:185][0m |          -0.0092 |         228.4862 |           0.5144 |
[32m[20221213 18:14:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 854.09
[32m[20221213 18:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.12
[32m[20221213 18:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.43
[32m[20221213 18:14:50 @agent_ppo2.py:143][0m Total time:      10.66 min
[32m[20221213 18:14:50 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221213 18:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 18:14:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:14:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0011 |         250.4704 |           0.5098 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0033 |         249.6363 |           0.5083 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0050 |         249.3304 |           0.5084 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0034 |         249.6537 |           0.5088 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |           0.0013 |         258.2309 |           0.5081 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0061 |         248.7761 |           0.5076 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0069 |         248.6671 |           0.5080 |
[32m[20221213 18:14:50 @agent_ppo2.py:185][0m |          -0.0020 |         250.2036 |           0.5074 |
[32m[20221213 18:14:51 @agent_ppo2.py:185][0m |          -0.0001 |         255.7211 |           0.5081 |
[32m[20221213 18:14:51 @agent_ppo2.py:185][0m |          -0.0069 |         248.2179 |           0.5077 |
[32m[20221213 18:14:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:14:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 996.07
[32m[20221213 18:14:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.58
[32m[20221213 18:14:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.02
[32m[20221213 18:14:51 @agent_ppo2.py:143][0m Total time:      10.68 min
[32m[20221213 18:14:51 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 18:14:51 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 18:14:51 @agent_ppo2.py:127][0m Sampling time: 0.31 s by 5 slaves
[32m[20221213 18:14:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:51 @agent_ppo2.py:185][0m |          -0.0032 |         253.5714 |           0.5211 |
[32m[20221213 18:14:51 @agent_ppo2.py:185][0m |           0.0073 |         287.1944 |           0.5201 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0075 |         249.8288 |           0.5196 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0101 |         248.7475 |           0.5203 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |           0.0002 |         273.7945 |           0.5203 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0092 |         247.1972 |           0.5201 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0104 |         246.5275 |           0.5200 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0073 |         248.3729 |           0.5201 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0063 |         250.5487 |           0.5208 |
[32m[20221213 18:14:52 @agent_ppo2.py:185][0m |          -0.0112 |         245.1418 |           0.5206 |
[32m[20221213 18:14:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:14:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.14
[32m[20221213 18:14:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.08
[32m[20221213 18:14:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 957.04
[32m[20221213 18:14:52 @agent_ppo2.py:143][0m Total time:      10.71 min
[32m[20221213 18:14:52 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221213 18:14:52 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 18:14:53 @agent_ppo2.py:127][0m Sampling time: 0.32 s by 5 slaves
[32m[20221213 18:14:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:53 @agent_ppo2.py:185][0m |          -0.0011 |         251.0248 |           0.5195 |
[32m[20221213 18:14:53 @agent_ppo2.py:185][0m |          -0.0041 |         249.6896 |           0.5188 |
[32m[20221213 18:14:53 @agent_ppo2.py:185][0m |          -0.0060 |         249.0941 |           0.5181 |
[32m[20221213 18:14:53 @agent_ppo2.py:185][0m |          -0.0063 |         248.7045 |           0.5189 |
[32m[20221213 18:14:54 @agent_ppo2.py:185][0m |          -0.0065 |         248.2955 |           0.5178 |
[32m[20221213 18:14:54 @agent_ppo2.py:185][0m |          -0.0070 |         247.9011 |           0.5183 |
[32m[20221213 18:14:54 @agent_ppo2.py:185][0m |          -0.0054 |         247.9217 |           0.5185 |
[32m[20221213 18:14:54 @agent_ppo2.py:185][0m |          -0.0076 |         247.2685 |           0.5182 |
[32m[20221213 18:14:54 @agent_ppo2.py:185][0m |          -0.0096 |         246.9431 |           0.5180 |
[32m[20221213 18:14:54 @agent_ppo2.py:185][0m |          -0.0076 |         246.7050 |           0.5180 |
[32m[20221213 18:14:54 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 18:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 923.56
[32m[20221213 18:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.56
[32m[20221213 18:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 963.74
[32m[20221213 18:14:54 @agent_ppo2.py:143][0m Total time:      10.74 min
[32m[20221213 18:14:54 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 18:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 18:14:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:14:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0023 |         250.7981 |           0.5187 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |           0.0042 |         261.8186 |           0.5183 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0058 |         247.5387 |           0.5182 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0044 |         248.8586 |           0.5181 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |           0.0063 |         282.1767 |           0.5183 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0089 |         246.3485 |           0.5175 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0057 |         245.8503 |           0.5175 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0096 |         245.7524 |           0.5178 |
[32m[20221213 18:14:55 @agent_ppo2.py:185][0m |          -0.0082 |         245.8312 |           0.5178 |
[32m[20221213 18:14:56 @agent_ppo2.py:185][0m |          -0.0034 |         247.5366 |           0.5175 |
[32m[20221213 18:14:56 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 18:14:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 853.48
[32m[20221213 18:14:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.19
[32m[20221213 18:14:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 901.36
[32m[20221213 18:14:56 @agent_ppo2.py:143][0m Total time:      10.77 min
[32m[20221213 18:14:56 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221213 18:14:56 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 18:14:56 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:14:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:56 @agent_ppo2.py:185][0m |          -0.0056 |         251.1136 |           0.5173 |
[32m[20221213 18:14:56 @agent_ppo2.py:185][0m |          -0.0070 |         248.7404 |           0.5160 |
[32m[20221213 18:14:56 @agent_ppo2.py:185][0m |          -0.0081 |         247.8391 |           0.5161 |
[32m[20221213 18:14:56 @agent_ppo2.py:185][0m |          -0.0084 |         247.6063 |           0.5156 |
[32m[20221213 18:14:57 @agent_ppo2.py:185][0m |          -0.0049 |         250.4690 |           0.5156 |
[32m[20221213 18:14:57 @agent_ppo2.py:185][0m |          -0.0096 |         246.7527 |           0.5157 |
[32m[20221213 18:14:57 @agent_ppo2.py:185][0m |          -0.0113 |         246.4228 |           0.5156 |
[32m[20221213 18:14:57 @agent_ppo2.py:185][0m |          -0.0094 |         245.8073 |           0.5155 |
[32m[20221213 18:14:57 @agent_ppo2.py:185][0m |          -0.0089 |         246.0107 |           0.5156 |
[32m[20221213 18:14:57 @agent_ppo2.py:185][0m |          -0.0109 |         245.5371 |           0.5158 |
[32m[20221213 18:14:57 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:14:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 882.17
[32m[20221213 18:14:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.60
[32m[20221213 18:14:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.53
[32m[20221213 18:14:57 @agent_ppo2.py:143][0m Total time:      10.79 min
[32m[20221213 18:14:57 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 18:14:57 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 18:14:58 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:14:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |           0.0196 |         287.8846 |           0.5352 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |          -0.0054 |         241.5554 |           0.5337 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |          -0.0061 |         240.8211 |           0.5341 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |          -0.0075 |         240.4398 |           0.5343 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |          -0.0048 |         240.2742 |           0.5347 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |          -0.0077 |         240.1174 |           0.5342 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |           0.0089 |         275.6241 |           0.5344 |
[32m[20221213 18:14:58 @agent_ppo2.py:185][0m |          -0.0080 |         239.7703 |           0.5341 |
[32m[20221213 18:14:59 @agent_ppo2.py:185][0m |          -0.0091 |         239.2794 |           0.5344 |
[32m[20221213 18:14:59 @agent_ppo2.py:185][0m |          -0.0077 |         239.1586 |           0.5340 |
[32m[20221213 18:14:59 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 18:14:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 917.91
[32m[20221213 18:14:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 980.95
[32m[20221213 18:14:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 957.61
[32m[20221213 18:14:59 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221213 18:14:59 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221213 18:14:59 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 18:14:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:14:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:14:59 @agent_ppo2.py:185][0m |          -0.0014 |         241.7696 |           0.5167 |
[32m[20221213 18:14:59 @agent_ppo2.py:185][0m |           0.0025 |         245.0797 |           0.5168 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0040 |         240.3624 |           0.5171 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0015 |         241.3699 |           0.5171 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0045 |         240.0010 |           0.5173 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |           0.0011 |         250.8294 |           0.5171 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0053 |         239.6375 |           0.5173 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0080 |         239.7215 |           0.5173 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0068 |         239.4333 |           0.5180 |
[32m[20221213 18:15:00 @agent_ppo2.py:185][0m |          -0.0079 |         239.6361 |           0.5181 |
[32m[20221213 18:15:00 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:15:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 945.05
[32m[20221213 18:15:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.88
[32m[20221213 18:15:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 889.80
[32m[20221213 18:15:00 @agent_ppo2.py:143][0m Total time:      10.84 min
[32m[20221213 18:15:00 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 18:15:00 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 18:15:01 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:01 @agent_ppo2.py:185][0m |          -0.0011 |         240.4844 |           0.5266 |
[32m[20221213 18:15:01 @agent_ppo2.py:185][0m |          -0.0058 |         238.9580 |           0.5259 |
[32m[20221213 18:15:01 @agent_ppo2.py:185][0m |          -0.0055 |         239.0860 |           0.5254 |
[32m[20221213 18:15:01 @agent_ppo2.py:185][0m |          -0.0078 |         239.0727 |           0.5258 |
[32m[20221213 18:15:01 @agent_ppo2.py:185][0m |          -0.0083 |         238.7390 |           0.5259 |
[32m[20221213 18:15:01 @agent_ppo2.py:185][0m |          -0.0080 |         238.8616 |           0.5260 |
[32m[20221213 18:15:02 @agent_ppo2.py:185][0m |          -0.0002 |         251.0197 |           0.5264 |
[32m[20221213 18:15:02 @agent_ppo2.py:185][0m |          -0.0097 |         238.6082 |           0.5254 |
[32m[20221213 18:15:02 @agent_ppo2.py:185][0m |          -0.0096 |         238.6306 |           0.5266 |
[32m[20221213 18:15:02 @agent_ppo2.py:185][0m |          -0.0028 |         249.4030 |           0.5263 |
[32m[20221213 18:15:02 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 18:15:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 995.08
[32m[20221213 18:15:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.73
[32m[20221213 18:15:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 914.00
[32m[20221213 18:15:02 @agent_ppo2.py:143][0m Total time:      10.87 min
[32m[20221213 18:15:02 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221213 18:15:02 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 18:15:02 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:15:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:02 @agent_ppo2.py:185][0m |          -0.0015 |         238.6024 |           0.5323 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |          -0.0022 |         231.8010 |           0.5317 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |           0.0034 |         232.0012 |           0.5314 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |          -0.0051 |         218.3460 |           0.5314 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |          -0.0056 |         216.0785 |           0.5320 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |          -0.0044 |         214.2545 |           0.5325 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |           0.0009 |         220.9669 |           0.5323 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |          -0.0056 |         212.4933 |           0.5320 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |           0.0082 |         242.6013 |           0.5324 |
[32m[20221213 18:15:03 @agent_ppo2.py:185][0m |          -0.0003 |         215.7282 |           0.5331 |
[32m[20221213 18:15:03 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 18:15:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 894.41
[32m[20221213 18:15:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.51
[32m[20221213 18:15:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.49
[32m[20221213 18:15:04 @agent_ppo2.py:143][0m Total time:      10.90 min
[32m[20221213 18:15:04 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 18:15:04 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 18:15:04 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:04 @agent_ppo2.py:185][0m |           0.0001 |         251.5804 |           0.5350 |
[32m[20221213 18:15:04 @agent_ppo2.py:185][0m |           0.0007 |         251.2601 |           0.5346 |
[32m[20221213 18:15:04 @agent_ppo2.py:185][0m |          -0.0037 |         246.7329 |           0.5348 |
[32m[20221213 18:15:04 @agent_ppo2.py:185][0m |          -0.0080 |         245.6284 |           0.5351 |
[32m[20221213 18:15:04 @agent_ppo2.py:185][0m |          -0.0078 |         245.3299 |           0.5341 |
[32m[20221213 18:15:05 @agent_ppo2.py:185][0m |          -0.0091 |         244.6920 |           0.5345 |
[32m[20221213 18:15:05 @agent_ppo2.py:185][0m |          -0.0013 |         256.5353 |           0.5347 |
[32m[20221213 18:15:05 @agent_ppo2.py:185][0m |          -0.0084 |         244.1040 |           0.5345 |
[32m[20221213 18:15:05 @agent_ppo2.py:185][0m |          -0.0077 |         245.5601 |           0.5352 |
[32m[20221213 18:15:05 @agent_ppo2.py:185][0m |          -0.0062 |         248.2347 |           0.5350 |
[32m[20221213 18:15:05 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 18:15:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 943.09
[32m[20221213 18:15:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.98
[32m[20221213 18:15:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.76
[32m[20221213 18:15:05 @agent_ppo2.py:143][0m Total time:      10.92 min
[32m[20221213 18:15:05 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221213 18:15:05 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 18:15:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0019 |         252.5214 |           0.5453 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |           0.0079 |         280.3406 |           0.5435 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0029 |         248.7384 |           0.5431 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0083 |         247.7655 |           0.5429 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0072 |         247.1325 |           0.5436 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0088 |         247.2078 |           0.5429 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0097 |         246.4437 |           0.5429 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0098 |         246.1273 |           0.5426 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0116 |         245.8017 |           0.5431 |
[32m[20221213 18:15:06 @agent_ppo2.py:185][0m |          -0.0087 |         246.4486 |           0.5423 |
[32m[20221213 18:15:06 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 18:15:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 879.86
[32m[20221213 18:15:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.59
[32m[20221213 18:15:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.34
[32m[20221213 18:15:07 @agent_ppo2.py:143][0m Total time:      10.95 min
[32m[20221213 18:15:07 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 18:15:07 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 18:15:07 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 18:15:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:07 @agent_ppo2.py:185][0m |           0.0002 |         246.8210 |           0.5570 |
[32m[20221213 18:15:07 @agent_ppo2.py:185][0m |          -0.0054 |         243.8509 |           0.5565 |
[32m[20221213 18:15:07 @agent_ppo2.py:185][0m |          -0.0048 |         242.3580 |           0.5561 |
[32m[20221213 18:15:07 @agent_ppo2.py:185][0m |          -0.0040 |         242.9947 |           0.5557 |
[32m[20221213 18:15:07 @agent_ppo2.py:185][0m |          -0.0048 |         241.2636 |           0.5552 |
[32m[20221213 18:15:08 @agent_ppo2.py:185][0m |          -0.0076 |         240.0298 |           0.5550 |
[32m[20221213 18:15:08 @agent_ppo2.py:185][0m |          -0.0080 |         239.3495 |           0.5548 |
[32m[20221213 18:15:08 @agent_ppo2.py:185][0m |          -0.0107 |         239.4621 |           0.5541 |
[32m[20221213 18:15:08 @agent_ppo2.py:185][0m |          -0.0076 |         239.1547 |           0.5540 |
[32m[20221213 18:15:08 @agent_ppo2.py:185][0m |          -0.0110 |         238.6673 |           0.5539 |
[32m[20221213 18:15:08 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 18:15:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 862.54
[32m[20221213 18:15:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.67
[32m[20221213 18:15:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.00
[32m[20221213 18:15:08 @agent_ppo2.py:143][0m Total time:      10.97 min
[32m[20221213 18:15:08 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221213 18:15:08 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 18:15:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0007 |         233.7391 |           0.5408 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0025 |         229.5828 |           0.5397 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |           0.0034 |         236.4409 |           0.5392 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0086 |         218.3121 |           0.5383 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |           0.0029 |         243.1638 |           0.5391 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0090 |         213.0434 |           0.5387 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0089 |         211.0549 |           0.5388 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0095 |         209.4787 |           0.5388 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0108 |         207.8852 |           0.5386 |
[32m[20221213 18:15:09 @agent_ppo2.py:185][0m |          -0.0088 |         208.2280 |           0.5388 |
[32m[20221213 18:15:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.36
[32m[20221213 18:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.58
[32m[20221213 18:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 945.15
[32m[20221213 18:15:10 @agent_ppo2.py:143][0m Total time:      11.00 min
[32m[20221213 18:15:10 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 18:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 18:15:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0030 |         247.0567 |           0.5507 |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0060 |         244.7180 |           0.5505 |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0065 |         242.7551 |           0.5503 |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0051 |         242.9488 |           0.5511 |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0081 |         241.4280 |           0.5506 |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0106 |         241.0426 |           0.5508 |
[32m[20221213 18:15:10 @agent_ppo2.py:185][0m |          -0.0098 |         240.7242 |           0.5510 |
[32m[20221213 18:15:11 @agent_ppo2.py:185][0m |          -0.0104 |         240.5603 |           0.5507 |
[32m[20221213 18:15:11 @agent_ppo2.py:185][0m |          -0.0111 |         240.2267 |           0.5512 |
[32m[20221213 18:15:11 @agent_ppo2.py:185][0m |          -0.0112 |         240.0653 |           0.5511 |
[32m[20221213 18:15:11 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.58
[32m[20221213 18:15:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.46
[32m[20221213 18:15:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 918.69
[32m[20221213 18:15:11 @agent_ppo2.py:143][0m Total time:      11.02 min
[32m[20221213 18:15:11 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221213 18:15:11 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 18:15:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:11 @agent_ppo2.py:185][0m |          -0.0027 |         242.6017 |           0.5405 |
[32m[20221213 18:15:11 @agent_ppo2.py:185][0m |           0.0018 |         244.1862 |           0.5400 |
[32m[20221213 18:15:11 @agent_ppo2.py:185][0m |          -0.0072 |         238.4222 |           0.5396 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0061 |         236.5785 |           0.5395 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0019 |         242.0171 |           0.5405 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0084 |         235.4036 |           0.5404 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0093 |         234.5682 |           0.5408 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0100 |         233.8412 |           0.5411 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0113 |         233.7151 |           0.5411 |
[32m[20221213 18:15:12 @agent_ppo2.py:185][0m |          -0.0108 |         233.4273 |           0.5417 |
[32m[20221213 18:15:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:15:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 912.99
[32m[20221213 18:15:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.30
[32m[20221213 18:15:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.97
[32m[20221213 18:15:12 @agent_ppo2.py:143][0m Total time:      11.04 min
[32m[20221213 18:15:12 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 18:15:12 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 18:15:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0005 |         250.0543 |           0.5538 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0061 |         235.9417 |           0.5526 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0058 |         232.8558 |           0.5522 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0106 |         230.1833 |           0.5521 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0089 |         229.8277 |           0.5516 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0115 |         227.9794 |           0.5516 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0026 |         236.0214 |           0.5514 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0100 |         224.4568 |           0.5512 |
[32m[20221213 18:15:13 @agent_ppo2.py:185][0m |          -0.0021 |         229.1705 |           0.5514 |
[32m[20221213 18:15:14 @agent_ppo2.py:185][0m |          -0.0099 |         222.8737 |           0.5510 |
[32m[20221213 18:15:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:15:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 888.06
[32m[20221213 18:15:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.43
[32m[20221213 18:15:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 874.01
[32m[20221213 18:15:14 @agent_ppo2.py:143][0m Total time:      11.07 min
[32m[20221213 18:15:14 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221213 18:15:14 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 18:15:14 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:14 @agent_ppo2.py:185][0m |          -0.0016 |         247.6200 |           0.5494 |
[32m[20221213 18:15:14 @agent_ppo2.py:185][0m |          -0.0039 |         242.9177 |           0.5486 |
[32m[20221213 18:15:14 @agent_ppo2.py:185][0m |          -0.0085 |         241.4345 |           0.5488 |
[32m[20221213 18:15:14 @agent_ppo2.py:185][0m |          -0.0037 |         240.6600 |           0.5487 |
[32m[20221213 18:15:15 @agent_ppo2.py:185][0m |          -0.0077 |         239.8544 |           0.5485 |
[32m[20221213 18:15:15 @agent_ppo2.py:185][0m |          -0.0060 |         239.0474 |           0.5484 |
[32m[20221213 18:15:15 @agent_ppo2.py:185][0m |          -0.0025 |         239.9257 |           0.5483 |
[32m[20221213 18:15:15 @agent_ppo2.py:185][0m |          -0.0070 |         239.6111 |           0.5483 |
[32m[20221213 18:15:15 @agent_ppo2.py:185][0m |          -0.0072 |         238.1204 |           0.5479 |
[32m[20221213 18:15:15 @agent_ppo2.py:185][0m |          -0.0086 |         238.1112 |           0.5482 |
[32m[20221213 18:15:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.33
[32m[20221213 18:15:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.07
[32m[20221213 18:15:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.01
[32m[20221213 18:15:15 @agent_ppo2.py:143][0m Total time:      11.09 min
[32m[20221213 18:15:15 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 18:15:15 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 18:15:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |           0.0006 |         237.5472 |           0.5389 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0017 |         236.1832 |           0.5385 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0056 |         235.2456 |           0.5382 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0046 |         235.1672 |           0.5379 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0056 |         234.4619 |           0.5383 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0077 |         233.7505 |           0.5375 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0084 |         233.2656 |           0.5378 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0084 |         232.7272 |           0.5379 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0103 |         232.6011 |           0.5375 |
[32m[20221213 18:15:16 @agent_ppo2.py:185][0m |          -0.0051 |         234.0670 |           0.5376 |
[32m[20221213 18:15:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:15:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 964.25
[32m[20221213 18:15:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.18
[32m[20221213 18:15:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.03
[32m[20221213 18:15:17 @agent_ppo2.py:143][0m Total time:      11.11 min
[32m[20221213 18:15:17 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221213 18:15:17 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 18:15:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:17 @agent_ppo2.py:185][0m |          -0.0019 |         233.5034 |           0.5707 |
[32m[20221213 18:15:17 @agent_ppo2.py:185][0m |          -0.0011 |         220.6059 |           0.5699 |
[32m[20221213 18:15:17 @agent_ppo2.py:185][0m |           0.0100 |         246.9138 |           0.5700 |
[32m[20221213 18:15:17 @agent_ppo2.py:185][0m |          -0.0053 |         215.8350 |           0.5695 |
[32m[20221213 18:15:17 @agent_ppo2.py:185][0m |          -0.0041 |         215.6780 |           0.5694 |
[32m[20221213 18:15:17 @agent_ppo2.py:185][0m |           0.0053 |         230.2785 |           0.5693 |
[32m[20221213 18:15:18 @agent_ppo2.py:185][0m |          -0.0066 |         213.5917 |           0.5689 |
[32m[20221213 18:15:18 @agent_ppo2.py:185][0m |          -0.0078 |         212.9717 |           0.5689 |
[32m[20221213 18:15:18 @agent_ppo2.py:185][0m |          -0.0073 |         212.0373 |           0.5689 |
[32m[20221213 18:15:18 @agent_ppo2.py:185][0m |           0.0024 |         231.6167 |           0.5687 |
[32m[20221213 18:15:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 914.50
[32m[20221213 18:15:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.53
[32m[20221213 18:15:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.98
[32m[20221213 18:15:18 @agent_ppo2.py:143][0m Total time:      11.14 min
[32m[20221213 18:15:18 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 18:15:18 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 18:15:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:18 @agent_ppo2.py:185][0m |          -0.0019 |         234.8244 |           0.5508 |
[32m[20221213 18:15:18 @agent_ppo2.py:185][0m |          -0.0011 |         229.1136 |           0.5498 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0081 |         222.3824 |           0.5492 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0081 |         220.3787 |           0.5493 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0082 |         219.4044 |           0.5491 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |           0.0034 |         231.5834 |           0.5486 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0076 |         217.7803 |           0.5490 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0025 |         224.3888 |           0.5489 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0016 |         223.9441 |           0.5490 |
[32m[20221213 18:15:19 @agent_ppo2.py:185][0m |          -0.0107 |         215.8068 |           0.5488 |
[32m[20221213 18:15:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:15:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 977.83
[32m[20221213 18:15:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.21
[32m[20221213 18:15:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.12
[32m[20221213 18:15:19 @agent_ppo2.py:143][0m Total time:      11.16 min
[32m[20221213 18:15:19 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221213 18:15:19 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 18:15:20 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0002 |         244.1428 |           0.5504 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0039 |         238.8653 |           0.5502 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0034 |         237.1009 |           0.5495 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0028 |         236.8026 |           0.5493 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0047 |         235.6506 |           0.5483 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0088 |         235.1616 |           0.5486 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |           0.0012 |         245.3479 |           0.5487 |
[32m[20221213 18:15:20 @agent_ppo2.py:185][0m |          -0.0070 |         234.8447 |           0.5488 |
[32m[20221213 18:15:21 @agent_ppo2.py:185][0m |          -0.0005 |         241.9683 |           0.5480 |
[32m[20221213 18:15:21 @agent_ppo2.py:185][0m |           0.0023 |         253.4464 |           0.5484 |
[32m[20221213 18:15:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:15:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.91
[32m[20221213 18:15:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.68
[32m[20221213 18:15:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.17
[32m[20221213 18:15:21 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 999.91
[32m[20221213 18:15:21 @agent_ppo2.py:143][0m Total time:      11.18 min
[32m[20221213 18:15:21 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 18:15:21 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 18:15:21 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:15:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:21 @agent_ppo2.py:185][0m |          -0.0012 |         240.2490 |           0.5595 |
[32m[20221213 18:15:21 @agent_ppo2.py:185][0m |          -0.0028 |         237.4610 |           0.5593 |
[32m[20221213 18:15:21 @agent_ppo2.py:185][0m |          -0.0060 |         236.2829 |           0.5592 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0094 |         235.2447 |           0.5599 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0078 |         234.4643 |           0.5599 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0038 |         238.1462 |           0.5602 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0009 |         237.7416 |           0.5597 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0090 |         232.9020 |           0.5597 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0096 |         232.6187 |           0.5600 |
[32m[20221213 18:15:22 @agent_ppo2.py:185][0m |          -0.0026 |         240.4654 |           0.5606 |
[32m[20221213 18:15:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 892.72
[32m[20221213 18:15:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.12
[32m[20221213 18:15:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.64
[32m[20221213 18:15:22 @agent_ppo2.py:143][0m Total time:      11.21 min
[32m[20221213 18:15:22 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221213 18:15:22 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 18:15:22 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0014 |         238.0765 |           0.5697 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0057 |         235.9650 |           0.5690 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0048 |         235.7555 |           0.5685 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0069 |         234.8583 |           0.5684 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0078 |         234.6017 |           0.5684 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0079 |         234.1050 |           0.5688 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0075 |         234.0398 |           0.5682 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0053 |         234.1723 |           0.5689 |
[32m[20221213 18:15:23 @agent_ppo2.py:185][0m |          -0.0081 |         233.6872 |           0.5690 |
[32m[20221213 18:15:24 @agent_ppo2.py:185][0m |          -0.0080 |         233.4041 |           0.5686 |
[32m[20221213 18:15:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 949.79
[32m[20221213 18:15:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.40
[32m[20221213 18:15:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.56
[32m[20221213 18:15:24 @agent_ppo2.py:143][0m Total time:      11.23 min
[32m[20221213 18:15:24 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 18:15:24 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 18:15:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:24 @agent_ppo2.py:185][0m |          -0.0023 |         242.8863 |           0.5669 |
[32m[20221213 18:15:24 @agent_ppo2.py:185][0m |           0.0055 |         254.9523 |           0.5660 |
[32m[20221213 18:15:24 @agent_ppo2.py:185][0m |          -0.0074 |         238.0676 |           0.5651 |
[32m[20221213 18:15:24 @agent_ppo2.py:185][0m |          -0.0050 |         237.0213 |           0.5650 |
[32m[20221213 18:15:24 @agent_ppo2.py:185][0m |          -0.0094 |         235.9966 |           0.5648 |
[32m[20221213 18:15:25 @agent_ppo2.py:185][0m |          -0.0100 |         235.2357 |           0.5640 |
[32m[20221213 18:15:25 @agent_ppo2.py:185][0m |          -0.0100 |         235.2400 |           0.5633 |
[32m[20221213 18:15:25 @agent_ppo2.py:185][0m |          -0.0095 |         234.7854 |           0.5641 |
[32m[20221213 18:15:25 @agent_ppo2.py:185][0m |          -0.0094 |         234.2881 |           0.5632 |
[32m[20221213 18:15:25 @agent_ppo2.py:185][0m |          -0.0103 |         234.1620 |           0.5632 |
[32m[20221213 18:15:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.45
[32m[20221213 18:15:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.60
[32m[20221213 18:15:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.21
[32m[20221213 18:15:25 @agent_ppo2.py:143][0m Total time:      11.26 min
[32m[20221213 18:15:25 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221213 18:15:25 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 18:15:25 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:25 @agent_ppo2.py:185][0m |           0.0097 |         257.8365 |           0.5632 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0050 |         237.0166 |           0.5617 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0014 |         237.0959 |           0.5616 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0063 |         235.3293 |           0.5614 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0065 |         234.9906 |           0.5610 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0016 |         236.3717 |           0.5613 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0084 |         234.5378 |           0.5606 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0082 |         234.5602 |           0.5602 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0085 |         234.1590 |           0.5601 |
[32m[20221213 18:15:26 @agent_ppo2.py:185][0m |          -0.0086 |         234.2797 |           0.5596 |
[32m[20221213 18:15:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:15:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 936.96
[32m[20221213 18:15:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.62
[32m[20221213 18:15:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 898.45
[32m[20221213 18:15:27 @agent_ppo2.py:143][0m Total time:      11.28 min
[32m[20221213 18:15:27 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 18:15:27 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 18:15:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0018 |         237.8817 |           0.5624 |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0053 |         234.9126 |           0.5612 |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0070 |         234.1630 |           0.5610 |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0072 |         233.1499 |           0.5609 |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0063 |         232.9858 |           0.5604 |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0074 |         232.7243 |           0.5604 |
[32m[20221213 18:15:27 @agent_ppo2.py:185][0m |          -0.0028 |         234.8812 |           0.5611 |
[32m[20221213 18:15:28 @agent_ppo2.py:185][0m |          -0.0068 |         234.1787 |           0.5614 |
[32m[20221213 18:15:28 @agent_ppo2.py:185][0m |          -0.0090 |         231.8514 |           0.5607 |
[32m[20221213 18:15:28 @agent_ppo2.py:185][0m |          -0.0088 |         231.4001 |           0.5610 |
[32m[20221213 18:15:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 856.36
[32m[20221213 18:15:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.39
[32m[20221213 18:15:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 970.21
[32m[20221213 18:15:28 @agent_ppo2.py:143][0m Total time:      11.30 min
[32m[20221213 18:15:28 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221213 18:15:28 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 18:15:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:28 @agent_ppo2.py:185][0m |          -0.0002 |         237.9080 |           0.5589 |
[32m[20221213 18:15:28 @agent_ppo2.py:185][0m |          -0.0032 |         235.8462 |           0.5581 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0047 |         235.1445 |           0.5588 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0062 |         234.1586 |           0.5592 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |           0.0016 |         243.0144 |           0.5589 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0055 |         233.3725 |           0.5591 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0066 |         233.0259 |           0.5596 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0059 |         232.9606 |           0.5594 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0060 |         233.1806 |           0.5594 |
[32m[20221213 18:15:29 @agent_ppo2.py:185][0m |          -0.0078 |         232.6696 |           0.5604 |
[32m[20221213 18:15:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:15:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.03
[32m[20221213 18:15:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.20
[32m[20221213 18:15:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.98
[32m[20221213 18:15:29 @agent_ppo2.py:143][0m Total time:      11.33 min
[32m[20221213 18:15:29 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 18:15:29 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 18:15:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0023 |         236.0734 |           0.5680 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0028 |         233.4172 |           0.5669 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0060 |         232.2219 |           0.5662 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0047 |         231.8466 |           0.5657 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0056 |         230.9117 |           0.5658 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0015 |         234.0895 |           0.5651 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0032 |         231.4798 |           0.5648 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0069 |         229.6593 |           0.5647 |
[32m[20221213 18:15:30 @agent_ppo2.py:185][0m |          -0.0063 |         229.2629 |           0.5644 |
[32m[20221213 18:15:31 @agent_ppo2.py:185][0m |           0.0011 |         245.7508 |           0.5644 |
[32m[20221213 18:15:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:15:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 893.73
[32m[20221213 18:15:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.92
[32m[20221213 18:15:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 913.70
[32m[20221213 18:15:31 @agent_ppo2.py:143][0m Total time:      11.35 min
[32m[20221213 18:15:31 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221213 18:15:31 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 18:15:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:31 @agent_ppo2.py:185][0m |           0.0001 |         231.1003 |           0.5631 |
[32m[20221213 18:15:31 @agent_ppo2.py:185][0m |          -0.0023 |         230.9925 |           0.5615 |
[32m[20221213 18:15:31 @agent_ppo2.py:185][0m |          -0.0058 |         229.2204 |           0.5608 |
[32m[20221213 18:15:31 @agent_ppo2.py:185][0m |           0.0016 |         242.3075 |           0.5598 |
[32m[20221213 18:15:32 @agent_ppo2.py:185][0m |          -0.0067 |         228.4737 |           0.5587 |
[32m[20221213 18:15:32 @agent_ppo2.py:185][0m |          -0.0082 |         227.9805 |           0.5596 |
[32m[20221213 18:15:32 @agent_ppo2.py:185][0m |          -0.0080 |         227.7840 |           0.5594 |
[32m[20221213 18:15:32 @agent_ppo2.py:185][0m |          -0.0088 |         227.5608 |           0.5594 |
[32m[20221213 18:15:32 @agent_ppo2.py:185][0m |          -0.0088 |         227.3312 |           0.5590 |
[32m[20221213 18:15:32 @agent_ppo2.py:185][0m |          -0.0087 |         226.8029 |           0.5597 |
[32m[20221213 18:15:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 18:15:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 923.60
[32m[20221213 18:15:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.10
[32m[20221213 18:15:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.80
[32m[20221213 18:15:32 @agent_ppo2.py:143][0m Total time:      11.37 min
[32m[20221213 18:15:32 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 18:15:32 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 18:15:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0041 |         241.2748 |           0.5597 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |           0.0073 |         256.5985 |           0.5592 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0068 |         235.4939 |           0.5594 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0067 |         234.6435 |           0.5595 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0082 |         234.0919 |           0.5602 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0082 |         233.2931 |           0.5604 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0034 |         235.6545 |           0.5609 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0083 |         232.2595 |           0.5611 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0090 |         231.7091 |           0.5616 |
[32m[20221213 18:15:33 @agent_ppo2.py:185][0m |          -0.0076 |         232.2261 |           0.5617 |
[32m[20221213 18:15:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:15:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.68
[32m[20221213 18:15:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.40
[32m[20221213 18:15:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.04
[32m[20221213 18:15:34 @agent_ppo2.py:143][0m Total time:      11.40 min
[32m[20221213 18:15:34 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221213 18:15:34 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 18:15:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:34 @agent_ppo2.py:185][0m |          -0.0024 |         244.6878 |           0.5667 |
[32m[20221213 18:15:34 @agent_ppo2.py:185][0m |          -0.0055 |         242.1296 |           0.5657 |
[32m[20221213 18:15:34 @agent_ppo2.py:185][0m |          -0.0080 |         240.9783 |           0.5654 |
[32m[20221213 18:15:34 @agent_ppo2.py:185][0m |          -0.0078 |         240.3335 |           0.5660 |
[32m[20221213 18:15:34 @agent_ppo2.py:185][0m |          -0.0082 |         239.9147 |           0.5654 |
[32m[20221213 18:15:34 @agent_ppo2.py:185][0m |           0.0015 |         264.8829 |           0.5659 |
[32m[20221213 18:15:35 @agent_ppo2.py:185][0m |          -0.0088 |         239.7130 |           0.5661 |
[32m[20221213 18:15:35 @agent_ppo2.py:185][0m |          -0.0093 |         239.0845 |           0.5666 |
[32m[20221213 18:15:35 @agent_ppo2.py:185][0m |          -0.0104 |         238.7474 |           0.5664 |
[32m[20221213 18:15:35 @agent_ppo2.py:185][0m |          -0.0100 |         239.0079 |           0.5666 |
[32m[20221213 18:15:35 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:15:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.99
[32m[20221213 18:15:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.62
[32m[20221213 18:15:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.41
[32m[20221213 18:15:35 @agent_ppo2.py:143][0m Total time:      11.42 min
[32m[20221213 18:15:35 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 18:15:35 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 18:15:35 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:15:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:35 @agent_ppo2.py:185][0m |          -0.0003 |         228.4263 |           0.5717 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0032 |         223.3194 |           0.5701 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0038 |         222.2316 |           0.5690 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0029 |         222.2140 |           0.5689 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |           0.0012 |         230.3200 |           0.5683 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |           0.0011 |         233.3175 |           0.5670 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0100 |         218.1439 |           0.5667 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0113 |         217.5887 |           0.5664 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0094 |         217.4918 |           0.5659 |
[32m[20221213 18:15:36 @agent_ppo2.py:185][0m |          -0.0102 |         216.8451 |           0.5652 |
[32m[20221213 18:15:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:15:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.11
[32m[20221213 18:15:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.84
[32m[20221213 18:15:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.34
[32m[20221213 18:15:36 @agent_ppo2.py:143][0m Total time:      11.44 min
[32m[20221213 18:15:36 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221213 18:15:36 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 18:15:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0024 |         240.8311 |           0.5785 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0031 |         238.9486 |           0.5770 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0062 |         237.3967 |           0.5756 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |           0.0064 |         250.1698 |           0.5765 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0065 |         236.2722 |           0.5753 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0064 |         235.8570 |           0.5742 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0082 |         235.6807 |           0.5751 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0020 |         240.0252 |           0.5746 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0090 |         235.3457 |           0.5746 |
[32m[20221213 18:15:37 @agent_ppo2.py:185][0m |          -0.0059 |         235.2894 |           0.5743 |
[32m[20221213 18:15:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:15:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 901.29
[32m[20221213 18:15:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.08
[32m[20221213 18:15:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.62
[32m[20221213 18:15:38 @agent_ppo2.py:143][0m Total time:      11.46 min
[32m[20221213 18:15:38 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 18:15:38 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 18:15:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0018 |         239.0860 |           0.5465 |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0045 |         237.2281 |           0.5460 |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0087 |         236.7282 |           0.5454 |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0084 |         236.0278 |           0.5449 |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0072 |         235.5626 |           0.5448 |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0096 |         235.2546 |           0.5442 |
[32m[20221213 18:15:38 @agent_ppo2.py:185][0m |          -0.0101 |         234.9855 |           0.5440 |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0081 |         235.0320 |           0.5442 |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0099 |         235.1566 |           0.5440 |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0108 |         234.5425 |           0.5435 |
[32m[20221213 18:15:39 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:15:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.05
[32m[20221213 18:15:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.60
[32m[20221213 18:15:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 885.66
[32m[20221213 18:15:39 @agent_ppo2.py:143][0m Total time:      11.48 min
[32m[20221213 18:15:39 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221213 18:15:39 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 18:15:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:15:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0026 |         237.1731 |           0.5441 |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0052 |         232.3981 |           0.5446 |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0056 |         228.4752 |           0.5442 |
[32m[20221213 18:15:39 @agent_ppo2.py:185][0m |          -0.0055 |         226.6517 |           0.5443 |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |          -0.0050 |         226.6164 |           0.5448 |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |          -0.0057 |         225.3584 |           0.5454 |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |          -0.0081 |         224.1939 |           0.5455 |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |           0.0036 |         239.3987 |           0.5455 |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |          -0.0011 |         228.2720 |           0.5453 |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |          -0.0076 |         220.7181 |           0.5452 |
[32m[20221213 18:15:40 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:15:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 959.18
[32m[20221213 18:15:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.14
[32m[20221213 18:15:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.72
[32m[20221213 18:15:40 @agent_ppo2.py:143][0m Total time:      11.51 min
[32m[20221213 18:15:40 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 18:15:40 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 18:15:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:40 @agent_ppo2.py:185][0m |          -0.0023 |         241.5759 |           0.5811 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0030 |         238.8122 |           0.5807 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0073 |         237.4422 |           0.5813 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0094 |         236.9140 |           0.5812 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0093 |         236.6909 |           0.5816 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0080 |         235.8641 |           0.5810 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0095 |         235.3915 |           0.5817 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0092 |         236.3199 |           0.5820 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0101 |         234.9330 |           0.5819 |
[32m[20221213 18:15:41 @agent_ppo2.py:185][0m |          -0.0094 |         234.7733 |           0.5817 |
[32m[20221213 18:15:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:15:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 909.11
[32m[20221213 18:15:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.71
[32m[20221213 18:15:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 914.82
[32m[20221213 18:15:41 @agent_ppo2.py:143][0m Total time:      11.53 min
[32m[20221213 18:15:41 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221213 18:15:41 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 18:15:42 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |           0.0030 |         250.8062 |           0.5510 |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |          -0.0053 |         244.5927 |           0.5506 |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |           0.0002 |         246.9638 |           0.5509 |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |          -0.0049 |         242.1899 |           0.5508 |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |          -0.0077 |         241.5955 |           0.5512 |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |          -0.0076 |         240.3787 |           0.5516 |
[32m[20221213 18:15:42 @agent_ppo2.py:185][0m |          -0.0074 |         239.9742 |           0.5514 |
[32m[20221213 18:15:43 @agent_ppo2.py:185][0m |          -0.0041 |         241.9974 |           0.5520 |
[32m[20221213 18:15:43 @agent_ppo2.py:185][0m |          -0.0083 |         239.1822 |           0.5520 |
[32m[20221213 18:15:43 @agent_ppo2.py:185][0m |          -0.0080 |         239.2150 |           0.5527 |
[32m[20221213 18:15:43 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 18:15:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.75
[32m[20221213 18:15:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 980.89
[32m[20221213 18:15:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 918.70
[32m[20221213 18:15:43 @agent_ppo2.py:143][0m Total time:      11.55 min
[32m[20221213 18:15:43 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 18:15:43 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 18:15:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:43 @agent_ppo2.py:185][0m |           0.0023 |         249.2917 |           0.5849 |
[32m[20221213 18:15:43 @agent_ppo2.py:185][0m |          -0.0053 |         242.5268 |           0.5841 |
[32m[20221213 18:15:43 @agent_ppo2.py:185][0m |          -0.0099 |         240.7466 |           0.5836 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0100 |         239.4029 |           0.5835 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0099 |         239.0298 |           0.5837 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0097 |         237.7454 |           0.5835 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0113 |         237.3197 |           0.5838 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0111 |         237.1096 |           0.5834 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0116 |         236.5257 |           0.5832 |
[32m[20221213 18:15:44 @agent_ppo2.py:185][0m |          -0.0129 |         235.6188 |           0.5832 |
[32m[20221213 18:15:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 851.22
[32m[20221213 18:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.39
[32m[20221213 18:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.55
[32m[20221213 18:15:44 @agent_ppo2.py:143][0m Total time:      11.57 min
[32m[20221213 18:15:44 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221213 18:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 18:15:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0025 |         236.7915 |           0.5653 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0056 |         234.8740 |           0.5650 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0070 |         233.5990 |           0.5656 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0089 |         233.0037 |           0.5654 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0085 |         232.6321 |           0.5656 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0103 |         232.0586 |           0.5657 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0109 |         231.6654 |           0.5669 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0108 |         231.1725 |           0.5662 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0117 |         231.0168 |           0.5674 |
[32m[20221213 18:15:45 @agent_ppo2.py:185][0m |          -0.0061 |         238.3793 |           0.5669 |
[32m[20221213 18:15:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:15:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 920.06
[32m[20221213 18:15:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.38
[32m[20221213 18:15:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 903.12
[32m[20221213 18:15:46 @agent_ppo2.py:143][0m Total time:      11.60 min
[32m[20221213 18:15:46 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 18:15:46 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 18:15:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:15:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:46 @agent_ppo2.py:185][0m |          -0.0007 |         241.1172 |           0.5903 |
[32m[20221213 18:15:46 @agent_ppo2.py:185][0m |          -0.0056 |         238.9664 |           0.5890 |
[32m[20221213 18:15:46 @agent_ppo2.py:185][0m |          -0.0076 |         237.8055 |           0.5886 |
[32m[20221213 18:15:46 @agent_ppo2.py:185][0m |          -0.0076 |         237.1261 |           0.5882 |
[32m[20221213 18:15:46 @agent_ppo2.py:185][0m |          -0.0086 |         236.8371 |           0.5878 |
[32m[20221213 18:15:46 @agent_ppo2.py:185][0m |           0.0035 |         263.6201 |           0.5881 |
[32m[20221213 18:15:47 @agent_ppo2.py:185][0m |          -0.0076 |         236.3634 |           0.5876 |
[32m[20221213 18:15:47 @agent_ppo2.py:185][0m |          -0.0107 |         235.9599 |           0.5871 |
[32m[20221213 18:15:47 @agent_ppo2.py:185][0m |          -0.0117 |         235.8158 |           0.5873 |
[32m[20221213 18:15:47 @agent_ppo2.py:185][0m |          -0.0116 |         235.6634 |           0.5870 |
[32m[20221213 18:15:47 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 18:15:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 948.46
[32m[20221213 18:15:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.19
[32m[20221213 18:15:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.57
[32m[20221213 18:15:47 @agent_ppo2.py:143][0m Total time:      11.62 min
[32m[20221213 18:15:47 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221213 18:15:47 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 18:15:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:47 @agent_ppo2.py:185][0m |          -0.0009 |         237.2254 |           0.5872 |
[32m[20221213 18:15:47 @agent_ppo2.py:185][0m |          -0.0061 |         235.2975 |           0.5856 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0074 |         234.5185 |           0.5850 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0068 |         233.9550 |           0.5839 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0050 |         233.4847 |           0.5841 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0082 |         233.2650 |           0.5842 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0063 |         233.9704 |           0.5841 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0087 |         232.8178 |           0.5837 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0073 |         232.4588 |           0.5838 |
[32m[20221213 18:15:48 @agent_ppo2.py:185][0m |          -0.0109 |         232.4055 |           0.5843 |
[32m[20221213 18:15:48 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 894.24
[32m[20221213 18:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.13
[32m[20221213 18:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.13
[32m[20221213 18:15:48 @agent_ppo2.py:143][0m Total time:      11.64 min
[32m[20221213 18:15:48 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 18:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 18:15:49 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:15:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0014 |         237.2736 |           0.5564 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0049 |         234.6521 |           0.5545 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |           0.0051 |         248.8895 |           0.5541 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0040 |         233.3396 |           0.5535 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0011 |         238.2804 |           0.5542 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0070 |         232.1361 |           0.5539 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0063 |         231.9171 |           0.5535 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0015 |         238.4553 |           0.5538 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0073 |         231.3251 |           0.5530 |
[32m[20221213 18:15:49 @agent_ppo2.py:185][0m |          -0.0095 |         230.9271 |           0.5528 |
[32m[20221213 18:15:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:15:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.36
[32m[20221213 18:15:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.51
[32m[20221213 18:15:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.32
[32m[20221213 18:15:50 @agent_ppo2.py:143][0m Total time:      11.66 min
[32m[20221213 18:15:50 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221213 18:15:50 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 18:15:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:50 @agent_ppo2.py:185][0m |          -0.0026 |         239.7788 |           0.5786 |
[32m[20221213 18:15:50 @agent_ppo2.py:185][0m |          -0.0049 |         236.2685 |           0.5777 |
[32m[20221213 18:15:50 @agent_ppo2.py:185][0m |           0.0068 |         253.3168 |           0.5764 |
[32m[20221213 18:15:50 @agent_ppo2.py:185][0m |          -0.0067 |         232.6798 |           0.5752 |
[32m[20221213 18:15:50 @agent_ppo2.py:185][0m |          -0.0076 |         231.8581 |           0.5759 |
[32m[20221213 18:15:50 @agent_ppo2.py:185][0m |          -0.0042 |         232.6677 |           0.5751 |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0077 |         231.3310 |           0.5754 |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0086 |         230.8743 |           0.5757 |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0049 |         232.7980 |           0.5757 |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0103 |         230.2233 |           0.5752 |
[32m[20221213 18:15:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:15:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.51
[32m[20221213 18:15:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.67
[32m[20221213 18:15:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 974.49
[32m[20221213 18:15:51 @agent_ppo2.py:143][0m Total time:      11.69 min
[32m[20221213 18:15:51 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 18:15:51 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 18:15:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0041 |         232.8274 |           0.5677 |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0021 |         224.8429 |           0.5670 |
[32m[20221213 18:15:51 @agent_ppo2.py:185][0m |          -0.0093 |         222.8751 |           0.5658 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |          -0.0076 |         221.0979 |           0.5665 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |           0.0129 |         265.5775 |           0.5655 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |           0.0033 |         243.1946 |           0.5641 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |          -0.0083 |         219.6078 |           0.5646 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |          -0.0062 |         225.6118 |           0.5646 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |          -0.0087 |         218.8929 |           0.5643 |
[32m[20221213 18:15:52 @agent_ppo2.py:185][0m |          -0.0096 |         218.0433 |           0.5645 |
[32m[20221213 18:15:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.57
[32m[20221213 18:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.11
[32m[20221213 18:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 959.88
[32m[20221213 18:15:52 @agent_ppo2.py:143][0m Total time:      11.71 min
[32m[20221213 18:15:52 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221213 18:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 18:15:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0027 |         236.6900 |           0.5792 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0034 |         234.7920 |           0.5777 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0071 |         233.8744 |           0.5773 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0068 |         233.4011 |           0.5773 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0070 |         232.8432 |           0.5768 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0079 |         232.5137 |           0.5767 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0086 |         232.5317 |           0.5768 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |           0.0041 |         262.1939 |           0.5773 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |           0.0070 |         266.7280 |           0.5757 |
[32m[20221213 18:15:53 @agent_ppo2.py:185][0m |          -0.0086 |         232.3161 |           0.5773 |
[32m[20221213 18:15:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:15:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 953.14
[32m[20221213 18:15:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.43
[32m[20221213 18:15:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.98
[32m[20221213 18:15:54 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221213 18:15:54 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 18:15:54 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 18:15:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:54 @agent_ppo2.py:185][0m |          -0.0000 |         234.3587 |           0.5760 |
[32m[20221213 18:15:54 @agent_ppo2.py:185][0m |          -0.0031 |         231.5347 |           0.5744 |
[32m[20221213 18:15:54 @agent_ppo2.py:185][0m |          -0.0051 |         230.9973 |           0.5750 |
[32m[20221213 18:15:54 @agent_ppo2.py:185][0m |          -0.0028 |         231.7074 |           0.5746 |
[32m[20221213 18:15:54 @agent_ppo2.py:185][0m |          -0.0054 |         230.2728 |           0.5745 |
[32m[20221213 18:15:54 @agent_ppo2.py:185][0m |          -0.0059 |         229.9234 |           0.5741 |
[32m[20221213 18:15:55 @agent_ppo2.py:185][0m |          -0.0073 |         229.6833 |           0.5750 |
[32m[20221213 18:15:55 @agent_ppo2.py:185][0m |          -0.0077 |         229.4443 |           0.5746 |
[32m[20221213 18:15:55 @agent_ppo2.py:185][0m |          -0.0049 |         230.1580 |           0.5745 |
[32m[20221213 18:15:55 @agent_ppo2.py:185][0m |          -0.0084 |         229.3251 |           0.5748 |
[32m[20221213 18:15:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:15:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.40
[32m[20221213 18:15:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.69
[32m[20221213 18:15:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.14
[32m[20221213 18:15:55 @agent_ppo2.py:143][0m Total time:      11.75 min
[32m[20221213 18:15:55 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221213 18:15:55 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 18:15:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:55 @agent_ppo2.py:185][0m |          -0.0017 |         230.5057 |           0.5499 |
[32m[20221213 18:15:55 @agent_ppo2.py:185][0m |          -0.0048 |         229.6629 |           0.5480 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0033 |         229.7553 |           0.5487 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0073 |         228.7376 |           0.5481 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0075 |         228.4737 |           0.5486 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0081 |         228.0284 |           0.5474 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0086 |         227.8988 |           0.5477 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0091 |         227.7634 |           0.5469 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0089 |         227.4559 |           0.5471 |
[32m[20221213 18:15:56 @agent_ppo2.py:185][0m |          -0.0071 |         227.4869 |           0.5464 |
[32m[20221213 18:15:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:15:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 968.15
[32m[20221213 18:15:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.08
[32m[20221213 18:15:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 959.47
[32m[20221213 18:15:56 @agent_ppo2.py:143][0m Total time:      11.78 min
[32m[20221213 18:15:56 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 18:15:56 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 18:15:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:15:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0023 |         236.8482 |           0.5738 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0057 |         235.0539 |           0.5726 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0072 |         234.6104 |           0.5727 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0079 |         233.6982 |           0.5726 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0083 |         233.0385 |           0.5726 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0084 |         232.6267 |           0.5720 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0092 |         232.4493 |           0.5717 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0103 |         231.9447 |           0.5717 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0047 |         235.4577 |           0.5713 |
[32m[20221213 18:15:57 @agent_ppo2.py:185][0m |          -0.0114 |         231.5781 |           0.5719 |
[32m[20221213 18:15:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:15:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 924.09
[32m[20221213 18:15:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.19
[32m[20221213 18:15:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.49
[32m[20221213 18:15:58 @agent_ppo2.py:143][0m Total time:      11.80 min
[32m[20221213 18:15:58 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221213 18:15:58 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 18:15:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:15:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |           0.0055 |         251.1443 |           0.5642 |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |          -0.0035 |         237.6233 |           0.5615 |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |          -0.0005 |         244.8359 |           0.5621 |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |          -0.0044 |         238.4483 |           0.5635 |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |          -0.0091 |         236.3287 |           0.5628 |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |          -0.0099 |         236.3145 |           0.5623 |
[32m[20221213 18:15:58 @agent_ppo2.py:185][0m |          -0.0087 |         235.8408 |           0.5622 |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |          -0.0101 |         235.5634 |           0.5617 |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |          -0.0082 |         235.6347 |           0.5617 |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |          -0.0079 |         235.2685 |           0.5626 |
[32m[20221213 18:15:59 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:15:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 945.27
[32m[20221213 18:15:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 982.83
[32m[20221213 18:15:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.00
[32m[20221213 18:15:59 @agent_ppo2.py:143][0m Total time:      11.82 min
[32m[20221213 18:15:59 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 18:15:59 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 18:15:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:15:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |           0.0007 |         243.8156 |           0.5619 |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |          -0.0050 |         242.4403 |           0.5608 |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |           0.0016 |         247.0013 |           0.5612 |
[32m[20221213 18:15:59 @agent_ppo2.py:185][0m |          -0.0066 |         241.3628 |           0.5611 |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |          -0.0053 |         241.0190 |           0.5609 |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |          -0.0058 |         240.8645 |           0.5606 |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |          -0.0038 |         240.7776 |           0.5610 |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |          -0.0076 |         240.3891 |           0.5605 |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |          -0.0078 |         240.1691 |           0.5615 |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |          -0.0053 |         240.9834 |           0.5610 |
[32m[20221213 18:16:00 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 897.57
[32m[20221213 18:16:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.23
[32m[20221213 18:16:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.81
[32m[20221213 18:16:00 @agent_ppo2.py:143][0m Total time:      11.84 min
[32m[20221213 18:16:00 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221213 18:16:00 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 18:16:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:00 @agent_ppo2.py:185][0m |           0.0015 |         248.1867 |           0.5675 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0015 |         244.6760 |           0.5680 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |           0.0017 |         252.5281 |           0.5677 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0033 |         243.3684 |           0.5677 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0044 |         243.0097 |           0.5676 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0071 |         242.4679 |           0.5676 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0085 |         242.4104 |           0.5671 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0079 |         242.0032 |           0.5673 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0083 |         241.8078 |           0.5673 |
[32m[20221213 18:16:01 @agent_ppo2.py:185][0m |          -0.0072 |         241.3288 |           0.5677 |
[32m[20221213 18:16:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:16:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 860.97
[32m[20221213 18:16:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.27
[32m[20221213 18:16:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 944.02
[32m[20221213 18:16:01 @agent_ppo2.py:143][0m Total time:      11.86 min
[32m[20221213 18:16:01 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 18:16:01 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 18:16:02 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:16:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0019 |         243.7992 |           0.5645 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0065 |         241.3858 |           0.5632 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0070 |         240.4037 |           0.5624 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0080 |         240.0338 |           0.5622 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0110 |         239.3516 |           0.5619 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0079 |         239.5735 |           0.5615 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0112 |         238.6310 |           0.5614 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |           0.0032 |         263.1810 |           0.5609 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0105 |         238.1719 |           0.5609 |
[32m[20221213 18:16:02 @agent_ppo2.py:185][0m |          -0.0093 |         237.6303 |           0.5608 |
[32m[20221213 18:16:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 855.45
[32m[20221213 18:16:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.27
[32m[20221213 18:16:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 920.81
[32m[20221213 18:16:03 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221213 18:16:03 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221213 18:16:03 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 18:16:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |          -0.0001 |         247.1593 |           0.5572 |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |           0.0049 |         250.6989 |           0.5566 |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |          -0.0043 |         241.1420 |           0.5553 |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |          -0.0062 |         240.3490 |           0.5560 |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |          -0.0075 |         239.8788 |           0.5559 |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |          -0.0061 |         239.4533 |           0.5555 |
[32m[20221213 18:16:03 @agent_ppo2.py:185][0m |          -0.0083 |         238.9964 |           0.5556 |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |          -0.0000 |         244.3583 |           0.5552 |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |          -0.0086 |         238.5675 |           0.5555 |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |          -0.0024 |         243.4958 |           0.5551 |
[32m[20221213 18:16:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.19
[32m[20221213 18:16:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.20
[32m[20221213 18:16:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.74
[32m[20221213 18:16:04 @agent_ppo2.py:143][0m Total time:      11.90 min
[32m[20221213 18:16:04 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 18:16:04 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 18:16:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |           0.0087 |         259.2731 |           0.5724 |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |          -0.0019 |         238.5627 |           0.5712 |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |          -0.0026 |         238.3554 |           0.5718 |
[32m[20221213 18:16:04 @agent_ppo2.py:185][0m |          -0.0045 |         237.3848 |           0.5720 |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |           0.0014 |         246.4666 |           0.5718 |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |          -0.0074 |         236.7039 |           0.5710 |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |          -0.0061 |         236.3975 |           0.5709 |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |          -0.0056 |         237.7575 |           0.5713 |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |          -0.0032 |         237.7513 |           0.5712 |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |          -0.0074 |         235.7796 |           0.5712 |
[32m[20221213 18:16:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 928.19
[32m[20221213 18:16:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.06
[32m[20221213 18:16:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.72
[32m[20221213 18:16:05 @agent_ppo2.py:143][0m Total time:      11.92 min
[32m[20221213 18:16:05 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221213 18:16:05 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 18:16:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:05 @agent_ppo2.py:185][0m |          -0.0012 |         240.2914 |           0.5449 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0055 |         238.4840 |           0.5439 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0068 |         238.1273 |           0.5442 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |           0.0049 |         249.6121 |           0.5440 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0076 |         237.3857 |           0.5433 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0002 |         250.4163 |           0.5433 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0062 |         237.7902 |           0.5441 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0084 |         236.6166 |           0.5439 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0084 |         236.2362 |           0.5436 |
[32m[20221213 18:16:06 @agent_ppo2.py:185][0m |          -0.0077 |         235.9925 |           0.5440 |
[32m[20221213 18:16:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.42
[32m[20221213 18:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.57
[32m[20221213 18:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.82
[32m[20221213 18:16:06 @agent_ppo2.py:143][0m Total time:      11.94 min
[32m[20221213 18:16:06 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 18:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 18:16:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0020 |         238.8101 |           0.5692 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0070 |         236.9717 |           0.5681 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0052 |         237.5122 |           0.5676 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0022 |         240.5264 |           0.5673 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0079 |         235.7256 |           0.5674 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0087 |         235.3086 |           0.5675 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0095 |         235.1654 |           0.5673 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0080 |         235.0440 |           0.5674 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0107 |         234.9950 |           0.5666 |
[32m[20221213 18:16:07 @agent_ppo2.py:185][0m |          -0.0095 |         234.7306 |           0.5671 |
[32m[20221213 18:16:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 914.05
[32m[20221213 18:16:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.05
[32m[20221213 18:16:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.27
[32m[20221213 18:16:08 @agent_ppo2.py:143][0m Total time:      11.96 min
[32m[20221213 18:16:08 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221213 18:16:08 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 18:16:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |           0.0037 |         236.1531 |           0.5784 |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |          -0.0043 |         233.2778 |           0.5782 |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |           0.0087 |         259.7006 |           0.5776 |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |          -0.0050 |         232.3989 |           0.5773 |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |          -0.0062 |         231.9707 |           0.5770 |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |          -0.0073 |         231.7037 |           0.5768 |
[32m[20221213 18:16:08 @agent_ppo2.py:185][0m |          -0.0019 |         235.2584 |           0.5764 |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |          -0.0067 |         230.9425 |           0.5758 |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |          -0.0071 |         231.1312 |           0.5763 |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |          -0.0014 |         233.0849 |           0.5765 |
[32m[20221213 18:16:09 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 947.82
[32m[20221213 18:16:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.18
[32m[20221213 18:16:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.91
[32m[20221213 18:16:09 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 18:16:09 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 18:16:09 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 18:16:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |          -0.0014 |         241.6924 |           0.5784 |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |           0.0005 |         240.7862 |           0.5787 |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |          -0.0030 |         237.5884 |           0.5782 |
[32m[20221213 18:16:09 @agent_ppo2.py:185][0m |          -0.0024 |         238.0727 |           0.5786 |
[32m[20221213 18:16:10 @agent_ppo2.py:185][0m |          -0.0063 |         236.5742 |           0.5784 |
[32m[20221213 18:16:10 @agent_ppo2.py:185][0m |          -0.0018 |         239.8264 |           0.5789 |
[32m[20221213 18:16:10 @agent_ppo2.py:185][0m |          -0.0075 |         235.8662 |           0.5789 |
[32m[20221213 18:16:10 @agent_ppo2.py:185][0m |          -0.0084 |         235.6281 |           0.5796 |
[32m[20221213 18:16:10 @agent_ppo2.py:185][0m |          -0.0067 |         235.7014 |           0.5792 |
[32m[20221213 18:16:10 @agent_ppo2.py:185][0m |          -0.0062 |         234.8511 |           0.5803 |
[32m[20221213 18:16:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:16:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 893.59
[32m[20221213 18:16:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.84
[32m[20221213 18:16:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.52
[32m[20221213 18:16:10 @agent_ppo2.py:143][0m Total time:      12.01 min
[32m[20221213 18:16:10 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221213 18:16:10 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 18:16:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0034 |         238.4568 |           0.5751 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0049 |         236.1308 |           0.5758 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0048 |         235.0342 |           0.5752 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0077 |         234.6399 |           0.5753 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0064 |         233.8575 |           0.5754 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0064 |         233.6840 |           0.5753 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0079 |         233.0606 |           0.5756 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0004 |         241.3187 |           0.5752 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0075 |         232.6391 |           0.5753 |
[32m[20221213 18:16:11 @agent_ppo2.py:185][0m |          -0.0090 |         232.3063 |           0.5757 |
[32m[20221213 18:16:11 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 971.54
[32m[20221213 18:16:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.69
[32m[20221213 18:16:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.71
[32m[20221213 18:16:11 @agent_ppo2.py:143][0m Total time:      12.03 min
[32m[20221213 18:16:11 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 18:16:11 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 18:16:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0018 |         238.9762 |           0.5835 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0040 |         233.3937 |           0.5819 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0065 |         231.7680 |           0.5809 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0035 |         230.8518 |           0.5811 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0065 |         230.5538 |           0.5809 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0079 |         229.7011 |           0.5802 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |           0.0045 |         253.6216 |           0.5802 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0086 |         228.8413 |           0.5795 |
[32m[20221213 18:16:12 @agent_ppo2.py:185][0m |          -0.0093 |         228.7070 |           0.5786 |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |          -0.0071 |         228.0020 |           0.5788 |
[32m[20221213 18:16:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:16:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 861.87
[32m[20221213 18:16:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.38
[32m[20221213 18:16:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.87
[32m[20221213 18:16:13 @agent_ppo2.py:143][0m Total time:      12.05 min
[32m[20221213 18:16:13 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221213 18:16:13 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 18:16:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |          -0.0004 |         238.8036 |           0.5846 |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |          -0.0007 |         236.5575 |           0.5839 |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |          -0.0036 |         234.9404 |           0.5827 |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |           0.0023 |         241.3387 |           0.5834 |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |          -0.0012 |         236.3244 |           0.5829 |
[32m[20221213 18:16:13 @agent_ppo2.py:185][0m |          -0.0055 |         234.0167 |           0.5830 |
[32m[20221213 18:16:14 @agent_ppo2.py:185][0m |          -0.0051 |         233.9089 |           0.5828 |
[32m[20221213 18:16:14 @agent_ppo2.py:185][0m |          -0.0060 |         233.5813 |           0.5836 |
[32m[20221213 18:16:14 @agent_ppo2.py:185][0m |          -0.0057 |         233.7572 |           0.5831 |
[32m[20221213 18:16:14 @agent_ppo2.py:185][0m |          -0.0068 |         233.7971 |           0.5829 |
[32m[20221213 18:16:14 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.40
[32m[20221213 18:16:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.77
[32m[20221213 18:16:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.34
[32m[20221213 18:16:14 @agent_ppo2.py:143][0m Total time:      12.07 min
[32m[20221213 18:16:14 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 18:16:14 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 18:16:14 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:16:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:14 @agent_ppo2.py:185][0m |           0.0002 |         219.7651 |           0.5707 |
[32m[20221213 18:16:14 @agent_ppo2.py:185][0m |          -0.0049 |         209.2418 |           0.5699 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0049 |         206.5523 |           0.5698 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0078 |         205.2504 |           0.5694 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0080 |         204.3948 |           0.5701 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0113 |         203.7316 |           0.5700 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0108 |         203.2584 |           0.5693 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |           0.0107 |         235.6521 |           0.5697 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0044 |         204.4757 |           0.5694 |
[32m[20221213 18:16:15 @agent_ppo2.py:185][0m |          -0.0138 |         201.3571 |           0.5699 |
[32m[20221213 18:16:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:16:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.45
[32m[20221213 18:16:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.72
[32m[20221213 18:16:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 974.31
[32m[20221213 18:16:15 @agent_ppo2.py:143][0m Total time:      12.09 min
[32m[20221213 18:16:15 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221213 18:16:15 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 18:16:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0023 |         249.2199 |           0.5612 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0031 |         238.6689 |           0.5616 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0056 |         236.6736 |           0.5611 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |           0.0032 |         243.5795 |           0.5612 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0042 |         234.4099 |           0.5609 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0054 |         233.7458 |           0.5620 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0077 |         233.2156 |           0.5614 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0066 |         233.1568 |           0.5618 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0045 |         233.6728 |           0.5617 |
[32m[20221213 18:16:16 @agent_ppo2.py:185][0m |          -0.0056 |         231.8455 |           0.5623 |
[32m[20221213 18:16:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.98
[32m[20221213 18:16:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.97
[32m[20221213 18:16:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.77
[32m[20221213 18:16:17 @agent_ppo2.py:143][0m Total time:      12.11 min
[32m[20221213 18:16:17 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 18:16:17 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 18:16:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |           0.0002 |         245.7002 |           0.5829 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0048 |         241.2889 |           0.5820 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0079 |         240.4260 |           0.5826 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0082 |         239.8560 |           0.5820 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0089 |         239.0882 |           0.5821 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0024 |         250.3757 |           0.5816 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0101 |         238.1606 |           0.5816 |
[32m[20221213 18:16:17 @agent_ppo2.py:185][0m |          -0.0110 |         238.2328 |           0.5814 |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0105 |         237.6568 |           0.5820 |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0067 |         238.3060 |           0.5816 |
[32m[20221213 18:16:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.36
[32m[20221213 18:16:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.76
[32m[20221213 18:16:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 931.94
[32m[20221213 18:16:18 @agent_ppo2.py:143][0m Total time:      12.13 min
[32m[20221213 18:16:18 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221213 18:16:18 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 18:16:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0025 |         246.1423 |           0.5831 |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0049 |         243.5248 |           0.5827 |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0056 |         242.7847 |           0.5823 |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0077 |         241.4739 |           0.5824 |
[32m[20221213 18:16:18 @agent_ppo2.py:185][0m |          -0.0079 |         240.4981 |           0.5817 |
[32m[20221213 18:16:19 @agent_ppo2.py:185][0m |          -0.0008 |         250.5799 |           0.5824 |
[32m[20221213 18:16:19 @agent_ppo2.py:185][0m |           0.0104 |         261.9813 |           0.5825 |
[32m[20221213 18:16:19 @agent_ppo2.py:185][0m |          -0.0087 |         239.0769 |           0.5829 |
[32m[20221213 18:16:19 @agent_ppo2.py:185][0m |          -0.0100 |         238.4841 |           0.5819 |
[32m[20221213 18:16:19 @agent_ppo2.py:185][0m |          -0.0101 |         238.0067 |           0.5821 |
[32m[20221213 18:16:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:16:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.69
[32m[20221213 18:16:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.96
[32m[20221213 18:16:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.46
[32m[20221213 18:16:19 @agent_ppo2.py:143][0m Total time:      12.16 min
[32m[20221213 18:16:19 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 18:16:19 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 18:16:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:19 @agent_ppo2.py:185][0m |           0.0071 |         253.2101 |           0.5640 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0048 |         241.9185 |           0.5638 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0056 |         240.1601 |           0.5630 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0072 |         239.3053 |           0.5620 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0011 |         245.5463 |           0.5619 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0070 |         238.6047 |           0.5616 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0081 |         237.9659 |           0.5614 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |           0.0011 |         263.0985 |           0.5609 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0031 |         238.1616 |           0.5595 |
[32m[20221213 18:16:20 @agent_ppo2.py:185][0m |          -0.0019 |         245.8452 |           0.5608 |
[32m[20221213 18:16:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:16:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 923.62
[32m[20221213 18:16:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.50
[32m[20221213 18:16:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.39
[32m[20221213 18:16:20 @agent_ppo2.py:143][0m Total time:      12.18 min
[32m[20221213 18:16:20 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221213 18:16:20 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 18:16:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0023 |         249.5356 |           0.5671 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0025 |         246.5882 |           0.5664 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0050 |         245.7077 |           0.5665 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0078 |         244.1432 |           0.5656 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0029 |         246.6885 |           0.5664 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0070 |         243.2463 |           0.5661 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0092 |         242.9929 |           0.5665 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |           0.0033 |         265.0614 |           0.5654 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0089 |         242.3706 |           0.5650 |
[32m[20221213 18:16:21 @agent_ppo2.py:185][0m |          -0.0084 |         242.2315 |           0.5655 |
[32m[20221213 18:16:21 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 878.30
[32m[20221213 18:16:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.39
[32m[20221213 18:16:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.10
[32m[20221213 18:16:22 @agent_ppo2.py:143][0m Total time:      12.20 min
[32m[20221213 18:16:22 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 18:16:22 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 18:16:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0039 |         252.8276 |           0.5759 |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0057 |         247.4575 |           0.5746 |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0048 |         243.6007 |           0.5742 |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0067 |         242.0530 |           0.5742 |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0078 |         241.0042 |           0.5735 |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0082 |         240.4624 |           0.5729 |
[32m[20221213 18:16:22 @agent_ppo2.py:185][0m |          -0.0062 |         239.9642 |           0.5724 |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |          -0.0093 |         239.3859 |           0.5724 |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |          -0.0082 |         239.5013 |           0.5716 |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |          -0.0097 |         238.8896 |           0.5718 |
[32m[20221213 18:16:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.79
[32m[20221213 18:16:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.06
[32m[20221213 18:16:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 974.06
[32m[20221213 18:16:23 @agent_ppo2.py:143][0m Total time:      12.22 min
[32m[20221213 18:16:23 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221213 18:16:23 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 18:16:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |          -0.0020 |         217.9654 |           0.5693 |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |          -0.0065 |         214.5821 |           0.5688 |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |           0.0040 |         230.5146 |           0.5694 |
[32m[20221213 18:16:23 @agent_ppo2.py:185][0m |          -0.0097 |         211.8689 |           0.5693 |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0074 |         209.5852 |           0.5695 |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0106 |         208.0665 |           0.5693 |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0101 |         206.2732 |           0.5698 |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0050 |         206.3121 |           0.5697 |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0106 |         203.2280 |           0.5695 |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0078 |         202.2014 |           0.5698 |
[32m[20221213 18:16:24 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.00
[32m[20221213 18:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.56
[32m[20221213 18:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.22
[32m[20221213 18:16:24 @agent_ppo2.py:143][0m Total time:      12.24 min
[32m[20221213 18:16:24 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 18:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 18:16:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:24 @agent_ppo2.py:185][0m |          -0.0006 |         221.2799 |           0.5747 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0079 |         215.9361 |           0.5741 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0038 |         214.9551 |           0.5743 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0095 |         213.6318 |           0.5741 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0084 |         212.8753 |           0.5745 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0079 |         212.2234 |           0.5747 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0030 |         212.0766 |           0.5746 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0115 |         211.6768 |           0.5751 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0061 |         211.7274 |           0.5751 |
[32m[20221213 18:16:25 @agent_ppo2.py:185][0m |          -0.0089 |         210.8890 |           0.5756 |
[32m[20221213 18:16:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.38
[32m[20221213 18:16:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.91
[32m[20221213 18:16:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.52
[32m[20221213 18:16:25 @agent_ppo2.py:143][0m Total time:      12.26 min
[32m[20221213 18:16:25 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221213 18:16:25 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 18:16:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0019 |         254.8383 |           0.5796 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0003 |         256.0222 |           0.5784 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0092 |         241.6604 |           0.5778 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0082 |         239.5879 |           0.5783 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0119 |         238.7506 |           0.5778 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |           0.0004 |         257.8990 |           0.5779 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0026 |         250.8918 |           0.5776 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0128 |         237.1003 |           0.5781 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0147 |         236.8946 |           0.5779 |
[32m[20221213 18:16:26 @agent_ppo2.py:185][0m |          -0.0088 |         247.6973 |           0.5783 |
[32m[20221213 18:16:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 860.19
[32m[20221213 18:16:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.27
[32m[20221213 18:16:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 930.79
[32m[20221213 18:16:27 @agent_ppo2.py:143][0m Total time:      12.28 min
[32m[20221213 18:16:27 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 18:16:27 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 18:16:27 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:16:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:27 @agent_ppo2.py:185][0m |           0.0075 |         263.6751 |           0.5884 |
[32m[20221213 18:16:27 @agent_ppo2.py:185][0m |          -0.0055 |         243.7120 |           0.5898 |
[32m[20221213 18:16:27 @agent_ppo2.py:185][0m |          -0.0073 |         241.5824 |           0.5897 |
[32m[20221213 18:16:27 @agent_ppo2.py:185][0m |          -0.0028 |         240.8140 |           0.5899 |
[32m[20221213 18:16:27 @agent_ppo2.py:185][0m |          -0.0103 |         239.1462 |           0.5901 |
[32m[20221213 18:16:27 @agent_ppo2.py:185][0m |          -0.0102 |         238.2378 |           0.5905 |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0092 |         237.6302 |           0.5914 |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0100 |         237.0631 |           0.5915 |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0092 |         236.5751 |           0.5920 |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0100 |         236.8435 |           0.5919 |
[32m[20221213 18:16:28 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 871.94
[32m[20221213 18:16:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.27
[32m[20221213 18:16:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.47
[32m[20221213 18:16:28 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221213 18:16:28 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221213 18:16:28 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 18:16:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0018 |         228.1948 |           0.5907 |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0019 |         227.3320 |           0.5896 |
[32m[20221213 18:16:28 @agent_ppo2.py:185][0m |          -0.0079 |         223.9999 |           0.5898 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |           0.0002 |         235.0320 |           0.5901 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |          -0.0084 |         223.2942 |           0.5906 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |          -0.0095 |         222.3828 |           0.5900 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |          -0.0104 |         222.3098 |           0.5899 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |          -0.0100 |         221.8932 |           0.5904 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |          -0.0107 |         221.6443 |           0.5901 |
[32m[20221213 18:16:29 @agent_ppo2.py:185][0m |          -0.0101 |         221.6262 |           0.5899 |
[32m[20221213 18:16:29 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 907.78
[32m[20221213 18:16:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.48
[32m[20221213 18:16:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.83
[32m[20221213 18:16:29 @agent_ppo2.py:143][0m Total time:      12.32 min
[32m[20221213 18:16:29 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 18:16:29 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 18:16:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0018 |         217.0881 |           0.5894 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0021 |         211.2309 |           0.5889 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |           0.0008 |         210.2812 |           0.5876 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0039 |         206.2637 |           0.5891 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0062 |         202.9877 |           0.5890 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0084 |         202.7374 |           0.5887 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0084 |         202.5098 |           0.5892 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0075 |         201.9570 |           0.5897 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0084 |         201.4786 |           0.5891 |
[32m[20221213 18:16:30 @agent_ppo2.py:185][0m |          -0.0046 |         209.1167 |           0.5900 |
[32m[20221213 18:16:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.89
[32m[20221213 18:16:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.30
[32m[20221213 18:16:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.31
[32m[20221213 18:16:30 @agent_ppo2.py:143][0m Total time:      12.34 min
[32m[20221213 18:16:30 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221213 18:16:30 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 18:16:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |           0.0030 |         240.9083 |           0.6130 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0068 |         234.0024 |           0.6112 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0087 |         232.0050 |           0.6103 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0051 |         231.3430 |           0.6098 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0098 |         229.6183 |           0.6095 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0092 |         229.0855 |           0.6087 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0097 |         228.3246 |           0.6087 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0105 |         227.8845 |           0.6084 |
[32m[20221213 18:16:31 @agent_ppo2.py:185][0m |          -0.0122 |         227.5208 |           0.6080 |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |          -0.0132 |         227.4355 |           0.6074 |
[32m[20221213 18:16:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 860.91
[32m[20221213 18:16:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.58
[32m[20221213 18:16:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.84
[32m[20221213 18:16:32 @agent_ppo2.py:143][0m Total time:      12.37 min
[32m[20221213 18:16:32 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 18:16:32 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 18:16:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |           0.0010 |         243.6401 |           0.6109 |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |          -0.0055 |         239.1929 |           0.6103 |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |          -0.0073 |         236.9205 |           0.6106 |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |          -0.0060 |         235.5642 |           0.6104 |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |          -0.0034 |         236.4452 |           0.6106 |
[32m[20221213 18:16:32 @agent_ppo2.py:185][0m |          -0.0032 |         237.3962 |           0.6102 |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |           0.0020 |         254.3657 |           0.6105 |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |          -0.0066 |         233.0221 |           0.6100 |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |          -0.0092 |         232.8286 |           0.6102 |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |          -0.0032 |         236.1286 |           0.6100 |
[32m[20221213 18:16:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 905.04
[32m[20221213 18:16:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.33
[32m[20221213 18:16:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 944.29
[32m[20221213 18:16:33 @agent_ppo2.py:143][0m Total time:      12.39 min
[32m[20221213 18:16:33 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221213 18:16:33 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 18:16:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |          -0.0003 |         241.3500 |           0.5933 |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |          -0.0045 |         235.7621 |           0.5922 |
[32m[20221213 18:16:33 @agent_ppo2.py:185][0m |          -0.0075 |         233.7307 |           0.5922 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0040 |         233.7173 |           0.5923 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0036 |         234.3659 |           0.5915 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0086 |         231.5836 |           0.5915 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0086 |         231.2846 |           0.5918 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0078 |         231.0161 |           0.5919 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0109 |         230.3779 |           0.5917 |
[32m[20221213 18:16:34 @agent_ppo2.py:185][0m |          -0.0099 |         230.0542 |           0.5918 |
[32m[20221213 18:16:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.07
[32m[20221213 18:16:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.26
[32m[20221213 18:16:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.57
[32m[20221213 18:16:34 @agent_ppo2.py:143][0m Total time:      12.41 min
[32m[20221213 18:16:34 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 18:16:34 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 18:16:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |           0.0124 |         254.2581 |           0.5952 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0048 |         226.2779 |           0.5948 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0067 |         224.6000 |           0.5944 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0072 |         223.9918 |           0.5936 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0073 |         223.9208 |           0.5941 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0089 |         223.9685 |           0.5936 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0078 |         223.3148 |           0.5941 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0064 |         223.0638 |           0.5937 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0094 |         223.2432 |           0.5937 |
[32m[20221213 18:16:35 @agent_ppo2.py:185][0m |          -0.0095 |         222.8893 |           0.5937 |
[32m[20221213 18:16:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 932.86
[32m[20221213 18:16:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.55
[32m[20221213 18:16:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.98
[32m[20221213 18:16:35 @agent_ppo2.py:143][0m Total time:      12.43 min
[32m[20221213 18:16:35 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221213 18:16:35 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 18:16:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |           0.0010 |         233.8542 |           0.6059 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |          -0.0007 |         232.9883 |           0.6062 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |           0.0066 |         240.6846 |           0.6057 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |           0.0001 |         234.5901 |           0.6065 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |          -0.0056 |         229.8458 |           0.6058 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |          -0.0072 |         229.8967 |           0.6067 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |          -0.0062 |         229.0990 |           0.6058 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |           0.0013 |         233.3664 |           0.6063 |
[32m[20221213 18:16:36 @agent_ppo2.py:185][0m |          -0.0092 |         228.7774 |           0.6065 |
[32m[20221213 18:16:37 @agent_ppo2.py:185][0m |          -0.0080 |         228.6131 |           0.6068 |
[32m[20221213 18:16:37 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 917.55
[32m[20221213 18:16:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.96
[32m[20221213 18:16:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.89
[32m[20221213 18:16:37 @agent_ppo2.py:143][0m Total time:      12.45 min
[32m[20221213 18:16:37 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 18:16:37 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 18:16:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:37 @agent_ppo2.py:185][0m |          -0.0007 |         239.0539 |           0.6088 |
[32m[20221213 18:16:37 @agent_ppo2.py:185][0m |           0.0025 |         248.1769 |           0.6062 |
[32m[20221213 18:16:37 @agent_ppo2.py:185][0m |           0.0033 |         250.3970 |           0.6049 |
[32m[20221213 18:16:37 @agent_ppo2.py:185][0m |           0.0056 |         261.4902 |           0.6046 |
[32m[20221213 18:16:37 @agent_ppo2.py:185][0m |          -0.0085 |         233.7434 |           0.6036 |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0071 |         232.2914 |           0.6041 |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0095 |         231.9430 |           0.6041 |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0083 |         231.8112 |           0.6041 |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0101 |         231.5259 |           0.6037 |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0103 |         230.4729 |           0.6041 |
[32m[20221213 18:16:38 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 895.11
[32m[20221213 18:16:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.71
[32m[20221213 18:16:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.72
[32m[20221213 18:16:38 @agent_ppo2.py:143][0m Total time:      12.47 min
[32m[20221213 18:16:38 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221213 18:16:38 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 18:16:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0008 |         236.1472 |           0.5968 |
[32m[20221213 18:16:38 @agent_ppo2.py:185][0m |          -0.0041 |         233.4550 |           0.5958 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0026 |         233.1678 |           0.5961 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0046 |         232.9736 |           0.5955 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0053 |         232.7008 |           0.5953 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |           0.0003 |         241.3895 |           0.5953 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0050 |         232.7192 |           0.5942 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0051 |         232.7506 |           0.5947 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0051 |         232.6566 |           0.5951 |
[32m[20221213 18:16:39 @agent_ppo2.py:185][0m |          -0.0047 |         232.3553 |           0.5948 |
[32m[20221213 18:16:39 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:16:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.48
[32m[20221213 18:16:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.68
[32m[20221213 18:16:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.51
[32m[20221213 18:16:39 @agent_ppo2.py:143][0m Total time:      12.49 min
[32m[20221213 18:16:39 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 18:16:39 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 18:16:39 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:16:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0029 |         235.1563 |           0.6062 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |           0.0021 |         243.9951 |           0.6045 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0052 |         231.9672 |           0.6042 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0052 |         231.4814 |           0.6032 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0003 |         233.4302 |           0.6039 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0068 |         230.5099 |           0.6023 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |           0.0065 |         259.7678 |           0.6023 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0078 |         229.9452 |           0.6019 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0089 |         229.7913 |           0.6008 |
[32m[20221213 18:16:40 @agent_ppo2.py:185][0m |          -0.0076 |         229.2308 |           0.6013 |
[32m[20221213 18:16:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.04
[32m[20221213 18:16:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 985.76
[32m[20221213 18:16:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 919.31
[32m[20221213 18:16:41 @agent_ppo2.py:143][0m Total time:      12.51 min
[32m[20221213 18:16:41 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221213 18:16:41 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 18:16:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0019 |         247.9623 |           0.5980 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |           0.0025 |         252.7392 |           0.5965 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0053 |         243.4731 |           0.5979 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0075 |         242.9777 |           0.5970 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0089 |         242.2228 |           0.5982 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0088 |         241.6968 |           0.5979 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0091 |         241.5143 |           0.5978 |
[32m[20221213 18:16:41 @agent_ppo2.py:185][0m |          -0.0086 |         241.1993 |           0.5986 |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0110 |         241.0295 |           0.5990 |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0114 |         240.6093 |           0.5992 |
[32m[20221213 18:16:42 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.28
[32m[20221213 18:16:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 909.64
[32m[20221213 18:16:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 908.15
[32m[20221213 18:16:42 @agent_ppo2.py:143][0m Total time:      12.53 min
[32m[20221213 18:16:42 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 18:16:42 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 18:16:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0018 |         277.0478 |           0.5930 |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0052 |         270.9817 |           0.5930 |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0076 |         270.0922 |           0.5929 |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0078 |         269.2721 |           0.5931 |
[32m[20221213 18:16:42 @agent_ppo2.py:185][0m |          -0.0083 |         268.7731 |           0.5922 |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0082 |         268.7485 |           0.5923 |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0072 |         268.2172 |           0.5925 |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0048 |         268.2725 |           0.5927 |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0079 |         267.4710 |           0.5927 |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0095 |         267.4653 |           0.5925 |
[32m[20221213 18:16:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 888.44
[32m[20221213 18:16:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.05
[32m[20221213 18:16:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.32
[32m[20221213 18:16:43 @agent_ppo2.py:143][0m Total time:      12.55 min
[32m[20221213 18:16:43 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221213 18:16:43 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 18:16:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0008 |         262.1263 |           0.6039 |
[32m[20221213 18:16:43 @agent_ppo2.py:185][0m |          -0.0025 |         253.5359 |           0.6036 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0058 |         247.4851 |           0.6027 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0037 |         244.3867 |           0.6026 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0044 |         243.0553 |           0.6022 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0057 |         242.0286 |           0.6018 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0025 |         250.2951 |           0.6018 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0072 |         241.2260 |           0.6006 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0061 |         240.6246 |           0.6005 |
[32m[20221213 18:16:44 @agent_ppo2.py:185][0m |          -0.0069 |         239.9419 |           0.5999 |
[32m[20221213 18:16:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 878.07
[32m[20221213 18:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.65
[32m[20221213 18:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.00
[32m[20221213 18:16:44 @agent_ppo2.py:143][0m Total time:      12.58 min
[32m[20221213 18:16:44 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 18:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 18:16:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0045 |         251.3391 |           0.5868 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0063 |         248.5713 |           0.5858 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0045 |         250.0414 |           0.5858 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0092 |         248.0768 |           0.5849 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0092 |         246.8727 |           0.5847 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0066 |         247.1632 |           0.5845 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0115 |         246.0733 |           0.5845 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0033 |         252.0633 |           0.5840 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0114 |         245.9161 |           0.5832 |
[32m[20221213 18:16:45 @agent_ppo2.py:185][0m |          -0.0108 |         245.5322 |           0.5829 |
[32m[20221213 18:16:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.56
[32m[20221213 18:16:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.73
[32m[20221213 18:16:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 993.74
[32m[20221213 18:16:46 @agent_ppo2.py:143][0m Total time:      12.60 min
[32m[20221213 18:16:46 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221213 18:16:46 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 18:16:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |          -0.0021 |         250.9846 |           0.5745 |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |          -0.0054 |         246.8094 |           0.5742 |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |          -0.0065 |         244.7670 |           0.5745 |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |          -0.0072 |         243.7003 |           0.5746 |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |           0.0045 |         273.4531 |           0.5749 |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |           0.0005 |         256.0477 |           0.5743 |
[32m[20221213 18:16:46 @agent_ppo2.py:185][0m |          -0.0089 |         241.0546 |           0.5745 |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |          -0.0029 |         246.9555 |           0.5756 |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |          -0.0105 |         240.6681 |           0.5749 |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |          -0.0112 |         240.3741 |           0.5761 |
[32m[20221213 18:16:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 896.17
[32m[20221213 18:16:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.32
[32m[20221213 18:16:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.05
[32m[20221213 18:16:47 @agent_ppo2.py:143][0m Total time:      12.62 min
[32m[20221213 18:16:47 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 18:16:47 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 18:16:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |           0.0086 |         257.5619 |           0.5958 |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |           0.0138 |         272.2448 |           0.5939 |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |          -0.0023 |         237.8538 |           0.5919 |
[32m[20221213 18:16:47 @agent_ppo2.py:185][0m |          -0.0043 |         237.4708 |           0.5933 |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |          -0.0062 |         237.1105 |           0.5941 |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |          -0.0057 |         236.9126 |           0.5944 |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |          -0.0055 |         236.5653 |           0.5947 |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |          -0.0053 |         236.7256 |           0.5942 |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |          -0.0063 |         236.3860 |           0.5951 |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |          -0.0026 |         237.9282 |           0.5945 |
[32m[20221213 18:16:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.14
[32m[20221213 18:16:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.34
[32m[20221213 18:16:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.89
[32m[20221213 18:16:48 @agent_ppo2.py:143][0m Total time:      12.64 min
[32m[20221213 18:16:48 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221213 18:16:48 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 18:16:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:48 @agent_ppo2.py:185][0m |           0.0009 |         230.9917 |           0.6007 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0021 |         226.7219 |           0.6001 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0097 |         223.1796 |           0.5998 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0070 |         222.7111 |           0.5997 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0087 |         222.2020 |           0.5999 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0102 |         222.1230 |           0.5999 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0029 |         226.9865 |           0.5998 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |           0.0011 |         242.0887 |           0.5997 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0069 |         224.0302 |           0.5994 |
[32m[20221213 18:16:49 @agent_ppo2.py:185][0m |          -0.0088 |         220.5535 |           0.5997 |
[32m[20221213 18:16:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.85
[32m[20221213 18:16:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.50
[32m[20221213 18:16:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.42
[32m[20221213 18:16:49 @agent_ppo2.py:143][0m Total time:      12.66 min
[32m[20221213 18:16:49 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 18:16:49 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 18:16:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0030 |         254.2455 |           0.5933 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0055 |         248.4806 |           0.5926 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0054 |         247.5169 |           0.5917 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0072 |         246.4524 |           0.5920 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |           0.0141 |         282.4526 |           0.5916 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |           0.0027 |         270.8937 |           0.5908 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0074 |         245.0150 |           0.5905 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0091 |         244.1994 |           0.5906 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0085 |         244.2562 |           0.5907 |
[32m[20221213 18:16:50 @agent_ppo2.py:185][0m |          -0.0081 |         243.9488 |           0.5897 |
[32m[20221213 18:16:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 952.77
[32m[20221213 18:16:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.98
[32m[20221213 18:16:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.71
[32m[20221213 18:16:51 @agent_ppo2.py:143][0m Total time:      12.68 min
[32m[20221213 18:16:51 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221213 18:16:51 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 18:16:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:16:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |           0.0156 |         285.6312 |           0.5816 |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |           0.0108 |         263.7478 |           0.5804 |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |          -0.0052 |         243.6115 |           0.5798 |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |          -0.0038 |         243.3526 |           0.5791 |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |           0.0022 |         257.3018 |           0.5783 |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |          -0.0089 |         241.2353 |           0.5776 |
[32m[20221213 18:16:51 @agent_ppo2.py:185][0m |          -0.0094 |         240.6392 |           0.5770 |
[32m[20221213 18:16:52 @agent_ppo2.py:185][0m |          -0.0084 |         240.2193 |           0.5765 |
[32m[20221213 18:16:52 @agent_ppo2.py:185][0m |          -0.0101 |         240.0575 |           0.5764 |
[32m[20221213 18:16:52 @agent_ppo2.py:185][0m |          -0.0103 |         239.8510 |           0.5758 |
[32m[20221213 18:16:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 903.22
[32m[20221213 18:16:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.61
[32m[20221213 18:16:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.81
[32m[20221213 18:16:52 @agent_ppo2.py:143][0m Total time:      12.70 min
[32m[20221213 18:16:52 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 18:16:52 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 18:16:52 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:16:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:52 @agent_ppo2.py:185][0m |          -0.0018 |         246.1061 |           0.5849 |
[32m[20221213 18:16:52 @agent_ppo2.py:185][0m |          -0.0003 |         242.8807 |           0.5846 |
[32m[20221213 18:16:52 @agent_ppo2.py:185][0m |           0.0058 |         256.3825 |           0.5845 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |           0.0025 |         244.6048 |           0.5839 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |          -0.0040 |         240.0209 |           0.5836 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |          -0.0066 |         239.4271 |           0.5837 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |          -0.0063 |         239.3641 |           0.5843 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |          -0.0063 |         239.0091 |           0.5843 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |          -0.0003 |         247.0941 |           0.5845 |
[32m[20221213 18:16:53 @agent_ppo2.py:185][0m |          -0.0075 |         239.3290 |           0.5840 |
[32m[20221213 18:16:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.85
[32m[20221213 18:16:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.99
[32m[20221213 18:16:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 910.84
[32m[20221213 18:16:53 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221213 18:16:53 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221213 18:16:53 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 18:16:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0012 |         245.0369 |           0.5719 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0012 |         244.8961 |           0.5719 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0059 |         242.5257 |           0.5707 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0061 |         241.6598 |           0.5711 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0054 |         241.1532 |           0.5706 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0071 |         241.0561 |           0.5705 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0068 |         241.2064 |           0.5709 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0055 |         240.6643 |           0.5708 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0081 |         240.3200 |           0.5710 |
[32m[20221213 18:16:54 @agent_ppo2.py:185][0m |          -0.0080 |         240.5291 |           0.5705 |
[32m[20221213 18:16:54 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:16:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.64
[32m[20221213 18:16:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.16
[32m[20221213 18:16:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.73
[32m[20221213 18:16:54 @agent_ppo2.py:143][0m Total time:      12.74 min
[32m[20221213 18:16:54 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 18:16:54 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 18:16:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0017 |         252.3908 |           0.6055 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0055 |         248.7199 |           0.6054 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0076 |         247.5512 |           0.6050 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0023 |         249.5100 |           0.6044 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0077 |         246.1180 |           0.6043 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0084 |         245.6955 |           0.6039 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0075 |         245.4840 |           0.6036 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0085 |         245.2006 |           0.6030 |
[32m[20221213 18:16:55 @agent_ppo2.py:185][0m |          -0.0026 |         250.9874 |           0.6034 |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |          -0.0085 |         244.8520 |           0.6023 |
[32m[20221213 18:16:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:16:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.16
[32m[20221213 18:16:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 951.66
[32m[20221213 18:16:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.62
[32m[20221213 18:16:56 @agent_ppo2.py:143][0m Total time:      12.77 min
[32m[20221213 18:16:56 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221213 18:16:56 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 18:16:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |           0.0037 |         245.4393 |           0.5749 |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |          -0.0033 |         238.3226 |           0.5746 |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |          -0.0036 |         236.5496 |           0.5751 |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |          -0.0037 |         235.6083 |           0.5740 |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |          -0.0014 |         237.1424 |           0.5738 |
[32m[20221213 18:16:56 @agent_ppo2.py:185][0m |          -0.0060 |         234.5241 |           0.5737 |
[32m[20221213 18:16:57 @agent_ppo2.py:185][0m |          -0.0042 |         234.2946 |           0.5740 |
[32m[20221213 18:16:57 @agent_ppo2.py:185][0m |          -0.0073 |         233.6415 |           0.5738 |
[32m[20221213 18:16:57 @agent_ppo2.py:185][0m |          -0.0068 |         233.4811 |           0.5734 |
[32m[20221213 18:16:57 @agent_ppo2.py:185][0m |          -0.0028 |         237.9197 |           0.5731 |
[32m[20221213 18:16:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:16:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 920.19
[32m[20221213 18:16:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.08
[32m[20221213 18:16:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 898.09
[32m[20221213 18:16:57 @agent_ppo2.py:143][0m Total time:      12.79 min
[32m[20221213 18:16:57 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 18:16:57 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 18:16:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:57 @agent_ppo2.py:185][0m |          -0.0023 |         256.5040 |           0.5657 |
[32m[20221213 18:16:57 @agent_ppo2.py:185][0m |          -0.0032 |         247.9813 |           0.5656 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0044 |         245.5501 |           0.5650 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0042 |         244.3456 |           0.5648 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0053 |         243.3892 |           0.5650 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0069 |         242.8819 |           0.5652 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0082 |         242.2348 |           0.5658 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0048 |         241.8055 |           0.5654 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0081 |         241.7332 |           0.5650 |
[32m[20221213 18:16:58 @agent_ppo2.py:185][0m |          -0.0067 |         241.0903 |           0.5655 |
[32m[20221213 18:16:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:16:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.74
[32m[20221213 18:16:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.79
[32m[20221213 18:16:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.83
[32m[20221213 18:16:58 @agent_ppo2.py:143][0m Total time:      12.81 min
[32m[20221213 18:16:58 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221213 18:16:58 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 18:16:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:16:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0009 |         242.8241 |           0.5728 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0035 |         239.4005 |           0.5720 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |           0.0066 |         260.1626 |           0.5717 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0056 |         238.0997 |           0.5714 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0012 |         238.1999 |           0.5710 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0063 |         237.2396 |           0.5708 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0069 |         236.9699 |           0.5704 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0065 |         237.2972 |           0.5704 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0075 |         236.4020 |           0.5700 |
[32m[20221213 18:16:59 @agent_ppo2.py:185][0m |          -0.0081 |         236.2897 |           0.5695 |
[32m[20221213 18:16:59 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.21
[32m[20221213 18:17:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.95
[32m[20221213 18:17:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.31
[32m[20221213 18:17:00 @agent_ppo2.py:143][0m Total time:      12.83 min
[32m[20221213 18:17:00 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 18:17:00 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 18:17:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |           0.0004 |         245.3119 |           0.5728 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |           0.0061 |         273.9984 |           0.5723 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |          -0.0051 |         242.1817 |           0.5709 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |          -0.0055 |         240.7756 |           0.5714 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |          -0.0073 |         240.8140 |           0.5717 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |          -0.0075 |         239.8172 |           0.5713 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |          -0.0084 |         239.9067 |           0.5714 |
[32m[20221213 18:17:00 @agent_ppo2.py:185][0m |          -0.0009 |         250.4737 |           0.5719 |
[32m[20221213 18:17:01 @agent_ppo2.py:185][0m |          -0.0034 |         245.1027 |           0.5706 |
[32m[20221213 18:17:01 @agent_ppo2.py:185][0m |          -0.0052 |         239.7153 |           0.5715 |
[32m[20221213 18:17:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.88
[32m[20221213 18:17:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.56
[32m[20221213 18:17:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.76
[32m[20221213 18:17:01 @agent_ppo2.py:143][0m Total time:      12.85 min
[32m[20221213 18:17:01 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221213 18:17:01 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 18:17:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:17:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:01 @agent_ppo2.py:185][0m |          -0.0023 |         241.1494 |           0.5765 |
[32m[20221213 18:17:01 @agent_ppo2.py:185][0m |          -0.0030 |         238.5698 |           0.5769 |
[32m[20221213 18:17:01 @agent_ppo2.py:185][0m |          -0.0042 |         237.6063 |           0.5769 |
[32m[20221213 18:17:01 @agent_ppo2.py:185][0m |          -0.0045 |         237.3508 |           0.5773 |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |          -0.0058 |         236.8994 |           0.5770 |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |           0.0063 |         250.0289 |           0.5779 |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |          -0.0011 |         244.3071 |           0.5780 |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |          -0.0063 |         236.7306 |           0.5776 |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |           0.0016 |         251.0264 |           0.5785 |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |          -0.0070 |         236.1750 |           0.5778 |
[32m[20221213 18:17:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:17:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 953.09
[32m[20221213 18:17:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.85
[32m[20221213 18:17:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.20
[32m[20221213 18:17:02 @agent_ppo2.py:143][0m Total time:      12.87 min
[32m[20221213 18:17:02 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 18:17:02 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 18:17:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:02 @agent_ppo2.py:185][0m |           0.0142 |         274.7768 |           0.5961 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0056 |         244.9215 |           0.5953 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0043 |         243.6216 |           0.5958 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0070 |         242.5777 |           0.5960 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0076 |         241.0025 |           0.5956 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0075 |         240.2734 |           0.5962 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0015 |         243.7954 |           0.5968 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0081 |         239.4667 |           0.5963 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0094 |         238.7752 |           0.5964 |
[32m[20221213 18:17:03 @agent_ppo2.py:185][0m |          -0.0007 |         250.3643 |           0.5958 |
[32m[20221213 18:17:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:17:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 947.93
[32m[20221213 18:17:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.96
[32m[20221213 18:17:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.78
[32m[20221213 18:17:03 @agent_ppo2.py:143][0m Total time:      12.89 min
[32m[20221213 18:17:03 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221213 18:17:03 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 18:17:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |          -0.0022 |         243.0431 |           0.5897 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |           0.0024 |         246.2617 |           0.5895 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |           0.0036 |         248.7626 |           0.5889 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |          -0.0036 |         240.9309 |           0.5896 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |          -0.0050 |         239.8136 |           0.5896 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |          -0.0039 |         240.1376 |           0.5896 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |          -0.0062 |         239.6479 |           0.5889 |
[32m[20221213 18:17:04 @agent_ppo2.py:185][0m |          -0.0079 |         239.4930 |           0.5892 |
[32m[20221213 18:17:05 @agent_ppo2.py:185][0m |          -0.0058 |         239.1116 |           0.5890 |
[32m[20221213 18:17:05 @agent_ppo2.py:185][0m |          -0.0081 |         239.1466 |           0.5892 |
[32m[20221213 18:17:05 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:17:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 954.53
[32m[20221213 18:17:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.05
[32m[20221213 18:17:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 879.62
[32m[20221213 18:17:05 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221213 18:17:05 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 18:17:05 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 18:17:05 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:17:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:05 @agent_ppo2.py:185][0m |           0.0072 |         253.9854 |           0.5920 |
[32m[20221213 18:17:05 @agent_ppo2.py:185][0m |          -0.0005 |         241.4888 |           0.5919 |
[32m[20221213 18:17:05 @agent_ppo2.py:185][0m |          -0.0054 |         236.0037 |           0.5922 |
[32m[20221213 18:17:05 @agent_ppo2.py:185][0m |          -0.0061 |         235.5576 |           0.5922 |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0067 |         234.9295 |           0.5915 |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0013 |         236.7539 |           0.5917 |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0098 |         234.0722 |           0.5916 |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0033 |         236.6341 |           0.5917 |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0090 |         233.9121 |           0.5920 |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0088 |         233.4569 |           0.5919 |
[32m[20221213 18:17:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:17:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 872.65
[32m[20221213 18:17:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.14
[32m[20221213 18:17:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.90
[32m[20221213 18:17:06 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221213 18:17:06 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221213 18:17:06 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 18:17:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:06 @agent_ppo2.py:185][0m |          -0.0018 |         240.4263 |           0.6147 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0044 |         234.4538 |           0.6141 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0078 |         231.4853 |           0.6141 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0078 |         230.0316 |           0.6131 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0088 |         229.5861 |           0.6138 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0057 |         230.8125 |           0.6130 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0088 |         228.9011 |           0.6128 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |           0.0014 |         245.1381 |           0.6127 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0060 |         229.4906 |           0.6132 |
[32m[20221213 18:17:07 @agent_ppo2.py:185][0m |          -0.0097 |         228.4593 |           0.6125 |
[32m[20221213 18:17:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:17:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 958.51
[32m[20221213 18:17:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.60
[32m[20221213 18:17:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 934.76
[32m[20221213 18:17:07 @agent_ppo2.py:143][0m Total time:      12.96 min
[32m[20221213 18:17:07 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 18:17:07 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 18:17:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |           0.0032 |         238.4650 |           0.6078 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0059 |         233.5764 |           0.6060 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0072 |         232.7551 |           0.6056 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0075 |         232.1994 |           0.6052 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0093 |         231.8826 |           0.6048 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0098 |         231.8442 |           0.6040 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |           0.0028 |         248.7618 |           0.6038 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0086 |         231.2741 |           0.6033 |
[32m[20221213 18:17:08 @agent_ppo2.py:185][0m |          -0.0089 |         231.2798 |           0.6032 |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0104 |         230.7369 |           0.6032 |
[32m[20221213 18:17:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.27
[32m[20221213 18:17:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.25
[32m[20221213 18:17:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 967.53
[32m[20221213 18:17:09 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221213 18:17:09 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221213 18:17:09 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 18:17:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0053 |         243.8340 |           0.6053 |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0075 |         239.6069 |           0.6031 |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0059 |         237.3617 |           0.6035 |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0099 |         236.9052 |           0.6027 |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0100 |         236.1532 |           0.6026 |
[32m[20221213 18:17:09 @agent_ppo2.py:185][0m |          -0.0104 |         236.1857 |           0.6023 |
[32m[20221213 18:17:10 @agent_ppo2.py:185][0m |          -0.0132 |         235.2398 |           0.6017 |
[32m[20221213 18:17:10 @agent_ppo2.py:185][0m |          -0.0088 |         234.7472 |           0.6017 |
[32m[20221213 18:17:10 @agent_ppo2.py:185][0m |          -0.0090 |         234.9039 |           0.6010 |
[32m[20221213 18:17:10 @agent_ppo2.py:185][0m |          -0.0097 |         234.3908 |           0.6014 |
[32m[20221213 18:17:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.59
[32m[20221213 18:17:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.26
[32m[20221213 18:17:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.72
[32m[20221213 18:17:10 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221213 18:17:10 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 18:17:10 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 18:17:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:10 @agent_ppo2.py:185][0m |           0.0044 |         248.0800 |           0.5935 |
[32m[20221213 18:17:10 @agent_ppo2.py:185][0m |          -0.0050 |         228.9205 |           0.5925 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0069 |         223.9620 |           0.5898 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0067 |         220.5609 |           0.5901 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0110 |         217.0882 |           0.5905 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |           0.0026 |         241.1387 |           0.5893 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0129 |         212.5198 |           0.5895 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0115 |         210.0440 |           0.5884 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0154 |         208.6261 |           0.5882 |
[32m[20221213 18:17:11 @agent_ppo2.py:185][0m |          -0.0091 |         209.5057 |           0.5882 |
[32m[20221213 18:17:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.19
[32m[20221213 18:17:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.68
[32m[20221213 18:17:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 973.85
[32m[20221213 18:17:11 @agent_ppo2.py:143][0m Total time:      13.03 min
[32m[20221213 18:17:11 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221213 18:17:11 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 18:17:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |           0.0066 |         243.3402 |           0.5679 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0077 |         226.6504 |           0.5674 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0090 |         221.3559 |           0.5670 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0090 |         218.1063 |           0.5666 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0094 |         215.6842 |           0.5668 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0138 |         213.3369 |           0.5669 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0128 |         211.5841 |           0.5666 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0123 |         209.2335 |           0.5664 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0131 |         208.2104 |           0.5664 |
[32m[20221213 18:17:12 @agent_ppo2.py:185][0m |          -0.0152 |         205.7856 |           0.5661 |
[32m[20221213 18:17:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:17:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.13
[32m[20221213 18:17:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 911.34
[32m[20221213 18:17:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 868.33
[32m[20221213 18:17:13 @agent_ppo2.py:143][0m Total time:      13.05 min
[32m[20221213 18:17:13 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 18:17:13 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 18:17:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0034 |         254.2318 |           0.5836 |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0086 |         245.7418 |           0.5841 |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0065 |         242.9753 |           0.5838 |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0030 |         245.3871 |           0.5837 |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0112 |         240.6067 |           0.5839 |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0060 |         241.7341 |           0.5840 |
[32m[20221213 18:17:13 @agent_ppo2.py:185][0m |          -0.0089 |         238.3418 |           0.5841 |
[32m[20221213 18:17:14 @agent_ppo2.py:185][0m |          -0.0099 |         238.1200 |           0.5836 |
[32m[20221213 18:17:14 @agent_ppo2.py:185][0m |          -0.0077 |         238.8184 |           0.5844 |
[32m[20221213 18:17:14 @agent_ppo2.py:185][0m |          -0.0091 |         238.2136 |           0.5840 |
[32m[20221213 18:17:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:17:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 898.86
[32m[20221213 18:17:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.93
[32m[20221213 18:17:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 908.64
[32m[20221213 18:17:14 @agent_ppo2.py:143][0m Total time:      13.07 min
[32m[20221213 18:17:14 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221213 18:17:14 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 18:17:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:14 @agent_ppo2.py:185][0m |          -0.0007 |         243.1144 |           0.5937 |
[32m[20221213 18:17:14 @agent_ppo2.py:185][0m |          -0.0028 |         238.6391 |           0.5928 |
[32m[20221213 18:17:14 @agent_ppo2.py:185][0m |          -0.0064 |         237.1053 |           0.5927 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |          -0.0020 |         238.6672 |           0.5931 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |          -0.0076 |         235.2998 |           0.5931 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |          -0.0065 |         234.5544 |           0.5927 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |          -0.0084 |         234.2761 |           0.5921 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |          -0.0094 |         234.0076 |           0.5921 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |          -0.0095 |         233.3075 |           0.5914 |
[32m[20221213 18:17:15 @agent_ppo2.py:185][0m |           0.0056 |         256.2924 |           0.5918 |
[32m[20221213 18:17:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.45
[32m[20221213 18:17:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.92
[32m[20221213 18:17:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.20
[32m[20221213 18:17:15 @agent_ppo2.py:143][0m Total time:      13.09 min
[32m[20221213 18:17:15 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 18:17:15 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 18:17:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:17:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |           0.0076 |         267.8118 |           0.5689 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |           0.0003 |         257.5877 |           0.5685 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0055 |         244.0276 |           0.5697 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0063 |         242.0656 |           0.5695 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0097 |         241.4020 |           0.5704 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0096 |         240.5442 |           0.5707 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0075 |         239.7052 |           0.5709 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0050 |         241.8212 |           0.5711 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0096 |         238.7597 |           0.5710 |
[32m[20221213 18:17:16 @agent_ppo2.py:185][0m |          -0.0010 |         265.7255 |           0.5714 |
[32m[20221213 18:17:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:17:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 920.70
[32m[20221213 18:17:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.41
[32m[20221213 18:17:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.01
[32m[20221213 18:17:16 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221213 18:17:16 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221213 18:17:16 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 18:17:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0024 |         240.8001 |           0.5916 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0047 |         238.4829 |           0.5910 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0061 |         236.4812 |           0.5912 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |           0.0102 |         270.3998 |           0.5914 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0044 |         235.0441 |           0.5911 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0066 |         234.1168 |           0.5920 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0054 |         234.1055 |           0.5930 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0080 |         233.5309 |           0.5934 |
[32m[20221213 18:17:17 @agent_ppo2.py:185][0m |          -0.0075 |         233.5881 |           0.5934 |
[32m[20221213 18:17:18 @agent_ppo2.py:185][0m |          -0.0083 |         232.7775 |           0.5938 |
[32m[20221213 18:17:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 908.43
[32m[20221213 18:17:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.93
[32m[20221213 18:17:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 882.28
[32m[20221213 18:17:18 @agent_ppo2.py:143][0m Total time:      13.13 min
[32m[20221213 18:17:18 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 18:17:18 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 18:17:18 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:17:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:18 @agent_ppo2.py:185][0m |          -0.0007 |         215.3560 |           0.5778 |
[32m[20221213 18:17:18 @agent_ppo2.py:185][0m |          -0.0040 |         202.7877 |           0.5766 |
[32m[20221213 18:17:18 @agent_ppo2.py:185][0m |           0.0049 |         217.3319 |           0.5761 |
[32m[20221213 18:17:18 @agent_ppo2.py:185][0m |          -0.0018 |         200.5838 |           0.5755 |
[32m[20221213 18:17:18 @agent_ppo2.py:185][0m |          -0.0063 |         193.8630 |           0.5759 |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0090 |         191.8147 |           0.5757 |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0076 |         191.2946 |           0.5757 |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0106 |         190.0714 |           0.5755 |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0106 |         188.6252 |           0.5757 |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0124 |         187.5047 |           0.5753 |
[32m[20221213 18:17:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.30
[32m[20221213 18:17:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 922.11
[32m[20221213 18:17:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 931.25
[32m[20221213 18:17:19 @agent_ppo2.py:143][0m Total time:      13.15 min
[32m[20221213 18:17:19 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221213 18:17:19 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 18:17:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0020 |         242.2559 |           0.5794 |
[32m[20221213 18:17:19 @agent_ppo2.py:185][0m |          -0.0066 |         238.1142 |           0.5782 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |          -0.0067 |         237.1472 |           0.5783 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |          -0.0060 |         236.4664 |           0.5781 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |          -0.0086 |         235.3862 |           0.5780 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |           0.0020 |         249.0475 |           0.5777 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |           0.0028 |         246.3251 |           0.5773 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |          -0.0080 |         233.3265 |           0.5776 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |          -0.0092 |         233.6007 |           0.5772 |
[32m[20221213 18:17:20 @agent_ppo2.py:185][0m |           0.0016 |         248.7002 |           0.5779 |
[32m[20221213 18:17:20 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 898.97
[32m[20221213 18:17:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.98
[32m[20221213 18:17:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.67
[32m[20221213 18:17:20 @agent_ppo2.py:143][0m Total time:      13.18 min
[32m[20221213 18:17:20 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 18:17:20 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 18:17:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |           0.0016 |         248.3633 |           0.6000 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0025 |         239.7989 |           0.5988 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |           0.0059 |         260.1306 |           0.5990 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0098 |         233.9030 |           0.5989 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0040 |         233.2709 |           0.5986 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0096 |         231.5894 |           0.5988 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0087 |         230.5666 |           0.5990 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0102 |         229.9633 |           0.5991 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0084 |         229.1194 |           0.5989 |
[32m[20221213 18:17:21 @agent_ppo2.py:185][0m |          -0.0137 |         228.9896 |           0.5987 |
[32m[20221213 18:17:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.88
[32m[20221213 18:17:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.82
[32m[20221213 18:17:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.23
[32m[20221213 18:17:22 @agent_ppo2.py:143][0m Total time:      13.20 min
[32m[20221213 18:17:22 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221213 18:17:22 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 18:17:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0013 |         231.2094 |           0.6048 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |           0.0028 |         238.0362 |           0.6033 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0083 |         225.7973 |           0.6025 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0069 |         224.7017 |           0.6030 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0077 |         224.2018 |           0.6024 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0069 |         223.9914 |           0.6024 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0107 |         223.6796 |           0.6023 |
[32m[20221213 18:17:22 @agent_ppo2.py:185][0m |          -0.0117 |         223.1240 |           0.6018 |
[32m[20221213 18:17:23 @agent_ppo2.py:185][0m |          -0.0019 |         231.5558 |           0.6022 |
[32m[20221213 18:17:23 @agent_ppo2.py:185][0m |          -0.0109 |         222.9885 |           0.6014 |
[32m[20221213 18:17:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 928.66
[32m[20221213 18:17:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.04
[32m[20221213 18:17:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 953.61
[32m[20221213 18:17:23 @agent_ppo2.py:143][0m Total time:      13.22 min
[32m[20221213 18:17:23 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 18:17:23 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 18:17:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:23 @agent_ppo2.py:185][0m |          -0.0014 |         236.0259 |           0.6008 |
[32m[20221213 18:17:23 @agent_ppo2.py:185][0m |           0.0012 |         239.2892 |           0.6002 |
[32m[20221213 18:17:23 @agent_ppo2.py:185][0m |          -0.0061 |         231.6795 |           0.5989 |
[32m[20221213 18:17:23 @agent_ppo2.py:185][0m |           0.0007 |         242.9469 |           0.5995 |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |          -0.0063 |         229.9553 |           0.5998 |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |          -0.0068 |         229.5048 |           0.5994 |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |           0.0022 |         243.0636 |           0.6004 |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |          -0.0074 |         228.9641 |           0.5997 |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |          -0.0082 |         228.5584 |           0.6001 |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |          -0.0072 |         228.3126 |           0.5999 |
[32m[20221213 18:17:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 909.21
[32m[20221213 18:17:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.90
[32m[20221213 18:17:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.07
[32m[20221213 18:17:24 @agent_ppo2.py:143][0m Total time:      13.24 min
[32m[20221213 18:17:24 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221213 18:17:24 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 18:17:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:24 @agent_ppo2.py:185][0m |          -0.0011 |         237.1488 |           0.5917 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0064 |         232.2787 |           0.5914 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0077 |         231.6638 |           0.5909 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0068 |         232.8007 |           0.5907 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0090 |         230.7201 |           0.5904 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0003 |         237.1964 |           0.5905 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0096 |         230.6101 |           0.5900 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0092 |         229.5464 |           0.5900 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0063 |         230.1069 |           0.5897 |
[32m[20221213 18:17:25 @agent_ppo2.py:185][0m |          -0.0102 |         228.9334 |           0.5898 |
[32m[20221213 18:17:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:17:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.51
[32m[20221213 18:17:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.88
[32m[20221213 18:17:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.03
[32m[20221213 18:17:25 @agent_ppo2.py:143][0m Total time:      13.26 min
[32m[20221213 18:17:25 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 18:17:25 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 18:17:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0027 |         230.3299 |           0.6025 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0057 |         212.0445 |           0.6025 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0077 |         205.5912 |           0.6018 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0092 |         199.8943 |           0.6017 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0117 |         195.4574 |           0.6012 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0102 |         192.3197 |           0.6007 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0117 |         190.2494 |           0.6002 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0076 |         193.4768 |           0.6001 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0127 |         186.4727 |           0.5998 |
[32m[20221213 18:17:26 @agent_ppo2.py:185][0m |          -0.0135 |         185.4656 |           0.5991 |
[32m[20221213 18:17:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 854.60
[32m[20221213 18:17:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.49
[32m[20221213 18:17:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.23
[32m[20221213 18:17:27 @agent_ppo2.py:143][0m Total time:      13.28 min
[32m[20221213 18:17:27 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221213 18:17:27 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 18:17:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:27 @agent_ppo2.py:185][0m |           0.0003 |         232.8186 |           0.5841 |
[32m[20221213 18:17:27 @agent_ppo2.py:185][0m |          -0.0068 |         229.0460 |           0.5835 |
[32m[20221213 18:17:27 @agent_ppo2.py:185][0m |          -0.0079 |         226.3230 |           0.5835 |
[32m[20221213 18:17:27 @agent_ppo2.py:185][0m |          -0.0090 |         225.0139 |           0.5828 |
[32m[20221213 18:17:27 @agent_ppo2.py:185][0m |          -0.0077 |         224.1163 |           0.5827 |
[32m[20221213 18:17:27 @agent_ppo2.py:185][0m |           0.0024 |         255.2402 |           0.5823 |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |          -0.0074 |         222.6924 |           0.5814 |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |          -0.0073 |         221.9221 |           0.5818 |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |          -0.0088 |         221.4114 |           0.5813 |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |          -0.0088 |         220.7501 |           0.5811 |
[32m[20221213 18:17:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.50
[32m[20221213 18:17:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.22
[32m[20221213 18:17:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 874.44
[32m[20221213 18:17:28 @agent_ppo2.py:143][0m Total time:      13.30 min
[32m[20221213 18:17:28 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 18:17:28 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 18:17:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |           0.0006 |         227.7249 |           0.5957 |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |          -0.0054 |         216.7320 |           0.5949 |
[32m[20221213 18:17:28 @agent_ppo2.py:185][0m |          -0.0080 |         211.7191 |           0.5947 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |          -0.0002 |         226.4881 |           0.5939 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |           0.0106 |         239.5336 |           0.5936 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |          -0.0102 |         205.5626 |           0.5932 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |          -0.0113 |         202.8820 |           0.5934 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |          -0.0116 |         201.2857 |           0.5933 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |          -0.0119 |         199.3939 |           0.5929 |
[32m[20221213 18:17:29 @agent_ppo2.py:185][0m |          -0.0148 |         198.4932 |           0.5928 |
[32m[20221213 18:17:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.36
[32m[20221213 18:17:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 932.63
[32m[20221213 18:17:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.54
[32m[20221213 18:17:29 @agent_ppo2.py:143][0m Total time:      13.32 min
[32m[20221213 18:17:29 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221213 18:17:29 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 18:17:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |           0.0005 |         220.8829 |           0.5642 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0060 |         208.3719 |           0.5639 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0087 |         203.2043 |           0.5632 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0088 |         199.9053 |           0.5628 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0106 |         198.5256 |           0.5627 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0052 |         199.5937 |           0.5626 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0108 |         196.5177 |           0.5622 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0079 |         196.2798 |           0.5616 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0102 |         195.7306 |           0.5603 |
[32m[20221213 18:17:30 @agent_ppo2.py:185][0m |          -0.0101 |         195.4918 |           0.5611 |
[32m[20221213 18:17:30 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 912.40
[32m[20221213 18:17:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.55
[32m[20221213 18:17:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 885.50
[32m[20221213 18:17:30 @agent_ppo2.py:143][0m Total time:      13.34 min
[32m[20221213 18:17:30 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 18:17:30 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 18:17:31 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:17:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0013 |         228.9559 |           0.5727 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0004 |         227.8938 |           0.5721 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0024 |         216.4733 |           0.5711 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0051 |         207.1665 |           0.5708 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0064 |         202.1290 |           0.5699 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0085 |         199.0082 |           0.5696 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0097 |         196.5520 |           0.5692 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0084 |         194.9180 |           0.5689 |
[32m[20221213 18:17:31 @agent_ppo2.py:185][0m |          -0.0092 |         193.6718 |           0.5685 |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0114 |         192.7517 |           0.5684 |
[32m[20221213 18:17:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.57
[32m[20221213 18:17:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.11
[32m[20221213 18:17:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.69
[32m[20221213 18:17:32 @agent_ppo2.py:143][0m Total time:      13.37 min
[32m[20221213 18:17:32 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221213 18:17:32 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 18:17:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0012 |         242.0770 |           0.5779 |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0062 |         236.5874 |           0.5777 |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0077 |         234.4443 |           0.5774 |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0069 |         232.6059 |           0.5776 |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0083 |         232.1020 |           0.5778 |
[32m[20221213 18:17:32 @agent_ppo2.py:185][0m |          -0.0070 |         231.1193 |           0.5782 |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |          -0.0111 |         230.1611 |           0.5781 |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |          -0.0025 |         245.9670 |           0.5784 |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |          -0.0121 |         229.4303 |           0.5788 |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |           0.0080 |         265.0943 |           0.5783 |
[32m[20221213 18:17:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 859.55
[32m[20221213 18:17:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.54
[32m[20221213 18:17:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.24
[32m[20221213 18:17:33 @agent_ppo2.py:143][0m Total time:      13.39 min
[32m[20221213 18:17:33 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 18:17:33 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 18:17:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |          -0.0031 |         241.4080 |           0.5704 |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |          -0.0049 |         231.7951 |           0.5705 |
[32m[20221213 18:17:33 @agent_ppo2.py:185][0m |          -0.0050 |         228.6445 |           0.5697 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |           0.0027 |         237.7068 |           0.5704 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |          -0.0067 |         226.6551 |           0.5693 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |           0.0055 |         254.3875 |           0.5699 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |          -0.0095 |         225.8475 |           0.5695 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |          -0.0104 |         225.5030 |           0.5694 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |          -0.0080 |         224.5965 |           0.5697 |
[32m[20221213 18:17:34 @agent_ppo2.py:185][0m |          -0.0079 |         224.2976 |           0.5700 |
[32m[20221213 18:17:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 906.17
[32m[20221213 18:17:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.24
[32m[20221213 18:17:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 926.71
[32m[20221213 18:17:34 @agent_ppo2.py:143][0m Total time:      13.41 min
[32m[20221213 18:17:34 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221213 18:17:34 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 18:17:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0030 |         223.8826 |           0.5713 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0023 |         216.0206 |           0.5700 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0064 |         212.3985 |           0.5703 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0063 |         209.5233 |           0.5706 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |           0.0022 |         224.5751 |           0.5699 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0087 |         206.1392 |           0.5701 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0090 |         204.8311 |           0.5707 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0118 |         203.6816 |           0.5707 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |           0.0070 |         229.6264 |           0.5706 |
[32m[20221213 18:17:35 @agent_ppo2.py:185][0m |          -0.0122 |         202.2955 |           0.5699 |
[32m[20221213 18:17:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.14
[32m[20221213 18:17:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.15
[32m[20221213 18:17:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.34
[32m[20221213 18:17:36 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221213 18:17:36 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 18:17:36 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 18:17:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |           0.0012 |         231.0790 |           0.5766 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0056 |         222.3939 |           0.5761 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0072 |         218.2672 |           0.5757 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0091 |         215.2270 |           0.5750 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0109 |         212.8421 |           0.5761 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0118 |         211.5265 |           0.5760 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0125 |         209.5716 |           0.5756 |
[32m[20221213 18:17:36 @agent_ppo2.py:185][0m |          -0.0080 |         208.4897 |           0.5762 |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |          -0.0034 |         220.9372 |           0.5765 |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |          -0.0106 |         207.2194 |           0.5765 |
[32m[20221213 18:17:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:17:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.01
[32m[20221213 18:17:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.89
[32m[20221213 18:17:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.34
[32m[20221213 18:17:37 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221213 18:17:37 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221213 18:17:37 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 18:17:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |          -0.0026 |         235.9564 |           0.5857 |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |          -0.0048 |         232.1960 |           0.5852 |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |           0.0065 |         252.6003 |           0.5850 |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |          -0.0070 |         229.7258 |           0.5850 |
[32m[20221213 18:17:37 @agent_ppo2.py:185][0m |          -0.0066 |         228.4733 |           0.5849 |
[32m[20221213 18:17:38 @agent_ppo2.py:185][0m |          -0.0077 |         227.7606 |           0.5849 |
[32m[20221213 18:17:38 @agent_ppo2.py:185][0m |          -0.0074 |         227.5649 |           0.5848 |
[32m[20221213 18:17:38 @agent_ppo2.py:185][0m |          -0.0097 |         227.0540 |           0.5849 |
[32m[20221213 18:17:38 @agent_ppo2.py:185][0m |          -0.0100 |         226.7160 |           0.5845 |
[32m[20221213 18:17:38 @agent_ppo2.py:185][0m |          -0.0090 |         226.4319 |           0.5846 |
[32m[20221213 18:17:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 876.09
[32m[20221213 18:17:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.60
[32m[20221213 18:17:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 945.64
[32m[20221213 18:17:38 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221213 18:17:38 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 18:17:38 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 18:17:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:38 @agent_ppo2.py:185][0m |           0.0015 |         240.3205 |           0.5712 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |           0.0084 |         259.6227 |           0.5715 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0049 |         233.4552 |           0.5703 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0071 |         232.1147 |           0.5708 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0065 |         231.2148 |           0.5709 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0067 |         230.7494 |           0.5703 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0081 |         230.1830 |           0.5716 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0096 |         229.7124 |           0.5705 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0081 |         229.4559 |           0.5715 |
[32m[20221213 18:17:39 @agent_ppo2.py:185][0m |          -0.0086 |         228.8722 |           0.5714 |
[32m[20221213 18:17:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 879.95
[32m[20221213 18:17:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 931.09
[32m[20221213 18:17:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.53
[32m[20221213 18:17:39 @agent_ppo2.py:143][0m Total time:      13.49 min
[32m[20221213 18:17:39 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221213 18:17:39 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 18:17:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0022 |         234.3492 |           0.5673 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0063 |         228.0807 |           0.5658 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0085 |         225.8276 |           0.5653 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0081 |         224.6594 |           0.5657 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |           0.0052 |         246.9638 |           0.5656 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |           0.0024 |         234.7375 |           0.5650 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0100 |         221.9579 |           0.5648 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0074 |         221.7753 |           0.5659 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0073 |         220.9019 |           0.5654 |
[32m[20221213 18:17:40 @agent_ppo2.py:185][0m |          -0.0098 |         220.7646 |           0.5650 |
[32m[20221213 18:17:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 875.86
[32m[20221213 18:17:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 931.17
[32m[20221213 18:17:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 905.65
[32m[20221213 18:17:41 @agent_ppo2.py:143][0m Total time:      13.51 min
[32m[20221213 18:17:41 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 18:17:41 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 18:17:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0034 |         235.9058 |           0.5721 |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0067 |         234.3311 |           0.5723 |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0073 |         233.2866 |           0.5716 |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0082 |         231.9137 |           0.5723 |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0081 |         230.3091 |           0.5725 |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0009 |         236.4684 |           0.5727 |
[32m[20221213 18:17:41 @agent_ppo2.py:185][0m |          -0.0084 |         228.3284 |           0.5734 |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0097 |         227.9531 |           0.5742 |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0096 |         227.7874 |           0.5741 |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0079 |         226.8492 |           0.5741 |
[32m[20221213 18:17:42 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.07
[32m[20221213 18:17:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.27
[32m[20221213 18:17:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.86
[32m[20221213 18:17:42 @agent_ppo2.py:143][0m Total time:      13.53 min
[32m[20221213 18:17:42 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221213 18:17:42 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 18:17:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0010 |         239.6188 |           0.5718 |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0061 |         235.7194 |           0.5706 |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0058 |         233.6600 |           0.5696 |
[32m[20221213 18:17:42 @agent_ppo2.py:185][0m |          -0.0082 |         232.3553 |           0.5693 |
[32m[20221213 18:17:43 @agent_ppo2.py:185][0m |          -0.0071 |         231.3405 |           0.5688 |
[32m[20221213 18:17:43 @agent_ppo2.py:185][0m |          -0.0102 |         230.6975 |           0.5684 |
[32m[20221213 18:17:43 @agent_ppo2.py:185][0m |          -0.0101 |         230.0811 |           0.5679 |
[32m[20221213 18:17:43 @agent_ppo2.py:185][0m |          -0.0103 |         229.6026 |           0.5682 |
[32m[20221213 18:17:43 @agent_ppo2.py:185][0m |          -0.0127 |         229.6469 |           0.5674 |
[32m[20221213 18:17:43 @agent_ppo2.py:185][0m |          -0.0048 |         231.2323 |           0.5674 |
[32m[20221213 18:17:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:17:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.93
[32m[20221213 18:17:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.90
[32m[20221213 18:17:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.87
[32m[20221213 18:17:43 @agent_ppo2.py:143][0m Total time:      13.56 min
[32m[20221213 18:17:43 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 18:17:43 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 18:17:43 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:17:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |           0.0025 |         236.3022 |           0.5764 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0067 |         226.5558 |           0.5745 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0029 |         229.1396 |           0.5756 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0085 |         224.9744 |           0.5745 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0008 |         230.7778 |           0.5754 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0085 |         224.5954 |           0.5746 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0082 |         223.9944 |           0.5746 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0100 |         223.6557 |           0.5741 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0060 |         224.6541 |           0.5742 |
[32m[20221213 18:17:44 @agent_ppo2.py:185][0m |          -0.0102 |         223.0705 |           0.5740 |
[32m[20221213 18:17:44 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 972.70
[32m[20221213 18:17:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.97
[32m[20221213 18:17:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 986.96
[32m[20221213 18:17:44 @agent_ppo2.py:143][0m Total time:      13.58 min
[32m[20221213 18:17:44 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221213 18:17:44 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 18:17:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0023 |         219.9042 |           0.5771 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0052 |         213.6149 |           0.5768 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0095 |         212.3631 |           0.5772 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0083 |         211.5289 |           0.5768 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0104 |         210.9601 |           0.5771 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0088 |         209.8198 |           0.5774 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0093 |         209.2455 |           0.5776 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0108 |         208.5329 |           0.5780 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0095 |         208.6104 |           0.5778 |
[32m[20221213 18:17:45 @agent_ppo2.py:185][0m |          -0.0135 |         207.9339 |           0.5783 |
[32m[20221213 18:17:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.20
[32m[20221213 18:17:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 925.52
[32m[20221213 18:17:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.37
[32m[20221213 18:17:46 @agent_ppo2.py:143][0m Total time:      13.60 min
[32m[20221213 18:17:46 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 18:17:46 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 18:17:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |           0.0040 |         246.2860 |           0.5848 |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |          -0.0014 |         238.6055 |           0.5836 |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |          -0.0067 |         234.3939 |           0.5840 |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |          -0.0071 |         233.9140 |           0.5844 |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |          -0.0081 |         232.9334 |           0.5840 |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |          -0.0077 |         233.0403 |           0.5842 |
[32m[20221213 18:17:46 @agent_ppo2.py:185][0m |          -0.0085 |         232.5831 |           0.5846 |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |          -0.0109 |         231.7666 |           0.5846 |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |           0.0008 |         250.3529 |           0.5851 |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |          -0.0090 |         231.0631 |           0.5847 |
[32m[20221213 18:17:47 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.48
[32m[20221213 18:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.63
[32m[20221213 18:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 896.34
[32m[20221213 18:17:47 @agent_ppo2.py:143][0m Total time:      13.62 min
[32m[20221213 18:17:47 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221213 18:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 18:17:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:17:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |           0.0031 |         201.6130 |           0.5755 |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |          -0.0046 |         186.1319 |           0.5745 |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |          -0.0077 |         180.5298 |           0.5743 |
[32m[20221213 18:17:47 @agent_ppo2.py:185][0m |          -0.0032 |         181.7434 |           0.5740 |
[32m[20221213 18:17:48 @agent_ppo2.py:185][0m |          -0.0069 |         175.6575 |           0.5733 |
[32m[20221213 18:17:48 @agent_ppo2.py:185][0m |          -0.0036 |         174.1486 |           0.5738 |
[32m[20221213 18:17:48 @agent_ppo2.py:185][0m |          -0.0092 |         172.8220 |           0.5739 |
[32m[20221213 18:17:48 @agent_ppo2.py:185][0m |          -0.0121 |         172.1137 |           0.5742 |
[32m[20221213 18:17:48 @agent_ppo2.py:185][0m |          -0.0113 |         171.0962 |           0.5743 |
[32m[20221213 18:17:48 @agent_ppo2.py:185][0m |          -0.0089 |         170.3425 |           0.5742 |
[32m[20221213 18:17:48 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.78
[32m[20221213 18:17:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.43
[32m[20221213 18:17:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.46
[32m[20221213 18:17:48 @agent_ppo2.py:143][0m Total time:      13.64 min
[32m[20221213 18:17:48 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 18:17:48 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 18:17:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0020 |         251.7577 |           0.6028 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0011 |         237.8770 |           0.6020 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |           0.0010 |         237.0202 |           0.6015 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |           0.0081 |         264.3710 |           0.6011 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0078 |         232.4403 |           0.5997 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0075 |         230.2773 |           0.6000 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0086 |         229.9514 |           0.6003 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0089 |         229.3721 |           0.6006 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0102 |         228.9662 |           0.5997 |
[32m[20221213 18:17:49 @agent_ppo2.py:185][0m |          -0.0072 |         228.1827 |           0.6003 |
[32m[20221213 18:17:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:17:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 930.61
[32m[20221213 18:17:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.72
[32m[20221213 18:17:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.13
[32m[20221213 18:17:49 @agent_ppo2.py:143][0m Total time:      13.66 min
[32m[20221213 18:17:49 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221213 18:17:49 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 18:17:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0032 |         244.2609 |           0.5892 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0028 |         243.0400 |           0.5881 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0070 |         240.3550 |           0.5878 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0075 |         239.9711 |           0.5877 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0071 |         239.2624 |           0.5872 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0080 |         238.9559 |           0.5862 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0075 |         238.5389 |           0.5866 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0072 |         238.7593 |           0.5868 |
[32m[20221213 18:17:50 @agent_ppo2.py:185][0m |          -0.0081 |         237.9590 |           0.5861 |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |           0.0044 |         248.5237 |           0.5858 |
[32m[20221213 18:17:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.54
[32m[20221213 18:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.22
[32m[20221213 18:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 886.52
[32m[20221213 18:17:51 @agent_ppo2.py:143][0m Total time:      13.68 min
[32m[20221213 18:17:51 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 18:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 18:17:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |           0.0124 |         269.7634 |           0.5922 |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |          -0.0007 |         249.3014 |           0.5925 |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |          -0.0022 |         248.7657 |           0.5920 |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |          -0.0064 |         243.9979 |           0.5924 |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |          -0.0056 |         242.4084 |           0.5919 |
[32m[20221213 18:17:51 @agent_ppo2.py:185][0m |          -0.0058 |         241.0247 |           0.5918 |
[32m[20221213 18:17:52 @agent_ppo2.py:185][0m |          -0.0045 |         241.2224 |           0.5921 |
[32m[20221213 18:17:52 @agent_ppo2.py:185][0m |           0.0035 |         257.5670 |           0.5917 |
[32m[20221213 18:17:52 @agent_ppo2.py:185][0m |          -0.0036 |         240.3164 |           0.5925 |
[32m[20221213 18:17:52 @agent_ppo2.py:185][0m |          -0.0075 |         238.5620 |           0.5927 |
[32m[20221213 18:17:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:17:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 883.10
[32m[20221213 18:17:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.08
[32m[20221213 18:17:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.42
[32m[20221213 18:17:52 @agent_ppo2.py:143][0m Total time:      13.70 min
[32m[20221213 18:17:52 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221213 18:17:52 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 18:17:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:52 @agent_ppo2.py:185][0m |           0.0014 |         232.5566 |           0.5906 |
[32m[20221213 18:17:52 @agent_ppo2.py:185][0m |          -0.0064 |         218.1653 |           0.5891 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0072 |         213.4124 |           0.5892 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0080 |         209.6314 |           0.5890 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0103 |         207.7344 |           0.5887 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |           0.0097 |         231.8098 |           0.5889 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0100 |         204.5866 |           0.5886 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0107 |         202.5625 |           0.5884 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0084 |         203.0573 |           0.5889 |
[32m[20221213 18:17:53 @agent_ppo2.py:185][0m |          -0.0122 |         199.9624 |           0.5888 |
[32m[20221213 18:17:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.57
[32m[20221213 18:17:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.68
[32m[20221213 18:17:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 876.59
[32m[20221213 18:17:53 @agent_ppo2.py:143][0m Total time:      13.72 min
[32m[20221213 18:17:53 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 18:17:53 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 18:17:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0017 |         249.2971 |           0.5905 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |           0.0137 |         267.6809 |           0.5902 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0058 |         242.9720 |           0.5902 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0054 |         241.4386 |           0.5908 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0065 |         240.7476 |           0.5908 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0055 |         240.0382 |           0.5907 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0055 |         239.5888 |           0.5910 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0073 |         239.4473 |           0.5905 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0033 |         241.3287 |           0.5911 |
[32m[20221213 18:17:54 @agent_ppo2.py:185][0m |          -0.0085 |         239.0172 |           0.5911 |
[32m[20221213 18:17:54 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:17:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.84
[32m[20221213 18:17:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.59
[32m[20221213 18:17:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.99
[32m[20221213 18:17:54 @agent_ppo2.py:143][0m Total time:      13.75 min
[32m[20221213 18:17:54 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221213 18:17:54 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 18:17:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0027 |         247.7749 |           0.5884 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0042 |         241.5881 |           0.5885 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0060 |         238.6128 |           0.5881 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0084 |         237.5698 |           0.5877 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0097 |         236.6724 |           0.5877 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0090 |         234.9459 |           0.5880 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0096 |         234.5855 |           0.5878 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0099 |         234.3320 |           0.5878 |
[32m[20221213 18:17:55 @agent_ppo2.py:185][0m |          -0.0101 |         234.4724 |           0.5872 |
[32m[20221213 18:17:56 @agent_ppo2.py:185][0m |          -0.0041 |         240.8616 |           0.5871 |
[32m[20221213 18:17:56 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 907.00
[32m[20221213 18:17:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.71
[32m[20221213 18:17:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 914.48
[32m[20221213 18:17:56 @agent_ppo2.py:143][0m Total time:      13.77 min
[32m[20221213 18:17:56 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 18:17:56 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 18:17:56 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:17:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:56 @agent_ppo2.py:185][0m |          -0.0015 |         246.7560 |           0.5922 |
[32m[20221213 18:17:56 @agent_ppo2.py:185][0m |          -0.0012 |         244.2693 |           0.5922 |
[32m[20221213 18:17:56 @agent_ppo2.py:185][0m |          -0.0035 |         241.2838 |           0.5913 |
[32m[20221213 18:17:56 @agent_ppo2.py:185][0m |          -0.0059 |         239.7938 |           0.5910 |
[32m[20221213 18:17:56 @agent_ppo2.py:185][0m |          -0.0075 |         239.0861 |           0.5911 |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0083 |         238.1826 |           0.5902 |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0020 |         243.3896 |           0.5906 |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0063 |         236.9015 |           0.5905 |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0078 |         236.2966 |           0.5905 |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0082 |         236.0456 |           0.5902 |
[32m[20221213 18:17:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:17:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.91
[32m[20221213 18:17:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.51
[32m[20221213 18:17:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.45
[32m[20221213 18:17:57 @agent_ppo2.py:143][0m Total time:      13.79 min
[32m[20221213 18:17:57 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221213 18:17:57 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 18:17:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0038 |         242.4678 |           0.5832 |
[32m[20221213 18:17:57 @agent_ppo2.py:185][0m |          -0.0081 |         238.7570 |           0.5836 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0042 |         237.2378 |           0.5834 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0062 |         235.1734 |           0.5832 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0071 |         234.7177 |           0.5827 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0107 |         233.7238 |           0.5831 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0096 |         233.1023 |           0.5831 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0079 |         232.9103 |           0.5830 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0109 |         232.0317 |           0.5824 |
[32m[20221213 18:17:58 @agent_ppo2.py:185][0m |          -0.0049 |         237.1127 |           0.5828 |
[32m[20221213 18:17:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:17:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.77
[32m[20221213 18:17:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.49
[32m[20221213 18:17:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 963.85
[32m[20221213 18:17:58 @agent_ppo2.py:143][0m Total time:      13.81 min
[32m[20221213 18:17:58 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 18:17:58 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 18:17:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:17:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0024 |         244.9159 |           0.5844 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0061 |         240.4071 |           0.5836 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0080 |         237.5495 |           0.5845 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0085 |         235.7912 |           0.5841 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0058 |         234.6571 |           0.5849 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0094 |         233.7030 |           0.5847 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0090 |         232.8449 |           0.5848 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0099 |         231.9230 |           0.5851 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0109 |         231.2706 |           0.5852 |
[32m[20221213 18:17:59 @agent_ppo2.py:185][0m |          -0.0070 |         231.1175 |           0.5853 |
[32m[20221213 18:17:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.18
[32m[20221213 18:18:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.29
[32m[20221213 18:18:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 928.25
[32m[20221213 18:18:00 @agent_ppo2.py:143][0m Total time:      13.83 min
[32m[20221213 18:18:00 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221213 18:18:00 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 18:18:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |           0.0010 |         241.4878 |           0.5911 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0033 |         236.2211 |           0.5907 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0052 |         234.0248 |           0.5906 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0056 |         232.0283 |           0.5903 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0065 |         230.7927 |           0.5899 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0071 |         229.2957 |           0.5902 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0071 |         227.4746 |           0.5899 |
[32m[20221213 18:18:00 @agent_ppo2.py:185][0m |          -0.0071 |         226.1753 |           0.5900 |
[32m[20221213 18:18:01 @agent_ppo2.py:185][0m |          -0.0088 |         224.4409 |           0.5899 |
[32m[20221213 18:18:01 @agent_ppo2.py:185][0m |          -0.0034 |         225.6868 |           0.5902 |
[32m[20221213 18:18:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 998.14
[32m[20221213 18:18:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.58
[32m[20221213 18:18:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.20
[32m[20221213 18:18:01 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221213 18:18:01 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 18:18:01 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 18:18:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:01 @agent_ppo2.py:185][0m |           0.0010 |         229.4353 |           0.5986 |
[32m[20221213 18:18:01 @agent_ppo2.py:185][0m |          -0.0028 |         222.3625 |           0.5978 |
[32m[20221213 18:18:01 @agent_ppo2.py:185][0m |          -0.0076 |         220.7257 |           0.5966 |
[32m[20221213 18:18:01 @agent_ppo2.py:185][0m |          -0.0062 |         220.0693 |           0.5968 |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0101 |         219.7707 |           0.5967 |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0076 |         219.4175 |           0.5960 |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0086 |         219.2306 |           0.5963 |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0093 |         218.8713 |           0.5963 |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0103 |         218.9009 |           0.5959 |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0096 |         218.5693 |           0.5955 |
[32m[20221213 18:18:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.30
[32m[20221213 18:18:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.05
[32m[20221213 18:18:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 884.59
[32m[20221213 18:18:02 @agent_ppo2.py:143][0m Total time:      13.87 min
[32m[20221213 18:18:02 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221213 18:18:02 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 18:18:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:02 @agent_ppo2.py:185][0m |          -0.0019 |         245.4873 |           0.5716 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0044 |         234.7445 |           0.5708 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0041 |         230.1849 |           0.5695 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0022 |         228.6248 |           0.5686 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |           0.0064 |         234.1375 |           0.5688 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0054 |         228.1414 |           0.5677 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0090 |         220.9434 |           0.5674 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0094 |         219.6802 |           0.5666 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0094 |         218.7014 |           0.5663 |
[32m[20221213 18:18:03 @agent_ppo2.py:185][0m |          -0.0102 |         217.3616 |           0.5664 |
[32m[20221213 18:18:03 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 894.47
[32m[20221213 18:18:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.14
[32m[20221213 18:18:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.11
[32m[20221213 18:18:03 @agent_ppo2.py:143][0m Total time:      13.89 min
[32m[20221213 18:18:03 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 18:18:03 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 18:18:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0035 |         229.3406 |           0.5861 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0043 |         220.5672 |           0.5853 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0074 |         217.9033 |           0.5849 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0068 |         216.9852 |           0.5847 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0111 |         216.0319 |           0.5842 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0072 |         215.3418 |           0.5848 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0092 |         213.8118 |           0.5844 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0099 |         212.2778 |           0.5843 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0099 |         211.1002 |           0.5840 |
[32m[20221213 18:18:04 @agent_ppo2.py:185][0m |          -0.0105 |         210.8379 |           0.5835 |
[32m[20221213 18:18:04 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:18:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.21
[32m[20221213 18:18:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.95
[32m[20221213 18:18:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 943.27
[32m[20221213 18:18:05 @agent_ppo2.py:143][0m Total time:      13.91 min
[32m[20221213 18:18:05 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221213 18:18:05 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 18:18:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |           0.0044 |         258.4204 |           0.5736 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0080 |         245.8139 |           0.5708 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0079 |         243.6761 |           0.5707 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0088 |         242.4432 |           0.5706 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0074 |         241.5351 |           0.5701 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0116 |         240.7758 |           0.5702 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0117 |         239.8527 |           0.5702 |
[32m[20221213 18:18:05 @agent_ppo2.py:185][0m |          -0.0079 |         239.5686 |           0.5701 |
[32m[20221213 18:18:06 @agent_ppo2.py:185][0m |          -0.0105 |         238.2438 |           0.5695 |
[32m[20221213 18:18:06 @agent_ppo2.py:185][0m |           0.0014 |         261.3121 |           0.5700 |
[32m[20221213 18:18:06 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.66
[32m[20221213 18:18:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.94
[32m[20221213 18:18:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 970.33
[32m[20221213 18:18:06 @agent_ppo2.py:143][0m Total time:      13.93 min
[32m[20221213 18:18:06 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 18:18:06 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 18:18:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:06 @agent_ppo2.py:185][0m |           0.0003 |         215.5258 |           0.5888 |
[32m[20221213 18:18:06 @agent_ppo2.py:185][0m |          -0.0011 |         210.2771 |           0.5888 |
[32m[20221213 18:18:06 @agent_ppo2.py:185][0m |          -0.0050 |         207.8332 |           0.5879 |
[32m[20221213 18:18:06 @agent_ppo2.py:185][0m |          -0.0057 |         206.5457 |           0.5875 |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |          -0.0070 |         206.7246 |           0.5874 |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |          -0.0087 |         205.3633 |           0.5873 |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |          -0.0057 |         204.8428 |           0.5872 |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |           0.0057 |         238.8680 |           0.5869 |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |          -0.0037 |         204.9294 |           0.5868 |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |          -0.0095 |         203.3883 |           0.5868 |
[32m[20221213 18:18:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.02
[32m[20221213 18:18:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 920.37
[32m[20221213 18:18:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.88
[32m[20221213 18:18:07 @agent_ppo2.py:143][0m Total time:      13.96 min
[32m[20221213 18:18:07 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221213 18:18:07 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 18:18:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:07 @agent_ppo2.py:185][0m |          -0.0035 |         250.0016 |           0.5747 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |           0.0030 |         266.0699 |           0.5744 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0067 |         246.6530 |           0.5749 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0082 |         246.4900 |           0.5754 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |           0.0110 |         281.2463 |           0.5753 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0067 |         245.9015 |           0.5757 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0102 |         245.5513 |           0.5758 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0104 |         245.4612 |           0.5764 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0111 |         245.3816 |           0.5761 |
[32m[20221213 18:18:08 @agent_ppo2.py:185][0m |          -0.0120 |         245.0757 |           0.5763 |
[32m[20221213 18:18:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 944.02
[32m[20221213 18:18:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.68
[32m[20221213 18:18:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.00
[32m[20221213 18:18:08 @agent_ppo2.py:143][0m Total time:      13.98 min
[32m[20221213 18:18:08 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 18:18:08 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 18:18:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:18:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |           0.0077 |         262.7754 |           0.5729 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0027 |         245.8702 |           0.5724 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0020 |         240.9882 |           0.5711 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0042 |         237.4242 |           0.5712 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0050 |         233.4734 |           0.5710 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0039 |         230.1858 |           0.5707 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0052 |         227.2021 |           0.5708 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0057 |         224.6314 |           0.5699 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0063 |         223.1322 |           0.5705 |
[32m[20221213 18:18:09 @agent_ppo2.py:185][0m |          -0.0044 |         222.5891 |           0.5698 |
[32m[20221213 18:18:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:18:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 995.26
[32m[20221213 18:18:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.74
[32m[20221213 18:18:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.26
[32m[20221213 18:18:10 @agent_ppo2.py:143][0m Total time:      14.00 min
[32m[20221213 18:18:10 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221213 18:18:10 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 18:18:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |           0.0050 |         275.7135 |           0.5745 |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |          -0.0054 |         254.5089 |           0.5744 |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |           0.0014 |         265.7662 |           0.5738 |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |          -0.0093 |         252.0316 |           0.5737 |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |          -0.0097 |         251.4852 |           0.5741 |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |          -0.0087 |         251.1976 |           0.5740 |
[32m[20221213 18:18:10 @agent_ppo2.py:185][0m |          -0.0088 |         250.7510 |           0.5741 |
[32m[20221213 18:18:11 @agent_ppo2.py:185][0m |          -0.0044 |         259.0113 |           0.5738 |
[32m[20221213 18:18:11 @agent_ppo2.py:185][0m |          -0.0097 |         250.1457 |           0.5740 |
[32m[20221213 18:18:11 @agent_ppo2.py:185][0m |          -0.0122 |         249.8704 |           0.5741 |
[32m[20221213 18:18:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 902.57
[32m[20221213 18:18:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.49
[32m[20221213 18:18:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.32
[32m[20221213 18:18:11 @agent_ppo2.py:143][0m Total time:      14.02 min
[32m[20221213 18:18:11 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 18:18:11 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 18:18:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:11 @agent_ppo2.py:185][0m |          -0.0008 |         254.0639 |           0.5739 |
[32m[20221213 18:18:11 @agent_ppo2.py:185][0m |          -0.0044 |         248.6813 |           0.5726 |
[32m[20221213 18:18:11 @agent_ppo2.py:185][0m |          -0.0061 |         247.6058 |           0.5728 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0081 |         247.0596 |           0.5720 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0090 |         246.4162 |           0.5723 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0077 |         246.2420 |           0.5720 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0122 |         245.6889 |           0.5718 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0098 |         245.6054 |           0.5714 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0131 |         244.9409 |           0.5719 |
[32m[20221213 18:18:12 @agent_ppo2.py:185][0m |          -0.0019 |         258.4316 |           0.5721 |
[32m[20221213 18:18:12 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 896.65
[32m[20221213 18:18:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.61
[32m[20221213 18:18:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.15
[32m[20221213 18:18:12 @agent_ppo2.py:143][0m Total time:      14.04 min
[32m[20221213 18:18:12 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221213 18:18:12 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 18:18:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0011 |         243.5849 |           0.5740 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0020 |         242.5825 |           0.5737 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0065 |         240.5087 |           0.5729 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0058 |         240.2516 |           0.5730 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0061 |         240.2514 |           0.5723 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0084 |         239.7819 |           0.5712 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0069 |         239.6798 |           0.5720 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0089 |         239.4629 |           0.5705 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0096 |         238.9509 |           0.5706 |
[32m[20221213 18:18:13 @agent_ppo2.py:185][0m |          -0.0097 |         239.2256 |           0.5704 |
[32m[20221213 18:18:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.65
[32m[20221213 18:18:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.27
[32m[20221213 18:18:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.36
[32m[20221213 18:18:13 @agent_ppo2.py:143][0m Total time:      14.06 min
[32m[20221213 18:18:13 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 18:18:13 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 18:18:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |           0.0032 |         241.5977 |           0.5677 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |           0.0017 |         233.7576 |           0.5678 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0057 |         226.1371 |           0.5669 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0086 |         224.8564 |           0.5671 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0100 |         223.4447 |           0.5667 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0091 |         223.1789 |           0.5665 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0073 |         222.5751 |           0.5662 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0096 |         221.8593 |           0.5662 |
[32m[20221213 18:18:14 @agent_ppo2.py:185][0m |          -0.0115 |         220.9547 |           0.5660 |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |          -0.0095 |         220.5769 |           0.5659 |
[32m[20221213 18:18:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 955.14
[32m[20221213 18:18:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.67
[32m[20221213 18:18:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.94
[32m[20221213 18:18:15 @agent_ppo2.py:143][0m Total time:      14.08 min
[32m[20221213 18:18:15 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221213 18:18:15 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 18:18:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |          -0.0057 |         229.3529 |           0.5807 |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |           0.0032 |         236.3977 |           0.5798 |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |          -0.0018 |         224.4875 |           0.5787 |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |          -0.0085 |         219.7495 |           0.5790 |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |          -0.0099 |         218.2432 |           0.5783 |
[32m[20221213 18:18:15 @agent_ppo2.py:185][0m |          -0.0095 |         217.5011 |           0.5786 |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |          -0.0111 |         216.8890 |           0.5783 |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |          -0.0090 |         216.1369 |           0.5784 |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |          -0.0126 |         215.7160 |           0.5781 |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |          -0.0084 |         216.5906 |           0.5783 |
[32m[20221213 18:18:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 856.17
[32m[20221213 18:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 986.35
[32m[20221213 18:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 983.59
[32m[20221213 18:18:16 @agent_ppo2.py:143][0m Total time:      14.10 min
[32m[20221213 18:18:16 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 18:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 18:18:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |           0.0204 |         244.5675 |           0.5737 |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |          -0.0043 |         189.0737 |           0.5731 |
[32m[20221213 18:18:16 @agent_ppo2.py:185][0m |          -0.0072 |         184.0537 |           0.5731 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0094 |         181.5802 |           0.5736 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0088 |         180.8505 |           0.5735 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0108 |         178.7144 |           0.5736 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0111 |         177.8312 |           0.5734 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0109 |         176.8444 |           0.5736 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0101 |         175.6544 |           0.5733 |
[32m[20221213 18:18:17 @agent_ppo2.py:185][0m |          -0.0078 |         175.0256 |           0.5733 |
[32m[20221213 18:18:17 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.21
[32m[20221213 18:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.15
[32m[20221213 18:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.90
[32m[20221213 18:18:17 @agent_ppo2.py:143][0m Total time:      14.12 min
[32m[20221213 18:18:17 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221213 18:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 18:18:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |           0.0097 |         275.2600 |           0.5796 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0046 |         244.7438 |           0.5769 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0093 |         238.2953 |           0.5782 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0096 |         234.6084 |           0.5771 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0096 |         233.8684 |           0.5776 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0085 |         231.3883 |           0.5764 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0131 |         229.7964 |           0.5764 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0013 |         242.9431 |           0.5763 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0091 |         227.6031 |           0.5751 |
[32m[20221213 18:18:18 @agent_ppo2.py:185][0m |          -0.0021 |         249.7108 |           0.5756 |
[32m[20221213 18:18:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.45
[32m[20221213 18:18:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.77
[32m[20221213 18:18:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.05
[32m[20221213 18:18:19 @agent_ppo2.py:143][0m Total time:      14.15 min
[32m[20221213 18:18:19 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 18:18:19 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 18:18:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |           0.0044 |         273.7342 |           0.5628 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |          -0.0047 |         247.4855 |           0.5629 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |           0.0079 |         264.3246 |           0.5628 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |          -0.0124 |         237.4446 |           0.5634 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |          -0.0130 |         234.9422 |           0.5633 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |          -0.0053 |         233.3097 |           0.5636 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |          -0.0105 |         230.8940 |           0.5643 |
[32m[20221213 18:18:19 @agent_ppo2.py:185][0m |          -0.0153 |         228.6465 |           0.5644 |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |           0.0018 |         246.5678 |           0.5646 |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |          -0.0174 |         225.5934 |           0.5652 |
[32m[20221213 18:18:20 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.16
[32m[20221213 18:18:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 896.09
[32m[20221213 18:18:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 874.41
[32m[20221213 18:18:20 @agent_ppo2.py:143][0m Total time:      14.17 min
[32m[20221213 18:18:20 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221213 18:18:20 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 18:18:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |          -0.0027 |         204.8323 |           0.5801 |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |          -0.0084 |         195.9598 |           0.5793 |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |          -0.0114 |         192.2266 |           0.5788 |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |          -0.0028 |         194.0624 |           0.5791 |
[32m[20221213 18:18:20 @agent_ppo2.py:185][0m |          -0.0078 |         188.5949 |           0.5786 |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0127 |         187.2156 |           0.5790 |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0121 |         185.8398 |           0.5790 |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0107 |         184.9650 |           0.5793 |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0116 |         183.8953 |           0.5791 |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0113 |         183.2509 |           0.5790 |
[32m[20221213 18:18:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.28
[32m[20221213 18:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 952.65
[32m[20221213 18:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 909.98
[32m[20221213 18:18:21 @agent_ppo2.py:143][0m Total time:      14.19 min
[32m[20221213 18:18:21 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 18:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 18:18:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:18:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0034 |         228.7754 |           0.5817 |
[32m[20221213 18:18:21 @agent_ppo2.py:185][0m |          -0.0039 |         221.7206 |           0.5809 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0083 |         219.0006 |           0.5803 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0060 |         217.1395 |           0.5801 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0117 |         216.2073 |           0.5796 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0102 |         214.7802 |           0.5800 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0115 |         213.9981 |           0.5800 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0107 |         213.2705 |           0.5796 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0082 |         216.1736 |           0.5798 |
[32m[20221213 18:18:22 @agent_ppo2.py:185][0m |          -0.0097 |         212.4319 |           0.5798 |
[32m[20221213 18:18:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:18:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.15
[32m[20221213 18:18:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.84
[32m[20221213 18:18:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.07
[32m[20221213 18:18:22 @agent_ppo2.py:143][0m Total time:      14.21 min
[32m[20221213 18:18:22 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221213 18:18:22 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 18:18:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0014 |         176.7324 |           0.5759 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0049 |         167.2074 |           0.5754 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0098 |         159.1870 |           0.5748 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0100 |         155.7822 |           0.5746 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0150 |         153.1434 |           0.5748 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0087 |         152.6075 |           0.5748 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0018 |         169.2265 |           0.5748 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0153 |         148.0479 |           0.5743 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0165 |         146.2518 |           0.5749 |
[32m[20221213 18:18:23 @agent_ppo2.py:185][0m |          -0.0166 |         144.6685 |           0.5747 |
[32m[20221213 18:18:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.28
[32m[20221213 18:18:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.71
[32m[20221213 18:18:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.99
[32m[20221213 18:18:24 @agent_ppo2.py:143][0m Total time:      14.23 min
[32m[20221213 18:18:24 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 18:18:24 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 18:18:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0031 |         257.6465 |           0.5685 |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0048 |         253.2287 |           0.5670 |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0030 |         251.0950 |           0.5680 |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0062 |         250.4744 |           0.5673 |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0075 |         249.4978 |           0.5676 |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0070 |         248.9492 |           0.5673 |
[32m[20221213 18:18:24 @agent_ppo2.py:185][0m |          -0.0061 |         248.6015 |           0.5678 |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |          -0.0088 |         248.0288 |           0.5680 |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |           0.0027 |         279.7905 |           0.5681 |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |          -0.0053 |         247.9035 |           0.5672 |
[32m[20221213 18:18:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 905.15
[32m[20221213 18:18:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.11
[32m[20221213 18:18:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 981.15
[32m[20221213 18:18:25 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221213 18:18:25 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221213 18:18:25 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 18:18:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |           0.0006 |         213.4139 |           0.5886 |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |          -0.0042 |         205.8163 |           0.5870 |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |          -0.0050 |         204.2198 |           0.5879 |
[32m[20221213 18:18:25 @agent_ppo2.py:185][0m |          -0.0080 |         203.1661 |           0.5869 |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |          -0.0083 |         202.3451 |           0.5876 |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |          -0.0090 |         201.2377 |           0.5871 |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |          -0.0059 |         202.1051 |           0.5875 |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |          -0.0094 |         199.9040 |           0.5880 |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |          -0.0095 |         199.3768 |           0.5875 |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |          -0.0084 |         198.9522 |           0.5877 |
[32m[20221213 18:18:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 895.03
[32m[20221213 18:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.03
[32m[20221213 18:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.74
[32m[20221213 18:18:26 @agent_ppo2.py:143][0m Total time:      14.27 min
[32m[20221213 18:18:26 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 18:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 18:18:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:26 @agent_ppo2.py:185][0m |           0.0042 |         251.3488 |           0.5965 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0047 |         239.6572 |           0.5955 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |           0.0039 |         254.4573 |           0.5955 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0076 |         236.3988 |           0.5949 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0069 |         235.6107 |           0.5937 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0080 |         234.8816 |           0.5941 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0090 |         234.7809 |           0.5938 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0099 |         233.9152 |           0.5941 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0102 |         233.8903 |           0.5946 |
[32m[20221213 18:18:27 @agent_ppo2.py:185][0m |          -0.0107 |         233.1141 |           0.5942 |
[32m[20221213 18:18:27 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 985.85
[32m[20221213 18:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.03
[32m[20221213 18:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.21
[32m[20221213 18:18:27 @agent_ppo2.py:143][0m Total time:      14.29 min
[32m[20221213 18:18:27 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221213 18:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 18:18:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0006 |         225.0067 |           0.5778 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0093 |         214.8160 |           0.5768 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0103 |         205.5206 |           0.5761 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0116 |         201.9547 |           0.5754 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0110 |         200.1432 |           0.5750 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0128 |         198.9706 |           0.5746 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0110 |         198.5871 |           0.5742 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0105 |         196.4828 |           0.5733 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0128 |         196.0849 |           0.5739 |
[32m[20221213 18:18:28 @agent_ppo2.py:185][0m |          -0.0055 |         216.2724 |           0.5733 |
[32m[20221213 18:18:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.09
[32m[20221213 18:18:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.38
[32m[20221213 18:18:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 922.30
[32m[20221213 18:18:29 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221213 18:18:29 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 18:18:29 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 18:18:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0034 |         247.1900 |           0.5837 |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0007 |         244.8147 |           0.5823 |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0079 |         239.0181 |           0.5828 |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0076 |         237.3820 |           0.5827 |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0022 |         245.0087 |           0.5829 |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0081 |         235.6479 |           0.5822 |
[32m[20221213 18:18:29 @agent_ppo2.py:185][0m |          -0.0056 |         235.9707 |           0.5827 |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |          -0.0009 |         242.9430 |           0.5823 |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |          -0.0113 |         234.4424 |           0.5826 |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |          -0.0104 |         234.1630 |           0.5826 |
[32m[20221213 18:18:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 915.73
[32m[20221213 18:18:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.52
[32m[20221213 18:18:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.44
[32m[20221213 18:18:30 @agent_ppo2.py:143][0m Total time:      14.34 min
[32m[20221213 18:18:30 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221213 18:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 18:18:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |          -0.0022 |         261.9102 |           0.5933 |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |           0.0017 |         261.3422 |           0.5922 |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |          -0.0079 |         241.6516 |           0.5916 |
[32m[20221213 18:18:30 @agent_ppo2.py:185][0m |          -0.0113 |         236.2468 |           0.5912 |
[32m[20221213 18:18:31 @agent_ppo2.py:185][0m |          -0.0117 |         232.5659 |           0.5912 |
[32m[20221213 18:18:31 @agent_ppo2.py:185][0m |          -0.0013 |         260.4090 |           0.5908 |
[32m[20221213 18:18:31 @agent_ppo2.py:185][0m |          -0.0004 |         238.8411 |           0.5908 |
[32m[20221213 18:18:31 @agent_ppo2.py:185][0m |          -0.0138 |         222.8380 |           0.5902 |
[32m[20221213 18:18:31 @agent_ppo2.py:185][0m |          -0.0125 |         219.7997 |           0.5902 |
[32m[20221213 18:18:31 @agent_ppo2.py:185][0m |          -0.0176 |         218.0632 |           0.5901 |
[32m[20221213 18:18:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.81
[32m[20221213 18:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.80
[32m[20221213 18:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 981.37
[32m[20221213 18:18:31 @agent_ppo2.py:143][0m Total time:      14.36 min
[32m[20221213 18:18:31 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 18:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 18:18:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |           0.0081 |         267.2742 |           0.5797 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0072 |         237.1257 |           0.5784 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0061 |         231.4657 |           0.5784 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0093 |         229.1273 |           0.5785 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0099 |         226.8295 |           0.5784 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0121 |         225.2788 |           0.5782 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0135 |         223.5510 |           0.5785 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0127 |         222.5219 |           0.5787 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0027 |         224.9967 |           0.5784 |
[32m[20221213 18:18:32 @agent_ppo2.py:185][0m |          -0.0119 |         222.1445 |           0.5782 |
[32m[20221213 18:18:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 852.54
[32m[20221213 18:18:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 951.25
[32m[20221213 18:18:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 963.08
[32m[20221213 18:18:32 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221213 18:18:32 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221213 18:18:32 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 18:18:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |           0.0063 |         254.7290 |           0.5792 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0066 |         241.4259 |           0.5785 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0086 |         238.6012 |           0.5786 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0107 |         236.4091 |           0.5785 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0088 |         235.3357 |           0.5783 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0122 |         233.7196 |           0.5789 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |           0.0000 |         258.7952 |           0.5787 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0136 |         231.6439 |           0.5790 |
[32m[20221213 18:18:33 @agent_ppo2.py:185][0m |          -0.0130 |         230.6950 |           0.5787 |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |          -0.0144 |         230.8086 |           0.5789 |
[32m[20221213 18:18:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 947.42
[32m[20221213 18:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.44
[32m[20221213 18:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.83
[32m[20221213 18:18:34 @agent_ppo2.py:143][0m Total time:      14.40 min
[32m[20221213 18:18:34 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 18:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 18:18:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |           0.0124 |         237.0252 |           0.6108 |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |          -0.0058 |         212.6842 |           0.6105 |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |          -0.0021 |         212.9969 |           0.6101 |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |          -0.0098 |         206.9219 |           0.6087 |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |          -0.0111 |         205.4281 |           0.6084 |
[32m[20221213 18:18:34 @agent_ppo2.py:185][0m |          -0.0021 |         209.5491 |           0.6080 |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |          -0.0144 |         202.8822 |           0.6070 |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |          -0.0139 |         202.1899 |           0.6066 |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |          -0.0138 |         201.2909 |           0.6060 |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |          -0.0145 |         200.4363 |           0.6055 |
[32m[20221213 18:18:35 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.21
[32m[20221213 18:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.05
[32m[20221213 18:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.41
[32m[20221213 18:18:35 @agent_ppo2.py:143][0m Total time:      14.42 min
[32m[20221213 18:18:35 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221213 18:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 18:18:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |           0.0000 |         251.7944 |           0.5885 |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |          -0.0063 |         246.1667 |           0.5881 |
[32m[20221213 18:18:35 @agent_ppo2.py:185][0m |           0.0007 |         259.0890 |           0.5881 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |          -0.0013 |         248.8716 |           0.5878 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |          -0.0093 |         242.5402 |           0.5874 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |          -0.0065 |         241.6631 |           0.5875 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |          -0.0103 |         240.7224 |           0.5872 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |          -0.0118 |         240.7424 |           0.5870 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |          -0.0081 |         239.9769 |           0.5871 |
[32m[20221213 18:18:36 @agent_ppo2.py:185][0m |           0.0035 |         268.9611 |           0.5871 |
[32m[20221213 18:18:36 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.59
[32m[20221213 18:18:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.58
[32m[20221213 18:18:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.74
[32m[20221213 18:18:36 @agent_ppo2.py:143][0m Total time:      14.44 min
[32m[20221213 18:18:36 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 18:18:36 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 18:18:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |           0.0004 |         248.7201 |           0.5722 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |          -0.0033 |         243.1987 |           0.5710 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |          -0.0064 |         241.1699 |           0.5712 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |           0.0060 |         274.8094 |           0.5702 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |          -0.0043 |         241.9271 |           0.5689 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |          -0.0078 |         238.7280 |           0.5690 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |           0.0011 |         246.4248 |           0.5688 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |           0.0011 |         249.5417 |           0.5678 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |          -0.0073 |         237.2534 |           0.5664 |
[32m[20221213 18:18:37 @agent_ppo2.py:185][0m |          -0.0098 |         236.5538 |           0.5676 |
[32m[20221213 18:18:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:18:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.51
[32m[20221213 18:18:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.20
[32m[20221213 18:18:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 969.46
[32m[20221213 18:18:37 @agent_ppo2.py:143][0m Total time:      14.46 min
[32m[20221213 18:18:37 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221213 18:18:37 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 18:18:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |           0.0066 |         257.1370 |           0.5636 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0057 |         233.3267 |           0.5623 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0006 |         235.7591 |           0.5616 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0118 |         221.8393 |           0.5605 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0113 |         217.6250 |           0.5601 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0149 |         214.6385 |           0.5602 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0124 |         210.9173 |           0.5598 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0146 |         208.0675 |           0.5590 |
[32m[20221213 18:18:38 @agent_ppo2.py:185][0m |          -0.0163 |         204.9043 |           0.5593 |
[32m[20221213 18:18:39 @agent_ppo2.py:185][0m |          -0.0170 |         202.1225 |           0.5589 |
[32m[20221213 18:18:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.19
[32m[20221213 18:18:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.83
[32m[20221213 18:18:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.21
[32m[20221213 18:18:39 @agent_ppo2.py:143][0m Total time:      14.48 min
[32m[20221213 18:18:39 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 18:18:39 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 18:18:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:39 @agent_ppo2.py:185][0m |           0.0007 |         250.8986 |           0.5565 |
[32m[20221213 18:18:39 @agent_ppo2.py:185][0m |          -0.0063 |         244.3783 |           0.5556 |
[32m[20221213 18:18:39 @agent_ppo2.py:185][0m |          -0.0056 |         241.0373 |           0.5553 |
[32m[20221213 18:18:39 @agent_ppo2.py:185][0m |          -0.0079 |         239.3288 |           0.5548 |
[32m[20221213 18:18:39 @agent_ppo2.py:185][0m |          -0.0088 |         238.1730 |           0.5542 |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |          -0.0049 |         240.4580 |           0.5544 |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |          -0.0065 |         235.8172 |           0.5536 |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |          -0.0102 |         234.6221 |           0.5542 |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |           0.0021 |         249.9609 |           0.5536 |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |          -0.0104 |         232.7326 |           0.5533 |
[32m[20221213 18:18:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 969.13
[32m[20221213 18:18:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.35
[32m[20221213 18:18:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 930.34
[32m[20221213 18:18:40 @agent_ppo2.py:143][0m Total time:      14.50 min
[32m[20221213 18:18:40 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221213 18:18:40 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 18:18:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |           0.0121 |         273.3616 |           0.5488 |
[32m[20221213 18:18:40 @agent_ppo2.py:185][0m |          -0.0067 |         243.0163 |           0.5474 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0069 |         240.7565 |           0.5470 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0083 |         239.9047 |           0.5464 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0079 |         238.3906 |           0.5461 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0097 |         238.1041 |           0.5465 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0105 |         237.5071 |           0.5458 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0105 |         237.0087 |           0.5456 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0111 |         236.5143 |           0.5454 |
[32m[20221213 18:18:41 @agent_ppo2.py:185][0m |          -0.0040 |         242.4052 |           0.5456 |
[32m[20221213 18:18:41 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.37
[32m[20221213 18:18:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.85
[32m[20221213 18:18:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.25
[32m[20221213 18:18:41 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221213 18:18:41 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 18:18:41 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 18:18:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0016 |         244.8018 |           0.5558 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0055 |         242.4759 |           0.5560 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0070 |         241.4047 |           0.5560 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0062 |         240.2251 |           0.5562 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0017 |         250.2858 |           0.5562 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |           0.0059 |         252.3118 |           0.5566 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0097 |         238.3761 |           0.5567 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0103 |         238.5058 |           0.5564 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0069 |         238.6001 |           0.5570 |
[32m[20221213 18:18:42 @agent_ppo2.py:185][0m |          -0.0114 |         236.9505 |           0.5573 |
[32m[20221213 18:18:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:18:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.12
[32m[20221213 18:18:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.66
[32m[20221213 18:18:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.60
[32m[20221213 18:18:43 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221213 18:18:43 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221213 18:18:43 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 18:18:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0012 |         247.0361 |           0.5591 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0038 |         243.8957 |           0.5576 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0049 |         241.4452 |           0.5573 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0072 |         240.0767 |           0.5566 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0088 |         239.3729 |           0.5562 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0104 |         239.2484 |           0.5563 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0092 |         238.0031 |           0.5556 |
[32m[20221213 18:18:43 @agent_ppo2.py:185][0m |          -0.0089 |         237.1503 |           0.5555 |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0085 |         236.8243 |           0.5549 |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0085 |         236.2940 |           0.5548 |
[32m[20221213 18:18:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 872.38
[32m[20221213 18:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 923.65
[32m[20221213 18:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.60
[32m[20221213 18:18:44 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221213 18:18:44 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 18:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 18:18:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0033 |         230.2145 |           0.5607 |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0081 |         220.7846 |           0.5590 |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0087 |         214.2269 |           0.5583 |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0017 |         223.2773 |           0.5575 |
[32m[20221213 18:18:44 @agent_ppo2.py:185][0m |          -0.0045 |         219.9772 |           0.5567 |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |          -0.0076 |         204.6329 |           0.5558 |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |          -0.0016 |         217.4672 |           0.5559 |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |           0.0017 |         229.8263 |           0.5554 |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |          -0.0027 |         209.8570 |           0.5538 |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |          -0.0141 |         197.2844 |           0.5540 |
[32m[20221213 18:18:45 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.95
[32m[20221213 18:18:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.71
[32m[20221213 18:18:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 981.75
[32m[20221213 18:18:45 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221213 18:18:45 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221213 18:18:45 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 18:18:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |           0.0043 |         258.3827 |           0.5563 |
[32m[20221213 18:18:45 @agent_ppo2.py:185][0m |          -0.0056 |         243.8471 |           0.5554 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0075 |         239.8158 |           0.5546 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0093 |         236.3354 |           0.5539 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0050 |         240.6576 |           0.5533 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0124 |         232.9496 |           0.5534 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0135 |         231.6716 |           0.5529 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0050 |         249.1722 |           0.5532 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0124 |         228.3727 |           0.5527 |
[32m[20221213 18:18:46 @agent_ppo2.py:185][0m |          -0.0087 |         237.9395 |           0.5532 |
[32m[20221213 18:18:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 910.76
[32m[20221213 18:18:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 993.77
[32m[20221213 18:18:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.18
[32m[20221213 18:18:46 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221213 18:18:46 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 18:18:46 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 18:18:47 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0005 |         243.9203 |           0.5343 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0037 |         238.2493 |           0.5329 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0075 |         237.8517 |           0.5326 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0004 |         243.3184 |           0.5324 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0068 |         235.3855 |           0.5318 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0068 |         234.6393 |           0.5315 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0096 |         234.4689 |           0.5311 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0087 |         234.1196 |           0.5317 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0036 |         235.8111 |           0.5312 |
[32m[20221213 18:18:47 @agent_ppo2.py:185][0m |          -0.0089 |         234.2019 |           0.5308 |
[32m[20221213 18:18:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.83
[32m[20221213 18:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.89
[32m[20221213 18:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.34
[32m[20221213 18:18:48 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221213 18:18:48 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221213 18:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 18:18:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |           0.0067 |         248.2717 |           0.5469 |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |          -0.0036 |         240.2813 |           0.5461 |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |          -0.0060 |         238.5432 |           0.5469 |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |          -0.0024 |         238.8690 |           0.5462 |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |          -0.0073 |         236.3109 |           0.5466 |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |          -0.0080 |         235.7882 |           0.5476 |
[32m[20221213 18:18:48 @agent_ppo2.py:185][0m |          -0.0085 |         235.0300 |           0.5471 |
[32m[20221213 18:18:49 @agent_ppo2.py:185][0m |          -0.0095 |         234.5999 |           0.5477 |
[32m[20221213 18:18:49 @agent_ppo2.py:185][0m |          -0.0085 |         234.5940 |           0.5476 |
[32m[20221213 18:18:49 @agent_ppo2.py:185][0m |          -0.0089 |         234.2075 |           0.5483 |
[32m[20221213 18:18:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 920.66
[32m[20221213 18:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.33
[32m[20221213 18:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 962.05
[32m[20221213 18:18:49 @agent_ppo2.py:143][0m Total time:      14.65 min
[32m[20221213 18:18:49 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 18:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 18:18:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:49 @agent_ppo2.py:185][0m |          -0.0008 |         242.7088 |           0.5503 |
[32m[20221213 18:18:49 @agent_ppo2.py:185][0m |          -0.0060 |         239.4610 |           0.5498 |
[32m[20221213 18:18:49 @agent_ppo2.py:185][0m |          -0.0076 |         237.6427 |           0.5489 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0075 |         236.4119 |           0.5486 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0101 |         235.5392 |           0.5487 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0067 |         237.8130 |           0.5486 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0105 |         234.2977 |           0.5479 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0107 |         234.3947 |           0.5479 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0113 |         234.0192 |           0.5477 |
[32m[20221213 18:18:50 @agent_ppo2.py:185][0m |          -0.0112 |         233.8390 |           0.5472 |
[32m[20221213 18:18:50 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:18:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 932.41
[32m[20221213 18:18:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 980.04
[32m[20221213 18:18:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.84
[32m[20221213 18:18:50 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221213 18:18:50 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221213 18:18:50 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 18:18:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |           0.0130 |         256.2873 |           0.5320 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0045 |         224.1668 |           0.5316 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0081 |         219.2799 |           0.5307 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0103 |         217.1859 |           0.5309 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0094 |         214.6687 |           0.5297 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0091 |         214.3036 |           0.5304 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |           0.0011 |         229.8105 |           0.5298 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0125 |         211.3437 |           0.5295 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0137 |         210.4243 |           0.5297 |
[32m[20221213 18:18:51 @agent_ppo2.py:185][0m |          -0.0134 |         216.1726 |           0.5294 |
[32m[20221213 18:18:51 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.33
[32m[20221213 18:18:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.84
[32m[20221213 18:18:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.98
[32m[20221213 18:18:51 @agent_ppo2.py:143][0m Total time:      14.69 min
[32m[20221213 18:18:51 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 18:18:51 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 18:18:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |           0.0032 |         257.5855 |           0.5524 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0064 |         228.4470 |           0.5520 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0074 |         221.8687 |           0.5514 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0124 |         218.7386 |           0.5514 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0114 |         216.1106 |           0.5511 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0049 |         221.2105 |           0.5512 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0153 |         212.3066 |           0.5511 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0061 |         224.6429 |           0.5508 |
[32m[20221213 18:18:52 @agent_ppo2.py:185][0m |          -0.0019 |         228.4426 |           0.5514 |
[32m[20221213 18:18:53 @agent_ppo2.py:185][0m |          -0.0145 |         208.7372 |           0.5510 |
[32m[20221213 18:18:53 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:18:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 912.56
[32m[20221213 18:18:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.33
[32m[20221213 18:18:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.50
[32m[20221213 18:18:53 @agent_ppo2.py:143][0m Total time:      14.72 min
[32m[20221213 18:18:53 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221213 18:18:53 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 18:18:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:18:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:53 @agent_ppo2.py:185][0m |          -0.0024 |         246.9530 |           0.5287 |
[32m[20221213 18:18:53 @agent_ppo2.py:185][0m |          -0.0040 |         243.3305 |           0.5271 |
[32m[20221213 18:18:53 @agent_ppo2.py:185][0m |           0.0011 |         254.1519 |           0.5278 |
[32m[20221213 18:18:53 @agent_ppo2.py:185][0m |           0.0042 |         262.3324 |           0.5276 |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |          -0.0055 |         240.2245 |           0.5276 |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |           0.0008 |         253.9234 |           0.5279 |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |          -0.0081 |         239.5693 |           0.5281 |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |          -0.0082 |         238.6795 |           0.5284 |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |          -0.0093 |         238.6942 |           0.5282 |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |          -0.0039 |         241.1386 |           0.5286 |
[32m[20221213 18:18:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 18:18:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 970.27
[32m[20221213 18:18:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.56
[32m[20221213 18:18:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.52
[32m[20221213 18:18:54 @agent_ppo2.py:143][0m Total time:      14.74 min
[32m[20221213 18:18:54 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 18:18:54 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 18:18:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:54 @agent_ppo2.py:185][0m |           0.0064 |         241.3209 |           0.5467 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |           0.0064 |         226.6879 |           0.5460 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0030 |         205.3512 |           0.5454 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0108 |         194.0200 |           0.5455 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0093 |         191.2357 |           0.5452 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0109 |         188.8143 |           0.5453 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0122 |         186.9502 |           0.5450 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0140 |         185.1917 |           0.5454 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0116 |         183.9545 |           0.5449 |
[32m[20221213 18:18:55 @agent_ppo2.py:185][0m |          -0.0076 |         182.9275 |           0.5453 |
[32m[20221213 18:18:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 895.10
[32m[20221213 18:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.52
[32m[20221213 18:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.14
[32m[20221213 18:18:55 @agent_ppo2.py:143][0m Total time:      14.76 min
[32m[20221213 18:18:55 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221213 18:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 18:18:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |           0.0013 |         253.1458 |           0.5459 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |          -0.0044 |         248.6951 |           0.5462 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |          -0.0071 |         247.4961 |           0.5463 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |          -0.0035 |         247.4669 |           0.5465 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |           0.0075 |         270.4097 |           0.5468 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |          -0.0077 |         245.0638 |           0.5460 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |          -0.0074 |         243.8130 |           0.5461 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |           0.0090 |         284.4841 |           0.5465 |
[32m[20221213 18:18:56 @agent_ppo2.py:185][0m |          -0.0085 |         243.4262 |           0.5468 |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0075 |         243.0094 |           0.5464 |
[32m[20221213 18:18:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 944.26
[32m[20221213 18:18:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.39
[32m[20221213 18:18:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.08
[32m[20221213 18:18:57 @agent_ppo2.py:143][0m Total time:      14.78 min
[32m[20221213 18:18:57 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 18:18:57 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 18:18:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0022 |         249.4207 |           0.5333 |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0057 |         246.0960 |           0.5334 |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0098 |         245.1873 |           0.5331 |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0067 |         244.3635 |           0.5332 |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0048 |         244.8485 |           0.5328 |
[32m[20221213 18:18:57 @agent_ppo2.py:185][0m |          -0.0097 |         243.2659 |           0.5329 |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0093 |         242.8482 |           0.5329 |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0075 |         243.9888 |           0.5327 |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0107 |         242.0935 |           0.5326 |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0107 |         241.9391 |           0.5325 |
[32m[20221213 18:18:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.12
[32m[20221213 18:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.94
[32m[20221213 18:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.99
[32m[20221213 18:18:58 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221213 18:18:58 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221213 18:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 18:18:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0048 |         246.3379 |           0.5444 |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0061 |         243.4718 |           0.5431 |
[32m[20221213 18:18:58 @agent_ppo2.py:185][0m |          -0.0072 |         242.7044 |           0.5430 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |           0.0030 |         260.2562 |           0.5425 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |           0.0015 |         271.0212 |           0.5420 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |          -0.0115 |         239.8987 |           0.5416 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |          -0.0119 |         238.9968 |           0.5421 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |          -0.0109 |         238.6756 |           0.5423 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |          -0.0075 |         240.8471 |           0.5421 |
[32m[20221213 18:18:59 @agent_ppo2.py:185][0m |          -0.0101 |         239.3927 |           0.5418 |
[32m[20221213 18:18:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 952.25
[32m[20221213 18:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.61
[32m[20221213 18:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 928.33
[32m[20221213 18:18:59 @agent_ppo2.py:143][0m Total time:      14.82 min
[32m[20221213 18:18:59 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 18:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 18:18:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:19:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0013 |         242.6360 |           0.5504 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |           0.0016 |         247.9992 |           0.5493 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0053 |         242.1304 |           0.5491 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0063 |         241.9028 |           0.5491 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0052 |         242.1296 |           0.5491 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0061 |         241.8599 |           0.5493 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0027 |         245.4088 |           0.5496 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0086 |         241.7015 |           0.5497 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0084 |         240.9444 |           0.5504 |
[32m[20221213 18:19:00 @agent_ppo2.py:185][0m |          -0.0017 |         246.6673 |           0.5500 |
[32m[20221213 18:19:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:19:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.64
[32m[20221213 18:19:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.83
[32m[20221213 18:19:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.71
[32m[20221213 18:19:01 @agent_ppo2.py:143][0m Total time:      14.85 min
[32m[20221213 18:19:01 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221213 18:19:01 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 18:19:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0002 |         247.6200 |           0.5454 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0014 |         246.3916 |           0.5454 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0053 |         244.6595 |           0.5449 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0061 |         244.2659 |           0.5446 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |           0.0015 |         254.6259 |           0.5446 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0017 |         247.1035 |           0.5449 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0081 |         243.2154 |           0.5451 |
[32m[20221213 18:19:01 @agent_ppo2.py:185][0m |          -0.0088 |         242.7624 |           0.5443 |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |           0.0006 |         258.4464 |           0.5450 |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |          -0.0065 |         242.5642 |           0.5448 |
[32m[20221213 18:19:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:19:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 915.09
[32m[20221213 18:19:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.13
[32m[20221213 18:19:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.02
[32m[20221213 18:19:02 @agent_ppo2.py:143][0m Total time:      14.87 min
[32m[20221213 18:19:02 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 18:19:02 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 18:19:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:19:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |          -0.0009 |         250.7037 |           0.5608 |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |          -0.0047 |         246.8008 |           0.5592 |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |          -0.0062 |         245.3256 |           0.5590 |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |          -0.0069 |         244.1046 |           0.5592 |
[32m[20221213 18:19:02 @agent_ppo2.py:185][0m |          -0.0084 |         243.6096 |           0.5587 |
[32m[20221213 18:19:03 @agent_ppo2.py:185][0m |           0.0023 |         255.8036 |           0.5590 |
[32m[20221213 18:19:03 @agent_ppo2.py:185][0m |          -0.0092 |         242.1850 |           0.5582 |
[32m[20221213 18:19:03 @agent_ppo2.py:185][0m |          -0.0017 |         248.7670 |           0.5583 |
[32m[20221213 18:19:03 @agent_ppo2.py:185][0m |          -0.0035 |         247.0267 |           0.5586 |
[32m[20221213 18:19:03 @agent_ppo2.py:185][0m |          -0.0110 |         240.7522 |           0.5589 |
[32m[20221213 18:19:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:19:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.87
[32m[20221213 18:19:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.14
[32m[20221213 18:19:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.88
[32m[20221213 18:19:03 @agent_ppo2.py:143][0m Total time:      14.89 min
[32m[20221213 18:19:03 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221213 18:19:03 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 18:19:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:03 @agent_ppo2.py:185][0m |           0.0006 |         250.4692 |           0.5600 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0046 |         246.5048 |           0.5591 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0055 |         245.4008 |           0.5584 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0026 |         249.8183 |           0.5587 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0026 |         251.5872 |           0.5571 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |           0.0044 |         268.9228 |           0.5566 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0034 |         245.0261 |           0.5577 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0111 |         241.5434 |           0.5571 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0019 |         246.8885 |           0.5564 |
[32m[20221213 18:19:04 @agent_ppo2.py:185][0m |          -0.0105 |         240.9644 |           0.5561 |
[32m[20221213 18:19:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:19:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 872.67
[32m[20221213 18:19:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.50
[32m[20221213 18:19:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.29
[32m[20221213 18:19:04 @agent_ppo2.py:143][0m Total time:      14.91 min
[32m[20221213 18:19:04 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 18:19:04 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 18:19:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |           0.0239 |         314.6629 |           0.5465 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0037 |         244.6341 |           0.5445 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |           0.0032 |         261.5766 |           0.5448 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0067 |         240.9282 |           0.5443 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0077 |         240.1873 |           0.5433 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0074 |         239.6093 |           0.5440 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0073 |         239.7942 |           0.5438 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0075 |         239.4679 |           0.5439 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |           0.0002 |         258.1088 |           0.5437 |
[32m[20221213 18:19:05 @agent_ppo2.py:185][0m |          -0.0100 |         238.5826 |           0.5428 |
[32m[20221213 18:19:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 962.16
[32m[20221213 18:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.50
[32m[20221213 18:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.82
[32m[20221213 18:19:06 @agent_ppo2.py:143][0m Total time:      14.93 min
[32m[20221213 18:19:06 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221213 18:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 18:19:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:06 @agent_ppo2.py:185][0m |           0.0003 |         248.0456 |           0.5513 |
[32m[20221213 18:19:06 @agent_ppo2.py:185][0m |           0.0052 |         256.2841 |           0.5505 |
[32m[20221213 18:19:06 @agent_ppo2.py:185][0m |          -0.0056 |         242.2224 |           0.5504 |
[32m[20221213 18:19:06 @agent_ppo2.py:185][0m |          -0.0068 |         240.6291 |           0.5499 |
[32m[20221213 18:19:06 @agent_ppo2.py:185][0m |          -0.0042 |         241.4876 |           0.5503 |
[32m[20221213 18:19:06 @agent_ppo2.py:185][0m |           0.0046 |         261.2703 |           0.5503 |
[32m[20221213 18:19:07 @agent_ppo2.py:185][0m |          -0.0061 |         239.1686 |           0.5497 |
[32m[20221213 18:19:07 @agent_ppo2.py:185][0m |          -0.0053 |         238.5486 |           0.5504 |
[32m[20221213 18:19:07 @agent_ppo2.py:185][0m |          -0.0077 |         238.3304 |           0.5505 |
[32m[20221213 18:19:07 @agent_ppo2.py:185][0m |          -0.0082 |         237.8405 |           0.5505 |
[32m[20221213 18:19:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:19:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.69
[32m[20221213 18:19:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.01
[32m[20221213 18:19:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 915.84
[32m[20221213 18:19:07 @agent_ppo2.py:143][0m Total time:      14.95 min
[32m[20221213 18:19:07 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 18:19:07 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 18:19:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:07 @agent_ppo2.py:185][0m |          -0.0023 |         238.9800 |           0.5383 |
[32m[20221213 18:19:07 @agent_ppo2.py:185][0m |          -0.0051 |         235.2264 |           0.5372 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0075 |         234.1401 |           0.5374 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0053 |         235.0460 |           0.5369 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0074 |         233.3462 |           0.5369 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0105 |         232.9454 |           0.5367 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0098 |         232.1828 |           0.5368 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0122 |         231.9919 |           0.5364 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0118 |         231.6900 |           0.5364 |
[32m[20221213 18:19:08 @agent_ppo2.py:185][0m |          -0.0108 |         231.8031 |           0.5364 |
[32m[20221213 18:19:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:19:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.09
[32m[20221213 18:19:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.31
[32m[20221213 18:19:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 905.00
[32m[20221213 18:19:08 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221213 18:19:08 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221213 18:19:08 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 18:19:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0005 |         246.3730 |           0.5462 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0025 |         242.5935 |           0.5461 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0081 |         239.4815 |           0.5474 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0082 |         237.9630 |           0.5468 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0085 |         237.1678 |           0.5478 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0090 |         236.7547 |           0.5480 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0103 |         237.0686 |           0.5491 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0076 |         237.1613 |           0.5494 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0111 |         235.2103 |           0.5495 |
[32m[20221213 18:19:09 @agent_ppo2.py:185][0m |          -0.0089 |         240.2255 |           0.5499 |
[32m[20221213 18:19:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.90
[32m[20221213 18:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.59
[32m[20221213 18:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.32
[32m[20221213 18:19:10 @agent_ppo2.py:143][0m Total time:      15.00 min
[32m[20221213 18:19:10 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 18:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 18:19:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:19:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0010 |         240.9601 |           0.5563 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0055 |         238.6294 |           0.5558 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0096 |         237.5789 |           0.5554 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0091 |         236.4429 |           0.5549 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0066 |         236.0004 |           0.5549 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0074 |         235.4300 |           0.5545 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0097 |         235.2755 |           0.5539 |
[32m[20221213 18:19:10 @agent_ppo2.py:185][0m |          -0.0101 |         234.6510 |           0.5536 |
[32m[20221213 18:19:11 @agent_ppo2.py:185][0m |          -0.0050 |         237.7809 |           0.5538 |
[32m[20221213 18:19:11 @agent_ppo2.py:185][0m |          -0.0099 |         233.9002 |           0.5529 |
[32m[20221213 18:19:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:19:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.75
[32m[20221213 18:19:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.89
[32m[20221213 18:19:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 940.02
[32m[20221213 18:19:11 @agent_ppo2.py:143][0m Total time:      15.02 min
[32m[20221213 18:19:11 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221213 18:19:11 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 18:19:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:11 @agent_ppo2.py:185][0m |          -0.0035 |         244.1155 |           0.5807 |
[32m[20221213 18:19:11 @agent_ppo2.py:185][0m |          -0.0027 |         243.3015 |           0.5801 |
[32m[20221213 18:19:11 @agent_ppo2.py:185][0m |           0.0041 |         249.7105 |           0.5798 |
[32m[20221213 18:19:11 @agent_ppo2.py:185][0m |          -0.0086 |         239.3990 |           0.5796 |
[32m[20221213 18:19:12 @agent_ppo2.py:185][0m |          -0.0100 |         238.8635 |           0.5799 |
[32m[20221213 18:19:12 @agent_ppo2.py:185][0m |          -0.0104 |         238.3838 |           0.5793 |
[32m[20221213 18:19:12 @agent_ppo2.py:185][0m |          -0.0101 |         238.0068 |           0.5796 |
[32m[20221213 18:19:12 @agent_ppo2.py:185][0m |          -0.0013 |         268.2903 |           0.5796 |
[32m[20221213 18:19:12 @agent_ppo2.py:185][0m |          -0.0061 |         242.7070 |           0.5785 |
[32m[20221213 18:19:12 @agent_ppo2.py:185][0m |          -0.0003 |         254.4192 |           0.5789 |
[32m[20221213 18:19:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:19:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.51
[32m[20221213 18:19:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.37
[32m[20221213 18:19:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.18
[32m[20221213 18:19:12 @agent_ppo2.py:143][0m Total time:      15.04 min
[32m[20221213 18:19:12 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 18:19:12 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 18:19:12 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0044 |         240.5326 |           0.5474 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0098 |         236.5580 |           0.5467 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0107 |         234.6330 |           0.5474 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0117 |         233.2433 |           0.5465 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0130 |         232.1885 |           0.5465 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0139 |         231.3928 |           0.5469 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0140 |         230.2452 |           0.5460 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0129 |         229.3600 |           0.5460 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0136 |         228.5220 |           0.5458 |
[32m[20221213 18:19:13 @agent_ppo2.py:185][0m |          -0.0149 |         227.6341 |           0.5456 |
[32m[20221213 18:19:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.62
[32m[20221213 18:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.45
[32m[20221213 18:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.30
[32m[20221213 18:19:13 @agent_ppo2.py:143][0m Total time:      15.06 min
[32m[20221213 18:19:13 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221213 18:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 18:19:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0037 |         245.1309 |           0.5597 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0059 |         242.5204 |           0.5583 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0057 |         241.6868 |           0.5586 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0074 |         241.1127 |           0.5578 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0081 |         239.7955 |           0.5581 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0060 |         239.2560 |           0.5580 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |           0.0066 |         273.3634 |           0.5581 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0097 |         237.5961 |           0.5571 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0091 |         237.3367 |           0.5572 |
[32m[20221213 18:19:14 @agent_ppo2.py:185][0m |          -0.0049 |         238.7904 |           0.5575 |
[32m[20221213 18:19:14 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:19:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 943.40
[32m[20221213 18:19:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.94
[32m[20221213 18:19:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.68
[32m[20221213 18:19:15 @agent_ppo2.py:143][0m Total time:      15.08 min
[32m[20221213 18:19:15 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 18:19:15 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 18:19:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |          -0.0012 |         245.0189 |           0.5613 |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |          -0.0062 |         242.5957 |           0.5603 |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |          -0.0090 |         241.2890 |           0.5595 |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |          -0.0059 |         241.0193 |           0.5588 |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |          -0.0066 |         241.9468 |           0.5579 |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |           0.0094 |         271.2064 |           0.5580 |
[32m[20221213 18:19:15 @agent_ppo2.py:185][0m |          -0.0002 |         253.4543 |           0.5567 |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0097 |         237.7811 |           0.5573 |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0121 |         237.3362 |           0.5569 |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0072 |         239.0308 |           0.5564 |
[32m[20221213 18:19:16 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:19:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.62
[32m[20221213 18:19:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.59
[32m[20221213 18:19:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.87
[32m[20221213 18:19:16 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221213 18:19:16 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221213 18:19:16 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 18:19:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0019 |         242.6195 |           0.5530 |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0000 |         246.1376 |           0.5524 |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0054 |         240.1115 |           0.5522 |
[32m[20221213 18:19:16 @agent_ppo2.py:185][0m |          -0.0039 |         239.5989 |           0.5519 |
[32m[20221213 18:19:17 @agent_ppo2.py:185][0m |          -0.0069 |         238.8724 |           0.5512 |
[32m[20221213 18:19:17 @agent_ppo2.py:185][0m |          -0.0011 |         248.0831 |           0.5514 |
[32m[20221213 18:19:17 @agent_ppo2.py:185][0m |          -0.0075 |         238.1985 |           0.5511 |
[32m[20221213 18:19:17 @agent_ppo2.py:185][0m |           0.0030 |         257.5168 |           0.5519 |
[32m[20221213 18:19:17 @agent_ppo2.py:185][0m |          -0.0082 |         237.7514 |           0.5504 |
[32m[20221213 18:19:17 @agent_ppo2.py:185][0m |          -0.0066 |         237.6090 |           0.5512 |
[32m[20221213 18:19:17 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:19:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 952.61
[32m[20221213 18:19:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.25
[32m[20221213 18:19:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.99
[32m[20221213 18:19:17 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 18:19:17 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 18:19:17 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 18:19:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0020 |         245.6637 |           0.5445 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0056 |         242.5491 |           0.5429 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0075 |         241.9137 |           0.5428 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0014 |         251.3110 |           0.5429 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0094 |         240.7666 |           0.5419 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0090 |         240.3763 |           0.5424 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0097 |         240.0580 |           0.5420 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0011 |         253.7880 |           0.5420 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0112 |         239.2321 |           0.5420 |
[32m[20221213 18:19:18 @agent_ppo2.py:185][0m |          -0.0123 |         239.1298 |           0.5419 |
[32m[20221213 18:19:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:19:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 923.65
[32m[20221213 18:19:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.33
[32m[20221213 18:19:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.92
[32m[20221213 18:19:18 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 999.92
[32m[20221213 18:19:18 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 999.92
[32m[20221213 18:19:18 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221213 18:19:18 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221213 18:19:18 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 18:19:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0015 |         244.8477 |           0.5480 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0025 |         243.6960 |           0.5474 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0079 |         241.3781 |           0.5467 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0070 |         240.0942 |           0.5476 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0073 |         239.2411 |           0.5474 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0042 |         239.7873 |           0.5473 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |           0.0023 |         249.8059 |           0.5474 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0103 |         237.4308 |           0.5478 |
[32m[20221213 18:19:19 @agent_ppo2.py:185][0m |          -0.0105 |         236.7748 |           0.5473 |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |          -0.0112 |         236.4978 |           0.5485 |
[32m[20221213 18:19:20 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.12
[32m[20221213 18:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 934.94
[32m[20221213 18:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 932.33
[32m[20221213 18:19:20 @agent_ppo2.py:143][0m Total time:      15.17 min
[32m[20221213 18:19:20 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 18:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 18:19:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |           0.0036 |         249.2537 |           0.5456 |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |          -0.0058 |         239.9356 |           0.5451 |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |          -0.0002 |         243.8117 |           0.5453 |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |          -0.0072 |         236.6205 |           0.5453 |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |           0.0058 |         255.3879 |           0.5457 |
[32m[20221213 18:19:20 @agent_ppo2.py:185][0m |          -0.0087 |         234.5235 |           0.5458 |
[32m[20221213 18:19:21 @agent_ppo2.py:185][0m |          -0.0039 |         239.0710 |           0.5456 |
[32m[20221213 18:19:21 @agent_ppo2.py:185][0m |          -0.0105 |         232.6896 |           0.5456 |
[32m[20221213 18:19:21 @agent_ppo2.py:185][0m |          -0.0106 |         231.6970 |           0.5467 |
[32m[20221213 18:19:21 @agent_ppo2.py:185][0m |           0.0011 |         242.8081 |           0.5461 |
[32m[20221213 18:19:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:19:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.21
[32m[20221213 18:19:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.11
[32m[20221213 18:19:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 934.83
[32m[20221213 18:19:21 @agent_ppo2.py:143][0m Total time:      15.19 min
[32m[20221213 18:19:21 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221213 18:19:21 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 18:19:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:19:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:21 @agent_ppo2.py:185][0m |           0.0002 |         242.8126 |           0.5588 |
[32m[20221213 18:19:21 @agent_ppo2.py:185][0m |          -0.0035 |         235.5284 |           0.5571 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0044 |         235.2482 |           0.5566 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0069 |         233.2588 |           0.5557 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0069 |         232.6603 |           0.5554 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0045 |         235.3802 |           0.5548 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0110 |         232.2279 |           0.5544 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0063 |         232.1184 |           0.5543 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0084 |         231.1633 |           0.5536 |
[32m[20221213 18:19:22 @agent_ppo2.py:185][0m |          -0.0074 |         230.7417 |           0.5535 |
[32m[20221213 18:19:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:19:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.49
[32m[20221213 18:19:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.10
[32m[20221213 18:19:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.16
[32m[20221213 18:19:22 @agent_ppo2.py:143][0m Total time:      15.21 min
[32m[20221213 18:19:22 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 18:19:22 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 18:19:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0011 |         235.8964 |           0.5410 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0005 |         234.0177 |           0.5415 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0070 |         231.8860 |           0.5408 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0065 |         231.3837 |           0.5412 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0083 |         230.3317 |           0.5411 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0061 |         231.3541 |           0.5410 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0056 |         229.4709 |           0.5410 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0086 |         229.1235 |           0.5414 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0120 |         228.4720 |           0.5412 |
[32m[20221213 18:19:23 @agent_ppo2.py:185][0m |          -0.0102 |         228.5681 |           0.5417 |
[32m[20221213 18:19:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.40
[32m[20221213 18:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.09
[32m[20221213 18:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.26
[32m[20221213 18:19:24 @agent_ppo2.py:143][0m Total time:      15.23 min
[32m[20221213 18:19:24 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221213 18:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 18:19:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0028 |         236.5715 |           0.5520 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0069 |         235.0418 |           0.5506 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0087 |         234.0389 |           0.5496 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0086 |         233.2520 |           0.5493 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0102 |         232.9820 |           0.5488 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0102 |         232.1196 |           0.5480 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0095 |         232.1555 |           0.5479 |
[32m[20221213 18:19:24 @agent_ppo2.py:185][0m |          -0.0099 |         231.5383 |           0.5474 |
[32m[20221213 18:19:25 @agent_ppo2.py:185][0m |          -0.0104 |         231.1809 |           0.5467 |
[32m[20221213 18:19:25 @agent_ppo2.py:185][0m |          -0.0107 |         230.7413 |           0.5464 |
[32m[20221213 18:19:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:19:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 983.53
[32m[20221213 18:19:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.81
[32m[20221213 18:19:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.01
[32m[20221213 18:19:25 @agent_ppo2.py:143][0m Total time:      15.25 min
[32m[20221213 18:19:25 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 18:19:25 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 18:19:25 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:19:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:25 @agent_ppo2.py:185][0m |          -0.0005 |         227.8361 |           0.5476 |
[32m[20221213 18:19:25 @agent_ppo2.py:185][0m |          -0.0014 |         212.2907 |           0.5466 |
[32m[20221213 18:19:25 @agent_ppo2.py:185][0m |          -0.0066 |         207.0585 |           0.5468 |
[32m[20221213 18:19:25 @agent_ppo2.py:185][0m |           0.0026 |         222.3716 |           0.5470 |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |          -0.0087 |         202.9703 |           0.5472 |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |          -0.0105 |         201.2557 |           0.5472 |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |          -0.0103 |         199.5584 |           0.5476 |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |           0.0035 |         217.5768 |           0.5476 |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |          -0.0116 |         197.8730 |           0.5467 |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |           0.0002 |         206.1974 |           0.5468 |
[32m[20221213 18:19:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.35
[32m[20221213 18:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.98
[32m[20221213 18:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.29
[32m[20221213 18:19:26 @agent_ppo2.py:143][0m Total time:      15.27 min
[32m[20221213 18:19:26 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221213 18:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 18:19:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:26 @agent_ppo2.py:185][0m |          -0.0014 |         250.9015 |           0.5428 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0046 |         243.8294 |           0.5423 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0061 |         242.1535 |           0.5422 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0081 |         240.7298 |           0.5419 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0062 |         240.2130 |           0.5420 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0079 |         239.0520 |           0.5418 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0083 |         239.2796 |           0.5420 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0049 |         241.5000 |           0.5415 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0106 |         237.6397 |           0.5418 |
[32m[20221213 18:19:27 @agent_ppo2.py:185][0m |          -0.0084 |         238.0755 |           0.5417 |
[32m[20221213 18:19:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:19:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 928.86
[32m[20221213 18:19:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.50
[32m[20221213 18:19:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.41
[32m[20221213 18:19:27 @agent_ppo2.py:143][0m Total time:      15.29 min
[32m[20221213 18:19:27 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 18:19:27 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 18:19:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:19:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0029 |         237.8795 |           0.5466 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0062 |         234.5293 |           0.5450 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |           0.0036 |         243.0099 |           0.5447 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0090 |         233.2112 |           0.5444 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0062 |         232.3953 |           0.5442 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0111 |         231.9407 |           0.5447 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0093 |         231.6359 |           0.5436 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0121 |         231.4328 |           0.5434 |
[32m[20221213 18:19:28 @agent_ppo2.py:185][0m |          -0.0054 |         236.0575 |           0.5430 |
[32m[20221213 18:19:29 @agent_ppo2.py:185][0m |          -0.0098 |         231.5276 |           0.5428 |
[32m[20221213 18:19:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:19:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 897.35
[32m[20221213 18:19:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.74
[32m[20221213 18:19:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.62
[32m[20221213 18:19:29 @agent_ppo2.py:143][0m Total time:      15.32 min
[32m[20221213 18:19:29 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221213 18:19:29 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 18:19:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:19:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:29 @agent_ppo2.py:185][0m |           0.0134 |         268.1120 |           0.5472 |
[32m[20221213 18:19:29 @agent_ppo2.py:185][0m |          -0.0064 |         238.3611 |           0.5459 |
[32m[20221213 18:19:29 @agent_ppo2.py:185][0m |          -0.0042 |         239.3421 |           0.5457 |
[32m[20221213 18:19:29 @agent_ppo2.py:185][0m |           0.0092 |         262.0771 |           0.5452 |
[32m[20221213 18:19:29 @agent_ppo2.py:185][0m |          -0.0097 |         235.5040 |           0.5445 |
[32m[20221213 18:19:30 @agent_ppo2.py:185][0m |          -0.0109 |         235.3211 |           0.5448 |
[32m[20221213 18:19:30 @agent_ppo2.py:185][0m |          -0.0117 |         234.6510 |           0.5446 |
[32m[20221213 18:19:30 @agent_ppo2.py:185][0m |          -0.0113 |         234.4585 |           0.5445 |
[32m[20221213 18:19:30 @agent_ppo2.py:185][0m |          -0.0121 |         234.0582 |           0.5446 |
[32m[20221213 18:19:30 @agent_ppo2.py:185][0m |          -0.0132 |         233.8088 |           0.5447 |
[32m[20221213 18:19:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:19:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.74
[32m[20221213 18:19:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.90
[32m[20221213 18:19:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 912.30
[32m[20221213 18:19:30 @agent_ppo2.py:143][0m Total time:      15.34 min
[32m[20221213 18:19:30 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 18:19:30 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 18:19:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0013 |         243.3698 |           0.5355 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0061 |         237.4486 |           0.5351 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0079 |         234.5153 |           0.5346 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0102 |         232.4194 |           0.5339 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0065 |         230.7665 |           0.5338 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0097 |         228.5398 |           0.5336 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0106 |         226.7364 |           0.5334 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0109 |         225.3436 |           0.5332 |
[32m[20221213 18:19:31 @agent_ppo2.py:185][0m |          -0.0121 |         223.7154 |           0.5330 |
[32m[20221213 18:19:32 @agent_ppo2.py:185][0m |          -0.0135 |         222.3067 |           0.5332 |
[32m[20221213 18:19:32 @agent_ppo2.py:130][0m Policy update time: 1.29 s
[32m[20221213 18:19:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.10
[32m[20221213 18:19:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.67
[32m[20221213 18:19:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 937.78
[32m[20221213 18:19:32 @agent_ppo2.py:143][0m Total time:      15.37 min
[32m[20221213 18:19:32 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221213 18:19:32 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 18:19:32 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 18:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:32 @agent_ppo2.py:185][0m |          -0.0025 |         244.5615 |           0.5441 |
[32m[20221213 18:19:32 @agent_ppo2.py:185][0m |          -0.0053 |         241.1250 |           0.5429 |
[32m[20221213 18:19:32 @agent_ppo2.py:185][0m |          -0.0076 |         239.5109 |           0.5429 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |          -0.0103 |         238.7257 |           0.5433 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |           0.0036 |         269.4772 |           0.5431 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |          -0.0091 |         237.3392 |           0.5424 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |          -0.0098 |         236.7803 |           0.5427 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |          -0.0106 |         236.6090 |           0.5432 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |          -0.0103 |         235.8884 |           0.5429 |
[32m[20221213 18:19:33 @agent_ppo2.py:185][0m |          -0.0101 |         235.8207 |           0.5425 |
[32m[20221213 18:19:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 987.65
[32m[20221213 18:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.56
[32m[20221213 18:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.42
[32m[20221213 18:19:33 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221213 18:19:33 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 18:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 18:19:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0015 |         246.7852 |           0.5454 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |           0.0016 |         242.9465 |           0.5447 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0062 |         240.4974 |           0.5443 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0060 |         239.1206 |           0.5443 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0052 |         239.8317 |           0.5439 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0056 |         237.2451 |           0.5437 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0064 |         236.5355 |           0.5434 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0080 |         235.6252 |           0.5430 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0092 |         234.8187 |           0.5428 |
[32m[20221213 18:19:34 @agent_ppo2.py:185][0m |          -0.0086 |         234.3687 |           0.5425 |
[32m[20221213 18:19:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.49
[32m[20221213 18:19:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.36
[32m[20221213 18:19:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.36
[32m[20221213 18:19:35 @agent_ppo2.py:143][0m Total time:      15.42 min
[32m[20221213 18:19:35 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221213 18:19:35 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 18:19:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:35 @agent_ppo2.py:185][0m |          -0.0003 |         243.2686 |           0.5391 |
[32m[20221213 18:19:35 @agent_ppo2.py:185][0m |           0.0031 |         247.0909 |           0.5385 |
[32m[20221213 18:19:35 @agent_ppo2.py:185][0m |          -0.0064 |         238.7567 |           0.5376 |
[32m[20221213 18:19:35 @agent_ppo2.py:185][0m |          -0.0072 |         237.7544 |           0.5379 |
[32m[20221213 18:19:35 @agent_ppo2.py:185][0m |          -0.0096 |         237.5540 |           0.5377 |
[32m[20221213 18:19:36 @agent_ppo2.py:185][0m |          -0.0078 |         237.1642 |           0.5376 |
[32m[20221213 18:19:36 @agent_ppo2.py:185][0m |          -0.0083 |         236.6217 |           0.5373 |
[32m[20221213 18:19:36 @agent_ppo2.py:185][0m |          -0.0064 |         236.5452 |           0.5373 |
[32m[20221213 18:19:36 @agent_ppo2.py:185][0m |          -0.0085 |         236.3059 |           0.5377 |
[32m[20221213 18:19:36 @agent_ppo2.py:185][0m |          -0.0101 |         235.7508 |           0.5372 |
[32m[20221213 18:19:36 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.69
[32m[20221213 18:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.73
[32m[20221213 18:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.00
[32m[20221213 18:19:36 @agent_ppo2.py:143][0m Total time:      15.44 min
[32m[20221213 18:19:36 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 18:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 18:19:36 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:19:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:36 @agent_ppo2.py:185][0m |           0.0045 |         240.2709 |           0.5442 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0030 |         234.9126 |           0.5430 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0032 |         235.3817 |           0.5430 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |           0.0003 |         242.3568 |           0.5429 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0076 |         232.7462 |           0.5427 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0085 |         232.5251 |           0.5435 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0095 |         232.2802 |           0.5427 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0090 |         232.1167 |           0.5425 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0104 |         232.2348 |           0.5417 |
[32m[20221213 18:19:37 @agent_ppo2.py:185][0m |          -0.0060 |         233.3103 |           0.5426 |
[32m[20221213 18:19:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 943.23
[32m[20221213 18:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 994.52
[32m[20221213 18:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.20
[32m[20221213 18:19:37 @agent_ppo2.py:143][0m Total time:      15.46 min
[32m[20221213 18:19:37 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221213 18:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 18:19:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0007 |         241.4565 |           0.5495 |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0050 |         238.7152 |           0.5498 |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0078 |         237.7437 |           0.5494 |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0071 |         237.1619 |           0.5491 |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0069 |         236.5587 |           0.5497 |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0086 |         236.1153 |           0.5494 |
[32m[20221213 18:19:38 @agent_ppo2.py:185][0m |          -0.0083 |         235.7316 |           0.5498 |
[32m[20221213 18:19:39 @agent_ppo2.py:185][0m |          -0.0082 |         235.2532 |           0.5496 |
[32m[20221213 18:19:39 @agent_ppo2.py:185][0m |          -0.0104 |         235.4339 |           0.5500 |
[32m[20221213 18:19:39 @agent_ppo2.py:185][0m |          -0.0092 |         234.8534 |           0.5500 |
[32m[20221213 18:19:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 945.62
[32m[20221213 18:19:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.24
[32m[20221213 18:19:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.66
[32m[20221213 18:19:39 @agent_ppo2.py:143][0m Total time:      15.49 min
[32m[20221213 18:19:39 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 18:19:39 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 18:19:39 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 18:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:39 @agent_ppo2.py:185][0m |          -0.0017 |         238.0018 |           0.5496 |
[32m[20221213 18:19:39 @agent_ppo2.py:185][0m |          -0.0035 |         237.1864 |           0.5486 |
[32m[20221213 18:19:39 @agent_ppo2.py:185][0m |          -0.0051 |         236.5250 |           0.5477 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |          -0.0053 |         236.1895 |           0.5477 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |          -0.0055 |         236.3947 |           0.5477 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |           0.0051 |         263.6485 |           0.5473 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |          -0.0058 |         235.9636 |           0.5466 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |          -0.0068 |         235.8080 |           0.5475 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |          -0.0030 |         239.6335 |           0.5472 |
[32m[20221213 18:19:40 @agent_ppo2.py:185][0m |          -0.0077 |         235.7997 |           0.5472 |
[32m[20221213 18:19:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 969.87
[32m[20221213 18:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.46
[32m[20221213 18:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.42
[32m[20221213 18:19:40 @agent_ppo2.py:143][0m Total time:      15.51 min
[32m[20221213 18:19:40 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221213 18:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 18:19:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0017 |         240.9633 |           0.5343 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0053 |         239.6349 |           0.5337 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0053 |         238.8309 |           0.5331 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0085 |         238.5138 |           0.5338 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0080 |         237.9402 |           0.5334 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0079 |         238.1398 |           0.5339 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0088 |         237.6715 |           0.5338 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0063 |         237.8315 |           0.5342 |
[32m[20221213 18:19:41 @agent_ppo2.py:185][0m |          -0.0087 |         237.1546 |           0.5340 |
[32m[20221213 18:19:42 @agent_ppo2.py:185][0m |          -0.0101 |         237.1166 |           0.5344 |
[32m[20221213 18:19:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:19:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 955.43
[32m[20221213 18:19:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.11
[32m[20221213 18:19:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.20
[32m[20221213 18:19:42 @agent_ppo2.py:143][0m Total time:      15.53 min
[32m[20221213 18:19:42 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 18:19:42 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 18:19:42 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:19:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:42 @agent_ppo2.py:185][0m |          -0.0018 |         240.0282 |           0.5574 |
[32m[20221213 18:19:42 @agent_ppo2.py:185][0m |          -0.0072 |         238.2634 |           0.5566 |
[32m[20221213 18:19:42 @agent_ppo2.py:185][0m |          -0.0073 |         237.0557 |           0.5562 |
[32m[20221213 18:19:42 @agent_ppo2.py:185][0m |          -0.0089 |         235.4051 |           0.5566 |
[32m[20221213 18:19:42 @agent_ppo2.py:185][0m |          -0.0095 |         234.2053 |           0.5560 |
[32m[20221213 18:19:43 @agent_ppo2.py:185][0m |          -0.0093 |         233.4074 |           0.5564 |
[32m[20221213 18:19:43 @agent_ppo2.py:185][0m |           0.0055 |         263.8913 |           0.5557 |
[32m[20221213 18:19:43 @agent_ppo2.py:185][0m |           0.0009 |         245.1535 |           0.5551 |
[32m[20221213 18:19:43 @agent_ppo2.py:185][0m |          -0.0092 |         231.9277 |           0.5552 |
[32m[20221213 18:19:43 @agent_ppo2.py:185][0m |          -0.0077 |         234.0105 |           0.5556 |
[32m[20221213 18:19:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:19:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 986.21
[32m[20221213 18:19:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.58
[32m[20221213 18:19:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 924.26
[32m[20221213 18:19:43 @agent_ppo2.py:143][0m Total time:      15.56 min
[32m[20221213 18:19:43 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221213 18:19:43 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 18:19:43 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |           0.0062 |         247.9840 |           0.5499 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0080 |         240.0282 |           0.5491 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0042 |         238.2538 |           0.5490 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0090 |         234.8220 |           0.5487 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0069 |         235.6304 |           0.5485 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0076 |         232.1982 |           0.5483 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0113 |         231.4989 |           0.5486 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0103 |         231.3515 |           0.5481 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0100 |         230.4520 |           0.5484 |
[32m[20221213 18:19:44 @agent_ppo2.py:185][0m |          -0.0109 |         230.0236 |           0.5478 |
[32m[20221213 18:19:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 896.07
[32m[20221213 18:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.47
[32m[20221213 18:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 973.54
[32m[20221213 18:19:45 @agent_ppo2.py:143][0m Total time:      15.58 min
[32m[20221213 18:19:45 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 18:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 18:19:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |           0.0045 |         250.0398 |           0.5511 |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |           0.0052 |         252.1860 |           0.5500 |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |          -0.0013 |         243.6462 |           0.5495 |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |           0.0055 |         247.6485 |           0.5493 |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |           0.0001 |         253.0540 |           0.5487 |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |          -0.0077 |         236.6247 |           0.5493 |
[32m[20221213 18:19:45 @agent_ppo2.py:185][0m |          -0.0094 |         236.4500 |           0.5486 |
[32m[20221213 18:19:46 @agent_ppo2.py:185][0m |          -0.0085 |         236.2103 |           0.5484 |
[32m[20221213 18:19:46 @agent_ppo2.py:185][0m |          -0.0036 |         242.2680 |           0.5475 |
[32m[20221213 18:19:46 @agent_ppo2.py:185][0m |           0.0071 |         261.7871 |           0.5476 |
[32m[20221213 18:19:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:19:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 930.36
[32m[20221213 18:19:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.14
[32m[20221213 18:19:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.08
[32m[20221213 18:19:46 @agent_ppo2.py:143][0m Total time:      15.60 min
[32m[20221213 18:19:46 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221213 18:19:46 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 18:19:46 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:19:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:46 @agent_ppo2.py:185][0m |          -0.0017 |         237.2649 |           0.5337 |
[32m[20221213 18:19:46 @agent_ppo2.py:185][0m |          -0.0041 |         235.9360 |           0.5333 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0052 |         235.4482 |           0.5325 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0046 |         235.1301 |           0.5330 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0059 |         234.4815 |           0.5328 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0054 |         234.4925 |           0.5332 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0027 |         236.8620 |           0.5324 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0036 |         235.6824 |           0.5323 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0073 |         233.8851 |           0.5317 |
[32m[20221213 18:19:47 @agent_ppo2.py:185][0m |          -0.0080 |         233.9764 |           0.5322 |
[32m[20221213 18:19:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:19:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.39
[32m[20221213 18:19:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.84
[32m[20221213 18:19:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.90
[32m[20221213 18:19:47 @agent_ppo2.py:143][0m Total time:      15.63 min
[32m[20221213 18:19:47 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 18:19:47 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 18:19:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |           0.0017 |         236.9328 |           0.5538 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |           0.0003 |         242.2609 |           0.5535 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |          -0.0049 |         233.0413 |           0.5523 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |          -0.0054 |         232.4468 |           0.5525 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |          -0.0078 |         231.9667 |           0.5528 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |          -0.0077 |         231.8058 |           0.5522 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |          -0.0089 |         231.3947 |           0.5520 |
[32m[20221213 18:19:48 @agent_ppo2.py:185][0m |          -0.0083 |         231.1088 |           0.5524 |
[32m[20221213 18:19:49 @agent_ppo2.py:185][0m |           0.0059 |         260.8581 |           0.5516 |
[32m[20221213 18:19:49 @agent_ppo2.py:185][0m |           0.0023 |         245.4525 |           0.5513 |
[32m[20221213 18:19:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 945.28
[32m[20221213 18:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.35
[32m[20221213 18:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.66
[32m[20221213 18:19:49 @agent_ppo2.py:143][0m Total time:      15.65 min
[32m[20221213 18:19:49 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221213 18:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 18:19:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:49 @agent_ppo2.py:185][0m |          -0.0015 |         233.9564 |           0.5446 |
[32m[20221213 18:19:49 @agent_ppo2.py:185][0m |          -0.0046 |         231.3967 |           0.5430 |
[32m[20221213 18:19:49 @agent_ppo2.py:185][0m |          -0.0039 |         229.3954 |           0.5427 |
[32m[20221213 18:19:49 @agent_ppo2.py:185][0m |          -0.0071 |         227.8465 |           0.5427 |
[32m[20221213 18:19:50 @agent_ppo2.py:185][0m |          -0.0061 |         226.5562 |           0.5421 |
[32m[20221213 18:19:50 @agent_ppo2.py:185][0m |          -0.0077 |         226.1617 |           0.5417 |
[32m[20221213 18:19:50 @agent_ppo2.py:185][0m |          -0.0074 |         225.0672 |           0.5418 |
[32m[20221213 18:19:50 @agent_ppo2.py:185][0m |          -0.0082 |         224.6364 |           0.5414 |
[32m[20221213 18:19:50 @agent_ppo2.py:185][0m |          -0.0079 |         223.5848 |           0.5406 |
[32m[20221213 18:19:50 @agent_ppo2.py:185][0m |          -0.0091 |         223.0750 |           0.5407 |
[32m[20221213 18:19:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.42
[32m[20221213 18:19:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.51
[32m[20221213 18:19:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.19
[32m[20221213 18:19:50 @agent_ppo2.py:143][0m Total time:      15.67 min
[32m[20221213 18:19:50 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 18:19:50 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 18:19:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |           0.0081 |         246.7793 |           0.5212 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |           0.0036 |         238.2969 |           0.5200 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0049 |         233.5847 |           0.5200 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0062 |         232.4815 |           0.5201 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0097 |         231.3378 |           0.5201 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0069 |         230.7895 |           0.5203 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0007 |         244.3648 |           0.5209 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0096 |         229.8213 |           0.5197 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0027 |         246.8592 |           0.5212 |
[32m[20221213 18:19:51 @agent_ppo2.py:185][0m |          -0.0116 |         229.0132 |           0.5206 |
[32m[20221213 18:19:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:19:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.12
[32m[20221213 18:19:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.63
[32m[20221213 18:19:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.01
[32m[20221213 18:19:52 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 18:19:52 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221213 18:19:52 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 18:19:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:52 @agent_ppo2.py:185][0m |          -0.0006 |         245.8085 |           0.5398 |
[32m[20221213 18:19:52 @agent_ppo2.py:185][0m |          -0.0044 |         215.3530 |           0.5393 |
[32m[20221213 18:19:52 @agent_ppo2.py:185][0m |          -0.0026 |         203.6496 |           0.5391 |
[32m[20221213 18:19:52 @agent_ppo2.py:185][0m |          -0.0079 |         190.7966 |           0.5390 |
[32m[20221213 18:19:52 @agent_ppo2.py:185][0m |          -0.0089 |         186.3705 |           0.5385 |
[32m[20221213 18:19:52 @agent_ppo2.py:185][0m |          -0.0075 |         184.0996 |           0.5387 |
[32m[20221213 18:19:53 @agent_ppo2.py:185][0m |          -0.0083 |         181.7140 |           0.5387 |
[32m[20221213 18:19:53 @agent_ppo2.py:185][0m |           0.0025 |         207.9942 |           0.5389 |
[32m[20221213 18:19:53 @agent_ppo2.py:185][0m |          -0.0096 |         179.8392 |           0.5385 |
[32m[20221213 18:19:53 @agent_ppo2.py:185][0m |          -0.0095 |         178.1365 |           0.5389 |
[32m[20221213 18:19:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 944.03
[32m[20221213 18:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.94
[32m[20221213 18:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.09
[32m[20221213 18:19:53 @agent_ppo2.py:143][0m Total time:      15.72 min
[32m[20221213 18:19:53 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 18:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 18:19:53 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 18:19:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:53 @agent_ppo2.py:185][0m |          -0.0018 |         249.7967 |           0.5422 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0041 |         243.0424 |           0.5402 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0079 |         241.1102 |           0.5396 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0094 |         239.8438 |           0.5395 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0042 |         241.7046 |           0.5397 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0082 |         238.3784 |           0.5394 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0064 |         238.1971 |           0.5390 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0099 |         237.7937 |           0.5391 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0096 |         237.7397 |           0.5387 |
[32m[20221213 18:19:54 @agent_ppo2.py:185][0m |          -0.0091 |         237.1403 |           0.5387 |
[32m[20221213 18:19:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:19:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.67
[32m[20221213 18:19:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.17
[32m[20221213 18:19:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 908.10
[32m[20221213 18:19:54 @agent_ppo2.py:143][0m Total time:      15.74 min
[32m[20221213 18:19:54 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221213 18:19:54 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 18:19:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |          -0.0012 |         241.2112 |           0.5279 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |           0.0027 |         249.1128 |           0.5273 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |          -0.0068 |         236.7305 |           0.5262 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |          -0.0027 |         238.1082 |           0.5251 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |          -0.0071 |         234.9287 |           0.5252 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |           0.0239 |         290.6388 |           0.5247 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |          -0.0040 |         235.4152 |           0.5225 |
[32m[20221213 18:19:55 @agent_ppo2.py:185][0m |          -0.0082 |         233.2256 |           0.5228 |
[32m[20221213 18:19:56 @agent_ppo2.py:185][0m |          -0.0084 |         232.3107 |           0.5230 |
[32m[20221213 18:19:56 @agent_ppo2.py:185][0m |          -0.0109 |         231.6462 |           0.5229 |
[32m[20221213 18:19:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:19:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 974.53
[32m[20221213 18:19:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.72
[32m[20221213 18:19:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 886.77
[32m[20221213 18:19:56 @agent_ppo2.py:143][0m Total time:      15.77 min
[32m[20221213 18:19:56 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 18:19:56 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 18:19:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:56 @agent_ppo2.py:185][0m |          -0.0022 |         241.8992 |           0.5368 |
[32m[20221213 18:19:56 @agent_ppo2.py:185][0m |          -0.0012 |         241.3539 |           0.5357 |
[32m[20221213 18:19:56 @agent_ppo2.py:185][0m |          -0.0080 |         237.8566 |           0.5345 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0081 |         236.4677 |           0.5336 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0007 |         247.4174 |           0.5330 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0111 |         235.4875 |           0.5328 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0095 |         234.4925 |           0.5319 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0102 |         234.4725 |           0.5315 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0044 |         242.7688 |           0.5306 |
[32m[20221213 18:19:57 @agent_ppo2.py:185][0m |          -0.0086 |         233.5342 |           0.5300 |
[32m[20221213 18:19:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.21
[32m[20221213 18:19:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.16
[32m[20221213 18:19:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 919.78
[32m[20221213 18:19:57 @agent_ppo2.py:143][0m Total time:      15.79 min
[32m[20221213 18:19:57 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221213 18:19:57 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 18:19:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0005 |         231.3067 |           0.5222 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |           0.0045 |         233.1990 |           0.5214 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0082 |         216.8443 |           0.5204 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0069 |         214.5763 |           0.5203 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0091 |         212.4143 |           0.5198 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0098 |         210.3683 |           0.5194 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0107 |         208.6572 |           0.5195 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0110 |         206.4981 |           0.5196 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0118 |         205.0718 |           0.5196 |
[32m[20221213 18:19:58 @agent_ppo2.py:185][0m |          -0.0114 |         203.6120 |           0.5190 |
[32m[20221213 18:19:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:19:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 954.18
[32m[20221213 18:19:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.86
[32m[20221213 18:19:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.70
[32m[20221213 18:19:59 @agent_ppo2.py:143][0m Total time:      15.82 min
[32m[20221213 18:19:59 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 18:19:59 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 18:19:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:19:59 @agent_ppo2.py:185][0m |          -0.0016 |         241.1017 |           0.5160 |
[32m[20221213 18:19:59 @agent_ppo2.py:185][0m |           0.0019 |         243.8004 |           0.5154 |
[32m[20221213 18:19:59 @agent_ppo2.py:185][0m |          -0.0081 |         233.3848 |           0.5145 |
[32m[20221213 18:19:59 @agent_ppo2.py:185][0m |          -0.0068 |         232.4654 |           0.5146 |
[32m[20221213 18:19:59 @agent_ppo2.py:185][0m |          -0.0068 |         231.6466 |           0.5144 |
[32m[20221213 18:20:00 @agent_ppo2.py:185][0m |          -0.0064 |         232.7567 |           0.5140 |
[32m[20221213 18:20:00 @agent_ppo2.py:185][0m |          -0.0089 |         230.8415 |           0.5138 |
[32m[20221213 18:20:00 @agent_ppo2.py:185][0m |          -0.0111 |         230.5897 |           0.5134 |
[32m[20221213 18:20:00 @agent_ppo2.py:185][0m |          -0.0092 |         230.0921 |           0.5135 |
[32m[20221213 18:20:00 @agent_ppo2.py:185][0m |          -0.0084 |         230.4000 |           0.5131 |
[32m[20221213 18:20:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:20:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.28
[32m[20221213 18:20:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.25
[32m[20221213 18:20:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.77
[32m[20221213 18:20:00 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221213 18:20:00 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221213 18:20:00 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 18:20:00 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:00 @agent_ppo2.py:185][0m |           0.0004 |         242.0740 |           0.5096 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0038 |         238.8083 |           0.5097 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0070 |         237.5008 |           0.5096 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |           0.0024 |         249.4949 |           0.5096 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0028 |         239.8483 |           0.5092 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0085 |         236.4510 |           0.5090 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0090 |         235.9587 |           0.5093 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0073 |         236.1376 |           0.5094 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0061 |         237.8365 |           0.5092 |
[32m[20221213 18:20:01 @agent_ppo2.py:185][0m |          -0.0087 |         235.4780 |           0.5094 |
[32m[20221213 18:20:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:20:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.91
[32m[20221213 18:20:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.59
[32m[20221213 18:20:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 901.75
[32m[20221213 18:20:01 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221213 18:20:01 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 18:20:01 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 18:20:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |          -0.0028 |         242.8712 |           0.5125 |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |          -0.0021 |         241.7917 |           0.5116 |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |          -0.0050 |         239.2256 |           0.5109 |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |          -0.0086 |         238.4472 |           0.5112 |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |          -0.0081 |         237.8761 |           0.5101 |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |           0.0039 |         271.3841 |           0.5104 |
[32m[20221213 18:20:02 @agent_ppo2.py:185][0m |          -0.0110 |         237.4578 |           0.5091 |
[32m[20221213 18:20:03 @agent_ppo2.py:185][0m |          -0.0045 |         242.7804 |           0.5092 |
[32m[20221213 18:20:03 @agent_ppo2.py:185][0m |          -0.0115 |         236.1429 |           0.5094 |
[32m[20221213 18:20:03 @agent_ppo2.py:185][0m |          -0.0089 |         236.5058 |           0.5095 |
[32m[20221213 18:20:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:20:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.71
[32m[20221213 18:20:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.75
[32m[20221213 18:20:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.43
[32m[20221213 18:20:03 @agent_ppo2.py:143][0m Total time:      15.89 min
[32m[20221213 18:20:03 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221213 18:20:03 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 18:20:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:03 @agent_ppo2.py:185][0m |          -0.0033 |         237.2943 |           0.5018 |
[32m[20221213 18:20:03 @agent_ppo2.py:185][0m |          -0.0045 |         234.9098 |           0.5010 |
[32m[20221213 18:20:03 @agent_ppo2.py:185][0m |          -0.0081 |         233.1789 |           0.5013 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0051 |         234.7016 |           0.5011 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0092 |         232.2015 |           0.5006 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0093 |         231.6254 |           0.5010 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0024 |         243.6723 |           0.5004 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0068 |         232.1779 |           0.5007 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0104 |         230.2184 |           0.5008 |
[32m[20221213 18:20:04 @agent_ppo2.py:185][0m |          -0.0101 |         229.6485 |           0.5008 |
[32m[20221213 18:20:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:20:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 932.91
[32m[20221213 18:20:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.26
[32m[20221213 18:20:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 959.34
[32m[20221213 18:20:04 @agent_ppo2.py:143][0m Total time:      15.91 min
[32m[20221213 18:20:04 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 18:20:04 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 18:20:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |           0.0003 |         236.6687 |           0.5197 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0037 |         234.4782 |           0.5201 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |           0.0007 |         243.4442 |           0.5204 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0054 |         232.7349 |           0.5202 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0077 |         231.8139 |           0.5207 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0073 |         231.1901 |           0.5202 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0085 |         231.0500 |           0.5206 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0069 |         230.5535 |           0.5207 |
[32m[20221213 18:20:05 @agent_ppo2.py:185][0m |          -0.0077 |         230.2750 |           0.5210 |
[32m[20221213 18:20:06 @agent_ppo2.py:185][0m |          -0.0083 |         230.0175 |           0.5208 |
[32m[20221213 18:20:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:20:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.18
[32m[20221213 18:20:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.72
[32m[20221213 18:20:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.69
[32m[20221213 18:20:06 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221213 18:20:06 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221213 18:20:06 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 18:20:06 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:06 @agent_ppo2.py:185][0m |           0.0107 |         255.9784 |           0.5173 |
[32m[20221213 18:20:06 @agent_ppo2.py:185][0m |          -0.0024 |         235.5587 |           0.5170 |
[32m[20221213 18:20:06 @agent_ppo2.py:185][0m |          -0.0052 |         235.1347 |           0.5172 |
[32m[20221213 18:20:06 @agent_ppo2.py:185][0m |          -0.0058 |         234.3297 |           0.5170 |
[32m[20221213 18:20:06 @agent_ppo2.py:185][0m |          -0.0058 |         233.6960 |           0.5172 |
[32m[20221213 18:20:07 @agent_ppo2.py:185][0m |          -0.0067 |         233.1536 |           0.5163 |
[32m[20221213 18:20:07 @agent_ppo2.py:185][0m |          -0.0066 |         232.6929 |           0.5169 |
[32m[20221213 18:20:07 @agent_ppo2.py:185][0m |          -0.0045 |         232.8071 |           0.5164 |
[32m[20221213 18:20:07 @agent_ppo2.py:185][0m |          -0.0034 |         235.0964 |           0.5169 |
[32m[20221213 18:20:07 @agent_ppo2.py:185][0m |          -0.0081 |         231.5766 |           0.5171 |
[32m[20221213 18:20:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:20:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 986.20
[32m[20221213 18:20:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.29
[32m[20221213 18:20:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.74
[32m[20221213 18:20:07 @agent_ppo2.py:143][0m Total time:      15.96 min
[32m[20221213 18:20:07 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 18:20:07 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 18:20:07 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |           0.0010 |         239.5079 |           0.5044 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0036 |         237.6399 |           0.5036 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0054 |         237.1966 |           0.5038 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0055 |         236.6714 |           0.5037 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0050 |         236.0683 |           0.5035 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0065 |         235.6787 |           0.5040 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0067 |         235.3163 |           0.5044 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0065 |         234.7856 |           0.5044 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0078 |         234.5919 |           0.5045 |
[32m[20221213 18:20:08 @agent_ppo2.py:185][0m |          -0.0071 |         234.1853 |           0.5045 |
[32m[20221213 18:20:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 999.45
[32m[20221213 18:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 999.77
[32m[20221213 18:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 941.74
[32m[20221213 18:20:09 @agent_ppo2.py:143][0m Total time:      15.98 min
[32m[20221213 18:20:09 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221213 18:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 18:20:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:09 @agent_ppo2.py:185][0m |          -0.0035 |         240.2311 |           0.5234 |
[32m[20221213 18:20:09 @agent_ppo2.py:185][0m |          -0.0094 |         238.2598 |           0.5229 |
[32m[20221213 18:20:09 @agent_ppo2.py:185][0m |          -0.0098 |         237.6974 |           0.5224 |
[32m[20221213 18:20:09 @agent_ppo2.py:185][0m |          -0.0114 |         236.9576 |           0.5223 |
[32m[20221213 18:20:09 @agent_ppo2.py:185][0m |          -0.0114 |         236.7139 |           0.5221 |
[32m[20221213 18:20:09 @agent_ppo2.py:185][0m |          -0.0135 |         236.0007 |           0.5213 |
[32m[20221213 18:20:10 @agent_ppo2.py:185][0m |          -0.0133 |         235.9388 |           0.5213 |
[32m[20221213 18:20:10 @agent_ppo2.py:185][0m |          -0.0133 |         235.8119 |           0.5210 |
[32m[20221213 18:20:10 @agent_ppo2.py:185][0m |          -0.0109 |         235.5737 |           0.5211 |
[32m[20221213 18:20:10 @agent_ppo2.py:185][0m |          -0.0146 |         234.9331 |           0.5207 |
[32m[20221213 18:20:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.19
[32m[20221213 18:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.68
[32m[20221213 18:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 925.37
[32m[20221213 18:20:10 @agent_ppo2.py:143][0m Total time:      16.00 min
[32m[20221213 18:20:10 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 18:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 18:20:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:10 @agent_ppo2.py:185][0m |           0.0047 |         252.4977 |           0.5256 |
[32m[20221213 18:20:10 @agent_ppo2.py:185][0m |          -0.0061 |         240.0502 |           0.5256 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |           0.0015 |         253.8354 |           0.5254 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0084 |         238.3583 |           0.5258 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0082 |         237.2539 |           0.5258 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0080 |         236.4571 |           0.5260 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0074 |         236.8185 |           0.5257 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0109 |         235.4868 |           0.5265 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0096 |         235.2062 |           0.5260 |
[32m[20221213 18:20:11 @agent_ppo2.py:185][0m |          -0.0097 |         235.1403 |           0.5263 |
[32m[20221213 18:20:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:20:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.55
[32m[20221213 18:20:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.80
[32m[20221213 18:20:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 930.60
[32m[20221213 18:20:11 @agent_ppo2.py:143][0m Total time:      16.03 min
[32m[20221213 18:20:11 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221213 18:20:11 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 18:20:12 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0017 |         253.1590 |           0.5273 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0044 |         248.7207 |           0.5275 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0081 |         246.7383 |           0.5283 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0088 |         245.1799 |           0.5283 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0095 |         243.9636 |           0.5284 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0110 |         243.3301 |           0.5285 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0075 |         243.5711 |           0.5288 |
[32m[20221213 18:20:12 @agent_ppo2.py:185][0m |          -0.0112 |         241.7916 |           0.5287 |
[32m[20221213 18:20:13 @agent_ppo2.py:185][0m |          -0.0001 |         264.0865 |           0.5289 |
[32m[20221213 18:20:13 @agent_ppo2.py:185][0m |          -0.0091 |         240.8553 |           0.5285 |
[32m[20221213 18:20:13 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 18:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 930.84
[32m[20221213 18:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.58
[32m[20221213 18:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.47
[32m[20221213 18:20:13 @agent_ppo2.py:143][0m Total time:      16.05 min
[32m[20221213 18:20:13 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 18:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 18:20:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:13 @agent_ppo2.py:185][0m |          -0.0034 |         250.7981 |           0.5288 |
[32m[20221213 18:20:13 @agent_ppo2.py:185][0m |          -0.0073 |         246.1040 |           0.5275 |
[32m[20221213 18:20:13 @agent_ppo2.py:185][0m |          -0.0109 |         243.8491 |           0.5267 |
[32m[20221213 18:20:13 @agent_ppo2.py:185][0m |          -0.0066 |         242.7981 |           0.5266 |
[32m[20221213 18:20:14 @agent_ppo2.py:185][0m |          -0.0103 |         240.9752 |           0.5266 |
[32m[20221213 18:20:14 @agent_ppo2.py:185][0m |          -0.0116 |         240.1364 |           0.5264 |
[32m[20221213 18:20:14 @agent_ppo2.py:185][0m |          -0.0039 |         244.2486 |           0.5263 |
[32m[20221213 18:20:14 @agent_ppo2.py:185][0m |          -0.0123 |         238.1573 |           0.5268 |
[32m[20221213 18:20:14 @agent_ppo2.py:185][0m |          -0.0132 |         236.7208 |           0.5270 |
[32m[20221213 18:20:14 @agent_ppo2.py:185][0m |          -0.0117 |         235.7778 |           0.5268 |
[32m[20221213 18:20:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.06
[32m[20221213 18:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.55
[32m[20221213 18:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 953.34
[32m[20221213 18:20:14 @agent_ppo2.py:143][0m Total time:      16.07 min
[32m[20221213 18:20:14 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221213 18:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 18:20:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |           0.0147 |         285.3282 |           0.5249 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0053 |         248.5883 |           0.5235 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0069 |         245.9130 |           0.5234 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0058 |         246.0980 |           0.5234 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0105 |         243.4575 |           0.5235 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0083 |         241.9161 |           0.5231 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0110 |         241.1160 |           0.5235 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0101 |         239.8664 |           0.5237 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0105 |         239.3992 |           0.5238 |
[32m[20221213 18:20:15 @agent_ppo2.py:185][0m |          -0.0103 |         238.1380 |           0.5238 |
[32m[20221213 18:20:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:20:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.27
[32m[20221213 18:20:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.05
[32m[20221213 18:20:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.57
[32m[20221213 18:20:16 @agent_ppo2.py:143][0m Total time:      16.10 min
[32m[20221213 18:20:16 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 18:20:16 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 18:20:16 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:16 @agent_ppo2.py:185][0m |          -0.0031 |         250.2492 |           0.5320 |
[32m[20221213 18:20:16 @agent_ppo2.py:185][0m |          -0.0060 |         244.9373 |           0.5302 |
[32m[20221213 18:20:16 @agent_ppo2.py:185][0m |          -0.0055 |         242.6893 |           0.5300 |
[32m[20221213 18:20:16 @agent_ppo2.py:185][0m |          -0.0077 |         241.1894 |           0.5299 |
[32m[20221213 18:20:16 @agent_ppo2.py:185][0m |          -0.0079 |         240.3408 |           0.5304 |
[32m[20221213 18:20:16 @agent_ppo2.py:185][0m |          -0.0079 |         239.5767 |           0.5297 |
[32m[20221213 18:20:17 @agent_ppo2.py:185][0m |          -0.0073 |         238.8474 |           0.5297 |
[32m[20221213 18:20:17 @agent_ppo2.py:185][0m |          -0.0095 |         237.9469 |           0.5298 |
[32m[20221213 18:20:17 @agent_ppo2.py:185][0m |          -0.0047 |         241.1358 |           0.5294 |
[32m[20221213 18:20:17 @agent_ppo2.py:185][0m |          -0.0109 |         237.1883 |           0.5295 |
[32m[20221213 18:20:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:20:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.09
[32m[20221213 18:20:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.51
[32m[20221213 18:20:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 925.39
[32m[20221213 18:20:17 @agent_ppo2.py:143][0m Total time:      16.12 min
[32m[20221213 18:20:17 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221213 18:20:17 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 18:20:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:17 @agent_ppo2.py:185][0m |          -0.0013 |         243.6455 |           0.5368 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0060 |         240.9994 |           0.5358 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0067 |         239.7695 |           0.5356 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0072 |         239.1848 |           0.5356 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0040 |         239.9403 |           0.5356 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0090 |         238.3839 |           0.5356 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0088 |         238.0341 |           0.5351 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0092 |         237.8625 |           0.5352 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0092 |         237.7702 |           0.5350 |
[32m[20221213 18:20:18 @agent_ppo2.py:185][0m |          -0.0112 |         237.3266 |           0.5350 |
[32m[20221213 18:20:18 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 18:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.53
[32m[20221213 18:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 952.32
[32m[20221213 18:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.12
[32m[20221213 18:20:18 @agent_ppo2.py:143][0m Total time:      16.15 min
[32m[20221213 18:20:18 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 18:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 18:20:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0004 |         241.5488 |           0.5390 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0007 |         240.5930 |           0.5392 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0051 |         238.6365 |           0.5387 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0055 |         237.5654 |           0.5391 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0055 |         237.2490 |           0.5389 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0060 |         236.9093 |           0.5389 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0012 |         239.5657 |           0.5390 |
[32m[20221213 18:20:19 @agent_ppo2.py:185][0m |          -0.0069 |         236.2526 |           0.5391 |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |          -0.0075 |         236.0092 |           0.5390 |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |          -0.0084 |         235.7363 |           0.5386 |
[32m[20221213 18:20:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.37
[32m[20221213 18:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.33
[32m[20221213 18:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 924.93
[32m[20221213 18:20:20 @agent_ppo2.py:143][0m Total time:      16.17 min
[32m[20221213 18:20:20 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221213 18:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 18:20:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |           0.0005 |         250.9658 |           0.5233 |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |          -0.0067 |         238.0403 |           0.5209 |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |          -0.0096 |         236.4760 |           0.5225 |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |          -0.0087 |         236.1024 |           0.5217 |
[32m[20221213 18:20:20 @agent_ppo2.py:185][0m |          -0.0105 |         236.1267 |           0.5214 |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |          -0.0120 |         235.1960 |           0.5213 |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |          -0.0097 |         235.3298 |           0.5210 |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |          -0.0129 |         234.8165 |           0.5213 |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |          -0.0143 |         234.8924 |           0.5210 |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |          -0.0129 |         234.4017 |           0.5209 |
[32m[20221213 18:20:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 953.15
[32m[20221213 18:20:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.74
[32m[20221213 18:20:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 902.50
[32m[20221213 18:20:21 @agent_ppo2.py:143][0m Total time:      16.19 min
[32m[20221213 18:20:21 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 18:20:21 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 18:20:21 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |           0.0050 |         250.3889 |           0.5374 |
[32m[20221213 18:20:21 @agent_ppo2.py:185][0m |          -0.0059 |         241.6821 |           0.5358 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0046 |         240.7440 |           0.5362 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0084 |         240.2726 |           0.5356 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0025 |         251.0116 |           0.5356 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0076 |         239.6254 |           0.5353 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |           0.0044 |         266.3009 |           0.5357 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0055 |         239.0005 |           0.5349 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0084 |         239.0675 |           0.5355 |
[32m[20221213 18:20:22 @agent_ppo2.py:185][0m |          -0.0097 |         238.7538 |           0.5356 |
[32m[20221213 18:20:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.77
[32m[20221213 18:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.00
[32m[20221213 18:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 909.89
[32m[20221213 18:20:22 @agent_ppo2.py:143][0m Total time:      16.21 min
[32m[20221213 18:20:22 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221213 18:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 18:20:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0011 |         247.4865 |           0.5215 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0036 |         243.0045 |           0.5205 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0026 |         240.3786 |           0.5210 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0063 |         239.4046 |           0.5204 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0019 |         242.3229 |           0.5207 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0075 |         238.4177 |           0.5201 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0083 |         238.3013 |           0.5197 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0019 |         242.0271 |           0.5202 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0080 |         237.8677 |           0.5208 |
[32m[20221213 18:20:23 @agent_ppo2.py:185][0m |          -0.0072 |         237.4403 |           0.5205 |
[32m[20221213 18:20:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.05
[32m[20221213 18:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.08
[32m[20221213 18:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 892.35
[32m[20221213 18:20:24 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221213 18:20:24 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 18:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 18:20:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0021 |         245.7482 |           0.5328 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0043 |         241.0129 |           0.5321 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0062 |         238.2845 |           0.5312 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0067 |         235.7524 |           0.5312 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0077 |         233.2451 |           0.5306 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0059 |         231.0029 |           0.5306 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0058 |         229.2707 |           0.5294 |
[32m[20221213 18:20:24 @agent_ppo2.py:185][0m |          -0.0085 |         227.1762 |           0.5293 |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |          -0.0098 |         225.6318 |           0.5288 |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |          -0.0081 |         224.0464 |           0.5278 |
[32m[20221213 18:20:25 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 948.37
[32m[20221213 18:20:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.38
[32m[20221213 18:20:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.33
[32m[20221213 18:20:25 @agent_ppo2.py:143][0m Total time:      16.25 min
[32m[20221213 18:20:25 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221213 18:20:25 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 18:20:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |          -0.0008 |         245.0459 |           0.5438 |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |          -0.0008 |         241.2062 |           0.5441 |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |          -0.0048 |         239.8728 |           0.5437 |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |           0.0009 |         244.2158 |           0.5433 |
[32m[20221213 18:20:25 @agent_ppo2.py:185][0m |          -0.0070 |         238.4528 |           0.5436 |
[32m[20221213 18:20:26 @agent_ppo2.py:185][0m |          -0.0066 |         237.8943 |           0.5432 |
[32m[20221213 18:20:26 @agent_ppo2.py:185][0m |          -0.0002 |         246.8898 |           0.5432 |
[32m[20221213 18:20:26 @agent_ppo2.py:185][0m |          -0.0071 |         237.3534 |           0.5427 |
[32m[20221213 18:20:26 @agent_ppo2.py:185][0m |          -0.0084 |         236.6264 |           0.5431 |
[32m[20221213 18:20:26 @agent_ppo2.py:185][0m |          -0.0098 |         236.4647 |           0.5429 |
[32m[20221213 18:20:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.73
[32m[20221213 18:20:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.51
[32m[20221213 18:20:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.99
[32m[20221213 18:20:26 @agent_ppo2.py:143][0m Total time:      16.27 min
[32m[20221213 18:20:26 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 18:20:26 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 18:20:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:26 @agent_ppo2.py:185][0m |          -0.0024 |         247.9908 |           0.5287 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0046 |         244.0544 |           0.5282 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0058 |         242.5013 |           0.5275 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0077 |         241.0619 |           0.5272 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |           0.0063 |         260.1473 |           0.5271 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0068 |         239.9701 |           0.5255 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0056 |         240.1005 |           0.5265 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0093 |         238.4395 |           0.5260 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0093 |         238.4075 |           0.5260 |
[32m[20221213 18:20:27 @agent_ppo2.py:185][0m |          -0.0056 |         239.8587 |           0.5258 |
[32m[20221213 18:20:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:20:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 894.58
[32m[20221213 18:20:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.74
[32m[20221213 18:20:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.09
[32m[20221213 18:20:27 @agent_ppo2.py:143][0m Total time:      16.29 min
[32m[20221213 18:20:27 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221213 18:20:27 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 18:20:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |           0.0021 |         242.8645 |           0.5309 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |           0.0056 |         249.5689 |           0.5311 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0043 |         237.0653 |           0.5305 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0056 |         236.8756 |           0.5305 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0087 |         235.7779 |           0.5308 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0081 |         235.2928 |           0.5310 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0090 |         234.8433 |           0.5311 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0083 |         235.0580 |           0.5310 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |           0.0020 |         251.1260 |           0.5316 |
[32m[20221213 18:20:28 @agent_ppo2.py:185][0m |          -0.0105 |         234.8784 |           0.5314 |
[32m[20221213 18:20:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.30
[32m[20221213 18:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.38
[32m[20221213 18:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 944.71
[32m[20221213 18:20:29 @agent_ppo2.py:143][0m Total time:      16.31 min
[32m[20221213 18:20:29 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 18:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 18:20:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |          -0.0031 |         247.3438 |           0.5164 |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |          -0.0011 |         241.6903 |           0.5148 |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |           0.0001 |         242.0566 |           0.5155 |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |          -0.0016 |         237.1216 |           0.5149 |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |          -0.0094 |         232.8538 |           0.5147 |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |          -0.0105 |         231.6510 |           0.5147 |
[32m[20221213 18:20:29 @agent_ppo2.py:185][0m |          -0.0002 |         242.7345 |           0.5138 |
[32m[20221213 18:20:30 @agent_ppo2.py:185][0m |          -0.0107 |         230.4942 |           0.5151 |
[32m[20221213 18:20:30 @agent_ppo2.py:185][0m |          -0.0100 |         230.2694 |           0.5146 |
[32m[20221213 18:20:30 @agent_ppo2.py:185][0m |          -0.0117 |         229.3400 |           0.5146 |
[32m[20221213 18:20:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 881.56
[32m[20221213 18:20:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 918.41
[32m[20221213 18:20:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 967.65
[32m[20221213 18:20:30 @agent_ppo2.py:143][0m Total time:      16.34 min
[32m[20221213 18:20:30 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221213 18:20:30 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 18:20:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:30 @agent_ppo2.py:185][0m |           0.0001 |         245.3820 |           0.5392 |
[32m[20221213 18:20:30 @agent_ppo2.py:185][0m |          -0.0036 |         240.9990 |           0.5381 |
[32m[20221213 18:20:30 @agent_ppo2.py:185][0m |          -0.0061 |         238.8099 |           0.5375 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0051 |         237.8843 |           0.5366 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0072 |         237.2914 |           0.5371 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0076 |         236.8913 |           0.5362 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0078 |         236.6786 |           0.5364 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0082 |         236.1910 |           0.5355 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0101 |         235.7404 |           0.5352 |
[32m[20221213 18:20:31 @agent_ppo2.py:185][0m |          -0.0047 |         238.2746 |           0.5356 |
[32m[20221213 18:20:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:20:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.25
[32m[20221213 18:20:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.67
[32m[20221213 18:20:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.58
[32m[20221213 18:20:31 @agent_ppo2.py:143][0m Total time:      16.36 min
[32m[20221213 18:20:31 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 18:20:31 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
[32m[20221213 18:20:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0008 |         241.6794 |           0.5192 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0053 |         235.7647 |           0.5191 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0061 |         233.6669 |           0.5181 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0098 |         232.1244 |           0.5179 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0100 |         230.4790 |           0.5181 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0115 |         228.6536 |           0.5175 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0118 |         228.1274 |           0.5176 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |          -0.0064 |         228.1768 |           0.5178 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |           0.0048 |         249.5596 |           0.5169 |
[32m[20221213 18:20:32 @agent_ppo2.py:185][0m |           0.0093 |         276.4145 |           0.5169 |
[32m[20221213 18:20:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 900.62
[32m[20221213 18:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.42
[32m[20221213 18:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 910.78
[32m[20221213 18:20:32 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221213 18:20:32 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221213 18:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 739 --------------------------#
[32m[20221213 18:20:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0013 |         241.1454 |           0.5036 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0032 |         232.2046 |           0.5032 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0062 |         227.5972 |           0.5030 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0082 |         225.7673 |           0.5028 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0044 |         225.8429 |           0.5023 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0079 |         223.6813 |           0.5024 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0082 |         222.8155 |           0.5023 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0084 |         222.0904 |           0.5017 |
[32m[20221213 18:20:33 @agent_ppo2.py:185][0m |          -0.0101 |         221.1405 |           0.5020 |
[32m[20221213 18:20:34 @agent_ppo2.py:185][0m |          -0.0083 |         220.7105 |           0.5021 |
[32m[20221213 18:20:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:20:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 857.33
[32m[20221213 18:20:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 952.21
[32m[20221213 18:20:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 880.50
[32m[20221213 18:20:34 @agent_ppo2.py:143][0m Total time:      16.40 min
[32m[20221213 18:20:34 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 18:20:34 @agent_ppo2.py:121][0m #------------------------ Iteration 740 --------------------------#
[32m[20221213 18:20:34 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:20:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:34 @agent_ppo2.py:185][0m |          -0.0027 |         236.6059 |           0.5055 |
[32m[20221213 18:20:34 @agent_ppo2.py:185][0m |          -0.0053 |         232.6033 |           0.5053 |
[32m[20221213 18:20:34 @agent_ppo2.py:185][0m |          -0.0070 |         230.6971 |           0.5054 |
[32m[20221213 18:20:34 @agent_ppo2.py:185][0m |          -0.0080 |         229.1348 |           0.5048 |
[32m[20221213 18:20:34 @agent_ppo2.py:185][0m |          -0.0063 |         228.4577 |           0.5048 |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0121 |         227.7146 |           0.5047 |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0077 |         226.9405 |           0.5045 |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0062 |         227.6779 |           0.5039 |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0104 |         226.1962 |           0.5042 |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0087 |         225.7425 |           0.5038 |
[32m[20221213 18:20:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 913.88
[32m[20221213 18:20:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.38
[32m[20221213 18:20:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 908.50
[32m[20221213 18:20:35 @agent_ppo2.py:143][0m Total time:      16.42 min
[32m[20221213 18:20:35 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221213 18:20:35 @agent_ppo2.py:121][0m #------------------------ Iteration 741 --------------------------#
[32m[20221213 18:20:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0024 |         231.8350 |           0.5164 |
[32m[20221213 18:20:35 @agent_ppo2.py:185][0m |          -0.0068 |         228.8206 |           0.5149 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |           0.0059 |         250.6831 |           0.5140 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |           0.0063 |         259.1662 |           0.5146 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |          -0.0090 |         225.6512 |           0.5137 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |          -0.0088 |         224.7051 |           0.5141 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |          -0.0108 |         224.3092 |           0.5143 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |          -0.0102 |         224.0025 |           0.5126 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |          -0.0083 |         224.6953 |           0.5140 |
[32m[20221213 18:20:36 @agent_ppo2.py:185][0m |          -0.0109 |         223.6598 |           0.5138 |
[32m[20221213 18:20:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 951.05
[32m[20221213 18:20:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.05
[32m[20221213 18:20:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.43
[32m[20221213 18:20:36 @agent_ppo2.py:143][0m Total time:      16.44 min
[32m[20221213 18:20:36 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 18:20:36 @agent_ppo2.py:121][0m #------------------------ Iteration 742 --------------------------#
[32m[20221213 18:20:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0028 |         238.7011 |           0.5102 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0054 |         233.6821 |           0.5092 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |           0.0031 |         246.4254 |           0.5086 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0077 |         232.7007 |           0.5090 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0091 |         231.6191 |           0.5088 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0079 |         231.4369 |           0.5083 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0052 |         237.5308 |           0.5080 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0087 |         231.1682 |           0.5075 |
[32m[20221213 18:20:37 @agent_ppo2.py:185][0m |          -0.0098 |         230.7116 |           0.5075 |
[32m[20221213 18:20:38 @agent_ppo2.py:185][0m |          -0.0002 |         248.2622 |           0.5078 |
[32m[20221213 18:20:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 926.23
[32m[20221213 18:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.23
[32m[20221213 18:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.84
[32m[20221213 18:20:38 @agent_ppo2.py:143][0m Total time:      16.47 min
[32m[20221213 18:20:38 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221213 18:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 743 --------------------------#
[32m[20221213 18:20:38 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 18:20:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:38 @agent_ppo2.py:185][0m |          -0.0027 |         233.9239 |           0.5178 |
[32m[20221213 18:20:38 @agent_ppo2.py:185][0m |           0.0008 |         237.5272 |           0.5161 |
[32m[20221213 18:20:38 @agent_ppo2.py:185][0m |          -0.0072 |         231.7519 |           0.5168 |
[32m[20221213 18:20:38 @agent_ppo2.py:185][0m |          -0.0075 |         231.4653 |           0.5167 |
[32m[20221213 18:20:38 @agent_ppo2.py:185][0m |          -0.0075 |         231.2235 |           0.5162 |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |          -0.0074 |         230.8681 |           0.5158 |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |           0.0002 |         242.5286 |           0.5156 |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |          -0.0078 |         230.2613 |           0.5160 |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |          -0.0089 |         229.9270 |           0.5152 |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |          -0.0096 |         230.0888 |           0.5152 |
[32m[20221213 18:20:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 990.02
[32m[20221213 18:20:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.77
[32m[20221213 18:20:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 933.12
[32m[20221213 18:20:39 @agent_ppo2.py:143][0m Total time:      16.49 min
[32m[20221213 18:20:39 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 18:20:39 @agent_ppo2.py:121][0m #------------------------ Iteration 744 --------------------------#
[32m[20221213 18:20:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |          -0.0010 |         246.4045 |           0.5123 |
[32m[20221213 18:20:39 @agent_ppo2.py:185][0m |          -0.0055 |         242.4237 |           0.5120 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0048 |         240.9482 |           0.5122 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0069 |         240.4134 |           0.5123 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0061 |         239.6566 |           0.5122 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0083 |         239.1983 |           0.5122 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0098 |         239.1081 |           0.5120 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0086 |         238.4567 |           0.5121 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0087 |         237.9996 |           0.5123 |
[32m[20221213 18:20:40 @agent_ppo2.py:185][0m |          -0.0042 |         246.0758 |           0.5121 |
[32m[20221213 18:20:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 898.68
[32m[20221213 18:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.53
[32m[20221213 18:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 893.99
[32m[20221213 18:20:40 @agent_ppo2.py:143][0m Total time:      16.51 min
[32m[20221213 18:20:40 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221213 18:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 745 --------------------------#
[32m[20221213 18:20:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0021 |         249.4009 |           0.5129 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0062 |         245.3781 |           0.5128 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0061 |         243.6494 |           0.5127 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0077 |         243.6754 |           0.5131 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0078 |         242.3885 |           0.5130 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0084 |         241.9517 |           0.5130 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0049 |         243.7459 |           0.5130 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0084 |         241.9087 |           0.5133 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0111 |         240.7549 |           0.5134 |
[32m[20221213 18:20:41 @agent_ppo2.py:185][0m |          -0.0089 |         240.3125 |           0.5133 |
[32m[20221213 18:20:41 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 907.38
[32m[20221213 18:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.40
[32m[20221213 18:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.60
[32m[20221213 18:20:42 @agent_ppo2.py:143][0m Total time:      16.53 min
[32m[20221213 18:20:42 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 18:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 746 --------------------------#
[32m[20221213 18:20:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0016 |         242.2222 |           0.5107 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0035 |         241.7076 |           0.5105 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0026 |         242.5915 |           0.5106 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0065 |         241.1201 |           0.5102 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0065 |         240.6918 |           0.5110 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0064 |         240.6405 |           0.5106 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0066 |         240.0386 |           0.5113 |
[32m[20221213 18:20:42 @agent_ppo2.py:185][0m |          -0.0031 |         243.1155 |           0.5115 |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |           0.0014 |         261.5771 |           0.5113 |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |          -0.0085 |         239.6370 |           0.5118 |
[32m[20221213 18:20:43 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:20:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 990.07
[32m[20221213 18:20:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.91
[32m[20221213 18:20:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 907.62
[32m[20221213 18:20:43 @agent_ppo2.py:143][0m Total time:      16.55 min
[32m[20221213 18:20:43 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221213 18:20:43 @agent_ppo2.py:121][0m #------------------------ Iteration 747 --------------------------#
[32m[20221213 18:20:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |          -0.0011 |         243.8166 |           0.5174 |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |          -0.0042 |         242.0369 |           0.5167 |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |          -0.0040 |         241.4392 |           0.5158 |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |          -0.0022 |         241.9520 |           0.5157 |
[32m[20221213 18:20:43 @agent_ppo2.py:185][0m |          -0.0055 |         240.8341 |           0.5158 |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |          -0.0061 |         240.3638 |           0.5146 |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |           0.0074 |         268.2567 |           0.5145 |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |          -0.0062 |         240.1362 |           0.5133 |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |          -0.0064 |         239.6947 |           0.5130 |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |          -0.0080 |         239.8866 |           0.5135 |
[32m[20221213 18:20:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 949.05
[32m[20221213 18:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.98
[32m[20221213 18:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.05
[32m[20221213 18:20:44 @agent_ppo2.py:143][0m Total time:      16.57 min
[32m[20221213 18:20:44 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 18:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 748 --------------------------#
[32m[20221213 18:20:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |          -0.0011 |         240.5165 |           0.5270 |
[32m[20221213 18:20:44 @agent_ppo2.py:185][0m |          -0.0052 |         238.5649 |           0.5270 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0073 |         237.8234 |           0.5272 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0041 |         238.2121 |           0.5270 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |           0.0064 |         272.6690 |           0.5266 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0064 |         237.3677 |           0.5258 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0075 |         237.0239 |           0.5259 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0080 |         237.0001 |           0.5260 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0089 |         236.7557 |           0.5262 |
[32m[20221213 18:20:45 @agent_ppo2.py:185][0m |          -0.0093 |         236.8275 |           0.5262 |
[32m[20221213 18:20:45 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:20:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 982.44
[32m[20221213 18:20:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.28
[32m[20221213 18:20:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.37
[32m[20221213 18:20:45 @agent_ppo2.py:143][0m Total time:      16.59 min
[32m[20221213 18:20:45 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221213 18:20:45 @agent_ppo2.py:121][0m #------------------------ Iteration 749 --------------------------#
[32m[20221213 18:20:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |           0.0129 |         279.8147 |           0.5141 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0041 |         241.8865 |           0.5131 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0052 |         239.7052 |           0.5140 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0019 |         241.2219 |           0.5142 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0074 |         238.6116 |           0.5147 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0069 |         237.4355 |           0.5152 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0079 |         236.9760 |           0.5153 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0092 |         236.9510 |           0.5155 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0031 |         240.7058 |           0.5160 |
[32m[20221213 18:20:46 @agent_ppo2.py:185][0m |          -0.0099 |         236.2370 |           0.5158 |
[32m[20221213 18:20:46 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 889.09
[32m[20221213 18:20:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 924.06
[32m[20221213 18:20:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.62
[32m[20221213 18:20:46 @agent_ppo2.py:143][0m Total time:      16.61 min
[32m[20221213 18:20:46 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 18:20:46 @agent_ppo2.py:121][0m #------------------------ Iteration 750 --------------------------#
[32m[20221213 18:20:47 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |           0.0022 |         235.0273 |           0.5176 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0037 |         232.7856 |           0.5179 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0028 |         231.5683 |           0.5178 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0012 |         232.1697 |           0.5172 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0066 |         230.1491 |           0.5173 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0055 |         229.6602 |           0.5170 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0056 |         228.9785 |           0.5170 |
[32m[20221213 18:20:47 @agent_ppo2.py:185][0m |          -0.0071 |         228.8774 |           0.5167 |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |          -0.0073 |         228.4089 |           0.5163 |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |          -0.0075 |         228.5430 |           0.5166 |
[32m[20221213 18:20:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:20:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 917.51
[32m[20221213 18:20:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.86
[32m[20221213 18:20:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 925.63
[32m[20221213 18:20:48 @agent_ppo2.py:143][0m Total time:      16.63 min
[32m[20221213 18:20:48 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221213 18:20:48 @agent_ppo2.py:121][0m #------------------------ Iteration 751 --------------------------#
[32m[20221213 18:20:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |           0.0044 |         244.8088 |           0.5164 |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |          -0.0075 |         217.5662 |           0.5145 |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |          -0.0087 |         212.3910 |           0.5145 |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |          -0.0098 |         206.0989 |           0.5137 |
[32m[20221213 18:20:48 @agent_ppo2.py:185][0m |          -0.0102 |         203.0996 |           0.5143 |
[32m[20221213 18:20:49 @agent_ppo2.py:185][0m |          -0.0105 |         200.7752 |           0.5142 |
[32m[20221213 18:20:49 @agent_ppo2.py:185][0m |          -0.0112 |         199.1691 |           0.5137 |
[32m[20221213 18:20:49 @agent_ppo2.py:185][0m |          -0.0106 |         197.6108 |           0.5145 |
[32m[20221213 18:20:49 @agent_ppo2.py:185][0m |          -0.0099 |         196.0987 |           0.5141 |
[32m[20221213 18:20:49 @agent_ppo2.py:185][0m |          -0.0109 |         194.5766 |           0.5132 |
[32m[20221213 18:20:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:20:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.49
[32m[20221213 18:20:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.46
[32m[20221213 18:20:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 896.29
[32m[20221213 18:20:49 @agent_ppo2.py:143][0m Total time:      16.66 min
[32m[20221213 18:20:49 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 18:20:49 @agent_ppo2.py:121][0m #------------------------ Iteration 752 --------------------------#
[32m[20221213 18:20:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:20:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:49 @agent_ppo2.py:185][0m |          -0.0018 |         263.5169 |           0.5179 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0050 |         255.3160 |           0.5171 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0059 |         251.5889 |           0.5160 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0028 |         257.3984 |           0.5149 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0075 |         248.7235 |           0.5149 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0046 |         249.6140 |           0.5142 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0122 |         246.4915 |           0.5146 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0102 |         245.6163 |           0.5135 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0090 |         244.7301 |           0.5132 |
[32m[20221213 18:20:50 @agent_ppo2.py:185][0m |          -0.0042 |         251.1599 |           0.5126 |
[32m[20221213 18:20:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 854.48
[32m[20221213 18:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 932.13
[32m[20221213 18:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 970.29
[32m[20221213 18:20:50 @agent_ppo2.py:143][0m Total time:      16.68 min
[32m[20221213 18:20:50 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221213 18:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 753 --------------------------#
[32m[20221213 18:20:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |           0.0032 |         247.5847 |           0.5151 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0048 |         242.7572 |           0.5151 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0049 |         241.5905 |           0.5146 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0052 |         240.7955 |           0.5149 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0061 |         240.2220 |           0.5151 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0061 |         239.3919 |           0.5144 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0017 |         243.3012 |           0.5147 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0063 |         238.6612 |           0.5147 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0087 |         238.8222 |           0.5144 |
[32m[20221213 18:20:51 @agent_ppo2.py:185][0m |          -0.0089 |         238.3889 |           0.5145 |
[32m[20221213 18:20:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:20:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.83
[32m[20221213 18:20:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.32
[32m[20221213 18:20:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 906.52
[32m[20221213 18:20:52 @agent_ppo2.py:143][0m Total time:      16.70 min
[32m[20221213 18:20:52 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 18:20:52 @agent_ppo2.py:121][0m #------------------------ Iteration 754 --------------------------#
[32m[20221213 18:20:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:52 @agent_ppo2.py:185][0m |           0.0036 |         253.9370 |           0.5174 |
[32m[20221213 18:20:52 @agent_ppo2.py:185][0m |          -0.0007 |         246.5669 |           0.5157 |
[32m[20221213 18:20:52 @agent_ppo2.py:185][0m |          -0.0093 |         241.0233 |           0.5151 |
[32m[20221213 18:20:52 @agent_ppo2.py:185][0m |          -0.0095 |         239.7020 |           0.5156 |
[32m[20221213 18:20:52 @agent_ppo2.py:185][0m |          -0.0089 |         238.4924 |           0.5151 |
[32m[20221213 18:20:52 @agent_ppo2.py:185][0m |          -0.0094 |         238.2164 |           0.5154 |
[32m[20221213 18:20:53 @agent_ppo2.py:185][0m |          -0.0106 |         237.0936 |           0.5152 |
[32m[20221213 18:20:53 @agent_ppo2.py:185][0m |          -0.0105 |         236.8838 |           0.5148 |
[32m[20221213 18:20:53 @agent_ppo2.py:185][0m |          -0.0093 |         236.2244 |           0.5151 |
[32m[20221213 18:20:53 @agent_ppo2.py:185][0m |          -0.0101 |         235.9508 |           0.5148 |
[32m[20221213 18:20:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 877.81
[32m[20221213 18:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 994.45
[32m[20221213 18:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.70
[32m[20221213 18:20:53 @agent_ppo2.py:143][0m Total time:      16.72 min
[32m[20221213 18:20:53 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221213 18:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 755 --------------------------#
[32m[20221213 18:20:53 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:53 @agent_ppo2.py:185][0m |          -0.0021 |         237.3767 |           0.5119 |
[32m[20221213 18:20:53 @agent_ppo2.py:185][0m |          -0.0072 |         227.2700 |           0.5110 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0083 |         223.1822 |           0.5104 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0090 |         221.1910 |           0.5106 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0112 |         219.7677 |           0.5106 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0110 |         217.8383 |           0.5106 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0129 |         216.0415 |           0.5110 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0136 |         214.9342 |           0.5112 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0150 |         213.9103 |           0.5111 |
[32m[20221213 18:20:54 @agent_ppo2.py:185][0m |          -0.0146 |         212.2212 |           0.5115 |
[32m[20221213 18:20:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 894.41
[32m[20221213 18:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 937.70
[32m[20221213 18:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.94
[32m[20221213 18:20:54 @agent_ppo2.py:143][0m Total time:      16.74 min
[32m[20221213 18:20:54 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 18:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 756 --------------------------#
[32m[20221213 18:20:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:20:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0025 |         249.7575 |           0.5284 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0066 |         243.9316 |           0.5273 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0063 |         241.6249 |           0.5278 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0073 |         240.5149 |           0.5276 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0073 |         239.8794 |           0.5283 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0083 |         238.7484 |           0.5278 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0079 |         238.1209 |           0.5277 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0087 |         237.6484 |           0.5281 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |          -0.0109 |         237.3208 |           0.5281 |
[32m[20221213 18:20:55 @agent_ppo2.py:185][0m |           0.0011 |         252.9467 |           0.5283 |
[32m[20221213 18:20:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:20:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 916.29
[32m[20221213 18:20:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.50
[32m[20221213 18:20:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 935.71
[32m[20221213 18:20:56 @agent_ppo2.py:143][0m Total time:      16.76 min
[32m[20221213 18:20:56 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221213 18:20:56 @agent_ppo2.py:121][0m #------------------------ Iteration 757 --------------------------#
[32m[20221213 18:20:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0010 |         248.8741 |           0.5317 |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0020 |         244.7493 |           0.5307 |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0047 |         243.2637 |           0.5309 |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0027 |         243.6334 |           0.5312 |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0060 |         241.4013 |           0.5309 |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0066 |         241.0871 |           0.5313 |
[32m[20221213 18:20:56 @agent_ppo2.py:185][0m |          -0.0102 |         240.6073 |           0.5311 |
[32m[20221213 18:20:57 @agent_ppo2.py:185][0m |          -0.0084 |         239.8410 |           0.5316 |
[32m[20221213 18:20:57 @agent_ppo2.py:185][0m |          -0.0079 |         240.2741 |           0.5315 |
[32m[20221213 18:20:57 @agent_ppo2.py:185][0m |          -0.0083 |         239.2167 |           0.5314 |
[32m[20221213 18:20:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.82
[32m[20221213 18:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.59
[32m[20221213 18:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 934.41
[32m[20221213 18:20:57 @agent_ppo2.py:143][0m Total time:      16.79 min
[32m[20221213 18:20:57 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 18:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 758 --------------------------#
[32m[20221213 18:20:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:57 @agent_ppo2.py:185][0m |          -0.0009 |         240.2443 |           0.5208 |
[32m[20221213 18:20:57 @agent_ppo2.py:185][0m |          -0.0004 |         240.4973 |           0.5207 |
[32m[20221213 18:20:57 @agent_ppo2.py:185][0m |          -0.0061 |         236.2440 |           0.5201 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |          -0.0052 |         236.0930 |           0.5208 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |          -0.0061 |         234.9736 |           0.5202 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |           0.0062 |         271.2243 |           0.5207 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |          -0.0080 |         234.7539 |           0.5196 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |          -0.0069 |         234.5458 |           0.5213 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |          -0.0080 |         234.4237 |           0.5206 |
[32m[20221213 18:20:58 @agent_ppo2.py:185][0m |          -0.0073 |         234.7023 |           0.5210 |
[32m[20221213 18:20:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 958.40
[32m[20221213 18:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.63
[32m[20221213 18:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 903.60
[32m[20221213 18:20:58 @agent_ppo2.py:143][0m Total time:      16.81 min
[32m[20221213 18:20:58 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221213 18:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 759 --------------------------#
[32m[20221213 18:20:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:20:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0010 |         246.1733 |           0.5296 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0057 |         240.7427 |           0.5289 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0084 |         237.1560 |           0.5292 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0082 |         235.0188 |           0.5299 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0097 |         233.1481 |           0.5296 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0091 |         231.9666 |           0.5295 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0002 |         249.1326 |           0.5298 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0115 |         229.9250 |           0.5296 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0104 |         228.6809 |           0.5299 |
[32m[20221213 18:20:59 @agent_ppo2.py:185][0m |          -0.0127 |         228.2109 |           0.5304 |
[32m[20221213 18:20:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:20:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 900.39
[32m[20221213 18:20:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.25
[32m[20221213 18:20:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.96
[32m[20221213 18:20:59 @agent_ppo2.py:143][0m Total time:      16.83 min
[32m[20221213 18:20:59 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 18:20:59 @agent_ppo2.py:121][0m #------------------------ Iteration 760 --------------------------#
[32m[20221213 18:21:00 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0001 |         267.7625 |           0.5407 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0024 |         255.7462 |           0.5404 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0055 |         250.7286 |           0.5401 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |           0.0075 |         276.0519 |           0.5401 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0071 |         245.9598 |           0.5396 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0060 |         244.9108 |           0.5403 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0100 |         243.2164 |           0.5400 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0105 |         241.9244 |           0.5403 |
[32m[20221213 18:21:00 @agent_ppo2.py:185][0m |          -0.0104 |         242.0768 |           0.5405 |
[32m[20221213 18:21:01 @agent_ppo2.py:185][0m |          -0.0107 |         240.1802 |           0.5402 |
[32m[20221213 18:21:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 850.34
[32m[20221213 18:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.73
[32m[20221213 18:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.24
[32m[20221213 18:21:01 @agent_ppo2.py:143][0m Total time:      16.85 min
[32m[20221213 18:21:01 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221213 18:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 761 --------------------------#
[32m[20221213 18:21:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:01 @agent_ppo2.py:185][0m |           0.0081 |         259.4166 |           0.5301 |
[32m[20221213 18:21:01 @agent_ppo2.py:185][0m |           0.0090 |         263.8017 |           0.5286 |
[32m[20221213 18:21:01 @agent_ppo2.py:185][0m |           0.0023 |         254.3769 |           0.5282 |
[32m[20221213 18:21:01 @agent_ppo2.py:185][0m |          -0.0061 |         242.8300 |           0.5279 |
[32m[20221213 18:21:01 @agent_ppo2.py:185][0m |          -0.0052 |         241.4322 |           0.5274 |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0110 |         240.4612 |           0.5276 |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0058 |         239.8502 |           0.5271 |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0076 |         239.4016 |           0.5268 |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0047 |         241.8540 |           0.5263 |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0109 |         237.8777 |           0.5263 |
[32m[20221213 18:21:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 870.10
[32m[20221213 18:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.87
[32m[20221213 18:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.52
[32m[20221213 18:21:02 @agent_ppo2.py:143][0m Total time:      16.87 min
[32m[20221213 18:21:02 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 18:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 762 --------------------------#
[32m[20221213 18:21:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0025 |         243.0756 |           0.5340 |
[32m[20221213 18:21:02 @agent_ppo2.py:185][0m |          -0.0042 |         239.3027 |           0.5329 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0069 |         236.8761 |           0.5325 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |           0.0069 |         264.8773 |           0.5316 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0073 |         235.0943 |           0.5308 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0084 |         234.4873 |           0.5309 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0055 |         234.3133 |           0.5303 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0089 |         233.8400 |           0.5298 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0088 |         233.5963 |           0.5298 |
[32m[20221213 18:21:03 @agent_ppo2.py:185][0m |          -0.0093 |         233.2271 |           0.5293 |
[32m[20221213 18:21:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 914.92
[32m[20221213 18:21:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.46
[32m[20221213 18:21:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 919.01
[32m[20221213 18:21:03 @agent_ppo2.py:143][0m Total time:      16.89 min
[32m[20221213 18:21:03 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221213 18:21:03 @agent_ppo2.py:121][0m #------------------------ Iteration 763 --------------------------#
[32m[20221213 18:21:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |           0.0092 |         252.9777 |           0.5342 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0071 |         222.5443 |           0.5327 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0006 |         223.4289 |           0.5329 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0098 |         217.6052 |           0.5325 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0125 |         216.1983 |           0.5325 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0107 |         214.2892 |           0.5321 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |           0.0021 |         238.1124 |           0.5313 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0095 |         212.3219 |           0.5310 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0118 |         210.9305 |           0.5305 |
[32m[20221213 18:21:04 @agent_ppo2.py:185][0m |          -0.0047 |         222.5096 |           0.5306 |
[32m[20221213 18:21:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:21:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 902.35
[32m[20221213 18:21:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.37
[32m[20221213 18:21:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 998.42
[32m[20221213 18:21:05 @agent_ppo2.py:143][0m Total time:      16.91 min
[32m[20221213 18:21:05 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 18:21:05 @agent_ppo2.py:121][0m #------------------------ Iteration 764 --------------------------#
[32m[20221213 18:21:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:05 @agent_ppo2.py:185][0m |          -0.0013 |         241.6425 |           0.5192 |
[32m[20221213 18:21:05 @agent_ppo2.py:185][0m |          -0.0055 |         236.7076 |           0.5179 |
[32m[20221213 18:21:05 @agent_ppo2.py:185][0m |           0.0039 |         257.9616 |           0.5174 |
[32m[20221213 18:21:05 @agent_ppo2.py:185][0m |          -0.0088 |         233.1465 |           0.5168 |
[32m[20221213 18:21:05 @agent_ppo2.py:185][0m |          -0.0098 |         231.7301 |           0.5166 |
[32m[20221213 18:21:05 @agent_ppo2.py:185][0m |          -0.0085 |         230.7027 |           0.5158 |
[32m[20221213 18:21:06 @agent_ppo2.py:185][0m |          -0.0105 |         229.9242 |           0.5160 |
[32m[20221213 18:21:06 @agent_ppo2.py:185][0m |          -0.0103 |         228.8645 |           0.5152 |
[32m[20221213 18:21:06 @agent_ppo2.py:185][0m |          -0.0111 |         228.7386 |           0.5148 |
[32m[20221213 18:21:06 @agent_ppo2.py:185][0m |          -0.0124 |         227.4565 |           0.5148 |
[32m[20221213 18:21:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.03
[32m[20221213 18:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.85
[32m[20221213 18:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 926.10
[32m[20221213 18:21:06 @agent_ppo2.py:143][0m Total time:      16.94 min
[32m[20221213 18:21:06 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221213 18:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 765 --------------------------#
[32m[20221213 18:21:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:06 @agent_ppo2.py:185][0m |          -0.0028 |         241.4312 |           0.5218 |
[32m[20221213 18:21:06 @agent_ppo2.py:185][0m |          -0.0036 |         237.4583 |           0.5201 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0071 |         235.6839 |           0.5199 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0073 |         234.3588 |           0.5193 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0094 |         233.4786 |           0.5193 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0102 |         232.8275 |           0.5195 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0067 |         233.7255 |           0.5191 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0109 |         231.3718 |           0.5186 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0032 |         242.0096 |           0.5190 |
[32m[20221213 18:21:07 @agent_ppo2.py:185][0m |          -0.0105 |         230.5863 |           0.5189 |
[32m[20221213 18:21:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:21:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 861.56
[32m[20221213 18:21:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.48
[32m[20221213 18:21:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.01
[32m[20221213 18:21:07 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221213 18:21:07 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 18:21:07 @agent_ppo2.py:121][0m #------------------------ Iteration 766 --------------------------#
[32m[20221213 18:21:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0031 |         237.3900 |           0.5034 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0048 |         235.1063 |           0.5016 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0070 |         233.0796 |           0.5014 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0076 |         231.9928 |           0.5007 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0098 |         231.1396 |           0.5009 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0086 |         230.5692 |           0.5002 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0099 |         230.2305 |           0.5003 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0112 |         229.5649 |           0.4995 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0108 |         228.8944 |           0.5000 |
[32m[20221213 18:21:08 @agent_ppo2.py:185][0m |          -0.0114 |         228.9230 |           0.5000 |
[32m[20221213 18:21:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 900.98
[32m[20221213 18:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 926.29
[32m[20221213 18:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.45
[32m[20221213 18:21:09 @agent_ppo2.py:143][0m Total time:      16.98 min
[32m[20221213 18:21:09 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221213 18:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 767 --------------------------#
[32m[20221213 18:21:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:09 @agent_ppo2.py:185][0m |          -0.0019 |         230.9250 |           0.5104 |
[32m[20221213 18:21:09 @agent_ppo2.py:185][0m |          -0.0075 |         221.9815 |           0.5098 |
[32m[20221213 18:21:09 @agent_ppo2.py:185][0m |          -0.0094 |         219.2023 |           0.5096 |
[32m[20221213 18:21:09 @agent_ppo2.py:185][0m |          -0.0079 |         218.0682 |           0.5092 |
[32m[20221213 18:21:09 @agent_ppo2.py:185][0m |           0.0053 |         238.0208 |           0.5090 |
[32m[20221213 18:21:09 @agent_ppo2.py:185][0m |          -0.0096 |         216.2902 |           0.5082 |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |          -0.0102 |         215.5464 |           0.5085 |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |          -0.0078 |         215.7689 |           0.5080 |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |          -0.0004 |         230.9194 |           0.5076 |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |          -0.0090 |         214.5352 |           0.5073 |
[32m[20221213 18:21:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:21:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 903.63
[32m[20221213 18:21:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.44
[32m[20221213 18:21:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.73
[32m[20221213 18:21:10 @agent_ppo2.py:143][0m Total time:      17.00 min
[32m[20221213 18:21:10 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 18:21:10 @agent_ppo2.py:121][0m #------------------------ Iteration 768 --------------------------#
[32m[20221213 18:21:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |           0.0006 |         235.5113 |           0.5092 |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |          -0.0028 |         231.9387 |           0.5082 |
[32m[20221213 18:21:10 @agent_ppo2.py:185][0m |          -0.0068 |         229.7454 |           0.5085 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0074 |         227.0635 |           0.5086 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0022 |         230.9418 |           0.5088 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0086 |         223.9558 |           0.5086 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0050 |         224.2219 |           0.5094 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0095 |         221.5212 |           0.5091 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0080 |         220.3213 |           0.5093 |
[32m[20221213 18:21:11 @agent_ppo2.py:185][0m |          -0.0100 |         219.3814 |           0.5094 |
[32m[20221213 18:21:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 969.47
[32m[20221213 18:21:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.27
[32m[20221213 18:21:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 974.25
[32m[20221213 18:21:11 @agent_ppo2.py:143][0m Total time:      17.02 min
[32m[20221213 18:21:11 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221213 18:21:11 @agent_ppo2.py:121][0m #------------------------ Iteration 769 --------------------------#
[32m[20221213 18:21:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0010 |         231.3798 |           0.5231 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0077 |         218.7358 |           0.5222 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0031 |         224.3506 |           0.5207 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0048 |         215.4491 |           0.5201 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0091 |         208.0671 |           0.5199 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0038 |         222.5963 |           0.5198 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0163 |         202.0476 |           0.5189 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0142 |         199.6995 |           0.5190 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0157 |         198.4195 |           0.5182 |
[32m[20221213 18:21:12 @agent_ppo2.py:185][0m |          -0.0029 |         229.6124 |           0.5177 |
[32m[20221213 18:21:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 888.56
[32m[20221213 18:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.99
[32m[20221213 18:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.56
[32m[20221213 18:21:13 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221213 18:21:13 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 18:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 770 --------------------------#
[32m[20221213 18:21:13 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:21:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:13 @agent_ppo2.py:185][0m |          -0.0019 |         217.6544 |           0.5051 |
[32m[20221213 18:21:13 @agent_ppo2.py:185][0m |          -0.0061 |         194.9740 |           0.5044 |
[32m[20221213 18:21:13 @agent_ppo2.py:185][0m |          -0.0081 |         187.0696 |           0.5034 |
[32m[20221213 18:21:13 @agent_ppo2.py:185][0m |          -0.0116 |         181.6053 |           0.5032 |
[32m[20221213 18:21:13 @agent_ppo2.py:185][0m |          -0.0106 |         178.1104 |           0.5023 |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0119 |         174.6191 |           0.5013 |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0040 |         174.0828 |           0.5013 |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0158 |         171.5242 |           0.5003 |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0118 |         168.4602 |           0.5004 |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0164 |         168.4092 |           0.4998 |
[32m[20221213 18:21:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.14
[32m[20221213 18:21:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 903.60
[32m[20221213 18:21:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.77
[32m[20221213 18:21:14 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 18:21:14 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221213 18:21:14 @agent_ppo2.py:121][0m #------------------------ Iteration 771 --------------------------#
[32m[20221213 18:21:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0018 |         242.2619 |           0.5115 |
[32m[20221213 18:21:14 @agent_ppo2.py:185][0m |          -0.0026 |         234.5945 |           0.5114 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0056 |         230.5116 |           0.5117 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0049 |         228.0007 |           0.5116 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0077 |         226.6103 |           0.5112 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0058 |         224.8420 |           0.5115 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0068 |         222.9693 |           0.5116 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0110 |         222.5245 |           0.5111 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0100 |         221.3536 |           0.5113 |
[32m[20221213 18:21:15 @agent_ppo2.py:185][0m |          -0.0098 |         220.8753 |           0.5114 |
[32m[20221213 18:21:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 870.98
[32m[20221213 18:21:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.71
[32m[20221213 18:21:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 905.43
[32m[20221213 18:21:15 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221213 18:21:15 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 18:21:15 @agent_ppo2.py:121][0m #------------------------ Iteration 772 --------------------------#
[32m[20221213 18:21:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |           0.0086 |         260.1858 |           0.5006 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0057 |         229.6297 |           0.5000 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0068 |         223.7978 |           0.5009 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0075 |         220.3778 |           0.5003 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0107 |         217.4983 |           0.5002 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0095 |         215.4328 |           0.5002 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0114 |         212.6846 |           0.5004 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0097 |         210.4311 |           0.5000 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0110 |         208.3685 |           0.5005 |
[32m[20221213 18:21:16 @agent_ppo2.py:185][0m |          -0.0086 |         207.3828 |           0.5002 |
[32m[20221213 18:21:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 948.73
[32m[20221213 18:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.51
[32m[20221213 18:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 938.39
[32m[20221213 18:21:17 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221213 18:21:17 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221213 18:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 773 --------------------------#
[32m[20221213 18:21:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:17 @agent_ppo2.py:185][0m |           0.0116 |         247.8887 |           0.5003 |
[32m[20221213 18:21:17 @agent_ppo2.py:185][0m |          -0.0033 |         218.5086 |           0.4984 |
[32m[20221213 18:21:17 @agent_ppo2.py:185][0m |          -0.0073 |         212.7186 |           0.4979 |
[32m[20221213 18:21:17 @agent_ppo2.py:185][0m |          -0.0079 |         210.1741 |           0.4978 |
[32m[20221213 18:21:17 @agent_ppo2.py:185][0m |           0.0015 |         216.8842 |           0.4966 |
[32m[20221213 18:21:17 @agent_ppo2.py:185][0m |           0.0021 |         221.2897 |           0.4963 |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0073 |         205.4215 |           0.4969 |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0108 |         204.8123 |           0.4963 |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0091 |         204.3373 |           0.4964 |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0000 |         235.4803 |           0.4957 |
[32m[20221213 18:21:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:21:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 992.96
[32m[20221213 18:21:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.34
[32m[20221213 18:21:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.13
[32m[20221213 18:21:18 @agent_ppo2.py:143][0m Total time:      17.14 min
[32m[20221213 18:21:18 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 18:21:18 @agent_ppo2.py:121][0m #------------------------ Iteration 774 --------------------------#
[32m[20221213 18:21:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0001 |         265.6117 |           0.4883 |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0029 |         253.5286 |           0.4877 |
[32m[20221213 18:21:18 @agent_ppo2.py:185][0m |          -0.0093 |         248.7118 |           0.4879 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0064 |         246.4808 |           0.4880 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0024 |         256.3658 |           0.4885 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0076 |         243.3277 |           0.4880 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0023 |         252.2301 |           0.4887 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0015 |         242.7877 |           0.4886 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0091 |         239.0450 |           0.4890 |
[32m[20221213 18:21:19 @agent_ppo2.py:185][0m |          -0.0150 |         238.1199 |           0.4890 |
[32m[20221213 18:21:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 880.48
[32m[20221213 18:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.64
[32m[20221213 18:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 881.80
[32m[20221213 18:21:19 @agent_ppo2.py:143][0m Total time:      17.16 min
[32m[20221213 18:21:19 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221213 18:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 775 --------------------------#
[32m[20221213 18:21:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |           0.0006 |         234.9672 |           0.5004 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |           0.0067 |         229.3317 |           0.4994 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |           0.0032 |         230.4629 |           0.4981 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |           0.0013 |         218.5165 |           0.4977 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |          -0.0032 |         217.3133 |           0.4968 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |          -0.0108 |         203.0644 |           0.4964 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |          -0.0120 |         200.7509 |           0.4964 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |          -0.0130 |         198.2952 |           0.4959 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |          -0.0159 |         195.6304 |           0.4956 |
[32m[20221213 18:21:20 @agent_ppo2.py:185][0m |          -0.0142 |         193.9630 |           0.4951 |
[32m[20221213 18:21:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 867.06
[32m[20221213 18:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.87
[32m[20221213 18:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 962.37
[32m[20221213 18:21:21 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221213 18:21:21 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 18:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 776 --------------------------#
[32m[20221213 18:21:21 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:21:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |           0.0148 |         268.1201 |           0.5119 |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |          -0.0047 |         230.5657 |           0.5115 |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |          -0.0063 |         225.0783 |           0.5116 |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |          -0.0079 |         221.6140 |           0.5108 |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |          -0.0090 |         219.3072 |           0.5110 |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |          -0.0010 |         240.0325 |           0.5107 |
[32m[20221213 18:21:21 @agent_ppo2.py:185][0m |           0.0074 |         240.3075 |           0.5105 |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |          -0.0084 |         214.7721 |           0.5096 |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |          -0.0128 |         213.1939 |           0.5106 |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |          -0.0122 |         211.9023 |           0.5101 |
[32m[20221213 18:21:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 870.56
[32m[20221213 18:21:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.82
[32m[20221213 18:21:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.25
[32m[20221213 18:21:22 @agent_ppo2.py:143][0m Total time:      17.20 min
[32m[20221213 18:21:22 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221213 18:21:22 @agent_ppo2.py:121][0m #------------------------ Iteration 777 --------------------------#
[32m[20221213 18:21:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |           0.0003 |         255.2162 |           0.5000 |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |          -0.0074 |         243.2544 |           0.4991 |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |          -0.0095 |         238.2859 |           0.4987 |
[32m[20221213 18:21:22 @agent_ppo2.py:185][0m |          -0.0100 |         234.2898 |           0.4984 |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0083 |         231.4940 |           0.4981 |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0110 |         228.7462 |           0.4977 |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0032 |         246.7436 |           0.4977 |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0085 |         224.8048 |           0.4979 |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0136 |         222.6915 |           0.4975 |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0093 |         223.2669 |           0.4971 |
[32m[20221213 18:21:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 900.41
[32m[20221213 18:21:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 969.39
[32m[20221213 18:21:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 909.11
[32m[20221213 18:21:23 @agent_ppo2.py:143][0m Total time:      17.22 min
[32m[20221213 18:21:23 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 18:21:23 @agent_ppo2.py:121][0m #------------------------ Iteration 778 --------------------------#
[32m[20221213 18:21:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:23 @agent_ppo2.py:185][0m |          -0.0015 |         240.3854 |           0.4791 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0050 |         235.0096 |           0.4790 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0057 |         232.3961 |           0.4790 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0052 |         230.9851 |           0.4794 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0070 |         231.5219 |           0.4793 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0099 |         227.5291 |           0.4798 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0101 |         226.4970 |           0.4801 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0117 |         225.6507 |           0.4800 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0117 |         225.0422 |           0.4801 |
[32m[20221213 18:21:24 @agent_ppo2.py:185][0m |          -0.0128 |         224.0868 |           0.4803 |
[32m[20221213 18:21:24 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:21:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 950.91
[32m[20221213 18:21:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 989.12
[32m[20221213 18:21:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 953.71
[32m[20221213 18:21:24 @agent_ppo2.py:143][0m Total time:      17.24 min
[32m[20221213 18:21:24 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221213 18:21:24 @agent_ppo2.py:121][0m #------------------------ Iteration 779 --------------------------#
[32m[20221213 18:21:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0028 |         223.1769 |           0.4877 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0081 |         213.3545 |           0.4867 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0097 |         208.4460 |           0.4865 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0008 |         237.4428 |           0.4871 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0117 |         203.0376 |           0.4858 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0118 |         199.8659 |           0.4866 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |           0.0044 |         227.3710 |           0.4865 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0117 |         196.3831 |           0.4865 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0113 |         195.4499 |           0.4867 |
[32m[20221213 18:21:25 @agent_ppo2.py:185][0m |          -0.0125 |         193.5841 |           0.4863 |
[32m[20221213 18:21:25 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 893.52
[32m[20221213 18:21:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.85
[32m[20221213 18:21:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 970.62
[32m[20221213 18:21:26 @agent_ppo2.py:143][0m Total time:      17.26 min
[32m[20221213 18:21:26 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 18:21:26 @agent_ppo2.py:121][0m #------------------------ Iteration 780 --------------------------#
[32m[20221213 18:21:26 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:21:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |           0.0131 |         248.3953 |           0.4959 |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |          -0.0048 |         215.8696 |           0.4947 |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |          -0.0072 |         211.6040 |           0.4944 |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |          -0.0093 |         209.0927 |           0.4941 |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |          -0.0100 |         207.1394 |           0.4943 |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |          -0.0039 |         210.1537 |           0.4942 |
[32m[20221213 18:21:26 @agent_ppo2.py:185][0m |          -0.0090 |         205.0607 |           0.4937 |
[32m[20221213 18:21:27 @agent_ppo2.py:185][0m |          -0.0092 |         203.3188 |           0.4941 |
[32m[20221213 18:21:27 @agent_ppo2.py:185][0m |          -0.0100 |         202.4863 |           0.4943 |
[32m[20221213 18:21:27 @agent_ppo2.py:185][0m |          -0.0124 |         201.1270 |           0.4940 |
[32m[20221213 18:21:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 912.89
[32m[20221213 18:21:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.46
[32m[20221213 18:21:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.46
[32m[20221213 18:21:27 @agent_ppo2.py:143][0m Total time:      17.29 min
[32m[20221213 18:21:27 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221213 18:21:27 @agent_ppo2.py:121][0m #------------------------ Iteration 781 --------------------------#
[32m[20221213 18:21:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:27 @agent_ppo2.py:185][0m |           0.0002 |         230.7591 |           0.5059 |
[32m[20221213 18:21:27 @agent_ppo2.py:185][0m |          -0.0077 |         219.0044 |           0.5055 |
[32m[20221213 18:21:27 @agent_ppo2.py:185][0m |          -0.0083 |         211.2701 |           0.5053 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0104 |         204.9258 |           0.5050 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0116 |         200.8173 |           0.5050 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0136 |         197.7012 |           0.5047 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0152 |         195.3123 |           0.5046 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0148 |         192.8160 |           0.5046 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0016 |         224.6790 |           0.5044 |
[32m[20221213 18:21:28 @agent_ppo2.py:185][0m |          -0.0021 |         213.7012 |           0.5039 |
[32m[20221213 18:21:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 876.49
[32m[20221213 18:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 929.49
[32m[20221213 18:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 936.77
[32m[20221213 18:21:28 @agent_ppo2.py:143][0m Total time:      17.31 min
[32m[20221213 18:21:28 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 18:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 782 --------------------------#
[32m[20221213 18:21:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0015 |         241.4404 |           0.4982 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0048 |         229.1310 |           0.4974 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0080 |         222.3583 |           0.4975 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0110 |         217.9995 |           0.4972 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0108 |         215.5963 |           0.4972 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0100 |         214.4628 |           0.4972 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0030 |         223.9757 |           0.4970 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0122 |         212.3234 |           0.4969 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0148 |         211.1644 |           0.4968 |
[32m[20221213 18:21:29 @agent_ppo2.py:185][0m |          -0.0100 |         210.6234 |           0.4964 |
[32m[20221213 18:21:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.56
[32m[20221213 18:21:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 995.42
[32m[20221213 18:21:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 934.41
[32m[20221213 18:21:29 @agent_ppo2.py:143][0m Total time:      17.33 min
[32m[20221213 18:21:29 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221213 18:21:29 @agent_ppo2.py:121][0m #------------------------ Iteration 783 --------------------------#
[32m[20221213 18:21:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0027 |         241.1057 |           0.5004 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0050 |         234.8786 |           0.4993 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |           0.0118 |         270.6776 |           0.4989 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0043 |         233.2764 |           0.4978 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0106 |         231.0566 |           0.4985 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0127 |         230.2820 |           0.4985 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0100 |         229.8298 |           0.4991 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0146 |         229.5665 |           0.4989 |
[32m[20221213 18:21:30 @agent_ppo2.py:185][0m |          -0.0120 |         229.0422 |           0.4987 |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |          -0.0121 |         228.7172 |           0.4992 |
[32m[20221213 18:21:31 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 975.51
[32m[20221213 18:21:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.32
[32m[20221213 18:21:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 945.94
[32m[20221213 18:21:31 @agent_ppo2.py:143][0m Total time:      17.35 min
[32m[20221213 18:21:31 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 18:21:31 @agent_ppo2.py:121][0m #------------------------ Iteration 784 --------------------------#
[32m[20221213 18:21:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |          -0.0037 |         235.3620 |           0.5027 |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |           0.0024 |         255.7830 |           0.5033 |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |           0.0006 |         236.2955 |           0.5034 |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |          -0.0101 |         225.9260 |           0.5034 |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |           0.0020 |         235.8311 |           0.5040 |
[32m[20221213 18:21:31 @agent_ppo2.py:185][0m |          -0.0104 |         224.0285 |           0.5038 |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |          -0.0114 |         223.1743 |           0.5045 |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |           0.0026 |         240.7485 |           0.5044 |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |          -0.0005 |         225.8154 |           0.5042 |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |          -0.0099 |         222.9038 |           0.5049 |
[32m[20221213 18:21:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 955.76
[32m[20221213 18:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.91
[32m[20221213 18:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 889.80
[32m[20221213 18:21:32 @agent_ppo2.py:143][0m Total time:      17.37 min
[32m[20221213 18:21:32 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221213 18:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 785 --------------------------#
[32m[20221213 18:21:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |          -0.0014 |         223.6477 |           0.5075 |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |          -0.0063 |         217.6392 |           0.5065 |
[32m[20221213 18:21:32 @agent_ppo2.py:185][0m |          -0.0078 |         214.8486 |           0.5060 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0085 |         212.6835 |           0.5057 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0040 |         216.2739 |           0.5052 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0095 |         208.4147 |           0.5048 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0099 |         206.7046 |           0.5045 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0100 |         205.1968 |           0.5045 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0048 |         210.9902 |           0.5039 |
[32m[20221213 18:21:33 @agent_ppo2.py:185][0m |          -0.0122 |         201.9085 |           0.5030 |
[32m[20221213 18:21:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 970.53
[32m[20221213 18:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.39
[32m[20221213 18:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.23
[32m[20221213 18:21:33 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221213 18:21:33 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 18:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 786 --------------------------#
[32m[20221213 18:21:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0025 |         238.9538 |           0.5164 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0030 |         235.4540 |           0.5161 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |           0.0016 |         240.2475 |           0.5162 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0036 |         233.9457 |           0.5164 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0081 |         229.4760 |           0.5162 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |           0.0019 |         237.6978 |           0.5166 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0072 |         227.3758 |           0.5167 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0088 |         226.5068 |           0.5172 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |           0.0002 |         245.7578 |           0.5173 |
[32m[20221213 18:21:34 @agent_ppo2.py:185][0m |          -0.0041 |         228.4316 |           0.5167 |
[32m[20221213 18:21:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.01
[32m[20221213 18:21:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.38
[32m[20221213 18:21:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.65
[32m[20221213 18:21:34 @agent_ppo2.py:143][0m Total time:      17.41 min
[32m[20221213 18:21:34 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221213 18:21:34 @agent_ppo2.py:121][0m #------------------------ Iteration 787 --------------------------#
[32m[20221213 18:21:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |           0.0050 |         242.9672 |           0.5059 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0015 |         240.6708 |           0.5055 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0063 |         235.6343 |           0.5055 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0083 |         234.4711 |           0.5058 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0067 |         236.4445 |           0.5061 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0107 |         232.2041 |           0.5062 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0092 |         232.4583 |           0.5064 |
[32m[20221213 18:21:35 @agent_ppo2.py:185][0m |          -0.0117 |         230.7606 |           0.5066 |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |          -0.0082 |         231.8076 |           0.5068 |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |          -0.0110 |         228.9814 |           0.5071 |
[32m[20221213 18:21:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 863.28
[32m[20221213 18:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.17
[32m[20221213 18:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.62
[32m[20221213 18:21:36 @agent_ppo2.py:143][0m Total time:      17.43 min
[32m[20221213 18:21:36 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 18:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 788 --------------------------#
[32m[20221213 18:21:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |           0.0009 |         245.7729 |           0.5110 |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |          -0.0059 |         234.0777 |           0.5097 |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |          -0.0099 |         229.7219 |           0.5093 |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |          -0.0100 |         226.8421 |           0.5091 |
[32m[20221213 18:21:36 @agent_ppo2.py:185][0m |          -0.0116 |         224.8721 |           0.5088 |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0117 |         222.7157 |           0.5083 |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0123 |         221.0433 |           0.5083 |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0025 |         230.1648 |           0.5077 |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0131 |         218.0398 |           0.5066 |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0129 |         216.8252 |           0.5072 |
[32m[20221213 18:21:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.04
[32m[20221213 18:21:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.24
[32m[20221213 18:21:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.69
[32m[20221213 18:21:37 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221213 18:21:37 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221213 18:21:37 @agent_ppo2.py:121][0m #------------------------ Iteration 789 --------------------------#
[32m[20221213 18:21:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0026 |         229.2444 |           0.5184 |
[32m[20221213 18:21:37 @agent_ppo2.py:185][0m |          -0.0069 |         220.8122 |           0.5173 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0096 |         217.6105 |           0.5165 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0118 |         215.0267 |           0.5172 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0105 |         212.9784 |           0.5166 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0111 |         211.0877 |           0.5167 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0135 |         208.8032 |           0.5168 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0154 |         206.7896 |           0.5167 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0148 |         204.9686 |           0.5163 |
[32m[20221213 18:21:38 @agent_ppo2.py:185][0m |          -0.0141 |         204.4570 |           0.5167 |
[32m[20221213 18:21:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 902.17
[32m[20221213 18:21:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.78
[32m[20221213 18:21:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 897.30
[32m[20221213 18:21:38 @agent_ppo2.py:143][0m Total time:      17.48 min
[32m[20221213 18:21:38 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 18:21:38 @agent_ppo2.py:121][0m #------------------------ Iteration 790 --------------------------#
[32m[20221213 18:21:39 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 18:21:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |           0.0036 |         242.8607 |           0.5168 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |           0.0034 |         244.4540 |           0.5167 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0047 |         234.6947 |           0.5171 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0074 |         233.2372 |           0.5171 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0090 |         232.3752 |           0.5177 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0018 |         236.4166 |           0.5179 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0109 |         231.0114 |           0.5186 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0095 |         230.6446 |           0.5186 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0115 |         230.0371 |           0.5187 |
[32m[20221213 18:21:39 @agent_ppo2.py:185][0m |          -0.0124 |         229.4977 |           0.5192 |
[32m[20221213 18:21:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 886.41
[32m[20221213 18:21:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.33
[32m[20221213 18:21:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 944.26
[32m[20221213 18:21:40 @agent_ppo2.py:143][0m Total time:      17.50 min
[32m[20221213 18:21:40 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221213 18:21:40 @agent_ppo2.py:121][0m #------------------------ Iteration 791 --------------------------#
[32m[20221213 18:21:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |          -0.0027 |         230.4999 |           0.5131 |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |          -0.0067 |         211.2019 |           0.5120 |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |          -0.0073 |         205.3138 |           0.5115 |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |          -0.0098 |         201.2574 |           0.5113 |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |          -0.0088 |         198.7194 |           0.5115 |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |          -0.0021 |         214.6390 |           0.5107 |
[32m[20221213 18:21:40 @agent_ppo2.py:185][0m |           0.0026 |         219.7892 |           0.5103 |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |          -0.0127 |         193.5278 |           0.5104 |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |          -0.0115 |         192.3573 |           0.5101 |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |          -0.0143 |         191.2464 |           0.5101 |
[32m[20221213 18:21:41 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.26
[32m[20221213 18:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.07
[32m[20221213 18:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.82
[32m[20221213 18:21:41 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221213 18:21:41 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 18:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 792 --------------------------#
[32m[20221213 18:21:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |          -0.0007 |         247.9628 |           0.5178 |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |          -0.0018 |         238.7209 |           0.5171 |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |           0.0045 |         255.7600 |           0.5171 |
[32m[20221213 18:21:41 @agent_ppo2.py:185][0m |          -0.0085 |         232.5539 |           0.5158 |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |          -0.0081 |         230.6335 |           0.5165 |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |          -0.0010 |         235.9517 |           0.5164 |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |           0.0012 |         243.5910 |           0.5163 |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |          -0.0098 |         228.1750 |           0.5168 |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |          -0.0055 |         227.9097 |           0.5165 |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |          -0.0098 |         227.0352 |           0.5167 |
[32m[20221213 18:21:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 854.77
[32m[20221213 18:21:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 920.65
[32m[20221213 18:21:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 958.56
[32m[20221213 18:21:42 @agent_ppo2.py:143][0m Total time:      17.54 min
[32m[20221213 18:21:42 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221213 18:21:42 @agent_ppo2.py:121][0m #------------------------ Iteration 793 --------------------------#
[32m[20221213 18:21:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:42 @agent_ppo2.py:185][0m |          -0.0025 |         237.8969 |           0.5178 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0062 |         234.2385 |           0.5165 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |           0.0011 |         242.7863 |           0.5161 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0074 |         232.1089 |           0.5155 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0097 |         230.6045 |           0.5158 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0105 |         229.9990 |           0.5153 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0116 |         229.2001 |           0.5154 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0088 |         229.7813 |           0.5149 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0123 |         226.9908 |           0.5145 |
[32m[20221213 18:21:43 @agent_ppo2.py:185][0m |          -0.0132 |         226.7306 |           0.5145 |
[32m[20221213 18:21:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.78
[32m[20221213 18:21:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.84
[32m[20221213 18:21:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 917.12
[32m[20221213 18:21:43 @agent_ppo2.py:143][0m Total time:      17.56 min
[32m[20221213 18:21:43 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 18:21:43 @agent_ppo2.py:121][0m #------------------------ Iteration 794 --------------------------#
[32m[20221213 18:21:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0020 |         245.1008 |           0.5041 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |           0.0068 |         261.1019 |           0.5038 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0074 |         238.1072 |           0.5030 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0087 |         236.0255 |           0.5029 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0088 |         234.3722 |           0.5023 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0076 |         232.8233 |           0.5024 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0066 |         233.4944 |           0.5019 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0080 |         230.6363 |           0.5019 |
[32m[20221213 18:21:44 @agent_ppo2.py:185][0m |          -0.0054 |         233.3286 |           0.5018 |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0071 |         231.0267 |           0.5014 |
[32m[20221213 18:21:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 871.26
[32m[20221213 18:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.64
[32m[20221213 18:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 917.99
[32m[20221213 18:21:45 @agent_ppo2.py:143][0m Total time:      17.58 min
[32m[20221213 18:21:45 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221213 18:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 795 --------------------------#
[32m[20221213 18:21:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0020 |         236.9040 |           0.5050 |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0051 |         230.4879 |           0.5038 |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0029 |         226.8280 |           0.5040 |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0061 |         222.9778 |           0.5037 |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0081 |         220.7712 |           0.5029 |
[32m[20221213 18:21:45 @agent_ppo2.py:185][0m |          -0.0043 |         220.7162 |           0.5029 |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0098 |         216.7482 |           0.5026 |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0106 |         215.0809 |           0.5024 |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0110 |         213.6611 |           0.5017 |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0106 |         212.5020 |           0.5020 |
[32m[20221213 18:21:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:21:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.37
[32m[20221213 18:21:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 950.35
[32m[20221213 18:21:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 959.92
[32m[20221213 18:21:46 @agent_ppo2.py:143][0m Total time:      17.60 min
[32m[20221213 18:21:46 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 18:21:46 @agent_ppo2.py:121][0m #------------------------ Iteration 796 --------------------------#
[32m[20221213 18:21:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0011 |         228.2345 |           0.4937 |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0071 |         215.6454 |           0.4934 |
[32m[20221213 18:21:46 @agent_ppo2.py:185][0m |          -0.0079 |         209.2887 |           0.4929 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0102 |         207.3791 |           0.4926 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0105 |         204.3870 |           0.4928 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0096 |         202.8417 |           0.4925 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0114 |         200.3214 |           0.4926 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0125 |         198.2607 |           0.4928 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0095 |         197.4221 |           0.4926 |
[32m[20221213 18:21:47 @agent_ppo2.py:185][0m |          -0.0124 |         195.1586 |           0.4926 |
[32m[20221213 18:21:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.74
[32m[20221213 18:21:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.88
[32m[20221213 18:21:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.18
[32m[20221213 18:21:47 @agent_ppo2.py:143][0m Total time:      17.63 min
[32m[20221213 18:21:47 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221213 18:21:47 @agent_ppo2.py:121][0m #------------------------ Iteration 797 --------------------------#
[32m[20221213 18:21:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0000 |         222.7581 |           0.5079 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0035 |         215.0365 |           0.5077 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0045 |         209.0837 |           0.5072 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0083 |         205.1625 |           0.5069 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |           0.0007 |         227.6068 |           0.5074 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0076 |         201.7104 |           0.5072 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0052 |         209.4928 |           0.5070 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0109 |         199.9729 |           0.5072 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0041 |         207.2465 |           0.5075 |
[32m[20221213 18:21:48 @agent_ppo2.py:185][0m |          -0.0119 |         198.2535 |           0.5075 |
[32m[20221213 18:21:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 939.88
[32m[20221213 18:21:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.66
[32m[20221213 18:21:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.19
[32m[20221213 18:21:49 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221213 18:21:49 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 18:21:49 @agent_ppo2.py:121][0m #------------------------ Iteration 798 --------------------------#
[32m[20221213 18:21:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0017 |         272.3216 |           0.5041 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0027 |         260.9863 |           0.5045 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0066 |         252.9081 |           0.5046 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0038 |         249.5933 |           0.5043 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0004 |         250.8034 |           0.5050 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |           0.0011 |         268.5421 |           0.5050 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0111 |         242.2106 |           0.5045 |
[32m[20221213 18:21:49 @agent_ppo2.py:185][0m |          -0.0113 |         240.5799 |           0.5049 |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |          -0.0098 |         239.5825 |           0.5055 |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |          -0.0115 |         238.0730 |           0.5054 |
[32m[20221213 18:21:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 892.13
[32m[20221213 18:21:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.18
[32m[20221213 18:21:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.95
[32m[20221213 18:21:50 @agent_ppo2.py:143][0m Total time:      17.67 min
[32m[20221213 18:21:50 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221213 18:21:50 @agent_ppo2.py:121][0m #------------------------ Iteration 799 --------------------------#
[32m[20221213 18:21:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |           0.0021 |         247.2883 |           0.5153 |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |           0.0041 |         253.7704 |           0.5139 |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |          -0.0060 |         235.6399 |           0.5130 |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |          -0.0031 |         242.4877 |           0.5128 |
[32m[20221213 18:21:50 @agent_ppo2.py:185][0m |          -0.0092 |         231.7978 |           0.5123 |
[32m[20221213 18:21:51 @agent_ppo2.py:185][0m |          -0.0065 |         230.8854 |           0.5126 |
[32m[20221213 18:21:51 @agent_ppo2.py:185][0m |          -0.0043 |         242.3424 |           0.5123 |
[32m[20221213 18:21:51 @agent_ppo2.py:185][0m |          -0.0133 |         228.5679 |           0.5124 |
[32m[20221213 18:21:51 @agent_ppo2.py:185][0m |          -0.0094 |         228.1059 |           0.5124 |
[32m[20221213 18:21:51 @agent_ppo2.py:185][0m |          -0.0100 |         226.9820 |           0.5123 |
[32m[20221213 18:21:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.82
[32m[20221213 18:21:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.29
[32m[20221213 18:21:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.91
[32m[20221213 18:21:51 @agent_ppo2.py:143][0m Total time:      17.69 min
[32m[20221213 18:21:51 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 18:21:51 @agent_ppo2.py:121][0m #------------------------ Iteration 800 --------------------------#
[32m[20221213 18:21:51 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:21:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:51 @agent_ppo2.py:185][0m |          -0.0009 |         242.1771 |           0.5074 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0031 |         232.8881 |           0.5073 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0080 |         227.6865 |           0.5067 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0100 |         224.8150 |           0.5069 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0080 |         222.7406 |           0.5067 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0070 |         222.1639 |           0.5068 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0083 |         220.9192 |           0.5065 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0089 |         218.2517 |           0.5065 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0094 |         217.3508 |           0.5065 |
[32m[20221213 18:21:52 @agent_ppo2.py:185][0m |          -0.0107 |         215.9578 |           0.5066 |
[32m[20221213 18:21:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.55
[32m[20221213 18:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.16
[32m[20221213 18:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 967.87
[32m[20221213 18:21:52 @agent_ppo2.py:143][0m Total time:      17.71 min
[32m[20221213 18:21:52 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221213 18:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 801 --------------------------#
[32m[20221213 18:21:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |           0.0006 |         238.7318 |           0.5102 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0044 |         232.0525 |           0.5097 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0082 |         227.2946 |           0.5084 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0095 |         223.7621 |           0.5085 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |           0.0002 |         226.9527 |           0.5084 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0053 |         224.7488 |           0.5078 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0133 |         219.1749 |           0.5077 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0147 |         218.2305 |           0.5079 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0117 |         216.7077 |           0.5080 |
[32m[20221213 18:21:53 @agent_ppo2.py:185][0m |          -0.0093 |         217.3605 |           0.5078 |
[32m[20221213 18:21:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:21:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.06
[32m[20221213 18:21:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.82
[32m[20221213 18:21:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.84
[32m[20221213 18:21:54 @agent_ppo2.py:143][0m Total time:      17.73 min
[32m[20221213 18:21:54 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 18:21:54 @agent_ppo2.py:121][0m #------------------------ Iteration 802 --------------------------#
[32m[20221213 18:21:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:21:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:54 @agent_ppo2.py:185][0m |          -0.0038 |         249.2201 |           0.5027 |
[32m[20221213 18:21:54 @agent_ppo2.py:185][0m |          -0.0050 |         244.5830 |           0.5023 |
[32m[20221213 18:21:54 @agent_ppo2.py:185][0m |          -0.0071 |         242.2644 |           0.5033 |
[32m[20221213 18:21:54 @agent_ppo2.py:185][0m |          -0.0082 |         241.3844 |           0.5032 |
[32m[20221213 18:21:54 @agent_ppo2.py:185][0m |          -0.0105 |         240.5215 |           0.5043 |
[32m[20221213 18:21:54 @agent_ppo2.py:185][0m |          -0.0102 |         239.9071 |           0.5042 |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |          -0.0086 |         239.5653 |           0.5045 |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |          -0.0075 |         240.2188 |           0.5053 |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |          -0.0099 |         238.7429 |           0.5049 |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |          -0.0118 |         238.3745 |           0.5053 |
[32m[20221213 18:21:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:21:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 951.20
[32m[20221213 18:21:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 995.08
[32m[20221213 18:21:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 917.15
[32m[20221213 18:21:55 @agent_ppo2.py:143][0m Total time:      17.75 min
[32m[20221213 18:21:55 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221213 18:21:55 @agent_ppo2.py:121][0m #------------------------ Iteration 803 --------------------------#
[32m[20221213 18:21:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |           0.0109 |         262.2224 |           0.5234 |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |          -0.0045 |         239.1534 |           0.5222 |
[32m[20221213 18:21:55 @agent_ppo2.py:185][0m |          -0.0074 |         237.5224 |           0.5231 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |          -0.0102 |         236.5698 |           0.5225 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |          -0.0082 |         235.8316 |           0.5226 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |          -0.0101 |         235.1249 |           0.5225 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |          -0.0104 |         234.4812 |           0.5225 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |          -0.0112 |         233.8849 |           0.5224 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |           0.0015 |         268.8005 |           0.5221 |
[32m[20221213 18:21:56 @agent_ppo2.py:185][0m |          -0.0112 |         233.0990 |           0.5215 |
[32m[20221213 18:21:56 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.36
[32m[20221213 18:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.35
[32m[20221213 18:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 964.55
[32m[20221213 18:21:56 @agent_ppo2.py:143][0m Total time:      17.77 min
[32m[20221213 18:21:56 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 18:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 804 --------------------------#
[32m[20221213 18:21:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |           0.0007 |         241.5143 |           0.5124 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0063 |         239.0010 |           0.5125 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0088 |         238.1240 |           0.5120 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0057 |         237.7289 |           0.5118 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0109 |         237.1014 |           0.5113 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |           0.0066 |         260.8630 |           0.5115 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0086 |         236.8561 |           0.5108 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0102 |         235.9175 |           0.5110 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0098 |         235.7561 |           0.5107 |
[32m[20221213 18:21:57 @agent_ppo2.py:185][0m |          -0.0120 |         235.4450 |           0.5107 |
[32m[20221213 18:21:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:21:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.99
[32m[20221213 18:21:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.09
[32m[20221213 18:21:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.60
[32m[20221213 18:21:58 @agent_ppo2.py:143][0m Total time:      17.80 min
[32m[20221213 18:21:58 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221213 18:21:58 @agent_ppo2.py:121][0m #------------------------ Iteration 805 --------------------------#
[32m[20221213 18:21:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:21:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |           0.0028 |         242.2720 |           0.5177 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |          -0.0080 |         231.7850 |           0.5163 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |          -0.0101 |         225.6914 |           0.5168 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |           0.0034 |         252.6905 |           0.5166 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |          -0.0098 |         218.2715 |           0.5165 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |          -0.0114 |         215.4632 |           0.5174 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |          -0.0034 |         227.1794 |           0.5171 |
[32m[20221213 18:21:58 @agent_ppo2.py:185][0m |          -0.0137 |         211.5243 |           0.5173 |
[32m[20221213 18:21:59 @agent_ppo2.py:185][0m |          -0.0127 |         211.0708 |           0.5177 |
[32m[20221213 18:21:59 @agent_ppo2.py:185][0m |          -0.0000 |         225.3914 |           0.5181 |
[32m[20221213 18:21:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:21:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.22
[32m[20221213 18:21:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 931.26
[32m[20221213 18:21:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.88
[32m[20221213 18:21:59 @agent_ppo2.py:143][0m Total time:      17.82 min
[32m[20221213 18:21:59 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 18:21:59 @agent_ppo2.py:121][0m #------------------------ Iteration 806 --------------------------#
[32m[20221213 18:21:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:21:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:21:59 @agent_ppo2.py:185][0m |          -0.0022 |         247.7195 |           0.5106 |
[32m[20221213 18:21:59 @agent_ppo2.py:185][0m |          -0.0054 |         242.5619 |           0.5102 |
[32m[20221213 18:21:59 @agent_ppo2.py:185][0m |          -0.0098 |         240.6398 |           0.5102 |
[32m[20221213 18:21:59 @agent_ppo2.py:185][0m |          -0.0071 |         239.6154 |           0.5099 |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0039 |         241.6767 |           0.5101 |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0093 |         238.2357 |           0.5097 |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0104 |         237.5456 |           0.5092 |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0102 |         237.0793 |           0.5097 |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0106 |         236.7390 |           0.5095 |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0103 |         236.3539 |           0.5096 |
[32m[20221213 18:22:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 908.67
[32m[20221213 18:22:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 953.97
[32m[20221213 18:22:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.26
[32m[20221213 18:22:00 @agent_ppo2.py:143][0m Total time:      17.84 min
[32m[20221213 18:22:00 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221213 18:22:00 @agent_ppo2.py:121][0m #------------------------ Iteration 807 --------------------------#
[32m[20221213 18:22:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:00 @agent_ppo2.py:185][0m |          -0.0009 |         244.9022 |           0.5202 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0033 |         240.6600 |           0.5183 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0077 |         239.0162 |           0.5170 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0083 |         237.3807 |           0.5159 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0095 |         236.4315 |           0.5150 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0079 |         236.0258 |           0.5151 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0085 |         235.4629 |           0.5140 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0108 |         234.2595 |           0.5141 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0118 |         233.8254 |           0.5135 |
[32m[20221213 18:22:01 @agent_ppo2.py:185][0m |          -0.0121 |         233.0739 |           0.5133 |
[32m[20221213 18:22:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.74
[32m[20221213 18:22:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.59
[32m[20221213 18:22:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 980.79
[32m[20221213 18:22:01 @agent_ppo2.py:143][0m Total time:      17.86 min
[32m[20221213 18:22:01 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 18:22:01 @agent_ppo2.py:121][0m #------------------------ Iteration 808 --------------------------#
[32m[20221213 18:22:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |           0.0000 |         241.6210 |           0.5089 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |           0.0126 |         264.6451 |           0.5091 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0049 |         237.7865 |           0.5077 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0045 |         236.5939 |           0.5080 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0072 |         235.4561 |           0.5070 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0090 |         234.8975 |           0.5064 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0097 |         234.0692 |           0.5057 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0065 |         234.8109 |           0.5050 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0106 |         232.8078 |           0.5051 |
[32m[20221213 18:22:02 @agent_ppo2.py:185][0m |          -0.0067 |         234.1460 |           0.5044 |
[32m[20221213 18:22:02 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.44
[32m[20221213 18:22:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 936.08
[32m[20221213 18:22:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 907.96
[32m[20221213 18:22:03 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221213 18:22:03 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221213 18:22:03 @agent_ppo2.py:121][0m #------------------------ Iteration 809 --------------------------#
[32m[20221213 18:22:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |           0.0010 |         238.4851 |           0.4987 |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |          -0.0052 |         235.1068 |           0.4982 |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |          -0.0053 |         234.3802 |           0.4983 |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |          -0.0093 |         233.4972 |           0.4982 |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |          -0.0081 |         233.5874 |           0.4989 |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |          -0.0097 |         232.7764 |           0.4991 |
[32m[20221213 18:22:03 @agent_ppo2.py:185][0m |          -0.0095 |         232.2152 |           0.4994 |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |          -0.0063 |         234.0884 |           0.4996 |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |          -0.0118 |         231.7348 |           0.4997 |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |          -0.0125 |         231.5921 |           0.4997 |
[32m[20221213 18:22:04 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 956.42
[32m[20221213 18:22:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.15
[32m[20221213 18:22:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.00
[32m[20221213 18:22:04 @agent_ppo2.py:143][0m Total time:      17.90 min
[32m[20221213 18:22:04 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 18:22:04 @agent_ppo2.py:121][0m #------------------------ Iteration 810 --------------------------#
[32m[20221213 18:22:04 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:22:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |          -0.0019 |         240.1470 |           0.5238 |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |          -0.0053 |         229.4432 |           0.5229 |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |          -0.0025 |         227.1962 |           0.5228 |
[32m[20221213 18:22:04 @agent_ppo2.py:185][0m |           0.0008 |         236.2964 |           0.5228 |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0048 |         223.9769 |           0.5227 |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0084 |         221.4537 |           0.5231 |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0095 |         220.3653 |           0.5227 |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0100 |         219.5432 |           0.5234 |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0106 |         218.2085 |           0.5233 |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0106 |         217.7976 |           0.5230 |
[32m[20221213 18:22:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.72
[32m[20221213 18:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.38
[32m[20221213 18:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 912.62
[32m[20221213 18:22:05 @agent_ppo2.py:143][0m Total time:      17.92 min
[32m[20221213 18:22:05 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221213 18:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 811 --------------------------#
[32m[20221213 18:22:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:05 @agent_ppo2.py:185][0m |          -0.0005 |         246.4871 |           0.5100 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0037 |         241.9828 |           0.5091 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0097 |         239.8470 |           0.5082 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |           0.0033 |         266.3843 |           0.5081 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0110 |         237.8536 |           0.5075 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0119 |         237.3479 |           0.5072 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0002 |         267.3359 |           0.5071 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0117 |         236.8821 |           0.5062 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0130 |         236.3506 |           0.5059 |
[32m[20221213 18:22:06 @agent_ppo2.py:185][0m |          -0.0141 |         235.9117 |           0.5056 |
[32m[20221213 18:22:06 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 861.72
[32m[20221213 18:22:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 944.12
[32m[20221213 18:22:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 914.11
[32m[20221213 18:22:06 @agent_ppo2.py:143][0m Total time:      17.94 min
[32m[20221213 18:22:06 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 18:22:06 @agent_ppo2.py:121][0m #------------------------ Iteration 812 --------------------------#
[32m[20221213 18:22:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0006 |         237.6973 |           0.4955 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0060 |         235.9197 |           0.4950 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0060 |         235.2229 |           0.4950 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0070 |         234.8660 |           0.4948 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0083 |         234.5617 |           0.4951 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0084 |         234.4199 |           0.4947 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |           0.0029 |         245.2826 |           0.4947 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |           0.0047 |         258.5531 |           0.4945 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0022 |         238.1437 |           0.4937 |
[32m[20221213 18:22:07 @agent_ppo2.py:185][0m |          -0.0086 |         233.8724 |           0.4944 |
[32m[20221213 18:22:07 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.07
[32m[20221213 18:22:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.05
[32m[20221213 18:22:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 927.14
[32m[20221213 18:22:08 @agent_ppo2.py:143][0m Total time:      17.96 min
[32m[20221213 18:22:08 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221213 18:22:08 @agent_ppo2.py:121][0m #------------------------ Iteration 813 --------------------------#
[32m[20221213 18:22:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |          -0.0007 |         235.9947 |           0.4914 |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |           0.0011 |         237.2399 |           0.4911 |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |           0.0039 |         246.6317 |           0.4912 |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |           0.0026 |         241.1644 |           0.4905 |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |          -0.0049 |         233.9594 |           0.4902 |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |          -0.0065 |         233.8160 |           0.4909 |
[32m[20221213 18:22:08 @agent_ppo2.py:185][0m |          -0.0070 |         233.9012 |           0.4911 |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0043 |         234.6474 |           0.4911 |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0052 |         234.3327 |           0.4914 |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0066 |         233.4025 |           0.4912 |
[32m[20221213 18:22:09 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 997.31
[32m[20221213 18:22:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.99
[32m[20221213 18:22:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.19
[32m[20221213 18:22:09 @agent_ppo2.py:143][0m Total time:      17.99 min
[32m[20221213 18:22:09 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 18:22:09 @agent_ppo2.py:121][0m #------------------------ Iteration 814 --------------------------#
[32m[20221213 18:22:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0039 |         219.4001 |           0.4959 |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0060 |         208.5848 |           0.4944 |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0031 |         201.6493 |           0.4945 |
[32m[20221213 18:22:09 @agent_ppo2.py:185][0m |          -0.0116 |         192.3914 |           0.4940 |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0140 |         188.3705 |           0.4941 |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0128 |         185.9099 |           0.4939 |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0144 |         181.7762 |           0.4940 |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0143 |         179.9239 |           0.4937 |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0152 |         177.8070 |           0.4947 |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0156 |         177.4201 |           0.4945 |
[32m[20221213 18:22:10 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 976.74
[32m[20221213 18:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.82
[32m[20221213 18:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 901.85
[32m[20221213 18:22:10 @agent_ppo2.py:143][0m Total time:      18.01 min
[32m[20221213 18:22:10 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221213 18:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 815 --------------------------#
[32m[20221213 18:22:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:10 @agent_ppo2.py:185][0m |          -0.0018 |         248.9360 |           0.5134 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0051 |         243.2084 |           0.5122 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0070 |         240.8474 |           0.5128 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0055 |         239.3732 |           0.5127 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0021 |         246.7178 |           0.5125 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |           0.0001 |         256.7458 |           0.5117 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0033 |         239.0077 |           0.5117 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0043 |         238.9237 |           0.5117 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0085 |         236.7732 |           0.5121 |
[32m[20221213 18:22:11 @agent_ppo2.py:185][0m |          -0.0077 |         236.9314 |           0.5121 |
[32m[20221213 18:22:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.63
[32m[20221213 18:22:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.51
[32m[20221213 18:22:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.33
[32m[20221213 18:22:11 @agent_ppo2.py:143][0m Total time:      18.03 min
[32m[20221213 18:22:11 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 18:22:11 @agent_ppo2.py:121][0m #------------------------ Iteration 816 --------------------------#
[32m[20221213 18:22:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0017 |         250.4336 |           0.5086 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0050 |         246.8633 |           0.5071 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0083 |         245.5704 |           0.5066 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0071 |         244.1929 |           0.5059 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0087 |         243.7774 |           0.5058 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0082 |         243.1075 |           0.5055 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0101 |         242.9063 |           0.5062 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0091 |         242.5437 |           0.5058 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0021 |         246.3753 |           0.5051 |
[32m[20221213 18:22:12 @agent_ppo2.py:185][0m |          -0.0090 |         242.1571 |           0.5056 |
[32m[20221213 18:22:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 928.21
[32m[20221213 18:22:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.60
[32m[20221213 18:22:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 937.72
[32m[20221213 18:22:13 @agent_ppo2.py:143][0m Total time:      18.05 min
[32m[20221213 18:22:13 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221213 18:22:13 @agent_ppo2.py:121][0m #------------------------ Iteration 817 --------------------------#
[32m[20221213 18:22:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:13 @agent_ppo2.py:185][0m |          -0.0037 |         249.2011 |           0.5121 |
[32m[20221213 18:22:13 @agent_ppo2.py:185][0m |           0.0070 |         266.0884 |           0.5104 |
[32m[20221213 18:22:13 @agent_ppo2.py:185][0m |          -0.0079 |         246.3146 |           0.5103 |
[32m[20221213 18:22:13 @agent_ppo2.py:185][0m |          -0.0098 |         245.1907 |           0.5095 |
[32m[20221213 18:22:13 @agent_ppo2.py:185][0m |          -0.0096 |         244.7299 |           0.5089 |
[32m[20221213 18:22:13 @agent_ppo2.py:185][0m |          -0.0117 |         244.1488 |           0.5085 |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0068 |         248.2650 |           0.5079 |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0049 |         249.2881 |           0.5077 |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0123 |         243.4778 |           0.5071 |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0125 |         243.0097 |           0.5072 |
[32m[20221213 18:22:14 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.14
[32m[20221213 18:22:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 964.12
[32m[20221213 18:22:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.36
[32m[20221213 18:22:14 @agent_ppo2.py:143][0m Total time:      18.07 min
[32m[20221213 18:22:14 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 18:22:14 @agent_ppo2.py:121][0m #------------------------ Iteration 818 --------------------------#
[32m[20221213 18:22:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0018 |         246.8789 |           0.5003 |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0063 |         242.6954 |           0.4994 |
[32m[20221213 18:22:14 @agent_ppo2.py:185][0m |          -0.0081 |         240.7144 |           0.4987 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0057 |         240.5561 |           0.4982 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0076 |         238.3169 |           0.4983 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0115 |         237.4353 |           0.4981 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0100 |         236.6117 |           0.4981 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0060 |         239.1402 |           0.4973 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0017 |         252.2427 |           0.4974 |
[32m[20221213 18:22:15 @agent_ppo2.py:185][0m |          -0.0121 |         234.3707 |           0.4971 |
[32m[20221213 18:22:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 931.82
[32m[20221213 18:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.61
[32m[20221213 18:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.65
[32m[20221213 18:22:15 @agent_ppo2.py:143][0m Total time:      18.09 min
[32m[20221213 18:22:15 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221213 18:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 819 --------------------------#
[32m[20221213 18:22:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |           0.0010 |         253.8009 |           0.4949 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0038 |         247.5763 |           0.4947 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0033 |         245.1405 |           0.4945 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0046 |         243.4098 |           0.4943 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0068 |         242.2357 |           0.4941 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0064 |         243.2703 |           0.4945 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |           0.0019 |         249.8919 |           0.4949 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0071 |         239.8680 |           0.4947 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0076 |         239.3055 |           0.4949 |
[32m[20221213 18:22:16 @agent_ppo2.py:185][0m |          -0.0063 |         239.1672 |           0.4947 |
[32m[20221213 18:22:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 850.57
[32m[20221213 18:22:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.32
[32m[20221213 18:22:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.47
[32m[20221213 18:22:16 @agent_ppo2.py:143][0m Total time:      18.11 min
[32m[20221213 18:22:16 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 18:22:16 @agent_ppo2.py:121][0m #------------------------ Iteration 820 --------------------------#
[32m[20221213 18:22:17 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:22:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0031 |         256.1030 |           0.4925 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |           0.0014 |         258.9789 |           0.4919 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0080 |         246.5574 |           0.4915 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0037 |         246.1048 |           0.4910 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0064 |         242.7931 |           0.4911 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0090 |         241.1600 |           0.4914 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0127 |         240.3124 |           0.4911 |
[32m[20221213 18:22:17 @agent_ppo2.py:185][0m |          -0.0099 |         239.6216 |           0.4910 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |          -0.0123 |         239.1928 |           0.4907 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |          -0.0036 |         252.7962 |           0.4907 |
[32m[20221213 18:22:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.92
[32m[20221213 18:22:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.64
[32m[20221213 18:22:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 26.11
[32m[20221213 18:22:18 @agent_ppo2.py:143][0m Total time:      18.13 min
[32m[20221213 18:22:18 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221213 18:22:18 @agent_ppo2.py:121][0m #------------------------ Iteration 821 --------------------------#
[32m[20221213 18:22:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |          -0.0032 |         249.0054 |           0.5158 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |           0.0005 |         249.9689 |           0.5151 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |          -0.0061 |         244.9442 |           0.5147 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |           0.0035 |         259.6011 |           0.5142 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |          -0.0054 |         244.6364 |           0.5141 |
[32m[20221213 18:22:18 @agent_ppo2.py:185][0m |          -0.0083 |         243.1550 |           0.5143 |
[32m[20221213 18:22:19 @agent_ppo2.py:185][0m |          -0.0125 |         242.9820 |           0.5141 |
[32m[20221213 18:22:19 @agent_ppo2.py:185][0m |          -0.0095 |         242.4173 |           0.5134 |
[32m[20221213 18:22:19 @agent_ppo2.py:185][0m |          -0.0088 |         241.9908 |           0.5141 |
[32m[20221213 18:22:19 @agent_ppo2.py:185][0m |          -0.0111 |         242.3207 |           0.5140 |
[32m[20221213 18:22:19 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:22:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 915.54
[32m[20221213 18:22:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.11
[32m[20221213 18:22:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 962.31
[32m[20221213 18:22:19 @agent_ppo2.py:143][0m Total time:      18.15 min
[32m[20221213 18:22:19 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 18:22:19 @agent_ppo2.py:121][0m #------------------------ Iteration 822 --------------------------#
[32m[20221213 18:22:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:19 @agent_ppo2.py:185][0m |           0.0035 |         248.7792 |           0.5192 |
[32m[20221213 18:22:19 @agent_ppo2.py:185][0m |          -0.0059 |         236.7757 |           0.5173 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |          -0.0091 |         235.3105 |           0.5177 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |           0.0148 |         272.4174 |           0.5168 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |          -0.0091 |         233.6217 |           0.5164 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |          -0.0106 |         232.5637 |           0.5166 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |           0.0014 |         267.8523 |           0.5158 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |          -0.0114 |         231.7659 |           0.5148 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |          -0.0088 |         233.2689 |           0.5161 |
[32m[20221213 18:22:20 @agent_ppo2.py:185][0m |          -0.0120 |         230.4911 |           0.5156 |
[32m[20221213 18:22:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 995.12
[32m[20221213 18:22:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.77
[32m[20221213 18:22:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 922.30
[32m[20221213 18:22:20 @agent_ppo2.py:143][0m Total time:      18.17 min
[32m[20221213 18:22:20 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221213 18:22:20 @agent_ppo2.py:121][0m #------------------------ Iteration 823 --------------------------#
[32m[20221213 18:22:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0023 |         245.4488 |           0.5114 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0049 |         242.7009 |           0.5102 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |           0.0111 |         276.7053 |           0.5091 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0043 |         242.1831 |           0.5089 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0098 |         241.6283 |           0.5093 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |           0.0102 |         277.3178 |           0.5087 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0079 |         241.6165 |           0.5084 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0086 |         241.4013 |           0.5085 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0096 |         241.3577 |           0.5088 |
[32m[20221213 18:22:21 @agent_ppo2.py:185][0m |          -0.0092 |         241.0612 |           0.5088 |
[32m[20221213 18:22:21 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 969.69
[32m[20221213 18:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.32
[32m[20221213 18:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.05
[32m[20221213 18:22:22 @agent_ppo2.py:143][0m Total time:      18.20 min
[32m[20221213 18:22:22 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 18:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 824 --------------------------#
[32m[20221213 18:22:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |           0.0034 |         261.5377 |           0.5058 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |          -0.0080 |         243.9577 |           0.5045 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |          -0.0099 |         242.5186 |           0.5059 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |          -0.0087 |         241.7126 |           0.5055 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |          -0.0107 |         241.2039 |           0.5056 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |           0.0040 |         275.8349 |           0.5066 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |          -0.0130 |         239.5603 |           0.5050 |
[32m[20221213 18:22:22 @agent_ppo2.py:185][0m |          -0.0120 |         239.2524 |           0.5055 |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |          -0.0132 |         239.1614 |           0.5055 |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |          -0.0119 |         238.7780 |           0.5051 |
[32m[20221213 18:22:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 954.07
[32m[20221213 18:22:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.76
[32m[20221213 18:22:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.28
[32m[20221213 18:22:23 @agent_ppo2.py:143][0m Total time:      18.22 min
[32m[20221213 18:22:23 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221213 18:22:23 @agent_ppo2.py:121][0m #------------------------ Iteration 825 --------------------------#
[32m[20221213 18:22:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |          -0.0024 |         244.8729 |           0.5006 |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |          -0.0046 |         244.2063 |           0.5005 |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |           0.0036 |         250.9488 |           0.5003 |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |          -0.0063 |         243.5006 |           0.5009 |
[32m[20221213 18:22:23 @agent_ppo2.py:185][0m |          -0.0066 |         243.0982 |           0.5008 |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0053 |         243.4001 |           0.5008 |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0077 |         242.7419 |           0.5000 |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0071 |         242.5743 |           0.5004 |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0073 |         242.2620 |           0.5007 |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0082 |         242.0670 |           0.5002 |
[32m[20221213 18:22:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 958.53
[32m[20221213 18:22:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.25
[32m[20221213 18:22:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.10
[32m[20221213 18:22:24 @agent_ppo2.py:143][0m Total time:      18.24 min
[32m[20221213 18:22:24 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 18:22:24 @agent_ppo2.py:121][0m #------------------------ Iteration 826 --------------------------#
[32m[20221213 18:22:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0028 |         244.2483 |           0.5020 |
[32m[20221213 18:22:24 @agent_ppo2.py:185][0m |          -0.0063 |         242.8148 |           0.5012 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0089 |         242.0367 |           0.5009 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0084 |         241.4301 |           0.5009 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0096 |         241.1623 |           0.5010 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0099 |         240.7332 |           0.5011 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0057 |         245.5132 |           0.5011 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0113 |         240.3390 |           0.5010 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0108 |         240.0672 |           0.5018 |
[32m[20221213 18:22:25 @agent_ppo2.py:185][0m |          -0.0010 |         259.7875 |           0.5014 |
[32m[20221213 18:22:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 924.86
[32m[20221213 18:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.52
[32m[20221213 18:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.29
[32m[20221213 18:22:25 @agent_ppo2.py:143][0m Total time:      18.26 min
[32m[20221213 18:22:25 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221213 18:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 827 --------------------------#
[32m[20221213 18:22:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0022 |         245.0540 |           0.5121 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0039 |         241.7265 |           0.5119 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0077 |         240.2584 |           0.5123 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0004 |         247.0206 |           0.5123 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0084 |         239.2597 |           0.5124 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0098 |         238.9635 |           0.5122 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |           0.0012 |         252.5232 |           0.5123 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0086 |         238.6995 |           0.5123 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0106 |         238.0182 |           0.5122 |
[32m[20221213 18:22:26 @agent_ppo2.py:185][0m |          -0.0093 |         237.5428 |           0.5124 |
[32m[20221213 18:22:26 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.12
[32m[20221213 18:22:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.69
[32m[20221213 18:22:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 973.24
[32m[20221213 18:22:27 @agent_ppo2.py:143][0m Total time:      18.28 min
[32m[20221213 18:22:27 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 18:22:27 @agent_ppo2.py:121][0m #------------------------ Iteration 828 --------------------------#
[32m[20221213 18:22:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0027 |         245.5215 |           0.5074 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0062 |         244.3615 |           0.5060 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0089 |         243.8250 |           0.5059 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0022 |         249.7358 |           0.5064 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0093 |         243.3664 |           0.5057 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0097 |         242.8020 |           0.5059 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0108 |         242.7141 |           0.5055 |
[32m[20221213 18:22:27 @agent_ppo2.py:185][0m |          -0.0055 |         246.3723 |           0.5057 |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |          -0.0109 |         242.0576 |           0.5052 |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |          -0.0044 |         246.5921 |           0.5057 |
[32m[20221213 18:22:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 945.89
[32m[20221213 18:22:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.17
[32m[20221213 18:22:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.30
[32m[20221213 18:22:28 @agent_ppo2.py:143][0m Total time:      18.30 min
[32m[20221213 18:22:28 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221213 18:22:28 @agent_ppo2.py:121][0m #------------------------ Iteration 829 --------------------------#
[32m[20221213 18:22:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |           0.0169 |         277.4659 |           0.5275 |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |          -0.0034 |         243.0731 |           0.5260 |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |          -0.0036 |         242.1768 |           0.5266 |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |          -0.0070 |         241.0436 |           0.5258 |
[32m[20221213 18:22:28 @agent_ppo2.py:185][0m |           0.0002 |         251.8718 |           0.5257 |
[32m[20221213 18:22:29 @agent_ppo2.py:185][0m |          -0.0073 |         240.3033 |           0.5251 |
[32m[20221213 18:22:29 @agent_ppo2.py:185][0m |          -0.0093 |         239.7829 |           0.5254 |
[32m[20221213 18:22:29 @agent_ppo2.py:185][0m |          -0.0039 |         243.0797 |           0.5254 |
[32m[20221213 18:22:29 @agent_ppo2.py:185][0m |          -0.0069 |         238.9638 |           0.5246 |
[32m[20221213 18:22:29 @agent_ppo2.py:185][0m |          -0.0070 |         239.5105 |           0.5248 |
[32m[20221213 18:22:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 967.28
[32m[20221213 18:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.38
[32m[20221213 18:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.07
[32m[20221213 18:22:29 @agent_ppo2.py:143][0m Total time:      18.32 min
[32m[20221213 18:22:29 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 18:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 830 --------------------------#
[32m[20221213 18:22:29 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:29 @agent_ppo2.py:185][0m |           0.0082 |         255.2899 |           0.5185 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0076 |         235.4431 |           0.5176 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0065 |         228.9674 |           0.5167 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0096 |         226.1544 |           0.5165 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0112 |         223.5902 |           0.5161 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0112 |         221.7969 |           0.5161 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0102 |         220.1091 |           0.5154 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0130 |         218.6421 |           0.5156 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0129 |         217.1517 |           0.5146 |
[32m[20221213 18:22:30 @agent_ppo2.py:185][0m |          -0.0150 |         216.2771 |           0.5146 |
[32m[20221213 18:22:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 892.72
[32m[20221213 18:22:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.49
[32m[20221213 18:22:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.07
[32m[20221213 18:22:30 @agent_ppo2.py:143][0m Total time:      18.34 min
[32m[20221213 18:22:30 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221213 18:22:30 @agent_ppo2.py:121][0m #------------------------ Iteration 831 --------------------------#
[32m[20221213 18:22:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |           0.0157 |         288.4449 |           0.4934 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0086 |         251.8422 |           0.4918 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0082 |         253.4283 |           0.4910 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0111 |         248.8740 |           0.4915 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0101 |         248.7560 |           0.4908 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0078 |         248.8854 |           0.4909 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0131 |         247.2942 |           0.4910 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0125 |         246.6187 |           0.4911 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0108 |         246.8240 |           0.4909 |
[32m[20221213 18:22:31 @agent_ppo2.py:185][0m |          -0.0088 |         246.6091 |           0.4903 |
[32m[20221213 18:22:31 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.48
[32m[20221213 18:22:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.52
[32m[20221213 18:22:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 945.42
[32m[20221213 18:22:32 @agent_ppo2.py:143][0m Total time:      18.36 min
[32m[20221213 18:22:32 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 18:22:32 @agent_ppo2.py:121][0m #------------------------ Iteration 832 --------------------------#
[32m[20221213 18:22:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |          -0.0010 |         249.7505 |           0.5119 |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |          -0.0050 |         243.7868 |           0.5108 |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |           0.0005 |         248.5989 |           0.5102 |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |          -0.0060 |         239.3473 |           0.5097 |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |          -0.0090 |         238.1598 |           0.5096 |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |          -0.0090 |         237.2880 |           0.5088 |
[32m[20221213 18:22:32 @agent_ppo2.py:185][0m |          -0.0095 |         236.2886 |           0.5091 |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |          -0.0094 |         235.6535 |           0.5089 |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |          -0.0070 |         237.8666 |           0.5090 |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |          -0.0111 |         234.5350 |           0.5089 |
[32m[20221213 18:22:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 923.45
[32m[20221213 18:22:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 942.45
[32m[20221213 18:22:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.80
[32m[20221213 18:22:33 @agent_ppo2.py:143][0m Total time:      18.39 min
[32m[20221213 18:22:33 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221213 18:22:33 @agent_ppo2.py:121][0m #------------------------ Iteration 833 --------------------------#
[32m[20221213 18:22:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |           0.0172 |         280.4714 |           0.5028 |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |          -0.0064 |         245.5818 |           0.5024 |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |          -0.0029 |         246.7261 |           0.5017 |
[32m[20221213 18:22:33 @agent_ppo2.py:185][0m |          -0.0074 |         242.8218 |           0.5014 |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0050 |         243.8489 |           0.5012 |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0095 |         242.2716 |           0.5009 |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0088 |         241.7062 |           0.5008 |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0089 |         241.1811 |           0.5007 |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0106 |         241.0527 |           0.5004 |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0096 |         241.0817 |           0.5001 |
[32m[20221213 18:22:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 944.78
[32m[20221213 18:22:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 971.75
[32m[20221213 18:22:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.74
[32m[20221213 18:22:34 @agent_ppo2.py:143][0m Total time:      18.41 min
[32m[20221213 18:22:34 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 18:22:34 @agent_ppo2.py:121][0m #------------------------ Iteration 834 --------------------------#
[32m[20221213 18:22:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:34 @agent_ppo2.py:185][0m |          -0.0020 |         246.6415 |           0.5051 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0008 |         248.2354 |           0.5033 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0056 |         243.8156 |           0.5030 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0088 |         242.7507 |           0.5023 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0093 |         241.8149 |           0.5023 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0082 |         241.8055 |           0.5021 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0049 |         245.5767 |           0.5016 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0105 |         240.5680 |           0.5023 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0012 |         262.3127 |           0.5020 |
[32m[20221213 18:22:35 @agent_ppo2.py:185][0m |          -0.0037 |         247.7208 |           0.5012 |
[32m[20221213 18:22:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 979.20
[32m[20221213 18:22:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.76
[32m[20221213 18:22:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.28
[32m[20221213 18:22:35 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221213 18:22:35 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221213 18:22:35 @agent_ppo2.py:121][0m #------------------------ Iteration 835 --------------------------#
[32m[20221213 18:22:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0008 |         246.0522 |           0.4924 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0043 |         242.8593 |           0.4922 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0053 |         241.5178 |           0.4929 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0062 |         240.6843 |           0.4925 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0056 |         241.7166 |           0.4925 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0071 |         239.8437 |           0.4931 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0085 |         239.6642 |           0.4934 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0089 |         239.3548 |           0.4934 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |           0.0046 |         260.0866 |           0.4932 |
[32m[20221213 18:22:36 @agent_ppo2.py:185][0m |          -0.0081 |         239.2746 |           0.4947 |
[32m[20221213 18:22:36 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 984.84
[32m[20221213 18:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 993.09
[32m[20221213 18:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 906.77
[32m[20221213 18:22:37 @agent_ppo2.py:143][0m Total time:      18.45 min
[32m[20221213 18:22:37 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 18:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 836 --------------------------#
[32m[20221213 18:22:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:37 @agent_ppo2.py:185][0m |           0.0010 |         248.7625 |           0.4962 |
[32m[20221213 18:22:37 @agent_ppo2.py:185][0m |          -0.0059 |         244.2354 |           0.4959 |
[32m[20221213 18:22:37 @agent_ppo2.py:185][0m |          -0.0036 |         244.0338 |           0.4951 |
[32m[20221213 18:22:37 @agent_ppo2.py:185][0m |          -0.0082 |         242.6713 |           0.4945 |
[32m[20221213 18:22:37 @agent_ppo2.py:185][0m |          -0.0101 |         242.3287 |           0.4940 |
[32m[20221213 18:22:37 @agent_ppo2.py:185][0m |          -0.0099 |         241.9072 |           0.4935 |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |          -0.0110 |         241.2993 |           0.4932 |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |          -0.0111 |         240.9176 |           0.4931 |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |          -0.0115 |         240.5370 |           0.4926 |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |          -0.0021 |         254.3524 |           0.4925 |
[32m[20221213 18:22:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 955.11
[32m[20221213 18:22:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.95
[32m[20221213 18:22:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.53
[32m[20221213 18:22:38 @agent_ppo2.py:143][0m Total time:      18.47 min
[32m[20221213 18:22:38 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221213 18:22:38 @agent_ppo2.py:121][0m #------------------------ Iteration 837 --------------------------#
[32m[20221213 18:22:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |           0.0024 |         250.3039 |           0.4947 |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |          -0.0069 |         241.5765 |           0.4942 |
[32m[20221213 18:22:38 @agent_ppo2.py:185][0m |          -0.0061 |         239.2387 |           0.4937 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0092 |         236.3212 |           0.4930 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0017 |         260.3676 |           0.4929 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0105 |         233.4114 |           0.4928 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0126 |         232.3288 |           0.4927 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0125 |         231.0149 |           0.4928 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0135 |         230.4858 |           0.4926 |
[32m[20221213 18:22:39 @agent_ppo2.py:185][0m |          -0.0052 |         246.9724 |           0.4924 |
[32m[20221213 18:22:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.37
[32m[20221213 18:22:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.68
[32m[20221213 18:22:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 957.17
[32m[20221213 18:22:39 @agent_ppo2.py:143][0m Total time:      18.49 min
[32m[20221213 18:22:39 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 18:22:39 @agent_ppo2.py:121][0m #------------------------ Iteration 838 --------------------------#
[32m[20221213 18:22:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |           0.0001 |         248.8831 |           0.4968 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0028 |         246.5751 |           0.4964 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0054 |         245.1078 |           0.4959 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |           0.0011 |         258.6800 |           0.4957 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0056 |         243.0165 |           0.4962 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0072 |         242.5805 |           0.4959 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |           0.0007 |         258.2243 |           0.4956 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0061 |         242.4233 |           0.4950 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0090 |         241.2327 |           0.4953 |
[32m[20221213 18:22:40 @agent_ppo2.py:185][0m |          -0.0103 |         240.7372 |           0.4957 |
[32m[20221213 18:22:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 994.56
[32m[20221213 18:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.41
[32m[20221213 18:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 894.85
[32m[20221213 18:22:40 @agent_ppo2.py:143][0m Total time:      18.51 min
[32m[20221213 18:22:40 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221213 18:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 839 --------------------------#
[32m[20221213 18:22:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |           0.0048 |         252.3898 |           0.5080 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |           0.0082 |         263.5887 |           0.5070 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |          -0.0010 |         247.8873 |           0.5064 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |          -0.0033 |         247.1774 |           0.5058 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |          -0.0042 |         246.7188 |           0.5060 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |          -0.0050 |         246.3205 |           0.5054 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |           0.0134 |         281.4050 |           0.5058 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |          -0.0055 |         246.6844 |           0.5047 |
[32m[20221213 18:22:41 @agent_ppo2.py:185][0m |          -0.0067 |         245.3722 |           0.5049 |
[32m[20221213 18:22:42 @agent_ppo2.py:185][0m |          -0.0055 |         245.4078 |           0.5049 |
[32m[20221213 18:22:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 18:22:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 955.40
[32m[20221213 18:22:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 973.61
[32m[20221213 18:22:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 976.58
[32m[20221213 18:22:42 @agent_ppo2.py:143][0m Total time:      18.53 min
[32m[20221213 18:22:42 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 18:22:42 @agent_ppo2.py:121][0m #------------------------ Iteration 840 --------------------------#
[32m[20221213 18:22:42 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:22:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:42 @agent_ppo2.py:185][0m |          -0.0021 |         247.6075 |           0.4977 |
[32m[20221213 18:22:42 @agent_ppo2.py:185][0m |          -0.0015 |         247.5548 |           0.4969 |
[32m[20221213 18:22:42 @agent_ppo2.py:185][0m |          -0.0065 |         245.8929 |           0.4966 |
[32m[20221213 18:22:42 @agent_ppo2.py:185][0m |          -0.0048 |         245.5384 |           0.4966 |
[32m[20221213 18:22:42 @agent_ppo2.py:185][0m |          -0.0071 |         244.9980 |           0.4958 |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |           0.0042 |         254.6112 |           0.4958 |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |          -0.0077 |         244.3819 |           0.4958 |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |          -0.0080 |         244.1571 |           0.4948 |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |          -0.0087 |         244.0136 |           0.4950 |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |           0.0032 |         272.3182 |           0.4949 |
[32m[20221213 18:22:43 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 944.72
[32m[20221213 18:22:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.18
[32m[20221213 18:22:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.93
[32m[20221213 18:22:43 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221213 18:22:43 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221213 18:22:43 @agent_ppo2.py:121][0m #------------------------ Iteration 841 --------------------------#
[32m[20221213 18:22:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |          -0.0005 |         249.0205 |           0.4862 |
[32m[20221213 18:22:43 @agent_ppo2.py:185][0m |          -0.0038 |         245.8322 |           0.4858 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0096 |         244.4482 |           0.4851 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0090 |         243.1996 |           0.4850 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0090 |         242.7939 |           0.4850 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0108 |         242.2580 |           0.4852 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0109 |         241.6841 |           0.4854 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0118 |         241.2253 |           0.4854 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0106 |         241.2731 |           0.4855 |
[32m[20221213 18:22:44 @agent_ppo2.py:185][0m |          -0.0130 |         240.4796 |           0.4854 |
[32m[20221213 18:22:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:22:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.41
[32m[20221213 18:22:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.16
[32m[20221213 18:22:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 882.35
[32m[20221213 18:22:44 @agent_ppo2.py:143][0m Total time:      18.58 min
[32m[20221213 18:22:44 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 18:22:44 @agent_ppo2.py:121][0m #------------------------ Iteration 842 --------------------------#
[32m[20221213 18:22:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0025 |         247.0768 |           0.4955 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0059 |         245.8032 |           0.4946 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0072 |         245.1913 |           0.4953 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0068 |         244.8108 |           0.4949 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0075 |         244.8366 |           0.4952 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0082 |         244.4930 |           0.4953 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0083 |         244.2361 |           0.4951 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0083 |         244.1492 |           0.4949 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0015 |         249.6693 |           0.4953 |
[32m[20221213 18:22:45 @agent_ppo2.py:185][0m |          -0.0074 |         244.4111 |           0.4954 |
[32m[20221213 18:22:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 960.13
[32m[20221213 18:22:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.17
[32m[20221213 18:22:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.63
[32m[20221213 18:22:46 @agent_ppo2.py:143][0m Total time:      18.60 min
[32m[20221213 18:22:46 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221213 18:22:46 @agent_ppo2.py:121][0m #------------------------ Iteration 843 --------------------------#
[32m[20221213 18:22:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0016 |         250.3114 |           0.4927 |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0068 |         246.5284 |           0.4920 |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0056 |         245.6991 |           0.4926 |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0103 |         244.5551 |           0.4930 |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0092 |         244.6920 |           0.4932 |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0088 |         244.0298 |           0.4938 |
[32m[20221213 18:22:46 @agent_ppo2.py:185][0m |          -0.0115 |         243.7248 |           0.4937 |
[32m[20221213 18:22:47 @agent_ppo2.py:185][0m |          -0.0120 |         243.3494 |           0.4940 |
[32m[20221213 18:22:47 @agent_ppo2.py:185][0m |          -0.0131 |         242.9657 |           0.4946 |
[32m[20221213 18:22:47 @agent_ppo2.py:185][0m |          -0.0135 |         242.9743 |           0.4944 |
[32m[20221213 18:22:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 18:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 895.13
[32m[20221213 18:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 924.13
[32m[20221213 18:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 953.01
[32m[20221213 18:22:47 @agent_ppo2.py:143][0m Total time:      18.62 min
[32m[20221213 18:22:47 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 18:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 844 --------------------------#
[32m[20221213 18:22:47 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:22:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:47 @agent_ppo2.py:185][0m |           0.0064 |         257.7608 |           0.5075 |
[32m[20221213 18:22:47 @agent_ppo2.py:185][0m |           0.0011 |         252.1898 |           0.5074 |
[32m[20221213 18:22:47 @agent_ppo2.py:185][0m |          -0.0044 |         245.1192 |           0.5080 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0080 |         244.4919 |           0.5076 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0069 |         244.5007 |           0.5079 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0091 |         243.9967 |           0.5080 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0075 |         243.8766 |           0.5082 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0095 |         243.3543 |           0.5081 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0092 |         243.3405 |           0.5081 |
[32m[20221213 18:22:48 @agent_ppo2.py:185][0m |          -0.0076 |         243.8680 |           0.5085 |
[32m[20221213 18:22:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:22:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 942.15
[32m[20221213 18:22:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.54
[32m[20221213 18:22:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.05
[32m[20221213 18:22:48 @agent_ppo2.py:143][0m Total time:      18.64 min
[32m[20221213 18:22:48 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221213 18:22:48 @agent_ppo2.py:121][0m #------------------------ Iteration 845 --------------------------#
[32m[20221213 18:22:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |           0.0155 |         278.3225 |           0.5052 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0047 |         243.1273 |           0.5043 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0061 |         241.4388 |           0.5031 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0060 |         241.3796 |           0.5025 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0071 |         240.0941 |           0.5023 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0073 |         239.9003 |           0.5024 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0071 |         239.2720 |           0.5025 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0062 |         239.5903 |           0.5020 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0081 |         239.2033 |           0.5023 |
[32m[20221213 18:22:49 @agent_ppo2.py:185][0m |          -0.0084 |         238.4201 |           0.5016 |
[32m[20221213 18:22:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 910.50
[32m[20221213 18:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 941.00
[32m[20221213 18:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 916.87
[32m[20221213 18:22:49 @agent_ppo2.py:143][0m Total time:      18.66 min
[32m[20221213 18:22:49 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 18:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 846 --------------------------#
[32m[20221213 18:22:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |           0.0009 |         253.2885 |           0.5081 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0061 |         247.6742 |           0.5066 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0015 |         249.5815 |           0.5059 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0054 |         245.3523 |           0.5059 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0103 |         241.7094 |           0.5061 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0097 |         240.8670 |           0.5054 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0106 |         239.3740 |           0.5054 |
[32m[20221213 18:22:50 @agent_ppo2.py:185][0m |          -0.0115 |         238.0737 |           0.5050 |
[32m[20221213 18:22:51 @agent_ppo2.py:185][0m |          -0.0114 |         237.6309 |           0.5051 |
[32m[20221213 18:22:51 @agent_ppo2.py:185][0m |          -0.0073 |         240.0017 |           0.5058 |
[32m[20221213 18:22:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:22:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 983.93
[32m[20221213 18:22:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 996.96
[32m[20221213 18:22:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 971.36
[32m[20221213 18:22:51 @agent_ppo2.py:143][0m Total time:      18.68 min
[32m[20221213 18:22:51 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221213 18:22:51 @agent_ppo2.py:121][0m #------------------------ Iteration 847 --------------------------#
[32m[20221213 18:22:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:51 @agent_ppo2.py:185][0m |           0.0068 |         263.5214 |           0.5118 |
[32m[20221213 18:22:51 @agent_ppo2.py:185][0m |          -0.0032 |         250.5341 |           0.5114 |
[32m[20221213 18:22:51 @agent_ppo2.py:185][0m |          -0.0032 |         248.8362 |           0.5111 |
[32m[20221213 18:22:51 @agent_ppo2.py:185][0m |           0.0018 |         254.0935 |           0.5105 |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |          -0.0027 |         246.7458 |           0.5110 |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |          -0.0054 |         246.1754 |           0.5105 |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |           0.0044 |         258.1899 |           0.5102 |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |          -0.0053 |         245.2638 |           0.5098 |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |          -0.0065 |         245.1273 |           0.5094 |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |          -0.0039 |         246.3100 |           0.5091 |
[32m[20221213 18:22:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 922.20
[32m[20221213 18:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.71
[32m[20221213 18:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.21
[32m[20221213 18:22:52 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221213 18:22:52 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 18:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 848 --------------------------#
[32m[20221213 18:22:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:52 @agent_ppo2.py:185][0m |          -0.0016 |         252.8181 |           0.5095 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |           0.0101 |         287.0044 |           0.5094 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0020 |         251.9271 |           0.5084 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0044 |         250.0012 |           0.5089 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0064 |         249.0765 |           0.5086 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0086 |         249.1119 |           0.5084 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0076 |         248.7470 |           0.5080 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0093 |         248.7733 |           0.5080 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0100 |         248.1081 |           0.5081 |
[32m[20221213 18:22:53 @agent_ppo2.py:185][0m |          -0.0103 |         247.9689 |           0.5082 |
[32m[20221213 18:22:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 18:22:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.60
[32m[20221213 18:22:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.37
[32m[20221213 18:22:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.81
[32m[20221213 18:22:53 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221213 18:22:53 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221213 18:22:53 @agent_ppo2.py:121][0m #------------------------ Iteration 849 --------------------------#
[32m[20221213 18:22:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0040 |         247.4794 |           0.5044 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0088 |         245.1193 |           0.5038 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0097 |         243.4660 |           0.5038 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0098 |         242.2450 |           0.5035 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0116 |         240.9927 |           0.5035 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0122 |         240.0159 |           0.5033 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0117 |         239.1408 |           0.5031 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0076 |         245.9938 |           0.5023 |
[32m[20221213 18:22:54 @agent_ppo2.py:185][0m |          -0.0123 |         237.8401 |           0.5026 |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0116 |         236.6128 |           0.5017 |
[32m[20221213 18:22:55 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 992.79
[32m[20221213 18:22:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.80
[32m[20221213 18:22:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 972.73
[32m[20221213 18:22:55 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221213 18:22:55 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 18:22:55 @agent_ppo2.py:121][0m #------------------------ Iteration 850 --------------------------#
[32m[20221213 18:22:55 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0015 |         252.9001 |           0.5017 |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0077 |         249.7992 |           0.5019 |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0067 |         248.1240 |           0.5015 |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0088 |         246.7813 |           0.5012 |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0132 |         246.0801 |           0.5009 |
[32m[20221213 18:22:55 @agent_ppo2.py:185][0m |          -0.0120 |         245.0924 |           0.5008 |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0129 |         244.4098 |           0.5010 |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0109 |         243.7026 |           0.5002 |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0129 |         243.0422 |           0.5006 |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0107 |         243.0227 |           0.5001 |
[32m[20221213 18:22:56 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.84
[32m[20221213 18:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.41
[32m[20221213 18:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.46
[32m[20221213 18:22:56 @agent_ppo2.py:143][0m Total time:      18.77 min
[32m[20221213 18:22:56 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221213 18:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 851 --------------------------#
[32m[20221213 18:22:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0004 |         244.1657 |           0.5076 |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0079 |         229.7055 |           0.5075 |
[32m[20221213 18:22:56 @agent_ppo2.py:185][0m |          -0.0094 |         222.9138 |           0.5069 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |          -0.0025 |         223.4089 |           0.5072 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |          -0.0081 |         215.2170 |           0.5076 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |          -0.0049 |         217.7011 |           0.5079 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |          -0.0132 |         209.2489 |           0.5074 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |          -0.0121 |         206.5491 |           0.5076 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |           0.0023 |         241.7233 |           0.5081 |
[32m[20221213 18:22:57 @agent_ppo2.py:185][0m |          -0.0064 |         204.4982 |           0.5076 |
[32m[20221213 18:22:57 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:22:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 985.65
[32m[20221213 18:22:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.59
[32m[20221213 18:22:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.44
[32m[20221213 18:22:57 @agent_ppo2.py:143][0m Total time:      18.79 min
[32m[20221213 18:22:57 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 18:22:57 @agent_ppo2.py:121][0m #------------------------ Iteration 852 --------------------------#
[32m[20221213 18:22:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:22:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0004 |         265.6611 |           0.5042 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |           0.0075 |         279.7514 |           0.5034 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0095 |         246.9456 |           0.5032 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0114 |         243.6902 |           0.5028 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0004 |         262.4054 |           0.5038 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0075 |         242.9213 |           0.5032 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0118 |         238.5394 |           0.5036 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0095 |         236.9783 |           0.5033 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0097 |         238.2105 |           0.5035 |
[32m[20221213 18:22:58 @agent_ppo2.py:185][0m |          -0.0070 |         238.5557 |           0.5034 |
[32m[20221213 18:22:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:22:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 935.52
[32m[20221213 18:22:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.60
[32m[20221213 18:22:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 975.91
[32m[20221213 18:22:58 @agent_ppo2.py:143][0m Total time:      18.81 min
[32m[20221213 18:22:58 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221213 18:22:58 @agent_ppo2.py:121][0m #------------------------ Iteration 853 --------------------------#
[32m[20221213 18:22:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |           0.0046 |         260.4335 |           0.5171 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0024 |         252.8727 |           0.5168 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0033 |         250.5858 |           0.5164 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0073 |         248.7803 |           0.5165 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0056 |         249.4825 |           0.5173 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0056 |         248.9203 |           0.5169 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0073 |         246.8499 |           0.5178 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0079 |         247.8455 |           0.5173 |
[32m[20221213 18:22:59 @agent_ppo2.py:185][0m |          -0.0087 |         246.2078 |           0.5175 |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |          -0.0095 |         246.2401 |           0.5176 |
[32m[20221213 18:23:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 920.61
[32m[20221213 18:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.29
[32m[20221213 18:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.18
[32m[20221213 18:23:00 @agent_ppo2.py:143][0m Total time:      18.83 min
[32m[20221213 18:23:00 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 18:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 854 --------------------------#
[32m[20221213 18:23:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |          -0.0037 |         252.8356 |           0.5040 |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |          -0.0065 |         250.9850 |           0.5041 |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |           0.0046 |         285.9791 |           0.5040 |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |          -0.0043 |         251.8801 |           0.5033 |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |          -0.0061 |         253.2053 |           0.5039 |
[32m[20221213 18:23:00 @agent_ppo2.py:185][0m |          -0.0096 |         249.0168 |           0.5034 |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0111 |         248.8381 |           0.5037 |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0112 |         248.7947 |           0.5033 |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0107 |         248.4331 |           0.5032 |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0124 |         248.2607 |           0.5028 |
[32m[20221213 18:23:01 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 956.95
[32m[20221213 18:23:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 966.40
[32m[20221213 18:23:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 907.90
[32m[20221213 18:23:01 @agent_ppo2.py:143][0m Total time:      18.85 min
[32m[20221213 18:23:01 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221213 18:23:01 @agent_ppo2.py:121][0m #------------------------ Iteration 855 --------------------------#
[32m[20221213 18:23:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0027 |         261.0411 |           0.5136 |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0065 |         248.6418 |           0.5125 |
[32m[20221213 18:23:01 @agent_ppo2.py:185][0m |          -0.0082 |         243.8100 |           0.5118 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0079 |         241.8062 |           0.5118 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0073 |         241.1580 |           0.5125 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0092 |         239.3006 |           0.5117 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0097 |         237.9338 |           0.5118 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0096 |         237.0380 |           0.5119 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0087 |         237.4158 |           0.5114 |
[32m[20221213 18:23:02 @agent_ppo2.py:185][0m |          -0.0111 |         235.4080 |           0.5119 |
[32m[20221213 18:23:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 908.19
[32m[20221213 18:23:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.54
[32m[20221213 18:23:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.42
[32m[20221213 18:23:02 @agent_ppo2.py:143][0m Total time:      18.87 min
[32m[20221213 18:23:02 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 18:23:02 @agent_ppo2.py:121][0m #------------------------ Iteration 856 --------------------------#
[32m[20221213 18:23:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0005 |         252.7013 |           0.5078 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0053 |         250.0937 |           0.5071 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0008 |         251.1396 |           0.5072 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0045 |         251.2776 |           0.5075 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0093 |         248.8359 |           0.5071 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0068 |         248.7246 |           0.5071 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0101 |         247.7586 |           0.5071 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0084 |         247.5447 |           0.5072 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0096 |         247.3970 |           0.5076 |
[32m[20221213 18:23:03 @agent_ppo2.py:185][0m |          -0.0093 |         247.4290 |           0.5069 |
[32m[20221213 18:23:03 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.04
[32m[20221213 18:23:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.49
[32m[20221213 18:23:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 942.42
[32m[20221213 18:23:03 @agent_ppo2.py:143][0m Total time:      18.89 min
[32m[20221213 18:23:03 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221213 18:23:03 @agent_ppo2.py:121][0m #------------------------ Iteration 857 --------------------------#
[32m[20221213 18:23:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0003 |         246.2050 |           0.5109 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |           0.0110 |         272.5865 |           0.5111 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0034 |         244.4260 |           0.5105 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0058 |         243.3047 |           0.5104 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0073 |         243.1205 |           0.5104 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |           0.0044 |         263.1305 |           0.5101 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0083 |         242.8601 |           0.5099 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0081 |         242.9479 |           0.5097 |
[32m[20221213 18:23:04 @agent_ppo2.py:185][0m |          -0.0058 |         242.4523 |           0.5094 |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |          -0.0095 |         242.4330 |           0.5097 |
[32m[20221213 18:23:05 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 934.53
[32m[20221213 18:23:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 948.56
[32m[20221213 18:23:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.36
[32m[20221213 18:23:05 @agent_ppo2.py:143][0m Total time:      18.92 min
[32m[20221213 18:23:05 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 18:23:05 @agent_ppo2.py:121][0m #------------------------ Iteration 858 --------------------------#
[32m[20221213 18:23:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |           0.0099 |         271.6504 |           0.5117 |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |          -0.0045 |         247.3356 |           0.5111 |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |          -0.0079 |         246.1763 |           0.5105 |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |          -0.0079 |         245.6424 |           0.5104 |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |          -0.0110 |         245.6180 |           0.5100 |
[32m[20221213 18:23:05 @agent_ppo2.py:185][0m |          -0.0125 |         245.4015 |           0.5101 |
[32m[20221213 18:23:06 @agent_ppo2.py:185][0m |          -0.0114 |         244.9488 |           0.5097 |
[32m[20221213 18:23:06 @agent_ppo2.py:185][0m |          -0.0121 |         244.7960 |           0.5099 |
[32m[20221213 18:23:06 @agent_ppo2.py:185][0m |          -0.0122 |         244.5758 |           0.5095 |
[32m[20221213 18:23:06 @agent_ppo2.py:185][0m |          -0.0125 |         244.3928 |           0.5090 |
[32m[20221213 18:23:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 18:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 937.03
[32m[20221213 18:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.45
[32m[20221213 18:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.28
[32m[20221213 18:23:06 @agent_ppo2.py:143][0m Total time:      18.94 min
[32m[20221213 18:23:06 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221213 18:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 859 --------------------------#
[32m[20221213 18:23:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:06 @agent_ppo2.py:185][0m |          -0.0035 |         255.5934 |           0.5130 |
[32m[20221213 18:23:06 @agent_ppo2.py:185][0m |          -0.0038 |         249.7727 |           0.5124 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0085 |         246.6402 |           0.5124 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0082 |         245.2080 |           0.5126 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0080 |         244.4967 |           0.5128 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0104 |         243.4612 |           0.5135 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0008 |         252.4124 |           0.5132 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0104 |         241.7270 |           0.5133 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0116 |         241.6576 |           0.5137 |
[32m[20221213 18:23:07 @agent_ppo2.py:185][0m |          -0.0078 |         241.9965 |           0.5134 |
[32m[20221213 18:23:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:23:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 905.58
[32m[20221213 18:23:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 938.77
[32m[20221213 18:23:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.83
[32m[20221213 18:23:07 @agent_ppo2.py:143][0m Total time:      18.96 min
[32m[20221213 18:23:07 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 18:23:07 @agent_ppo2.py:121][0m #------------------------ Iteration 860 --------------------------#
[32m[20221213 18:23:08 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:23:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0021 |         245.9678 |           0.5007 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0048 |         245.1402 |           0.5001 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0033 |         245.2410 |           0.5011 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0077 |         244.4085 |           0.5005 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0077 |         243.9966 |           0.5020 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0082 |         244.0008 |           0.5015 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0086 |         244.0068 |           0.5015 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0077 |         244.3311 |           0.5018 |
[32m[20221213 18:23:08 @agent_ppo2.py:185][0m |          -0.0107 |         243.5085 |           0.5019 |
[32m[20221213 18:23:09 @agent_ppo2.py:185][0m |          -0.0087 |         243.3794 |           0.5025 |
[32m[20221213 18:23:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 18:23:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 976.07
[32m[20221213 18:23:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 977.46
[32m[20221213 18:23:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 965.29
[32m[20221213 18:23:09 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221213 18:23:09 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221213 18:23:09 @agent_ppo2.py:121][0m #------------------------ Iteration 861 --------------------------#
[32m[20221213 18:23:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:09 @agent_ppo2.py:185][0m |           0.0002 |         248.5780 |           0.5193 |
[32m[20221213 18:23:09 @agent_ppo2.py:185][0m |           0.0002 |         249.7404 |           0.5187 |
[32m[20221213 18:23:09 @agent_ppo2.py:185][0m |          -0.0047 |         245.6235 |           0.5189 |
[32m[20221213 18:23:09 @agent_ppo2.py:185][0m |          -0.0070 |         245.1774 |           0.5190 |
[32m[20221213 18:23:09 @agent_ppo2.py:185][0m |          -0.0093 |         245.0615 |           0.5186 |
[32m[20221213 18:23:10 @agent_ppo2.py:185][0m |           0.0009 |         260.6514 |           0.5188 |
[32m[20221213 18:23:10 @agent_ppo2.py:185][0m |          -0.0078 |         245.0245 |           0.5186 |
[32m[20221213 18:23:10 @agent_ppo2.py:185][0m |          -0.0082 |         244.7343 |           0.5185 |
[32m[20221213 18:23:10 @agent_ppo2.py:185][0m |          -0.0093 |         244.3150 |           0.5182 |
[32m[20221213 18:23:10 @agent_ppo2.py:185][0m |          -0.0003 |         254.7940 |           0.5189 |
[32m[20221213 18:23:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.44
[32m[20221213 18:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 968.94
[32m[20221213 18:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 893.03
[32m[20221213 18:23:10 @agent_ppo2.py:143][0m Total time:      19.01 min
[32m[20221213 18:23:10 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 18:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 862 --------------------------#
[32m[20221213 18:23:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:10 @agent_ppo2.py:185][0m |           0.0091 |         270.4643 |           0.5197 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |           0.0087 |         274.9790 |           0.5192 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0037 |         248.5055 |           0.5183 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0056 |         248.0657 |           0.5185 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0065 |         247.7359 |           0.5179 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0060 |         247.2488 |           0.5174 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0033 |         247.7392 |           0.5171 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0030 |         248.9773 |           0.5167 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0075 |         246.4106 |           0.5157 |
[32m[20221213 18:23:11 @agent_ppo2.py:185][0m |          -0.0086 |         246.6167 |           0.5158 |
[32m[20221213 18:23:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:23:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 949.40
[32m[20221213 18:23:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 976.16
[32m[20221213 18:23:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.26
[32m[20221213 18:23:11 @agent_ppo2.py:143][0m Total time:      19.03 min
[32m[20221213 18:23:11 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221213 18:23:11 @agent_ppo2.py:121][0m #------------------------ Iteration 863 --------------------------#
[32m[20221213 18:23:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0012 |         247.5833 |           0.5160 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0019 |         245.3145 |           0.5156 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |           0.0111 |         270.4455 |           0.5152 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0042 |         243.8349 |           0.5136 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0058 |         243.9447 |           0.5142 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0064 |         242.4772 |           0.5150 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0073 |         242.8715 |           0.5146 |
[32m[20221213 18:23:12 @agent_ppo2.py:185][0m |          -0.0079 |         241.8012 |           0.5148 |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0057 |         242.9014 |           0.5146 |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0085 |         241.8675 |           0.5147 |
[32m[20221213 18:23:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:23:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 950.30
[32m[20221213 18:23:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 974.53
[32m[20221213 18:23:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 948.66
[32m[20221213 18:23:13 @agent_ppo2.py:143][0m Total time:      19.05 min
[32m[20221213 18:23:13 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 18:23:13 @agent_ppo2.py:121][0m #------------------------ Iteration 864 --------------------------#
[32m[20221213 18:23:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0017 |         247.8050 |           0.5099 |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0053 |         244.6517 |           0.5083 |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0077 |         243.7344 |           0.5078 |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0060 |         243.2515 |           0.5075 |
[32m[20221213 18:23:13 @agent_ppo2.py:185][0m |          -0.0054 |         243.7031 |           0.5073 |
[32m[20221213 18:23:14 @agent_ppo2.py:185][0m |          -0.0036 |         243.7900 |           0.5068 |
[32m[20221213 18:23:14 @agent_ppo2.py:185][0m |          -0.0069 |         242.4852 |           0.5072 |
[32m[20221213 18:23:14 @agent_ppo2.py:185][0m |          -0.0095 |         242.2384 |           0.5069 |
[32m[20221213 18:23:14 @agent_ppo2.py:185][0m |          -0.0089 |         242.0275 |           0.5065 |
[32m[20221213 18:23:14 @agent_ppo2.py:185][0m |          -0.0106 |         241.9190 |           0.5064 |
[32m[20221213 18:23:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 18:23:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 984.85
[32m[20221213 18:23:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.95
[32m[20221213 18:23:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 915.48
[32m[20221213 18:23:14 @agent_ppo2.py:143][0m Total time:      19.07 min
[32m[20221213 18:23:14 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221213 18:23:14 @agent_ppo2.py:121][0m #------------------------ Iteration 865 --------------------------#
[32m[20221213 18:23:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0039 |         251.3381 |           0.5137 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0083 |         247.9304 |           0.5132 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0024 |         255.3368 |           0.5129 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0109 |         245.5819 |           0.5127 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0118 |         244.2402 |           0.5127 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0126 |         243.5967 |           0.5126 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0124 |         242.9150 |           0.5124 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0008 |         267.5218 |           0.5126 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0136 |         242.0214 |           0.5120 |
[32m[20221213 18:23:15 @agent_ppo2.py:185][0m |          -0.0150 |         241.3163 |           0.5124 |
[32m[20221213 18:23:15 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 901.94
[32m[20221213 18:23:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.04
[32m[20221213 18:23:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 950.16
[32m[20221213 18:23:15 @agent_ppo2.py:143][0m Total time:      19.09 min
[32m[20221213 18:23:15 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 18:23:15 @agent_ppo2.py:121][0m #------------------------ Iteration 866 --------------------------#
[32m[20221213 18:23:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |           0.0019 |         247.7714 |           0.5045 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0038 |         246.1284 |           0.5051 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0066 |         245.6946 |           0.5050 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0072 |         245.4911 |           0.5048 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0095 |         245.3695 |           0.5043 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0088 |         244.9972 |           0.5051 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0086 |         244.7336 |           0.5047 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0087 |         244.9440 |           0.5045 |
[32m[20221213 18:23:16 @agent_ppo2.py:185][0m |          -0.0111 |         244.4309 |           0.5044 |
[32m[20221213 18:23:17 @agent_ppo2.py:185][0m |          -0.0046 |         250.1642 |           0.5046 |
[32m[20221213 18:23:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 18:23:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 989.20
[32m[20221213 18:23:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.74
[32m[20221213 18:23:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 955.98
[32m[20221213 18:23:17 @agent_ppo2.py:143][0m Total time:      19.12 min
[32m[20221213 18:23:17 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221213 18:23:17 @agent_ppo2.py:121][0m #------------------------ Iteration 867 --------------------------#
[32m[20221213 18:23:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:17 @agent_ppo2.py:185][0m |          -0.0011 |         249.4030 |           0.5192 |
[32m[20221213 18:23:17 @agent_ppo2.py:185][0m |          -0.0037 |         246.0950 |           0.5177 |
[32m[20221213 18:23:17 @agent_ppo2.py:185][0m |          -0.0077 |         243.8228 |           0.5173 |
[32m[20221213 18:23:17 @agent_ppo2.py:185][0m |          -0.0095 |         242.8315 |           0.5174 |
[32m[20221213 18:23:17 @agent_ppo2.py:185][0m |          -0.0084 |         241.7057 |           0.5167 |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0110 |         240.5712 |           0.5173 |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0107 |         239.7637 |           0.5171 |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0116 |         239.3550 |           0.5175 |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0059 |         245.6595 |           0.5169 |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0129 |         238.5751 |           0.5160 |
[32m[20221213 18:23:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.75
[32m[20221213 18:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 947.62
[32m[20221213 18:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 944.99
[32m[20221213 18:23:18 @agent_ppo2.py:143][0m Total time:      19.14 min
[32m[20221213 18:23:18 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 18:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 868 --------------------------#
[32m[20221213 18:23:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0015 |         247.5597 |           0.5072 |
[32m[20221213 18:23:18 @agent_ppo2.py:185][0m |          -0.0033 |         244.2708 |           0.5073 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0053 |         243.2118 |           0.5067 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |           0.0026 |         262.8889 |           0.5064 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0062 |         242.6201 |           0.5058 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0077 |         242.4872 |           0.5065 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0002 |         260.4137 |           0.5070 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0061 |         242.6023 |           0.5069 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0055 |         243.2887 |           0.5073 |
[32m[20221213 18:23:19 @agent_ppo2.py:185][0m |          -0.0085 |         241.6616 |           0.5069 |
[32m[20221213 18:23:19 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 949.38
[32m[20221213 18:23:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.30
[32m[20221213 18:23:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 967.97
[32m[20221213 18:23:19 @agent_ppo2.py:143][0m Total time:      19.16 min
[32m[20221213 18:23:19 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221213 18:23:19 @agent_ppo2.py:121][0m #------------------------ Iteration 869 --------------------------#
[32m[20221213 18:23:20 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 18:23:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0029 |         243.4987 |           0.5153 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0068 |         241.6547 |           0.5143 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0080 |         239.8193 |           0.5143 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0095 |         238.2770 |           0.5144 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0090 |         237.2846 |           0.5141 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0091 |         236.9822 |           0.5145 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0002 |         245.4581 |           0.5147 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0102 |         235.5250 |           0.5137 |
[32m[20221213 18:23:20 @agent_ppo2.py:185][0m |          -0.0111 |         234.7614 |           0.5145 |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0128 |         234.7348 |           0.5145 |
[32m[20221213 18:23:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 974.47
[32m[20221213 18:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.17
[32m[20221213 18:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 959.49
[32m[20221213 18:23:21 @agent_ppo2.py:143][0m Total time:      19.18 min
[32m[20221213 18:23:21 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 18:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 870 --------------------------#
[32m[20221213 18:23:21 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:23:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0012 |         247.4300 |           0.5186 |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0061 |         245.5623 |           0.5181 |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0079 |         244.2947 |           0.5176 |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0090 |         243.4461 |           0.5176 |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0032 |         245.2719 |           0.5176 |
[32m[20221213 18:23:21 @agent_ppo2.py:185][0m |          -0.0027 |         250.3530 |           0.5178 |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |          -0.0096 |         241.2518 |           0.5170 |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |          -0.0117 |         240.8816 |           0.5173 |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |          -0.0079 |         240.5143 |           0.5175 |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |          -0.0113 |         239.8926 |           0.5173 |
[32m[20221213 18:23:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.32
[32m[20221213 18:23:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 954.55
[32m[20221213 18:23:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.76
[32m[20221213 18:23:22 @agent_ppo2.py:143][0m Total time:      19.20 min
[32m[20221213 18:23:22 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221213 18:23:22 @agent_ppo2.py:121][0m #------------------------ Iteration 871 --------------------------#
[32m[20221213 18:23:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |           0.0007 |         247.5095 |           0.5210 |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |          -0.0050 |         244.6072 |           0.5201 |
[32m[20221213 18:23:22 @agent_ppo2.py:185][0m |          -0.0041 |         243.5004 |           0.5202 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |          -0.0067 |         242.9466 |           0.5195 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |          -0.0081 |         242.5847 |           0.5200 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |          -0.0075 |         242.2635 |           0.5191 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |          -0.0082 |         242.1621 |           0.5195 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |          -0.0082 |         242.1089 |           0.5197 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |          -0.0062 |         241.8557 |           0.5191 |
[32m[20221213 18:23:23 @agent_ppo2.py:185][0m |           0.0002 |         248.4469 |           0.5192 |
[32m[20221213 18:23:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:23:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 943.85
[32m[20221213 18:23:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 979.41
[32m[20221213 18:23:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.49
[32m[20221213 18:23:23 @agent_ppo2.py:143][0m Total time:      19.22 min
[32m[20221213 18:23:23 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 18:23:23 @agent_ppo2.py:121][0m #------------------------ Iteration 872 --------------------------#
[32m[20221213 18:23:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |           0.0112 |         258.4715 |           0.5256 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0052 |         238.5881 |           0.5249 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0076 |         236.2123 |           0.5246 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0028 |         234.9801 |           0.5246 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0089 |         231.6727 |           0.5248 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0093 |         230.3441 |           0.5250 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0095 |         228.6105 |           0.5251 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0098 |         227.1279 |           0.5250 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0086 |         226.0122 |           0.5241 |
[32m[20221213 18:23:24 @agent_ppo2.py:185][0m |          -0.0026 |         239.4319 |           0.5245 |
[32m[20221213 18:23:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 955.27
[32m[20221213 18:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.25
[32m[20221213 18:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.00
[32m[20221213 18:23:25 @agent_ppo2.py:143][0m Total time:      19.25 min
[32m[20221213 18:23:25 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221213 18:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 873 --------------------------#
[32m[20221213 18:23:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |          -0.0014 |         246.5504 |           0.5249 |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |           0.0072 |         252.6266 |           0.5242 |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |          -0.0045 |         243.5973 |           0.5246 |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |          -0.0059 |         243.0511 |           0.5237 |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |          -0.0066 |         242.7076 |           0.5237 |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |          -0.0066 |         242.3424 |           0.5240 |
[32m[20221213 18:23:25 @agent_ppo2.py:185][0m |          -0.0071 |         242.1410 |           0.5240 |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0076 |         242.1024 |           0.5239 |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0106 |         241.7411 |           0.5235 |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0079 |         241.6372 |           0.5234 |
[32m[20221213 18:23:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 984.74
[32m[20221213 18:23:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.32
[32m[20221213 18:23:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 953.42
[32m[20221213 18:23:26 @agent_ppo2.py:143][0m Total time:      19.27 min
[32m[20221213 18:23:26 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 18:23:26 @agent_ppo2.py:121][0m #------------------------ Iteration 874 --------------------------#
[32m[20221213 18:23:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0025 |         249.3638 |           0.5309 |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0051 |         247.1998 |           0.5293 |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0080 |         245.7843 |           0.5292 |
[32m[20221213 18:23:26 @agent_ppo2.py:185][0m |          -0.0068 |         244.4315 |           0.5284 |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0080 |         243.3559 |           0.5280 |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0035 |         248.3252 |           0.5281 |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0013 |         246.9261 |           0.5269 |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0100 |         240.8094 |           0.5268 |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0088 |         240.3470 |           0.5268 |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0096 |         239.6972 |           0.5262 |
[32m[20221213 18:23:27 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 907.53
[32m[20221213 18:23:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.03
[32m[20221213 18:23:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 929.62
[32m[20221213 18:23:27 @agent_ppo2.py:143][0m Total time:      19.29 min
[32m[20221213 18:23:27 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221213 18:23:27 @agent_ppo2.py:121][0m #------------------------ Iteration 875 --------------------------#
[32m[20221213 18:23:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:27 @agent_ppo2.py:185][0m |          -0.0007 |         244.4030 |           0.5189 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0012 |         245.7824 |           0.5184 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0070 |         241.7497 |           0.5179 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0069 |         241.2019 |           0.5178 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |           0.0036 |         265.8471 |           0.5173 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0073 |         240.7187 |           0.5168 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0096 |         240.4073 |           0.5169 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0092 |         240.4472 |           0.5167 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0091 |         240.0930 |           0.5168 |
[32m[20221213 18:23:28 @agent_ppo2.py:185][0m |          -0.0102 |         239.9193 |           0.5168 |
[32m[20221213 18:23:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 997.83
[32m[20221213 18:23:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.88
[32m[20221213 18:23:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 969.68
[32m[20221213 18:23:28 @agent_ppo2.py:143][0m Total time:      19.31 min
[32m[20221213 18:23:28 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 18:23:28 @agent_ppo2.py:121][0m #------------------------ Iteration 876 --------------------------#
[32m[20221213 18:23:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0014 |         247.5007 |           0.5153 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0067 |         246.1601 |           0.5146 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |           0.0021 |         268.4277 |           0.5132 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0080 |         244.6495 |           0.5132 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0088 |         244.2328 |           0.5134 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0039 |         246.9344 |           0.5136 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0099 |         243.2733 |           0.5133 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0022 |         257.2686 |           0.5138 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0104 |         242.6053 |           0.5132 |
[32m[20221213 18:23:29 @agent_ppo2.py:185][0m |          -0.0120 |         242.3007 |           0.5138 |
[32m[20221213 18:23:29 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 952.95
[32m[20221213 18:23:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 975.17
[32m[20221213 18:23:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.79
[32m[20221213 18:23:30 @agent_ppo2.py:143][0m Total time:      19.33 min
[32m[20221213 18:23:30 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221213 18:23:30 @agent_ppo2.py:121][0m #------------------------ Iteration 877 --------------------------#
[32m[20221213 18:23:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0013 |         244.1022 |           0.5245 |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0044 |         242.9779 |           0.5247 |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0061 |         242.6793 |           0.5241 |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0068 |         242.2576 |           0.5248 |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0072 |         241.9813 |           0.5239 |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0060 |         242.5841 |           0.5249 |
[32m[20221213 18:23:30 @agent_ppo2.py:185][0m |          -0.0080 |         241.6456 |           0.5242 |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |          -0.0043 |         243.3986 |           0.5239 |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |          -0.0087 |         241.5018 |           0.5245 |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |          -0.0053 |         243.0525 |           0.5247 |
[32m[20221213 18:23:31 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 961.59
[32m[20221213 18:23:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.91
[32m[20221213 18:23:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 956.96
[32m[20221213 18:23:31 @agent_ppo2.py:143][0m Total time:      19.35 min
[32m[20221213 18:23:31 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 18:23:31 @agent_ppo2.py:121][0m #------------------------ Iteration 878 --------------------------#
[32m[20221213 18:23:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |          -0.0027 |         244.7712 |           0.5223 |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |          -0.0061 |         243.2139 |           0.5217 |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |          -0.0071 |         242.3793 |           0.5213 |
[32m[20221213 18:23:31 @agent_ppo2.py:185][0m |           0.0010 |         252.9030 |           0.5210 |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |           0.0007 |         264.2475 |           0.5213 |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |          -0.0104 |         241.6605 |           0.5211 |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |          -0.0102 |         240.8822 |           0.5206 |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |          -0.0119 |         240.8083 |           0.5204 |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |          -0.0063 |         243.0426 |           0.5196 |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |          -0.0083 |         241.2542 |           0.5200 |
[32m[20221213 18:23:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 919.40
[32m[20221213 18:23:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.76
[32m[20221213 18:23:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 933.69
[32m[20221213 18:23:32 @agent_ppo2.py:143][0m Total time:      19.37 min
[32m[20221213 18:23:32 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221213 18:23:32 @agent_ppo2.py:121][0m #------------------------ Iteration 879 --------------------------#
[32m[20221213 18:23:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:32 @agent_ppo2.py:185][0m |          -0.0038 |         247.9852 |           0.5201 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0060 |         244.4373 |           0.5187 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0076 |         243.0896 |           0.5181 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0042 |         254.2391 |           0.5178 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0110 |         240.9984 |           0.5176 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |           0.0016 |         259.6730 |           0.5171 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0115 |         240.3167 |           0.5166 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0019 |         248.1899 |           0.5167 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0110 |         239.4484 |           0.5163 |
[32m[20221213 18:23:33 @agent_ppo2.py:185][0m |          -0.0111 |         238.3434 |           0.5161 |
[32m[20221213 18:23:33 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.70
[32m[20221213 18:23:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.58
[32m[20221213 18:23:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 977.71
[32m[20221213 18:23:33 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221213 18:23:33 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 18:23:33 @agent_ppo2.py:121][0m #------------------------ Iteration 880 --------------------------#
[32m[20221213 18:23:34 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:23:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0009 |         244.5467 |           0.5229 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0024 |         244.1403 |           0.5224 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0056 |         242.8140 |           0.5219 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0072 |         242.6083 |           0.5211 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0069 |         242.3295 |           0.5211 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |           0.0008 |         260.5155 |           0.5209 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |           0.0013 |         247.9709 |           0.5207 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0078 |         241.4071 |           0.5199 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0080 |         241.8176 |           0.5203 |
[32m[20221213 18:23:34 @agent_ppo2.py:185][0m |          -0.0099 |         241.6235 |           0.5198 |
[32m[20221213 18:23:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.12
[32m[20221213 18:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 957.32
[32m[20221213 18:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.98
[32m[20221213 18:23:35 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221213 18:23:35 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221213 18:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 881 --------------------------#
[32m[20221213 18:23:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0014 |         248.8437 |           0.4976 |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0026 |         246.3567 |           0.4964 |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0069 |         244.1829 |           0.4974 |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0078 |         242.8107 |           0.4976 |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0074 |         242.4585 |           0.4974 |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0100 |         241.6332 |           0.4975 |
[32m[20221213 18:23:35 @agent_ppo2.py:185][0m |          -0.0084 |         241.6657 |           0.4968 |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |          -0.0047 |         241.6976 |           0.4972 |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |           0.0080 |         266.1270 |           0.4976 |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |          -0.0098 |         242.0800 |           0.4976 |
[32m[20221213 18:23:36 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 913.55
[32m[20221213 18:23:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 956.18
[32m[20221213 18:23:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.61
[32m[20221213 18:23:36 @agent_ppo2.py:143][0m Total time:      19.44 min
[32m[20221213 18:23:36 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 18:23:36 @agent_ppo2.py:121][0m #------------------------ Iteration 882 --------------------------#
[32m[20221213 18:23:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |           0.0007 |         249.5997 |           0.5254 |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |          -0.0042 |         246.9748 |           0.5246 |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |          -0.0056 |         245.8157 |           0.5243 |
[32m[20221213 18:23:36 @agent_ppo2.py:185][0m |          -0.0027 |         249.6498 |           0.5237 |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |          -0.0072 |         245.3952 |           0.5235 |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |          -0.0075 |         244.9021 |           0.5239 |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |          -0.0068 |         244.4704 |           0.5229 |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |           0.0047 |         280.7847 |           0.5231 |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |          -0.0098 |         244.1438 |           0.5224 |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |          -0.0085 |         244.5609 |           0.5233 |
[32m[20221213 18:23:37 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 941.47
[32m[20221213 18:23:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.81
[32m[20221213 18:23:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.84
[32m[20221213 18:23:37 @agent_ppo2.py:143][0m Total time:      19.46 min
[32m[20221213 18:23:37 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221213 18:23:37 @agent_ppo2.py:121][0m #------------------------ Iteration 883 --------------------------#
[32m[20221213 18:23:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:37 @agent_ppo2.py:185][0m |          -0.0014 |         245.0602 |           0.5195 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |           0.0096 |         261.3872 |           0.5186 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0013 |         242.8665 |           0.5180 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0073 |         241.1655 |           0.5179 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0081 |         241.0671 |           0.5178 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0088 |         240.8020 |           0.5173 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0040 |         243.2125 |           0.5177 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |           0.0005 |         254.2659 |           0.5178 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0088 |         239.8145 |           0.5176 |
[32m[20221213 18:23:38 @agent_ppo2.py:185][0m |          -0.0100 |         239.5762 |           0.5168 |
[32m[20221213 18:23:38 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.66
[32m[20221213 18:23:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 949.56
[32m[20221213 18:23:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.04
[32m[20221213 18:23:38 @agent_ppo2.py:143][0m Total time:      19.48 min
[32m[20221213 18:23:38 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 18:23:38 @agent_ppo2.py:121][0m #------------------------ Iteration 884 --------------------------#
[32m[20221213 18:23:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |           0.0016 |         246.7013 |           0.5244 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |           0.0086 |         270.2042 |           0.5242 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |           0.0038 |         262.1898 |           0.5229 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |          -0.0049 |         239.8889 |           0.5233 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |          -0.0058 |         238.4445 |           0.5227 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |          -0.0060 |         237.8741 |           0.5235 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |          -0.0065 |         237.9551 |           0.5241 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |          -0.0089 |         237.6604 |           0.5240 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |          -0.0094 |         237.2831 |           0.5241 |
[32m[20221213 18:23:39 @agent_ppo2.py:185][0m |           0.0012 |         256.9599 |           0.5243 |
[32m[20221213 18:23:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.93
[32m[20221213 18:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 972.78
[32m[20221213 18:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.88
[32m[20221213 18:23:40 @agent_ppo2.py:143][0m Total time:      19.50 min
[32m[20221213 18:23:40 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221213 18:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 885 --------------------------#
[32m[20221213 18:23:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:40 @agent_ppo2.py:185][0m |          -0.0026 |         245.3197 |           0.5177 |
[32m[20221213 18:23:40 @agent_ppo2.py:185][0m |          -0.0057 |         243.4093 |           0.5177 |
[32m[20221213 18:23:40 @agent_ppo2.py:185][0m |          -0.0064 |         242.4661 |           0.5171 |
[32m[20221213 18:23:40 @agent_ppo2.py:185][0m |          -0.0088 |         242.1718 |           0.5174 |
[32m[20221213 18:23:40 @agent_ppo2.py:185][0m |          -0.0098 |         241.6361 |           0.5172 |
[32m[20221213 18:23:40 @agent_ppo2.py:185][0m |          -0.0088 |         241.6718 |           0.5175 |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |          -0.0104 |         241.4744 |           0.5175 |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |          -0.0100 |         241.4638 |           0.5177 |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |          -0.0113 |         241.3752 |           0.5179 |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |          -0.0117 |         240.8182 |           0.5178 |
[32m[20221213 18:23:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 910.11
[32m[20221213 18:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 962.97
[32m[20221213 18:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.38
[32m[20221213 18:23:41 @agent_ppo2.py:143][0m Total time:      19.52 min
[32m[20221213 18:23:41 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 18:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 886 --------------------------#
[32m[20221213 18:23:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |           0.0003 |         245.1185 |           0.5130 |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |          -0.0051 |         242.0827 |           0.5133 |
[32m[20221213 18:23:41 @agent_ppo2.py:185][0m |          -0.0068 |         240.7301 |           0.5128 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0048 |         244.0122 |           0.5129 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0093 |         239.9386 |           0.5130 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0108 |         239.5338 |           0.5130 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0094 |         239.3474 |           0.5133 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0101 |         239.0946 |           0.5132 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0113 |         238.9379 |           0.5135 |
[32m[20221213 18:23:42 @agent_ppo2.py:185][0m |          -0.0101 |         239.4632 |           0.5140 |
[32m[20221213 18:23:42 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 882.02
[32m[20221213 18:23:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 921.54
[32m[20221213 18:23:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 946.02
[32m[20221213 18:23:42 @agent_ppo2.py:143][0m Total time:      19.54 min
[32m[20221213 18:23:42 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221213 18:23:42 @agent_ppo2.py:121][0m #------------------------ Iteration 887 --------------------------#
[32m[20221213 18:23:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |           0.0114 |         254.5414 |           0.5137 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0030 |         242.5686 |           0.5129 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0064 |         241.0810 |           0.5136 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0071 |         240.2459 |           0.5130 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0079 |         239.8572 |           0.5122 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0097 |         238.8496 |           0.5119 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0111 |         238.5951 |           0.5119 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0077 |         239.1053 |           0.5119 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0108 |         237.7515 |           0.5116 |
[32m[20221213 18:23:43 @agent_ppo2.py:185][0m |          -0.0111 |         237.2639 |           0.5114 |
[32m[20221213 18:23:43 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.81
[32m[20221213 18:23:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.90
[32m[20221213 18:23:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 939.23
[32m[20221213 18:23:43 @agent_ppo2.py:143][0m Total time:      19.56 min
[32m[20221213 18:23:43 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 18:23:43 @agent_ppo2.py:121][0m #------------------------ Iteration 888 --------------------------#
[32m[20221213 18:23:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |           0.0019 |         249.3106 |           0.5185 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0042 |         244.0033 |           0.5169 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0003 |         246.1717 |           0.5167 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0064 |         242.2490 |           0.5163 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0045 |         245.2195 |           0.5156 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0092 |         241.1383 |           0.5155 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0098 |         240.7556 |           0.5144 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0097 |         240.6329 |           0.5143 |
[32m[20221213 18:23:44 @agent_ppo2.py:185][0m |          -0.0112 |         240.2778 |           0.5140 |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |          -0.0114 |         240.2917 |           0.5133 |
[32m[20221213 18:23:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 940.42
[32m[20221213 18:23:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 951.27
[32m[20221213 18:23:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 954.13
[32m[20221213 18:23:45 @agent_ppo2.py:143][0m Total time:      19.58 min
[32m[20221213 18:23:45 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221213 18:23:45 @agent_ppo2.py:121][0m #------------------------ Iteration 889 --------------------------#
[32m[20221213 18:23:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |           0.0008 |         249.1615 |           0.5240 |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |          -0.0044 |         245.9172 |           0.5232 |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |          -0.0061 |         243.1142 |           0.5226 |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |          -0.0059 |         238.9258 |           0.5220 |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |          -0.0075 |         235.2080 |           0.5212 |
[32m[20221213 18:23:45 @agent_ppo2.py:185][0m |          -0.0072 |         234.0615 |           0.5211 |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |          -0.0082 |         232.9035 |           0.5209 |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |          -0.0053 |         235.9880 |           0.5206 |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |          -0.0080 |         232.8717 |           0.5204 |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |          -0.0104 |         231.4266 |           0.5196 |
[32m[20221213 18:23:46 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 891.55
[32m[20221213 18:23:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 918.17
[32m[20221213 18:23:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.51
[32m[20221213 18:23:46 @agent_ppo2.py:143][0m Total time:      19.60 min
[32m[20221213 18:23:46 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 18:23:46 @agent_ppo2.py:121][0m #------------------------ Iteration 890 --------------------------#
[32m[20221213 18:23:46 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:23:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |           0.0000 |         249.5943 |           0.4991 |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |           0.0038 |         270.4620 |           0.4979 |
[32m[20221213 18:23:46 @agent_ppo2.py:185][0m |          -0.0063 |         243.2097 |           0.4968 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0096 |         242.1000 |           0.4967 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0095 |         241.4293 |           0.4964 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0040 |         243.6720 |           0.4958 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0069 |         242.2183 |           0.4957 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0078 |         243.8727 |           0.4952 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0112 |         240.3866 |           0.4950 |
[32m[20221213 18:23:47 @agent_ppo2.py:185][0m |          -0.0104 |         240.1470 |           0.4947 |
[32m[20221213 18:23:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 18:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 887.47
[32m[20221213 18:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.86
[32m[20221213 18:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 951.77
[32m[20221213 18:23:47 @agent_ppo2.py:143][0m Total time:      19.62 min
[32m[20221213 18:23:47 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221213 18:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 891 --------------------------#
[32m[20221213 18:23:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0031 |         240.0957 |           0.5032 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0052 |         238.1653 |           0.5010 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0062 |         236.7280 |           0.5017 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0066 |         235.8283 |           0.5020 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0073 |         235.5556 |           0.5020 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0045 |         236.1732 |           0.5021 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0007 |         246.9875 |           0.5018 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0072 |         234.5038 |           0.5015 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0080 |         233.9567 |           0.5016 |
[32m[20221213 18:23:48 @agent_ppo2.py:185][0m |          -0.0085 |         234.1682 |           0.5019 |
[32m[20221213 18:23:48 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 953.03
[32m[20221213 18:23:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 978.06
[32m[20221213 18:23:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.42
[32m[20221213 18:23:48 @agent_ppo2.py:143][0m Total time:      19.65 min
[32m[20221213 18:23:48 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 18:23:48 @agent_ppo2.py:121][0m #------------------------ Iteration 892 --------------------------#
[32m[20221213 18:23:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0020 |         242.8779 |           0.5033 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0042 |         240.7214 |           0.5023 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0043 |         240.1275 |           0.5018 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0047 |         239.1755 |           0.5011 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0052 |         239.2589 |           0.5013 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0081 |         238.3679 |           0.5003 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0097 |         237.9426 |           0.5002 |
[32m[20221213 18:23:49 @agent_ppo2.py:185][0m |          -0.0100 |         237.5639 |           0.5005 |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |          -0.0089 |         237.2307 |           0.5002 |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |          -0.0095 |         237.0568 |           0.4998 |
[32m[20221213 18:23:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:23:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.48
[32m[20221213 18:23:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 943.56
[32m[20221213 18:23:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 961.84
[32m[20221213 18:23:50 @agent_ppo2.py:143][0m Total time:      19.67 min
[32m[20221213 18:23:50 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221213 18:23:50 @agent_ppo2.py:121][0m #------------------------ Iteration 893 --------------------------#
[32m[20221213 18:23:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |           0.0007 |         244.5887 |           0.5120 |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |          -0.0077 |         240.9328 |           0.5116 |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |          -0.0088 |         239.3553 |           0.5115 |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |          -0.0082 |         238.0542 |           0.5110 |
[32m[20221213 18:23:50 @agent_ppo2.py:185][0m |          -0.0103 |         236.6398 |           0.5116 |
[32m[20221213 18:23:51 @agent_ppo2.py:185][0m |          -0.0098 |         235.6727 |           0.5109 |
[32m[20221213 18:23:51 @agent_ppo2.py:185][0m |          -0.0079 |         236.0263 |           0.5109 |
[32m[20221213 18:23:51 @agent_ppo2.py:185][0m |          -0.0024 |         239.0498 |           0.5105 |
[32m[20221213 18:23:51 @agent_ppo2.py:185][0m |          -0.0125 |         233.6511 |           0.5106 |
[32m[20221213 18:23:51 @agent_ppo2.py:185][0m |          -0.0116 |         233.1338 |           0.5103 |
[32m[20221213 18:23:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 18:23:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 929.42
[32m[20221213 18:23:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 967.26
[32m[20221213 18:23:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 967.30
[32m[20221213 18:23:51 @agent_ppo2.py:143][0m Total time:      19.69 min
[32m[20221213 18:23:51 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 18:23:51 @agent_ppo2.py:121][0m #------------------------ Iteration 894 --------------------------#
[32m[20221213 18:23:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:51 @agent_ppo2.py:185][0m |          -0.0027 |         247.5785 |           0.5088 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0074 |         244.8135 |           0.5099 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0103 |         243.6059 |           0.5096 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0107 |         242.3821 |           0.5094 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0119 |         241.6090 |           0.5088 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0131 |         240.6935 |           0.5081 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0138 |         239.9025 |           0.5086 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0078 |         245.9534 |           0.5081 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0116 |         239.4576 |           0.5075 |
[32m[20221213 18:23:52 @agent_ppo2.py:185][0m |          -0.0152 |         238.1868 |           0.5077 |
[32m[20221213 18:23:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 921.69
[32m[20221213 18:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 945.29
[32m[20221213 18:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.38
[32m[20221213 18:23:52 @agent_ppo2.py:143][0m Total time:      19.71 min
[32m[20221213 18:23:52 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221213 18:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 895 --------------------------#
[32m[20221213 18:23:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:23:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |           0.0000 |         243.6169 |           0.4859 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0058 |         239.0560 |           0.4853 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0045 |         235.8029 |           0.4846 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0072 |         232.8644 |           0.4836 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0080 |         230.5100 |           0.4837 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0082 |         228.7165 |           0.4836 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0104 |         227.3306 |           0.4834 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0109 |         226.1983 |           0.4832 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |           0.0013 |         252.9653 |           0.4831 |
[32m[20221213 18:23:53 @agent_ppo2.py:185][0m |          -0.0107 |         224.1297 |           0.4821 |
[32m[20221213 18:23:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 927.42
[32m[20221213 18:23:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.79
[32m[20221213 18:23:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.23
[32m[20221213 18:23:54 @agent_ppo2.py:143][0m Total time:      19.73 min
[32m[20221213 18:23:54 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 18:23:54 @agent_ppo2.py:121][0m #------------------------ Iteration 896 --------------------------#
[32m[20221213 18:23:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |          -0.0036 |         247.9498 |           0.4989 |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |           0.0079 |         278.7766 |           0.4966 |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |          -0.0064 |         242.5695 |           0.4966 |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |          -0.0030 |         244.7506 |           0.4962 |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |           0.0064 |         272.4161 |           0.4956 |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |          -0.0092 |         239.9679 |           0.4945 |
[32m[20221213 18:23:54 @agent_ppo2.py:185][0m |          -0.0096 |         239.6539 |           0.4954 |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |          -0.0096 |         238.7303 |           0.4943 |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |          -0.0106 |         238.4798 |           0.4943 |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |          -0.0147 |         237.8946 |           0.4945 |
[32m[20221213 18:23:55 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 995.33
[32m[20221213 18:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.49
[32m[20221213 18:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.21
[32m[20221213 18:23:55 @agent_ppo2.py:143][0m Total time:      19.75 min
[32m[20221213 18:23:55 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221213 18:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 897 --------------------------#
[32m[20221213 18:23:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |          -0.0013 |         246.5763 |           0.4918 |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |          -0.0033 |         243.9320 |           0.4914 |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |          -0.0058 |         241.3594 |           0.4906 |
[32m[20221213 18:23:55 @agent_ppo2.py:185][0m |           0.0075 |         271.3288 |           0.4902 |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0061 |         237.1291 |           0.4894 |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0021 |         241.1980 |           0.4895 |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0091 |         234.2141 |           0.4894 |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0088 |         233.1995 |           0.4893 |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0038 |         235.5100 |           0.4893 |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0085 |         231.4528 |           0.4887 |
[32m[20221213 18:23:56 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:23:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 963.94
[32m[20221213 18:23:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 980.05
[32m[20221213 18:23:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 931.67
[32m[20221213 18:23:56 @agent_ppo2.py:143][0m Total time:      19.77 min
[32m[20221213 18:23:56 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 18:23:56 @agent_ppo2.py:121][0m #------------------------ Iteration 898 --------------------------#
[32m[20221213 18:23:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:56 @agent_ppo2.py:185][0m |          -0.0046 |         250.7734 |           0.5012 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0108 |         242.6863 |           0.4998 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0092 |         237.1150 |           0.4993 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0109 |         233.8760 |           0.4994 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0114 |         231.9090 |           0.4990 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0106 |         229.8129 |           0.4989 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0127 |         227.4892 |           0.4989 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0090 |         231.1146 |           0.4990 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0172 |         223.1441 |           0.4984 |
[32m[20221213 18:23:57 @agent_ppo2.py:185][0m |          -0.0137 |         221.7315 |           0.4985 |
[32m[20221213 18:23:57 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:23:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 977.16
[32m[20221213 18:23:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 997.44
[32m[20221213 18:23:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 978.52
[32m[20221213 18:23:57 @agent_ppo2.py:143][0m Total time:      19.79 min
[32m[20221213 18:23:57 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221213 18:23:57 @agent_ppo2.py:121][0m #------------------------ Iteration 899 --------------------------#
[32m[20221213 18:23:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:23:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |           0.0106 |         292.9897 |           0.5014 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0006 |         257.4205 |           0.5000 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0084 |         249.1508 |           0.4992 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0064 |         252.2224 |           0.4983 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0105 |         244.6692 |           0.4980 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0119 |         243.4993 |           0.4977 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0093 |         242.2209 |           0.4979 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0113 |         241.2898 |           0.4974 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0131 |         241.0719 |           0.4969 |
[32m[20221213 18:23:58 @agent_ppo2.py:185][0m |          -0.0147 |         240.2583 |           0.4968 |
[32m[20221213 18:23:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.11
[32m[20221213 18:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 960.70
[32m[20221213 18:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 960.38
[32m[20221213 18:23:59 @agent_ppo2.py:143][0m Total time:      19.81 min
[32m[20221213 18:23:59 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 18:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 900 --------------------------#
[32m[20221213 18:23:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:23:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0006 |         246.6659 |           0.4909 |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0049 |         244.0302 |           0.4894 |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0082 |         243.1133 |           0.4889 |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0103 |         243.0245 |           0.4889 |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0088 |         243.0954 |           0.4887 |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0089 |         243.3819 |           0.4883 |
[32m[20221213 18:23:59 @agent_ppo2.py:185][0m |          -0.0113 |         242.4486 |           0.4876 |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0125 |         242.3873 |           0.4879 |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0130 |         242.3709 |           0.4878 |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0100 |         243.1880 |           0.4876 |
[32m[20221213 18:24:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:24:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 997.18
[32m[20221213 18:24:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 998.24
[32m[20221213 18:24:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 979.16
[32m[20221213 18:24:00 @agent_ppo2.py:143][0m Total time:      19.84 min
[32m[20221213 18:24:00 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221213 18:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 901 --------------------------#
[32m[20221213 18:24:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0039 |         251.0269 |           0.4764 |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0079 |         244.9249 |           0.4753 |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0073 |         241.4314 |           0.4757 |
[32m[20221213 18:24:00 @agent_ppo2.py:185][0m |          -0.0072 |         239.3690 |           0.4751 |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0070 |         238.0855 |           0.4751 |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0094 |         235.5866 |           0.4750 |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0107 |         233.8258 |           0.4748 |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0130 |         233.0753 |           0.4744 |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0125 |         231.5953 |           0.4749 |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0127 |         231.1734 |           0.4745 |
[32m[20221213 18:24:01 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:24:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 899.94
[32m[20221213 18:24:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 963.60
[32m[20221213 18:24:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 918.44
[32m[20221213 18:24:01 @agent_ppo2.py:143][0m Total time:      19.86 min
[32m[20221213 18:24:01 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 18:24:01 @agent_ppo2.py:121][0m #------------------------ Iteration 902 --------------------------#
[32m[20221213 18:24:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:01 @agent_ppo2.py:185][0m |          -0.0022 |         254.4501 |           0.4831 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0051 |         248.7588 |           0.4828 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0063 |         246.9724 |           0.4824 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0086 |         245.9870 |           0.4822 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |           0.0029 |         262.7470 |           0.4821 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0088 |         244.4031 |           0.4822 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0034 |         252.4347 |           0.4817 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0091 |         243.2938 |           0.4818 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0113 |         243.1815 |           0.4815 |
[32m[20221213 18:24:02 @agent_ppo2.py:185][0m |          -0.0106 |         242.8481 |           0.4813 |
[32m[20221213 18:24:02 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 18:24:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 918.37
[32m[20221213 18:24:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 959.35
[32m[20221213 18:24:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 968.40
[32m[20221213 18:24:02 @agent_ppo2.py:143][0m Total time:      19.88 min
[32m[20221213 18:24:02 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221213 18:24:02 @agent_ppo2.py:121][0m #------------------------ Iteration 903 --------------------------#
[32m[20221213 18:24:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0029 |         245.4435 |           0.4843 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |           0.0002 |         252.1122 |           0.4827 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0094 |         237.7878 |           0.4812 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |           0.0061 |         264.4027 |           0.4806 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0109 |         234.9285 |           0.4809 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0033 |         247.1236 |           0.4802 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0134 |         232.1106 |           0.4800 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0141 |         231.4522 |           0.4799 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0105 |         232.7069 |           0.4790 |
[32m[20221213 18:24:03 @agent_ppo2.py:185][0m |          -0.0139 |         229.7469 |           0.4798 |
[32m[20221213 18:24:03 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:24:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.32
[32m[20221213 18:24:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 939.84
[32m[20221213 18:24:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 949.91
[32m[20221213 18:24:04 @agent_ppo2.py:143][0m Total time:      19.90 min
[32m[20221213 18:24:04 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 18:24:04 @agent_ppo2.py:121][0m #------------------------ Iteration 904 --------------------------#
[32m[20221213 18:24:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |          -0.0025 |         249.7263 |           0.4715 |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |          -0.0047 |         246.3640 |           0.4708 |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |          -0.0078 |         244.9884 |           0.4705 |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |          -0.0055 |         243.9182 |           0.4704 |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |          -0.0026 |         248.8424 |           0.4703 |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |           0.0003 |         254.1051 |           0.4702 |
[32m[20221213 18:24:04 @agent_ppo2.py:185][0m |          -0.0084 |         241.4557 |           0.4701 |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0108 |         240.5280 |           0.4703 |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0088 |         240.3896 |           0.4702 |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0009 |         250.1706 |           0.4705 |
[32m[20221213 18:24:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 18:24:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.75
[32m[20221213 18:24:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 970.00
[32m[20221213 18:24:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 926.86
[32m[20221213 18:24:05 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221213 18:24:05 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221213 18:24:05 @agent_ppo2.py:121][0m #------------------------ Iteration 905 --------------------------#
[32m[20221213 18:24:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:24:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0013 |         245.7394 |           0.4775 |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0082 |         242.6150 |           0.4778 |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0042 |         242.1985 |           0.4770 |
[32m[20221213 18:24:05 @agent_ppo2.py:185][0m |          -0.0087 |         240.8196 |           0.4781 |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0086 |         240.3618 |           0.4772 |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0095 |         239.7907 |           0.4774 |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0093 |         239.4460 |           0.4774 |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0110 |         239.3280 |           0.4778 |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0061 |         242.6706 |           0.4772 |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0091 |         238.9322 |           0.4769 |
[32m[20221213 18:24:06 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:24:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 910.35
[32m[20221213 18:24:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 958.43
[32m[20221213 18:24:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 931.36
[32m[20221213 18:24:06 @agent_ppo2.py:143][0m Total time:      19.94 min
[32m[20221213 18:24:06 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 18:24:06 @agent_ppo2.py:121][0m #------------------------ Iteration 906 --------------------------#
[32m[20221213 18:24:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:06 @agent_ppo2.py:185][0m |          -0.0020 |         247.4169 |           0.4764 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0049 |         245.0307 |           0.4759 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0096 |         240.3778 |           0.4757 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0074 |         240.4749 |           0.4753 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0049 |         251.3724 |           0.4754 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0117 |         235.7312 |           0.4751 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0031 |         259.0250 |           0.4749 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0129 |         233.2855 |           0.4736 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0146 |         232.3367 |           0.4744 |
[32m[20221213 18:24:07 @agent_ppo2.py:185][0m |          -0.0159 |         231.7621 |           0.4741 |
[32m[20221213 18:24:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:24:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 911.70
[32m[20221213 18:24:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 940.05
[32m[20221213 18:24:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 921.05
[32m[20221213 18:24:07 @agent_ppo2.py:143][0m Total time:      19.96 min
[32m[20221213 18:24:07 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221213 18:24:07 @agent_ppo2.py:121][0m #------------------------ Iteration 907 --------------------------#
[32m[20221213 18:24:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0007 |         249.1133 |           0.4789 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0045 |         245.1074 |           0.4778 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0087 |         241.9995 |           0.4777 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0111 |         240.7581 |           0.4779 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0101 |         239.3855 |           0.4779 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0132 |         238.5544 |           0.4779 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0141 |         237.9218 |           0.4780 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0143 |         237.2767 |           0.4783 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0130 |         236.5675 |           0.4782 |
[32m[20221213 18:24:08 @agent_ppo2.py:185][0m |          -0.0142 |         236.0632 |           0.4786 |
[32m[20221213 18:24:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 18:24:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 890.95
[32m[20221213 18:24:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 955.47
[32m[20221213 18:24:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 966.43
[32m[20221213 18:24:09 @agent_ppo2.py:143][0m Total time:      19.98 min
[32m[20221213 18:24:09 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 18:24:09 @agent_ppo2.py:121][0m #------------------------ Iteration 908 --------------------------#
[32m[20221213 18:24:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |           0.0014 |         250.0820 |           0.4840 |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |          -0.0082 |         245.7034 |           0.4832 |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |          -0.0078 |         242.6559 |           0.4829 |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |          -0.0101 |         239.7336 |           0.4830 |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |          -0.0121 |         237.4441 |           0.4828 |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |          -0.0118 |         236.2019 |           0.4830 |
[32m[20221213 18:24:09 @agent_ppo2.py:185][0m |          -0.0088 |         236.8381 |           0.4827 |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |          -0.0111 |         233.5780 |           0.4826 |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |          -0.0110 |         232.0008 |           0.4827 |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |           0.0131 |         297.7128 |           0.4824 |
[32m[20221213 18:24:10 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 18:24:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 904.72
[32m[20221213 18:24:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 961.23
[32m[20221213 18:24:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.20
[32m[20221213 18:24:10 @agent_ppo2.py:143][0m Total time:      20.00 min
[32m[20221213 18:24:10 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221213 18:24:10 @agent_ppo2.py:121][0m #------------------------ Iteration 909 --------------------------#
[32m[20221213 18:24:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 18:24:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |           0.0069 |         253.3113 |           0.4735 |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |          -0.0078 |         243.7466 |           0.4733 |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |          -0.0093 |         240.9478 |           0.4725 |
[32m[20221213 18:24:10 @agent_ppo2.py:185][0m |          -0.0090 |         240.1110 |           0.4730 |
[32m[20221213 18:24:11 @agent_ppo2.py:185][0m |          -0.0082 |         240.3129 |           0.4727 |
[32m[20221213 18:24:11 @agent_ppo2.py:185][0m |          -0.0120 |         238.0589 |           0.4731 |
[32m[20221213 18:24:11 @agent_ppo2.py:185][0m |          -0.0081 |         240.6382 |           0.4730 |
[32m[20221213 18:24:11 @agent_ppo2.py:185][0m |          -0.0131 |         237.0947 |           0.4732 |
[32m[20221213 18:24:11 @agent_ppo2.py:185][0m |          -0.0087 |         248.3871 |           0.4735 |
[32m[20221213 18:24:11 @agent_ppo2.py:185][0m |          -0.0101 |         237.3453 |           0.4728 |
[32m[20221213 18:24:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:24:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 879.66
[32m[20221213 18:24:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 918.29
[32m[20221213 18:24:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 952.43
[32m[20221213 18:24:11 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221213 18:24:11 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 18:24:11 @agent_ppo2.py:121][0m #------------------------ Iteration 910 --------------------------#
[32m[20221213 18:24:11 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 18:24:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0008 |         241.2171 |           0.4819 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0043 |         228.8766 |           0.4814 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0096 |         222.2231 |           0.4810 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0123 |         217.4141 |           0.4799 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0116 |         214.0763 |           0.4795 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0137 |         211.6878 |           0.4797 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0151 |         209.8069 |           0.4793 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0131 |         208.5791 |           0.4792 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0093 |         213.6399 |           0.4787 |
[32m[20221213 18:24:12 @agent_ppo2.py:185][0m |          -0.0143 |         206.2610 |           0.4782 |
[32m[20221213 18:24:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 18:24:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 925.10
[32m[20221213 18:24:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 946.73
[32m[20221213 18:24:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 999.48
[32m[20221213 18:24:12 @agent_ppo2.py:143][0m Total time:      20.05 min
[32m[20221213 18:24:12 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221213 18:24:12 @agent_ppo2.py:121][0m #------------------------ Iteration 911 --------------------------#
[32m[20221213 18:24:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 18:24:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |           0.0084 |         258.4269 |           0.4697 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0017 |         245.2033 |           0.4690 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0046 |         247.5422 |           0.4697 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0116 |         236.1024 |           0.4701 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0124 |         234.3990 |           0.4703 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0126 |         232.9785 |           0.4708 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0139 |         230.1739 |           0.4707 |
[32m[20221213 18:24:13 @agent_ppo2.py:185][0m |          -0.0154 |         227.9558 |           0.4710 |
[32m[20221213 18:24:14 @agent_ppo2.py:185][0m |          -0.0162 |         226.7109 |           0.4711 |
[32m[20221213 18:24:14 @agent_ppo2.py:185][0m |          -0.0176 |         225.5168 |           0.4711 |
[32m[20221213 18:24:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 18:24:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 938.40
[32m[20221213 18:24:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 965.39
[32m[20221213 18:24:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 947.98
[32m[20221213 18:24:14 @agent_ppo2.py:143][0m Total time:      20.07 min
[32m[20221213 18:24:14 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 18:24:14 @agent_ppo2.py:121][0m #------------------------ Iteration 912 --------------------------#
[32m[20221213 18:24:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 18:24:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 18:24:14 @agent_ppo2.py:185][0m |          -0.0020 |         246.9547 |           0.4716 |
