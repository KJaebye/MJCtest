[32m[20221209 01:38:02 @logger.py:105][0m Log file set to ./tmp/walker/stand/20221209_013802/log/walker_stand-20221209_013802.log
[32m[20221209 01:38:02 @agent_ppo2.py:115][0m #------------------------ Iteration 0 --------------------------#
[32m[20221209 01:38:03 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221209 01:38:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0037 |           0.1851 |           0.2305 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0163 |           0.0911 |           0.2297 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0189 |           0.0779 |           0.2301 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0216 |           0.0691 |           0.2297 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0223 |           0.0614 |           0.2293 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0231 |           0.0550 |           0.2290 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0255 |           0.0494 |           0.2285 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0259 |           0.0446 |           0.2282 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0267 |           0.0405 |           0.2280 |
[32m[20221209 01:38:03 @agent_ppo2.py:179][0m |          -0.0274 |           0.0369 |           0.2275 |
[32m[20221209 01:38:03 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221209 01:38:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.56
[32m[20221209 01:38:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 25.12
[32m[20221209 01:38:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.46
[32m[20221209 01:38:04 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 29.46
[32m[20221209 01:38:04 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 29.46
[32m[20221209 01:38:04 @agent_ppo2.py:137][0m Total time:       0.03 min
[32m[20221209 01:38:04 @agent_ppo2.py:139][0m 2048 total steps have happened
[32m[20221209 01:38:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1 --------------------------#
[32m[20221209 01:38:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221209 01:38:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221209 01:38:04 @agent_ppo2.py:179][0m |           0.0001 |           0.0589 |           0.2308 |
[32m[20221209 01:38:04 @agent_ppo2.py:179][0m |          -0.0112 |           0.0375 |           0.2309 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0138 |           0.0349 |           0.2316 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0158 |           0.0328 |           0.2321 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0171 |           0.0311 |           0.2323 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0199 |           0.0299 |           0.2324 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0195 |           0.0286 |           0.2327 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0218 |           0.0277 |           0.2330 |
[32m[20221209 01:38:05 @agent_ppo2.py:179][0m |          -0.0216 |           0.0266 |           0.2328 |
