[32m[20221208 14:36:34 @logger.py:105][0m Log file set to ./tmp/walker/stand/20221208_143634/log/walker_stand-20221208_143634.log
[32m[20221208 14:36:34 @agent_ppo2.py:115][0m #------------------------ Iteration 0 --------------------------#
[32m[20221208 14:36:35 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:36:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0037 |           0.1851 |           0.2305 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0163 |           0.0911 |           0.2297 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0189 |           0.0779 |           0.2301 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0216 |           0.0691 |           0.2297 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0223 |           0.0614 |           0.2293 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0231 |           0.0550 |           0.2290 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0255 |           0.0494 |           0.2285 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0259 |           0.0446 |           0.2282 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0267 |           0.0405 |           0.2280 |
[32m[20221208 14:36:35 @agent_ppo2.py:179][0m |          -0.0274 |           0.0369 |           0.2275 |
[32m[20221208 14:36:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:36:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.56
[32m[20221208 14:36:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 25.12
[32m[20221208 14:36:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.46
[32m[20221208 14:36:36 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 29.46
[32m[20221208 14:36:36 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 29.46
[32m[20221208 14:36:36 @agent_ppo2.py:137][0m Total time:       0.03 min
[32m[20221208 14:36:36 @agent_ppo2.py:139][0m 2048 total steps have happened
[32m[20221208 14:36:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1 --------------------------#
[32m[20221208 14:36:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:36:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:36 @agent_ppo2.py:179][0m |           0.0001 |           0.0589 |           0.2308 |
[32m[20221208 14:36:36 @agent_ppo2.py:179][0m |          -0.0112 |           0.0375 |           0.2309 |
[32m[20221208 14:36:36 @agent_ppo2.py:179][0m |          -0.0138 |           0.0349 |           0.2316 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0158 |           0.0328 |           0.2321 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0171 |           0.0311 |           0.2323 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0199 |           0.0299 |           0.2324 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0195 |           0.0286 |           0.2327 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0218 |           0.0277 |           0.2330 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0216 |           0.0266 |           0.2328 |
[32m[20221208 14:36:37 @agent_ppo2.py:179][0m |          -0.0241 |           0.0256 |           0.2335 |
[32m[20221208 14:36:37 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:36:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.86
[32m[20221208 14:36:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.72
[32m[20221208 14:36:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.22
[32m[20221208 14:36:37 @agent_ppo2.py:137][0m Total time:       0.05 min
[32m[20221208 14:36:37 @agent_ppo2.py:139][0m 4096 total steps have happened
[32m[20221208 14:36:37 @agent_ppo2.py:115][0m #------------------------ Iteration 2 --------------------------#
[32m[20221208 14:36:38 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:36:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |           0.0019 |           0.0577 |           0.2327 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0119 |           0.0375 |           0.2317 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0163 |           0.0363 |           0.2311 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0154 |           0.0356 |           0.2306 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0176 |           0.0350 |           0.2301 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0196 |           0.0347 |           0.2302 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0204 |           0.0343 |           0.2300 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0217 |           0.0335 |           0.2302 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0213 |           0.0332 |           0.2297 |
[32m[20221208 14:36:38 @agent_ppo2.py:179][0m |          -0.0240 |           0.0330 |           0.2293 |
[32m[20221208 14:36:38 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.31
[32m[20221208 14:36:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 27.96
[32m[20221208 14:36:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.77
[32m[20221208 14:36:39 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 31.77
[32m[20221208 14:36:39 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 31.77
[32m[20221208 14:36:39 @agent_ppo2.py:137][0m Total time:       0.08 min
[32m[20221208 14:36:39 @agent_ppo2.py:139][0m 6144 total steps have happened
[32m[20221208 14:36:39 @agent_ppo2.py:115][0m #------------------------ Iteration 3 --------------------------#
[32m[20221208 14:36:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:36:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:39 @agent_ppo2.py:179][0m |           0.0041 |           0.0524 |           0.2298 |
[32m[20221208 14:36:39 @agent_ppo2.py:179][0m |          -0.0051 |           0.0402 |           0.2294 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0095 |           0.0394 |           0.2283 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0136 |           0.0389 |           0.2294 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0147 |           0.0393 |           0.2294 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0166 |           0.0389 |           0.2293 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0180 |           0.0381 |           0.2294 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0195 |           0.0379 |           0.2293 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0207 |           0.0374 |           0.2296 |
[32m[20221208 14:36:40 @agent_ppo2.py:179][0m |          -0.0216 |           0.0370 |           0.2302 |
[32m[20221208 14:36:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.16
[32m[20221208 14:36:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.62
[32m[20221208 14:36:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.44
[32m[20221208 14:36:40 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 35.44
[32m[20221208 14:36:40 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 35.44
[32m[20221208 14:36:40 @agent_ppo2.py:137][0m Total time:       0.10 min
[32m[20221208 14:36:40 @agent_ppo2.py:139][0m 8192 total steps have happened
[32m[20221208 14:36:40 @agent_ppo2.py:115][0m #------------------------ Iteration 4 --------------------------#
[32m[20221208 14:36:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:36:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |           0.0015 |           0.0763 |           0.2369 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0110 |           0.0610 |           0.2359 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0141 |           0.0610 |           0.2356 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0167 |           0.0607 |           0.2359 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0192 |           0.0592 |           0.2358 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0190 |           0.0585 |           0.2355 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0218 |           0.0579 |           0.2352 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0212 |           0.0588 |           0.2352 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0223 |           0.0575 |           0.2354 |
[32m[20221208 14:36:41 @agent_ppo2.py:179][0m |          -0.0239 |           0.0571 |           0.2349 |
[32m[20221208 14:36:41 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 28.01
[32m[20221208 14:36:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.21
[32m[20221208 14:36:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.70
[32m[20221208 14:36:42 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 43.70
[32m[20221208 14:36:42 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 43.70
[32m[20221208 14:36:42 @agent_ppo2.py:137][0m Total time:       0.13 min
[32m[20221208 14:36:42 @agent_ppo2.py:139][0m 10240 total steps have happened
[32m[20221208 14:36:42 @agent_ppo2.py:115][0m #------------------------ Iteration 5 --------------------------#
[32m[20221208 14:36:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:42 @agent_ppo2.py:179][0m |           0.0006 |           0.0973 |           0.2381 |
[32m[20221208 14:36:42 @agent_ppo2.py:179][0m |          -0.0109 |           0.0840 |           0.2377 |
[32m[20221208 14:36:42 @agent_ppo2.py:179][0m |          -0.0155 |           0.0832 |           0.2375 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0176 |           0.0819 |           0.2376 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0191 |           0.0815 |           0.2370 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0220 |           0.0809 |           0.2367 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0237 |           0.0809 |           0.2368 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0243 |           0.0817 |           0.2372 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0255 |           0.0803 |           0.2366 |
[32m[20221208 14:36:43 @agent_ppo2.py:179][0m |          -0.0264 |           0.0798 |           0.2368 |
[32m[20221208 14:36:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:36:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 27.91
[32m[20221208 14:36:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 29.62
[32m[20221208 14:36:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.93
[32m[20221208 14:36:43 @agent_ppo2.py:137][0m Total time:       0.15 min
[32m[20221208 14:36:43 @agent_ppo2.py:139][0m 12288 total steps have happened
[32m[20221208 14:36:43 @agent_ppo2.py:115][0m #------------------------ Iteration 6 --------------------------#
[32m[20221208 14:36:44 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:36:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |           0.0022 |           0.0858 |           0.2406 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0105 |           0.0775 |           0.2409 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0140 |           0.0764 |           0.2417 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0166 |           0.0757 |           0.2421 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0170 |           0.0765 |           0.2427 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0174 |           0.0758 |           0.2429 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0206 |           0.0753 |           0.2435 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0216 |           0.0756 |           0.2440 |
[32m[20221208 14:36:44 @agent_ppo2.py:179][0m |          -0.0215 |           0.0762 |           0.2434 |
[32m[20221208 14:36:45 @agent_ppo2.py:179][0m |          -0.0233 |           0.0752 |           0.2439 |
[32m[20221208 14:36:45 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:36:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 28.47
[32m[20221208 14:36:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.90
[32m[20221208 14:36:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 24.48
[32m[20221208 14:36:45 @agent_ppo2.py:137][0m Total time:       0.18 min
[32m[20221208 14:36:45 @agent_ppo2.py:139][0m 14336 total steps have happened
[32m[20221208 14:36:45 @agent_ppo2.py:115][0m #------------------------ Iteration 7 --------------------------#
[32m[20221208 14:36:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:36:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:45 @agent_ppo2.py:179][0m |           0.0005 |           0.1328 |           0.2422 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0109 |           0.1170 |           0.2413 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0169 |           0.1150 |           0.2412 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0174 |           0.1138 |           0.2419 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0209 |           0.1125 |           0.2409 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0207 |           0.1110 |           0.2419 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0229 |           0.1098 |           0.2424 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0261 |           0.1094 |           0.2425 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0254 |           0.1080 |           0.2422 |
[32m[20221208 14:36:46 @agent_ppo2.py:179][0m |          -0.0261 |           0.1078 |           0.2429 |
[32m[20221208 14:36:46 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:36:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.51
[32m[20221208 14:36:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 35.79
[32m[20221208 14:36:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.04
[32m[20221208 14:36:46 @agent_ppo2.py:137][0m Total time:       0.20 min
[32m[20221208 14:36:46 @agent_ppo2.py:139][0m 16384 total steps have happened
[32m[20221208 14:36:46 @agent_ppo2.py:115][0m #------------------------ Iteration 8 --------------------------#
[32m[20221208 14:36:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |           0.0006 |           0.1360 |           0.2479 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0135 |           0.1234 |           0.2491 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0164 |           0.1207 |           0.2493 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0189 |           0.1208 |           0.2497 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0211 |           0.1196 |           0.2506 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0226 |           0.1194 |           0.2501 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0222 |           0.1193 |           0.2512 |
[32m[20221208 14:36:47 @agent_ppo2.py:179][0m |          -0.0245 |           0.1207 |           0.2511 |
[32m[20221208 14:36:48 @agent_ppo2.py:179][0m |          -0.0259 |           0.1177 |           0.2516 |
[32m[20221208 14:36:48 @agent_ppo2.py:179][0m |          -0.0250 |           0.1178 |           0.2519 |
[32m[20221208 14:36:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:36:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.12
[32m[20221208 14:36:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 35.69
[32m[20221208 14:36:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.85
[32m[20221208 14:36:48 @agent_ppo2.py:137][0m Total time:       0.23 min
[32m[20221208 14:36:48 @agent_ppo2.py:139][0m 18432 total steps have happened
[32m[20221208 14:36:48 @agent_ppo2.py:115][0m #------------------------ Iteration 9 --------------------------#
[32m[20221208 14:36:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0011 |           0.1540 |           0.2658 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0130 |           0.1404 |           0.2646 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0178 |           0.1374 |           0.2649 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0213 |           0.1358 |           0.2651 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0232 |           0.1355 |           0.2657 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0233 |           0.1344 |           0.2661 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0260 |           0.1345 |           0.2655 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0263 |           0.1312 |           0.2662 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0283 |           0.1296 |           0.2664 |
[32m[20221208 14:36:49 @agent_ppo2.py:179][0m |          -0.0271 |           0.1307 |           0.2666 |
[32m[20221208 14:36:49 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:36:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.44
[32m[20221208 14:36:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 37.13
[32m[20221208 14:36:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.95
[32m[20221208 14:36:50 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 50.95
[32m[20221208 14:36:50 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 50.95
[32m[20221208 14:36:50 @agent_ppo2.py:137][0m Total time:       0.25 min
[32m[20221208 14:36:50 @agent_ppo2.py:139][0m 20480 total steps have happened
[32m[20221208 14:36:50 @agent_ppo2.py:115][0m #------------------------ Iteration 10 --------------------------#
[32m[20221208 14:36:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0029 |           0.2379 |           0.2626 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0137 |           0.2106 |           0.2625 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0178 |           0.2091 |           0.2623 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0202 |           0.2056 |           0.2628 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0216 |           0.2005 |           0.2627 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0244 |           0.1992 |           0.2628 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0263 |           0.1967 |           0.2627 |
[32m[20221208 14:36:50 @agent_ppo2.py:179][0m |          -0.0257 |           0.1983 |           0.2634 |
[32m[20221208 14:36:51 @agent_ppo2.py:179][0m |          -0.0282 |           0.1942 |           0.2633 |
[32m[20221208 14:36:51 @agent_ppo2.py:179][0m |          -0.0300 |           0.1954 |           0.2632 |
[32m[20221208 14:36:51 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.21
[32m[20221208 14:36:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.40
[32m[20221208 14:36:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.69
[32m[20221208 14:36:51 @agent_ppo2.py:137][0m Total time:       0.28 min
[32m[20221208 14:36:51 @agent_ppo2.py:139][0m 22528 total steps have happened
[32m[20221208 14:36:51 @agent_ppo2.py:115][0m #------------------------ Iteration 11 --------------------------#
[32m[20221208 14:36:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0014 |           0.2017 |           0.2714 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0144 |           0.1776 |           0.2691 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0172 |           0.1730 |           0.2695 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0201 |           0.1720 |           0.2690 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0209 |           0.1702 |           0.2687 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0224 |           0.1698 |           0.2685 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0233 |           0.1663 |           0.2684 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0254 |           0.1669 |           0.2681 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0258 |           0.1656 |           0.2683 |
[32m[20221208 14:36:52 @agent_ppo2.py:179][0m |          -0.0240 |           0.1665 |           0.2691 |
[32m[20221208 14:36:52 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:36:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.28
[32m[20221208 14:36:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.90
[32m[20221208 14:36:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.71
[32m[20221208 14:36:53 @agent_ppo2.py:137][0m Total time:       0.30 min
[32m[20221208 14:36:53 @agent_ppo2.py:139][0m 24576 total steps have happened
[32m[20221208 14:36:53 @agent_ppo2.py:115][0m #------------------------ Iteration 12 --------------------------#
[32m[20221208 14:36:53 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:36:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |           0.0011 |           0.2535 |           0.2734 |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |          -0.0097 |           0.2442 |           0.2728 |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |          -0.0151 |           0.2406 |           0.2729 |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |          -0.0183 |           0.2410 |           0.2733 |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |          -0.0211 |           0.2407 |           0.2731 |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |          -0.0223 |           0.2397 |           0.2726 |
[32m[20221208 14:36:53 @agent_ppo2.py:179][0m |          -0.0239 |           0.2404 |           0.2723 |
[32m[20221208 14:36:54 @agent_ppo2.py:179][0m |          -0.0244 |           0.2391 |           0.2726 |
[32m[20221208 14:36:54 @agent_ppo2.py:179][0m |          -0.0235 |           0.2397 |           0.2723 |
[32m[20221208 14:36:54 @agent_ppo2.py:179][0m |          -0.0233 |           0.2395 |           0.2728 |
[32m[20221208 14:36:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.43
[32m[20221208 14:36:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.81
[32m[20221208 14:36:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.33
[32m[20221208 14:36:54 @agent_ppo2.py:137][0m Total time:       0.33 min
[32m[20221208 14:36:54 @agent_ppo2.py:139][0m 26624 total steps have happened
[32m[20221208 14:36:54 @agent_ppo2.py:115][0m #------------------------ Iteration 13 --------------------------#
[32m[20221208 14:36:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0005 |           0.3611 |           0.2749 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0093 |           0.3354 |           0.2749 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0146 |           0.3303 |           0.2745 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0154 |           0.3281 |           0.2745 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0182 |           0.3230 |           0.2749 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0183 |           0.3212 |           0.2745 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0212 |           0.3163 |           0.2751 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0230 |           0.3144 |           0.2747 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0240 |           0.3095 |           0.2751 |
[32m[20221208 14:36:55 @agent_ppo2.py:179][0m |          -0.0241 |           0.3084 |           0.2742 |
[32m[20221208 14:36:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.17
[32m[20221208 14:36:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.44
[32m[20221208 14:36:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.83
[32m[20221208 14:36:56 @agent_ppo2.py:137][0m Total time:       0.35 min
[32m[20221208 14:36:56 @agent_ppo2.py:139][0m 28672 total steps have happened
[32m[20221208 14:36:56 @agent_ppo2.py:115][0m #------------------------ Iteration 14 --------------------------#
[32m[20221208 14:36:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:36:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |           0.0045 |           0.3908 |           0.2684 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0116 |           0.3610 |           0.2686 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0140 |           0.3534 |           0.2690 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0180 |           0.3507 |           0.2691 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0203 |           0.3488 |           0.2692 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0237 |           0.3465 |           0.2690 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0255 |           0.3417 |           0.2692 |
[32m[20221208 14:36:56 @agent_ppo2.py:179][0m |          -0.0247 |           0.3440 |           0.2703 |
[32m[20221208 14:36:57 @agent_ppo2.py:179][0m |          -0.0279 |           0.3389 |           0.2704 |
[32m[20221208 14:36:57 @agent_ppo2.py:179][0m |          -0.0294 |           0.3369 |           0.2706 |
[32m[20221208 14:36:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:36:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.53
[32m[20221208 14:36:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.78
[32m[20221208 14:36:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.68
[32m[20221208 14:36:57 @agent_ppo2.py:137][0m Total time:       0.38 min
[32m[20221208 14:36:57 @agent_ppo2.py:139][0m 30720 total steps have happened
[32m[20221208 14:36:57 @agent_ppo2.py:115][0m #------------------------ Iteration 15 --------------------------#
[32m[20221208 14:36:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:36:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |           0.0052 |           0.3736 |           0.2737 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0042 |           0.3537 |           0.2748 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0103 |           0.3516 |           0.2747 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0120 |           0.3474 |           0.2748 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0136 |           0.3491 |           0.2746 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0144 |           0.3440 |           0.2752 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0173 |           0.3454 |           0.2757 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0160 |           0.3441 |           0.2754 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0202 |           0.3431 |           0.2753 |
[32m[20221208 14:36:58 @agent_ppo2.py:179][0m |          -0.0202 |           0.3443 |           0.2762 |
[32m[20221208 14:36:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:36:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.53
[32m[20221208 14:36:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.25
[32m[20221208 14:36:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.03
[32m[20221208 14:36:59 @agent_ppo2.py:137][0m Total time:       0.41 min
[32m[20221208 14:36:59 @agent_ppo2.py:139][0m 32768 total steps have happened
[32m[20221208 14:36:59 @agent_ppo2.py:115][0m #------------------------ Iteration 16 --------------------------#
[32m[20221208 14:36:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:36:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0004 |           0.4129 |           0.2847 |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0122 |           0.3929 |           0.2837 |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0155 |           0.3932 |           0.2843 |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0172 |           0.3898 |           0.2842 |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0198 |           0.3898 |           0.2854 |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0204 |           0.3852 |           0.2850 |
[32m[20221208 14:36:59 @agent_ppo2.py:179][0m |          -0.0213 |           0.3863 |           0.2857 |
[32m[20221208 14:37:00 @agent_ppo2.py:179][0m |          -0.0240 |           0.3841 |           0.2860 |
[32m[20221208 14:37:00 @agent_ppo2.py:179][0m |          -0.0233 |           0.3850 |           0.2865 |
[32m[20221208 14:37:00 @agent_ppo2.py:179][0m |          -0.0241 |           0.3825 |           0.2856 |
[32m[20221208 14:37:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:37:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.28
[32m[20221208 14:37:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.64
[32m[20221208 14:37:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.84
[32m[20221208 14:37:00 @agent_ppo2.py:137][0m Total time:       0.43 min
[32m[20221208 14:37:00 @agent_ppo2.py:139][0m 34816 total steps have happened
[32m[20221208 14:37:00 @agent_ppo2.py:115][0m #------------------------ Iteration 17 --------------------------#
[32m[20221208 14:37:01 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |           0.0032 |           0.4086 |           0.2886 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0118 |           0.3688 |           0.2879 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0152 |           0.3604 |           0.2884 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0194 |           0.3576 |           0.2887 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0189 |           0.3576 |           0.2870 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0223 |           0.3517 |           0.2894 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0243 |           0.3525 |           0.2889 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0268 |           0.3508 |           0.2893 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0262 |           0.3485 |           0.2904 |
[32m[20221208 14:37:01 @agent_ppo2.py:179][0m |          -0.0275 |           0.3537 |           0.2905 |
[32m[20221208 14:37:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.35
[32m[20221208 14:37:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.36
[32m[20221208 14:37:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.58
[32m[20221208 14:37:02 @agent_ppo2.py:137][0m Total time:       0.46 min
[32m[20221208 14:37:02 @agent_ppo2.py:139][0m 36864 total steps have happened
[32m[20221208 14:37:02 @agent_ppo2.py:115][0m #------------------------ Iteration 18 --------------------------#
[32m[20221208 14:37:02 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:02 @agent_ppo2.py:179][0m |          -0.0035 |           0.3962 |           0.3114 |
[32m[20221208 14:37:02 @agent_ppo2.py:179][0m |          -0.0132 |           0.3761 |           0.3101 |
[32m[20221208 14:37:02 @agent_ppo2.py:179][0m |          -0.0186 |           0.3735 |           0.3092 |
[32m[20221208 14:37:02 @agent_ppo2.py:179][0m |          -0.0187 |           0.3703 |           0.3086 |
[32m[20221208 14:37:02 @agent_ppo2.py:179][0m |          -0.0196 |           0.3710 |           0.3092 |
[32m[20221208 14:37:02 @agent_ppo2.py:179][0m |          -0.0217 |           0.3708 |           0.3093 |
[32m[20221208 14:37:03 @agent_ppo2.py:179][0m |          -0.0214 |           0.3758 |           0.3092 |
[32m[20221208 14:37:03 @agent_ppo2.py:179][0m |          -0.0232 |           0.3743 |           0.3100 |
[32m[20221208 14:37:03 @agent_ppo2.py:179][0m |          -0.0247 |           0.3778 |           0.3102 |
[32m[20221208 14:37:03 @agent_ppo2.py:179][0m |          -0.0252 |           0.3705 |           0.3104 |
[32m[20221208 14:37:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:37:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.85
[32m[20221208 14:37:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.67
[32m[20221208 14:37:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.22
[32m[20221208 14:37:03 @agent_ppo2.py:137][0m Total time:       0.48 min
[32m[20221208 14:37:03 @agent_ppo2.py:139][0m 38912 total steps have happened
[32m[20221208 14:37:03 @agent_ppo2.py:115][0m #------------------------ Iteration 19 --------------------------#
[32m[20221208 14:37:04 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |           0.0092 |           0.3902 |           0.3085 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0054 |           0.3856 |           0.3081 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0060 |           0.3802 |           0.3090 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0114 |           0.3785 |           0.3085 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0122 |           0.3792 |           0.3094 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0150 |           0.3802 |           0.3092 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0143 |           0.3774 |           0.3090 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0146 |           0.3762 |           0.3088 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0150 |           0.3783 |           0.3094 |
[32m[20221208 14:37:04 @agent_ppo2.py:179][0m |          -0.0163 |           0.3764 |           0.3081 |
[32m[20221208 14:37:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:37:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.53
[32m[20221208 14:37:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.22
[32m[20221208 14:37:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.82
[32m[20221208 14:37:05 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 53.82
[32m[20221208 14:37:05 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 53.82
[32m[20221208 14:37:05 @agent_ppo2.py:137][0m Total time:       0.51 min
[32m[20221208 14:37:05 @agent_ppo2.py:139][0m 40960 total steps have happened
[32m[20221208 14:37:05 @agent_ppo2.py:115][0m #------------------------ Iteration 20 --------------------------#
[32m[20221208 14:37:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:05 @agent_ppo2.py:179][0m |           0.0018 |           0.4097 |           0.2995 |
[32m[20221208 14:37:05 @agent_ppo2.py:179][0m |          -0.0066 |           0.4070 |           0.2981 |
[32m[20221208 14:37:05 @agent_ppo2.py:179][0m |          -0.0121 |           0.4034 |           0.2992 |
[32m[20221208 14:37:05 @agent_ppo2.py:179][0m |          -0.0093 |           0.3996 |           0.2982 |
[32m[20221208 14:37:05 @agent_ppo2.py:179][0m |          -0.0126 |           0.3987 |           0.2978 |
[32m[20221208 14:37:06 @agent_ppo2.py:179][0m |          -0.0144 |           0.4012 |           0.2978 |
[32m[20221208 14:37:06 @agent_ppo2.py:179][0m |          -0.0140 |           0.4024 |           0.2967 |
[32m[20221208 14:37:06 @agent_ppo2.py:179][0m |          -0.0150 |           0.3991 |           0.2969 |
[32m[20221208 14:37:06 @agent_ppo2.py:179][0m |          -0.0165 |           0.3961 |           0.2974 |
[32m[20221208 14:37:06 @agent_ppo2.py:179][0m |          -0.0175 |           0.3968 |           0.2969 |
[32m[20221208 14:37:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.96
[32m[20221208 14:37:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.65
[32m[20221208 14:37:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.05
[32m[20221208 14:37:06 @agent_ppo2.py:137][0m Total time:       0.53 min
[32m[20221208 14:37:06 @agent_ppo2.py:139][0m 43008 total steps have happened
[32m[20221208 14:37:06 @agent_ppo2.py:115][0m #------------------------ Iteration 21 --------------------------#
[32m[20221208 14:37:07 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |           0.0028 |           0.4211 |           0.3111 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0061 |           0.4154 |           0.3093 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0081 |           0.4158 |           0.3098 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0103 |           0.4160 |           0.3092 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0121 |           0.4173 |           0.3091 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0124 |           0.4128 |           0.3091 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0144 |           0.4119 |           0.3095 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0151 |           0.4145 |           0.3088 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0176 |           0.4127 |           0.3087 |
[32m[20221208 14:37:07 @agent_ppo2.py:179][0m |          -0.0149 |           0.4163 |           0.3082 |
[32m[20221208 14:37:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:37:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.86
[32m[20221208 14:37:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.42
[32m[20221208 14:37:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.88
[32m[20221208 14:37:08 @agent_ppo2.py:137][0m Total time:       0.56 min
[32m[20221208 14:37:08 @agent_ppo2.py:139][0m 45056 total steps have happened
[32m[20221208 14:37:08 @agent_ppo2.py:115][0m #------------------------ Iteration 22 --------------------------#
[32m[20221208 14:37:08 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:37:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:08 @agent_ppo2.py:179][0m |           0.0007 |           0.4192 |           0.2953 |
[32m[20221208 14:37:08 @agent_ppo2.py:179][0m |          -0.0067 |           0.4204 |           0.2961 |
[32m[20221208 14:37:08 @agent_ppo2.py:179][0m |          -0.0081 |           0.4170 |           0.2944 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0097 |           0.4153 |           0.2936 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0104 |           0.4129 |           0.2930 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0106 |           0.4145 |           0.2932 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0130 |           0.4109 |           0.2922 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0126 |           0.4146 |           0.2916 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0144 |           0.4140 |           0.2923 |
[32m[20221208 14:37:09 @agent_ppo2.py:179][0m |          -0.0158 |           0.4137 |           0.2919 |
[32m[20221208 14:37:09 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:37:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.30
[32m[20221208 14:37:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.13
[32m[20221208 14:37:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.42
[32m[20221208 14:37:09 @agent_ppo2.py:137][0m Total time:       0.58 min
[32m[20221208 14:37:09 @agent_ppo2.py:139][0m 47104 total steps have happened
[32m[20221208 14:37:09 @agent_ppo2.py:115][0m #------------------------ Iteration 23 --------------------------#
[32m[20221208 14:37:10 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |           0.0027 |           0.4680 |           0.2941 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0055 |           0.4551 |           0.2918 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0079 |           0.4535 |           0.2909 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0096 |           0.4507 |           0.2913 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0120 |           0.4518 |           0.2914 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0111 |           0.4498 |           0.2912 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0117 |           0.4500 |           0.2920 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0136 |           0.4473 |           0.2911 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0163 |           0.4440 |           0.2925 |
[32m[20221208 14:37:10 @agent_ppo2.py:179][0m |          -0.0164 |           0.4481 |           0.2917 |
[32m[20221208 14:37:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.09
[32m[20221208 14:37:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.49
[32m[20221208 14:37:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.08
[32m[20221208 14:37:11 @agent_ppo2.py:137][0m Total time:       0.61 min
[32m[20221208 14:37:11 @agent_ppo2.py:139][0m 49152 total steps have happened
[32m[20221208 14:37:11 @agent_ppo2.py:115][0m #------------------------ Iteration 24 --------------------------#
[32m[20221208 14:37:11 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:11 @agent_ppo2.py:179][0m |           0.0038 |           0.4628 |           0.2882 |
[32m[20221208 14:37:11 @agent_ppo2.py:179][0m |          -0.0068 |           0.4616 |           0.2895 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0103 |           0.4581 |           0.2897 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0093 |           0.4532 |           0.2902 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0113 |           0.4557 |           0.2891 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0101 |           0.4549 |           0.2896 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0134 |           0.4517 |           0.2899 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0134 |           0.4586 |           0.2904 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0133 |           0.4531 |           0.2895 |
[32m[20221208 14:37:12 @agent_ppo2.py:179][0m |          -0.0157 |           0.4613 |           0.2894 |
[32m[20221208 14:37:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.41
[32m[20221208 14:37:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.79
[32m[20221208 14:37:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.55
[32m[20221208 14:37:12 @agent_ppo2.py:137][0m Total time:       0.63 min
[32m[20221208 14:37:12 @agent_ppo2.py:139][0m 51200 total steps have happened
[32m[20221208 14:37:12 @agent_ppo2.py:115][0m #------------------------ Iteration 25 --------------------------#
[32m[20221208 14:37:13 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |           0.0041 |           0.5202 |           0.3036 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0052 |           0.4871 |           0.3041 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0118 |           0.4814 |           0.3054 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0149 |           0.4760 |           0.3057 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0174 |           0.4885 |           0.3063 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0182 |           0.4790 |           0.3065 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0219 |           0.4769 |           0.3066 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0213 |           0.4721 |           0.3064 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0230 |           0.4719 |           0.3074 |
[32m[20221208 14:37:13 @agent_ppo2.py:179][0m |          -0.0247 |           0.4710 |           0.3081 |
[32m[20221208 14:37:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:37:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.12
[32m[20221208 14:37:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.46
[32m[20221208 14:37:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.05
[32m[20221208 14:37:14 @agent_ppo2.py:137][0m Total time:       0.66 min
[32m[20221208 14:37:14 @agent_ppo2.py:139][0m 53248 total steps have happened
[32m[20221208 14:37:14 @agent_ppo2.py:115][0m #------------------------ Iteration 26 --------------------------#
[32m[20221208 14:37:14 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:14 @agent_ppo2.py:179][0m |           0.0044 |           0.5448 |           0.3023 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0073 |           0.5335 |           0.3020 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0101 |           0.5260 |           0.3027 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0126 |           0.5206 |           0.3027 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0155 |           0.5187 |           0.3024 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0177 |           0.5184 |           0.3028 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0185 |           0.5180 |           0.3025 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0176 |           0.5143 |           0.3023 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0204 |           0.5142 |           0.3026 |
[32m[20221208 14:37:15 @agent_ppo2.py:179][0m |          -0.0201 |           0.5157 |           0.3022 |
[32m[20221208 14:37:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.74
[32m[20221208 14:37:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.18
[32m[20221208 14:37:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.10
[32m[20221208 14:37:15 @agent_ppo2.py:137][0m Total time:       0.69 min
[32m[20221208 14:37:15 @agent_ppo2.py:139][0m 55296 total steps have happened
[32m[20221208 14:37:15 @agent_ppo2.py:115][0m #------------------------ Iteration 27 --------------------------#
[32m[20221208 14:37:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |           0.0078 |           0.5256 |           0.3114 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0090 |           0.5132 |           0.3101 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0152 |           0.5111 |           0.3095 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0172 |           0.5067 |           0.3095 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0196 |           0.5096 |           0.3086 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0186 |           0.5065 |           0.3083 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0191 |           0.5053 |           0.3094 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0225 |           0.5031 |           0.3086 |
[32m[20221208 14:37:16 @agent_ppo2.py:179][0m |          -0.0218 |           0.5017 |           0.3097 |
[32m[20221208 14:37:17 @agent_ppo2.py:179][0m |          -0.0238 |           0.4993 |           0.3100 |
[32m[20221208 14:37:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.19
[32m[20221208 14:37:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.21
[32m[20221208 14:37:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.15
[32m[20221208 14:37:17 @agent_ppo2.py:137][0m Total time:       0.71 min
[32m[20221208 14:37:17 @agent_ppo2.py:139][0m 57344 total steps have happened
[32m[20221208 14:37:17 @agent_ppo2.py:115][0m #------------------------ Iteration 28 --------------------------#
[32m[20221208 14:37:17 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |           0.0020 |           0.5180 |           0.3090 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0059 |           0.5046 |           0.3078 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0123 |           0.5012 |           0.3075 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0122 |           0.4985 |           0.3086 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0130 |           0.4946 |           0.3099 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0139 |           0.4929 |           0.3092 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0167 |           0.4952 |           0.3106 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0194 |           0.4952 |           0.3097 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0205 |           0.4922 |           0.3103 |
[32m[20221208 14:37:18 @agent_ppo2.py:179][0m |          -0.0208 |           0.4969 |           0.3095 |
[32m[20221208 14:37:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.81
[32m[20221208 14:37:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.12
[32m[20221208 14:37:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.98
[32m[20221208 14:37:18 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 62.98
[32m[20221208 14:37:18 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 62.98
[32m[20221208 14:37:18 @agent_ppo2.py:137][0m Total time:       0.74 min
[32m[20221208 14:37:18 @agent_ppo2.py:139][0m 59392 total steps have happened
[32m[20221208 14:37:18 @agent_ppo2.py:115][0m #------------------------ Iteration 29 --------------------------#
[32m[20221208 14:37:19 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0023 |           0.5128 |           0.3228 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0113 |           0.5045 |           0.3212 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0148 |           0.5002 |           0.3215 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0169 |           0.5008 |           0.3206 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0189 |           0.4967 |           0.3205 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0180 |           0.5037 |           0.3202 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0202 |           0.4984 |           0.3209 |
[32m[20221208 14:37:19 @agent_ppo2.py:179][0m |          -0.0206 |           0.5002 |           0.3206 |
[32m[20221208 14:37:20 @agent_ppo2.py:179][0m |          -0.0210 |           0.4972 |           0.3201 |
[32m[20221208 14:37:20 @agent_ppo2.py:179][0m |          -0.0217 |           0.4946 |           0.3204 |
[32m[20221208 14:37:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.71
[32m[20221208 14:37:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.95
[32m[20221208 14:37:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.26
[32m[20221208 14:37:20 @agent_ppo2.py:137][0m Total time:       0.76 min
[32m[20221208 14:37:20 @agent_ppo2.py:139][0m 61440 total steps have happened
[32m[20221208 14:37:20 @agent_ppo2.py:115][0m #------------------------ Iteration 30 --------------------------#
[32m[20221208 14:37:21 @agent_ppo2.py:121][0m Sampling time: 0.55 s by 1 slaves
[32m[20221208 14:37:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |           0.0017 |           0.5433 |           0.3184 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0085 |           0.5339 |           0.3168 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0122 |           0.5355 |           0.3184 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0145 |           0.5306 |           0.3182 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0166 |           0.5251 |           0.3167 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0195 |           0.5311 |           0.3177 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0185 |           0.5277 |           0.3182 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0198 |           0.5269 |           0.3173 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0219 |           0.5292 |           0.3171 |
[32m[20221208 14:37:21 @agent_ppo2.py:179][0m |          -0.0220 |           0.5262 |           0.3176 |
[32m[20221208 14:37:21 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:37:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.78
[32m[20221208 14:37:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.00
[32m[20221208 14:37:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.29
[32m[20221208 14:37:22 @agent_ppo2.py:137][0m Total time:       0.79 min
[32m[20221208 14:37:22 @agent_ppo2.py:139][0m 63488 total steps have happened
[32m[20221208 14:37:22 @agent_ppo2.py:115][0m #------------------------ Iteration 31 --------------------------#
[32m[20221208 14:37:22 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:37:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:22 @agent_ppo2.py:179][0m |           0.0011 |           0.5445 |           0.3144 |
[32m[20221208 14:37:22 @agent_ppo2.py:179][0m |          -0.0071 |           0.5369 |           0.3121 |
[32m[20221208 14:37:22 @agent_ppo2.py:179][0m |          -0.0090 |           0.5371 |           0.3126 |
[32m[20221208 14:37:22 @agent_ppo2.py:179][0m |          -0.0116 |           0.5396 |           0.3107 |
[32m[20221208 14:37:23 @agent_ppo2.py:179][0m |          -0.0114 |           0.5306 |           0.3117 |
[32m[20221208 14:37:23 @agent_ppo2.py:179][0m |          -0.0100 |           0.5320 |           0.3105 |
[32m[20221208 14:37:23 @agent_ppo2.py:179][0m |          -0.0138 |           0.5348 |           0.3103 |
[32m[20221208 14:37:23 @agent_ppo2.py:179][0m |          -0.0140 |           0.5284 |           0.3096 |
[32m[20221208 14:37:23 @agent_ppo2.py:179][0m |          -0.0171 |           0.5299 |           0.3108 |
[32m[20221208 14:37:23 @agent_ppo2.py:179][0m |          -0.0149 |           0.5352 |           0.3105 |
[32m[20221208 14:37:23 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:37:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.76
[32m[20221208 14:37:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.88
[32m[20221208 14:37:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.17
[32m[20221208 14:37:23 @agent_ppo2.py:137][0m Total time:       0.82 min
[32m[20221208 14:37:23 @agent_ppo2.py:139][0m 65536 total steps have happened
[32m[20221208 14:37:23 @agent_ppo2.py:115][0m #------------------------ Iteration 32 --------------------------#
[32m[20221208 14:37:24 @agent_ppo2.py:121][0m Sampling time: 0.54 s by 1 slaves
[32m[20221208 14:37:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:24 @agent_ppo2.py:179][0m |           0.0012 |           0.5491 |           0.3054 |
[32m[20221208 14:37:24 @agent_ppo2.py:179][0m |          -0.0089 |           0.5428 |           0.3038 |
[32m[20221208 14:37:24 @agent_ppo2.py:179][0m |          -0.0093 |           0.5396 |           0.3024 |
[32m[20221208 14:37:24 @agent_ppo2.py:179][0m |          -0.0121 |           0.5446 |           0.3019 |
[32m[20221208 14:37:24 @agent_ppo2.py:179][0m |          -0.0132 |           0.5444 |           0.3009 |
[32m[20221208 14:37:25 @agent_ppo2.py:179][0m |          -0.0144 |           0.5419 |           0.3016 |
[32m[20221208 14:37:25 @agent_ppo2.py:179][0m |          -0.0169 |           0.5360 |           0.3011 |
[32m[20221208 14:37:25 @agent_ppo2.py:179][0m |          -0.0173 |           0.5372 |           0.3011 |
[32m[20221208 14:37:25 @agent_ppo2.py:179][0m |          -0.0166 |           0.5409 |           0.3002 |
[32m[20221208 14:37:25 @agent_ppo2.py:179][0m |          -0.0164 |           0.5358 |           0.2996 |
[32m[20221208 14:37:25 @agent_ppo2.py:124][0m Policy update time: 1.18 s
[32m[20221208 14:37:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.67
[32m[20221208 14:37:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.23
[32m[20221208 14:37:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.92
[32m[20221208 14:37:25 @agent_ppo2.py:137][0m Total time:       0.85 min
[32m[20221208 14:37:25 @agent_ppo2.py:139][0m 67584 total steps have happened
[32m[20221208 14:37:25 @agent_ppo2.py:115][0m #------------------------ Iteration 33 --------------------------#
[32m[20221208 14:37:26 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:37:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |           0.0089 |           0.5654 |           0.3006 |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |          -0.0092 |           0.5269 |           0.2991 |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |          -0.0130 |           0.5196 |           0.2981 |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |          -0.0145 |           0.5188 |           0.2979 |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |          -0.0168 |           0.5134 |           0.2974 |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |          -0.0186 |           0.5079 |           0.2968 |
[32m[20221208 14:37:26 @agent_ppo2.py:179][0m |          -0.0238 |           0.5061 |           0.2970 |
[32m[20221208 14:37:27 @agent_ppo2.py:179][0m |          -0.0199 |           0.5019 |           0.2971 |
[32m[20221208 14:37:27 @agent_ppo2.py:179][0m |          -0.0232 |           0.4969 |           0.2972 |
[32m[20221208 14:37:27 @agent_ppo2.py:179][0m |          -0.0244 |           0.5045 |           0.2970 |
[32m[20221208 14:37:27 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:37:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.87
[32m[20221208 14:37:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.10
[32m[20221208 14:37:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.06
[32m[20221208 14:37:27 @agent_ppo2.py:137][0m Total time:       0.88 min
[32m[20221208 14:37:27 @agent_ppo2.py:139][0m 69632 total steps have happened
[32m[20221208 14:37:27 @agent_ppo2.py:115][0m #------------------------ Iteration 34 --------------------------#
[32m[20221208 14:37:28 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:37:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |           0.0045 |           0.5764 |           0.3054 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0063 |           0.5422 |           0.3038 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0123 |           0.5296 |           0.3039 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0177 |           0.5285 |           0.3034 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0177 |           0.5262 |           0.3036 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0218 |           0.5222 |           0.3035 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0238 |           0.5175 |           0.3019 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0257 |           0.5177 |           0.3026 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0275 |           0.5137 |           0.3026 |
[32m[20221208 14:37:28 @agent_ppo2.py:179][0m |          -0.0263 |           0.5143 |           0.3025 |
[32m[20221208 14:37:28 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:37:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.31
[32m[20221208 14:37:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.12
[32m[20221208 14:37:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.69
[32m[20221208 14:37:29 @agent_ppo2.py:137][0m Total time:       0.91 min
[32m[20221208 14:37:29 @agent_ppo2.py:139][0m 71680 total steps have happened
[32m[20221208 14:37:29 @agent_ppo2.py:115][0m #------------------------ Iteration 35 --------------------------#
[32m[20221208 14:37:29 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:37:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:29 @agent_ppo2.py:179][0m |           0.0041 |           0.5583 |           0.3015 |
[32m[20221208 14:37:29 @agent_ppo2.py:179][0m |          -0.0050 |           0.5349 |           0.3017 |
[32m[20221208 14:37:29 @agent_ppo2.py:179][0m |          -0.0079 |           0.5343 |           0.3016 |
[32m[20221208 14:37:29 @agent_ppo2.py:179][0m |          -0.0108 |           0.5346 |           0.3019 |
[32m[20221208 14:37:30 @agent_ppo2.py:179][0m |          -0.0136 |           0.5326 |           0.3022 |
[32m[20221208 14:37:30 @agent_ppo2.py:179][0m |          -0.0143 |           0.5265 |           0.3012 |
[32m[20221208 14:37:30 @agent_ppo2.py:179][0m |          -0.0148 |           0.5263 |           0.3028 |
[32m[20221208 14:37:30 @agent_ppo2.py:179][0m |          -0.0169 |           0.5311 |           0.3026 |
[32m[20221208 14:37:30 @agent_ppo2.py:179][0m |          -0.0150 |           0.5282 |           0.3028 |
[32m[20221208 14:37:30 @agent_ppo2.py:179][0m |          -0.0206 |           0.5200 |           0.3034 |
[32m[20221208 14:37:30 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:37:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.21
[32m[20221208 14:37:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.53
[32m[20221208 14:37:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.39
[32m[20221208 14:37:30 @agent_ppo2.py:137][0m Total time:       0.94 min
[32m[20221208 14:37:30 @agent_ppo2.py:139][0m 73728 total steps have happened
[32m[20221208 14:37:30 @agent_ppo2.py:115][0m #------------------------ Iteration 36 --------------------------#
[32m[20221208 14:37:31 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:37:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0003 |           0.5893 |           0.2979 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0104 |           0.5827 |           0.2966 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0149 |           0.5707 |           0.2970 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0178 |           0.5656 |           0.2968 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0205 |           0.5659 |           0.2977 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0198 |           0.5716 |           0.2976 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0240 |           0.5610 |           0.2982 |
[32m[20221208 14:37:31 @agent_ppo2.py:179][0m |          -0.0243 |           0.5570 |           0.2989 |
[32m[20221208 14:37:32 @agent_ppo2.py:179][0m |          -0.0270 |           0.5575 |           0.2986 |
[32m[20221208 14:37:32 @agent_ppo2.py:179][0m |          -0.0253 |           0.5566 |           0.2987 |
[32m[20221208 14:37:32 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:37:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.28
[32m[20221208 14:37:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.68
[32m[20221208 14:37:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.21
[32m[20221208 14:37:32 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 63.21
[32m[20221208 14:37:32 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 63.21
[32m[20221208 14:37:32 @agent_ppo2.py:137][0m Total time:       0.96 min
[32m[20221208 14:37:32 @agent_ppo2.py:139][0m 75776 total steps have happened
[32m[20221208 14:37:32 @agent_ppo2.py:115][0m #------------------------ Iteration 37 --------------------------#
[32m[20221208 14:37:32 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |           0.0019 |           0.7630 |           0.3012 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0154 |           0.7316 |           0.2995 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0205 |           0.7159 |           0.2999 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0228 |           0.7142 |           0.2991 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0275 |           0.7016 |           0.3002 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0264 |           0.6912 |           0.2997 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0316 |           0.6925 |           0.2995 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0318 |           0.6796 |           0.3003 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0340 |           0.6723 |           0.3011 |
[32m[20221208 14:37:33 @agent_ppo2.py:179][0m |          -0.0356 |           0.6677 |           0.3007 |
[32m[20221208 14:37:33 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:37:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.03
[32m[20221208 14:37:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.84
[32m[20221208 14:37:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.17
[32m[20221208 14:37:34 @agent_ppo2.py:137][0m Total time:       0.99 min
[32m[20221208 14:37:34 @agent_ppo2.py:139][0m 77824 total steps have happened
[32m[20221208 14:37:34 @agent_ppo2.py:115][0m #------------------------ Iteration 38 --------------------------#
[32m[20221208 14:37:34 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:34 @agent_ppo2.py:179][0m |           0.0043 |           0.9464 |           0.3084 |
[32m[20221208 14:37:34 @agent_ppo2.py:179][0m |          -0.0109 |           0.9088 |           0.3071 |
[32m[20221208 14:37:34 @agent_ppo2.py:179][0m |          -0.0177 |           0.8746 |           0.3080 |
[32m[20221208 14:37:34 @agent_ppo2.py:179][0m |          -0.0211 |           0.8743 |           0.3080 |
[32m[20221208 14:37:34 @agent_ppo2.py:179][0m |          -0.0230 |           0.8581 |           0.3070 |
[32m[20221208 14:37:34 @agent_ppo2.py:179][0m |          -0.0258 |           0.8480 |           0.3081 |
[32m[20221208 14:37:35 @agent_ppo2.py:179][0m |          -0.0294 |           0.8493 |           0.3078 |
[32m[20221208 14:37:35 @agent_ppo2.py:179][0m |          -0.0310 |           0.8614 |           0.3086 |
[32m[20221208 14:37:35 @agent_ppo2.py:179][0m |          -0.0336 |           0.8413 |           0.3078 |
[32m[20221208 14:37:35 @agent_ppo2.py:179][0m |          -0.0330 |           0.8420 |           0.3075 |
[32m[20221208 14:37:35 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:37:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.80
[32m[20221208 14:37:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.16
[32m[20221208 14:37:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.21
[32m[20221208 14:37:35 @agent_ppo2.py:137][0m Total time:       1.01 min
[32m[20221208 14:37:35 @agent_ppo2.py:139][0m 79872 total steps have happened
[32m[20221208 14:37:35 @agent_ppo2.py:115][0m #------------------------ Iteration 39 --------------------------#
[32m[20221208 14:37:36 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |           0.0012 |           0.6417 |           0.3127 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0104 |           0.6211 |           0.3114 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0182 |           0.6116 |           0.3129 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0190 |           0.5994 |           0.3125 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0216 |           0.6005 |           0.3119 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0261 |           0.6002 |           0.3125 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0274 |           0.5937 |           0.3132 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0287 |           0.5912 |           0.3127 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0310 |           0.5893 |           0.3135 |
[32m[20221208 14:37:36 @agent_ppo2.py:179][0m |          -0.0326 |           0.5885 |           0.3139 |
[32m[20221208 14:37:36 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 14:37:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.04
[32m[20221208 14:37:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.41
[32m[20221208 14:37:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.07
[32m[20221208 14:37:37 @agent_ppo2.py:137][0m Total time:       1.04 min
[32m[20221208 14:37:37 @agent_ppo2.py:139][0m 81920 total steps have happened
[32m[20221208 14:37:37 @agent_ppo2.py:115][0m #------------------------ Iteration 40 --------------------------#
[32m[20221208 14:37:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:37 @agent_ppo2.py:179][0m |           0.0032 |           0.5528 |           0.3233 |
[32m[20221208 14:37:37 @agent_ppo2.py:179][0m |          -0.0095 |           0.5202 |           0.3232 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0124 |           0.5148 |           0.3227 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0161 |           0.5113 |           0.3221 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0184 |           0.5032 |           0.3222 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0196 |           0.5046 |           0.3224 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0208 |           0.5029 |           0.3230 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0220 |           0.5076 |           0.3228 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0225 |           0.5002 |           0.3215 |
[32m[20221208 14:37:38 @agent_ppo2.py:179][0m |          -0.0208 |           0.5017 |           0.3222 |
[32m[20221208 14:37:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:37:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.68
[32m[20221208 14:37:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.43
[32m[20221208 14:37:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.41
[32m[20221208 14:37:38 @agent_ppo2.py:137][0m Total time:       1.07 min
[32m[20221208 14:37:38 @agent_ppo2.py:139][0m 83968 total steps have happened
[32m[20221208 14:37:38 @agent_ppo2.py:115][0m #------------------------ Iteration 41 --------------------------#
[32m[20221208 14:37:39 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |           0.0049 |           0.5121 |           0.3008 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0106 |           0.4865 |           0.3011 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0137 |           0.4834 |           0.3011 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0147 |           0.4786 |           0.3017 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0200 |           0.4715 |           0.3012 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0200 |           0.4649 |           0.3017 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0228 |           0.4613 |           0.2996 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0213 |           0.4572 |           0.2994 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0222 |           0.4553 |           0.3004 |
[32m[20221208 14:37:39 @agent_ppo2.py:179][0m |          -0.0250 |           0.4527 |           0.3006 |
[32m[20221208 14:37:39 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:37:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.72
[32m[20221208 14:37:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.69
[32m[20221208 14:37:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.53
[32m[20221208 14:37:40 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 65.53
[32m[20221208 14:37:40 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 65.53
[32m[20221208 14:37:40 @agent_ppo2.py:137][0m Total time:       1.09 min
[32m[20221208 14:37:40 @agent_ppo2.py:139][0m 86016 total steps have happened
[32m[20221208 14:37:40 @agent_ppo2.py:115][0m #------------------------ Iteration 42 --------------------------#
[32m[20221208 14:37:40 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:40 @agent_ppo2.py:179][0m |           0.0031 |           0.6190 |           0.3051 |
[32m[20221208 14:37:40 @agent_ppo2.py:179][0m |          -0.0098 |           0.5651 |           0.3062 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0166 |           0.5553 |           0.3062 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0199 |           0.5577 |           0.3074 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0243 |           0.5525 |           0.3081 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0266 |           0.5487 |           0.3088 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0291 |           0.5423 |           0.3081 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0314 |           0.5402 |           0.3089 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0323 |           0.5470 |           0.3090 |
[32m[20221208 14:37:41 @agent_ppo2.py:179][0m |          -0.0328 |           0.5395 |           0.3101 |
[32m[20221208 14:37:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:37:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.45
[32m[20221208 14:37:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.76
[32m[20221208 14:37:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.98
[32m[20221208 14:37:41 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 66.98
[32m[20221208 14:37:41 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 66.98
[32m[20221208 14:37:41 @agent_ppo2.py:137][0m Total time:       1.12 min
[32m[20221208 14:37:41 @agent_ppo2.py:139][0m 88064 total steps have happened
[32m[20221208 14:37:41 @agent_ppo2.py:115][0m #------------------------ Iteration 43 --------------------------#
[32m[20221208 14:37:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:37:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |           0.0045 |           0.6375 |           0.2999 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0082 |           0.5931 |           0.3001 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0144 |           0.5907 |           0.2988 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0177 |           0.5854 |           0.2993 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0230 |           0.5837 |           0.3001 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0242 |           0.5816 |           0.3001 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0271 |           0.5770 |           0.3007 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0298 |           0.5758 |           0.3008 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0294 |           0.5729 |           0.3012 |
[32m[20221208 14:37:42 @agent_ppo2.py:179][0m |          -0.0310 |           0.5740 |           0.3013 |
[32m[20221208 14:37:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:37:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.84
[32m[20221208 14:37:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.23
[32m[20221208 14:37:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.09
[32m[20221208 14:37:43 @agent_ppo2.py:137][0m Total time:       1.14 min
[32m[20221208 14:37:43 @agent_ppo2.py:139][0m 90112 total steps have happened
[32m[20221208 14:37:43 @agent_ppo2.py:115][0m #------------------------ Iteration 44 --------------------------#
[32m[20221208 14:37:43 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:37:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |           0.0058 |           0.4998 |           0.3306 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0042 |           0.4871 |           0.3314 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0074 |           0.4822 |           0.3324 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0126 |           0.4855 |           0.3329 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0120 |           0.4785 |           0.3318 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0158 |           0.4751 |           0.3324 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0146 |           0.4745 |           0.3321 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0174 |           0.4821 |           0.3321 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0165 |           0.4775 |           0.3327 |
[32m[20221208 14:37:44 @agent_ppo2.py:179][0m |          -0.0187 |           0.4748 |           0.3320 |
[32m[20221208 14:37:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:37:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.35
[32m[20221208 14:37:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.22
[32m[20221208 14:37:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.97
[32m[20221208 14:37:44 @agent_ppo2.py:137][0m Total time:       1.17 min
[32m[20221208 14:37:44 @agent_ppo2.py:139][0m 92160 total steps have happened
[32m[20221208 14:37:44 @agent_ppo2.py:115][0m #------------------------ Iteration 45 --------------------------#
[32m[20221208 14:37:45 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:37:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |           0.0049 |           0.4948 |           0.3326 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0040 |           0.4687 |           0.3325 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0098 |           0.4684 |           0.3322 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0145 |           0.4725 |           0.3323 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0191 |           0.4655 |           0.3326 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0177 |           0.4594 |           0.3326 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0235 |           0.4621 |           0.3332 |
[32m[20221208 14:37:45 @agent_ppo2.py:179][0m |          -0.0218 |           0.4566 |           0.3328 |
[32m[20221208 14:37:46 @agent_ppo2.py:179][0m |          -0.0234 |           0.4611 |           0.3341 |
[32m[20221208 14:37:46 @agent_ppo2.py:179][0m |          -0.0273 |           0.4600 |           0.3327 |
[32m[20221208 14:37:46 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:37:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.48
[32m[20221208 14:37:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.34
[32m[20221208 14:37:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.33
[32m[20221208 14:37:46 @agent_ppo2.py:137][0m Total time:       1.20 min
[32m[20221208 14:37:46 @agent_ppo2.py:139][0m 94208 total steps have happened
[32m[20221208 14:37:46 @agent_ppo2.py:115][0m #------------------------ Iteration 46 --------------------------#
[32m[20221208 14:37:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |           0.0024 |           0.4905 |           0.3232 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0073 |           0.4589 |           0.3221 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0123 |           0.4508 |           0.3242 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0198 |           0.4451 |           0.3238 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0232 |           0.4459 |           0.3251 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0271 |           0.4362 |           0.3256 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0283 |           0.4329 |           0.3271 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0309 |           0.4285 |           0.3266 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0331 |           0.4308 |           0.3276 |
[32m[20221208 14:37:47 @agent_ppo2.py:179][0m |          -0.0350 |           0.4392 |           0.3273 |
[32m[20221208 14:37:47 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:37:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.61
[32m[20221208 14:37:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.86
[32m[20221208 14:37:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.41
[32m[20221208 14:37:48 @agent_ppo2.py:137][0m Total time:       1.22 min
[32m[20221208 14:37:48 @agent_ppo2.py:139][0m 96256 total steps have happened
[32m[20221208 14:37:48 @agent_ppo2.py:115][0m #------------------------ Iteration 47 --------------------------#
[32m[20221208 14:37:48 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:48 @agent_ppo2.py:179][0m |           0.0076 |           0.6094 |           0.3378 |
[32m[20221208 14:37:48 @agent_ppo2.py:179][0m |          -0.0060 |           0.5645 |           0.3377 |
[32m[20221208 14:37:48 @agent_ppo2.py:179][0m |          -0.0134 |           0.5510 |           0.3377 |
[32m[20221208 14:37:48 @agent_ppo2.py:179][0m |          -0.0173 |           0.5326 |           0.3372 |
[32m[20221208 14:37:48 @agent_ppo2.py:179][0m |          -0.0219 |           0.5288 |           0.3388 |
[32m[20221208 14:37:49 @agent_ppo2.py:179][0m |          -0.0231 |           0.5173 |           0.3386 |
[32m[20221208 14:37:49 @agent_ppo2.py:179][0m |          -0.0270 |           0.5092 |           0.3396 |
[32m[20221208 14:37:49 @agent_ppo2.py:179][0m |          -0.0257 |           0.4952 |           0.3398 |
[32m[20221208 14:37:49 @agent_ppo2.py:179][0m |          -0.0284 |           0.4937 |           0.3400 |
[32m[20221208 14:37:49 @agent_ppo2.py:179][0m |          -0.0324 |           0.4838 |           0.3406 |
[32m[20221208 14:37:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:37:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.48
[32m[20221208 14:37:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 64.65
[32m[20221208 14:37:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.69
[32m[20221208 14:37:49 @agent_ppo2.py:137][0m Total time:       1.25 min
[32m[20221208 14:37:49 @agent_ppo2.py:139][0m 98304 total steps have happened
[32m[20221208 14:37:49 @agent_ppo2.py:115][0m #------------------------ Iteration 48 --------------------------#
[32m[20221208 14:37:50 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |           0.0042 |           1.2160 |           0.3425 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0095 |           1.1882 |           0.3414 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0156 |           1.1777 |           0.3395 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0168 |           1.1574 |           0.3389 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0191 |           1.1664 |           0.3387 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0224 |           1.1392 |           0.3396 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0261 |           1.1354 |           0.3393 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0248 |           1.1204 |           0.3395 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0285 |           1.1207 |           0.3403 |
[32m[20221208 14:37:50 @agent_ppo2.py:179][0m |          -0.0278 |           1.1272 |           0.3392 |
[32m[20221208 14:37:50 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.66
[32m[20221208 14:37:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.69
[32m[20221208 14:37:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.73
[32m[20221208 14:37:51 @agent_ppo2.py:137][0m Total time:       1.27 min
[32m[20221208 14:37:51 @agent_ppo2.py:139][0m 100352 total steps have happened
[32m[20221208 14:37:51 @agent_ppo2.py:115][0m #------------------------ Iteration 49 --------------------------#
[32m[20221208 14:37:51 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:51 @agent_ppo2.py:179][0m |           0.0036 |           0.6878 |           0.3431 |
[32m[20221208 14:37:51 @agent_ppo2.py:179][0m |          -0.0095 |           0.6526 |           0.3415 |
[32m[20221208 14:37:51 @agent_ppo2.py:179][0m |          -0.0156 |           0.6446 |           0.3407 |
[32m[20221208 14:37:51 @agent_ppo2.py:179][0m |          -0.0157 |           0.6418 |           0.3394 |
[32m[20221208 14:37:52 @agent_ppo2.py:179][0m |          -0.0208 |           0.6350 |           0.3403 |
[32m[20221208 14:37:52 @agent_ppo2.py:179][0m |          -0.0232 |           0.6298 |           0.3396 |
[32m[20221208 14:37:52 @agent_ppo2.py:179][0m |          -0.0245 |           0.6254 |           0.3395 |
[32m[20221208 14:37:52 @agent_ppo2.py:179][0m |          -0.0287 |           0.6244 |           0.3388 |
[32m[20221208 14:37:52 @agent_ppo2.py:179][0m |          -0.0299 |           0.6216 |           0.3404 |
[32m[20221208 14:37:52 @agent_ppo2.py:179][0m |          -0.0335 |           0.6172 |           0.3407 |
[32m[20221208 14:37:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.72
[32m[20221208 14:37:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.09
[32m[20221208 14:37:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.14
[32m[20221208 14:37:52 @agent_ppo2.py:137][0m Total time:       1.30 min
[32m[20221208 14:37:52 @agent_ppo2.py:139][0m 102400 total steps have happened
[32m[20221208 14:37:52 @agent_ppo2.py:115][0m #------------------------ Iteration 50 --------------------------#
[32m[20221208 14:37:53 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |           0.0057 |           0.8529 |           0.3407 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0119 |           0.8034 |           0.3388 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0144 |           0.8001 |           0.3357 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0213 |           0.7875 |           0.3348 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0224 |           0.7821 |           0.3339 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0253 |           0.7751 |           0.3329 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0271 |           0.7725 |           0.3326 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0286 |           0.7684 |           0.3314 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0307 |           0.7726 |           0.3309 |
[32m[20221208 14:37:53 @agent_ppo2.py:179][0m |          -0.0320 |           0.7669 |           0.3305 |
[32m[20221208 14:37:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:37:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.24
[32m[20221208 14:37:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.57
[32m[20221208 14:37:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.87
[32m[20221208 14:37:54 @agent_ppo2.py:137][0m Total time:       1.33 min
[32m[20221208 14:37:54 @agent_ppo2.py:139][0m 104448 total steps have happened
[32m[20221208 14:37:54 @agent_ppo2.py:115][0m #------------------------ Iteration 51 --------------------------#
[32m[20221208 14:37:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:37:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:54 @agent_ppo2.py:179][0m |           0.0033 |           1.3778 |           0.3200 |
[32m[20221208 14:37:54 @agent_ppo2.py:179][0m |          -0.0137 |           1.3127 |           0.3196 |
[32m[20221208 14:37:54 @agent_ppo2.py:179][0m |          -0.0184 |           1.3220 |           0.3191 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0247 |           1.2996 |           0.3210 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0287 |           1.2782 |           0.3214 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0321 |           1.2420 |           0.3220 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0353 |           1.2321 |           0.3234 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0375 |           1.2290 |           0.3243 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0389 |           1.2068 |           0.3255 |
[32m[20221208 14:37:55 @agent_ppo2.py:179][0m |          -0.0410 |           1.1928 |           0.3252 |
[32m[20221208 14:37:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.70
[32m[20221208 14:37:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.67
[32m[20221208 14:37:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.80
[32m[20221208 14:37:55 @agent_ppo2.py:137][0m Total time:       1.35 min
[32m[20221208 14:37:55 @agent_ppo2.py:139][0m 106496 total steps have happened
[32m[20221208 14:37:55 @agent_ppo2.py:115][0m #------------------------ Iteration 52 --------------------------#
[32m[20221208 14:37:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:37:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |           0.0018 |           0.8772 |           0.3220 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0133 |           0.8039 |           0.3213 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0160 |           0.7819 |           0.3214 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0247 |           0.7714 |           0.3225 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0294 |           0.7636 |           0.3241 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0294 |           0.7512 |           0.3240 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0335 |           0.7563 |           0.3249 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0334 |           0.7393 |           0.3251 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0369 |           0.7359 |           0.3248 |
[32m[20221208 14:37:56 @agent_ppo2.py:179][0m |          -0.0387 |           0.7313 |           0.3253 |
[32m[20221208 14:37:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.87
[32m[20221208 14:37:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.53
[32m[20221208 14:37:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.14
[32m[20221208 14:37:57 @agent_ppo2.py:137][0m Total time:       1.38 min
[32m[20221208 14:37:57 @agent_ppo2.py:139][0m 108544 total steps have happened
[32m[20221208 14:37:57 @agent_ppo2.py:115][0m #------------------------ Iteration 53 --------------------------#
[32m[20221208 14:37:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:37:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:57 @agent_ppo2.py:179][0m |          -0.0026 |           1.3655 |           0.3265 |
[32m[20221208 14:37:57 @agent_ppo2.py:179][0m |          -0.0084 |           1.3101 |           0.3252 |
[32m[20221208 14:37:57 @agent_ppo2.py:179][0m |          -0.0191 |           1.2751 |           0.3270 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0259 |           1.2627 |           0.3271 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0302 |           1.2536 |           0.3279 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0328 |           1.2431 |           0.3279 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0364 |           1.2249 |           0.3272 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0395 |           1.2286 |           0.3272 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0406 |           1.2191 |           0.3287 |
[32m[20221208 14:37:58 @agent_ppo2.py:179][0m |          -0.0416 |           1.2088 |           0.3290 |
[32m[20221208 14:37:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:37:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.58
[32m[20221208 14:37:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.75
[32m[20221208 14:37:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.59
[32m[20221208 14:37:58 @agent_ppo2.py:137][0m Total time:       1.40 min
[32m[20221208 14:37:58 @agent_ppo2.py:139][0m 110592 total steps have happened
[32m[20221208 14:37:58 @agent_ppo2.py:115][0m #------------------------ Iteration 54 --------------------------#
[32m[20221208 14:37:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:37:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |           0.0064 |           0.9973 |           0.3510 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0081 |           0.9482 |           0.3504 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0164 |           0.9499 |           0.3515 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0178 |           0.9305 |           0.3514 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0220 |           0.9256 |           0.3525 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0239 |           0.9279 |           0.3531 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0273 |           0.9222 |           0.3535 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0272 |           0.9268 |           0.3546 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0287 |           0.9255 |           0.3541 |
[32m[20221208 14:37:59 @agent_ppo2.py:179][0m |          -0.0319 |           0.9152 |           0.3550 |
[32m[20221208 14:37:59 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.45
[32m[20221208 14:38:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.87
[32m[20221208 14:38:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.53
[32m[20221208 14:38:00 @agent_ppo2.py:137][0m Total time:       1.43 min
[32m[20221208 14:38:00 @agent_ppo2.py:139][0m 112640 total steps have happened
[32m[20221208 14:38:00 @agent_ppo2.py:115][0m #------------------------ Iteration 55 --------------------------#
[32m[20221208 14:38:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:00 @agent_ppo2.py:179][0m |           0.0050 |           1.0381 |           0.3469 |
[32m[20221208 14:38:00 @agent_ppo2.py:179][0m |          -0.0019 |           1.0069 |           0.3441 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0110 |           1.0006 |           0.3448 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0154 |           1.0062 |           0.3434 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0171 |           0.9920 |           0.3419 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0207 |           0.9817 |           0.3423 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0219 |           0.9831 |           0.3420 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0229 |           0.9805 |           0.3421 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0237 |           0.9768 |           0.3418 |
[32m[20221208 14:38:01 @agent_ppo2.py:179][0m |          -0.0237 |           0.9707 |           0.3423 |
[32m[20221208 14:38:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.46
[32m[20221208 14:38:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.03
[32m[20221208 14:38:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.12
[32m[20221208 14:38:01 @agent_ppo2.py:137][0m Total time:       1.45 min
[32m[20221208 14:38:01 @agent_ppo2.py:139][0m 114688 total steps have happened
[32m[20221208 14:38:01 @agent_ppo2.py:115][0m #------------------------ Iteration 56 --------------------------#
[32m[20221208 14:38:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |           0.0082 |           0.7645 |           0.3337 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0096 |           0.7388 |           0.3323 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0092 |           0.7348 |           0.3323 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0167 |           0.7233 |           0.3318 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0174 |           0.7220 |           0.3334 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0206 |           0.7211 |           0.3319 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0238 |           0.7165 |           0.3329 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0248 |           0.7179 |           0.3330 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0270 |           0.7137 |           0.3325 |
[32m[20221208 14:38:02 @agent_ppo2.py:179][0m |          -0.0288 |           0.7117 |           0.3330 |
[32m[20221208 14:38:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:38:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.08
[32m[20221208 14:38:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.48
[32m[20221208 14:38:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.25
[32m[20221208 14:38:03 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 67.25
[32m[20221208 14:38:03 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 67.25
[32m[20221208 14:38:03 @agent_ppo2.py:137][0m Total time:       1.48 min
[32m[20221208 14:38:03 @agent_ppo2.py:139][0m 116736 total steps have happened
[32m[20221208 14:38:03 @agent_ppo2.py:115][0m #------------------------ Iteration 57 --------------------------#
[32m[20221208 14:38:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:03 @agent_ppo2.py:179][0m |           0.0067 |           1.3499 |           0.3294 |
[32m[20221208 14:38:03 @agent_ppo2.py:179][0m |          -0.0042 |           1.3276 |           0.3275 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0114 |           1.3040 |           0.3307 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0201 |           1.3078 |           0.3313 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0254 |           1.2923 |           0.3320 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0257 |           1.2790 |           0.3331 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0302 |           1.2773 |           0.3337 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0320 |           1.2717 |           0.3355 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0302 |           1.2629 |           0.3349 |
[32m[20221208 14:38:04 @agent_ppo2.py:179][0m |          -0.0347 |           1.2563 |           0.3351 |
[32m[20221208 14:38:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.68
[32m[20221208 14:38:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.34
[32m[20221208 14:38:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.23
[32m[20221208 14:38:04 @agent_ppo2.py:137][0m Total time:       1.50 min
[32m[20221208 14:38:04 @agent_ppo2.py:139][0m 118784 total steps have happened
[32m[20221208 14:38:04 @agent_ppo2.py:115][0m #------------------------ Iteration 58 --------------------------#
[32m[20221208 14:38:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |           0.0080 |           1.0757 |           0.3460 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0051 |           1.0463 |           0.3460 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0129 |           1.0279 |           0.3466 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0181 |           1.0279 |           0.3473 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0234 |           1.0196 |           0.3483 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0254 |           1.0099 |           0.3495 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0284 |           1.0086 |           0.3494 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0283 |           1.0161 |           0.3479 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0328 |           1.0141 |           0.3494 |
[32m[20221208 14:38:05 @agent_ppo2.py:179][0m |          -0.0337 |           1.0044 |           0.3491 |
[32m[20221208 14:38:05 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.75
[32m[20221208 14:38:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.20
[32m[20221208 14:38:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.21
[32m[20221208 14:38:06 @agent_ppo2.py:137][0m Total time:       1.53 min
[32m[20221208 14:38:06 @agent_ppo2.py:139][0m 120832 total steps have happened
[32m[20221208 14:38:06 @agent_ppo2.py:115][0m #------------------------ Iteration 59 --------------------------#
[32m[20221208 14:38:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:06 @agent_ppo2.py:179][0m |           0.0122 |           1.2840 |           0.3509 |
[32m[20221208 14:38:06 @agent_ppo2.py:179][0m |          -0.0073 |           1.2358 |           0.3508 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0162 |           1.2346 |           0.3503 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0209 |           1.2198 |           0.3489 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0211 |           1.2221 |           0.3489 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0251 |           1.2103 |           0.3484 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0267 |           1.1970 |           0.3489 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0302 |           1.2019 |           0.3488 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0309 |           1.1924 |           0.3502 |
[32m[20221208 14:38:07 @agent_ppo2.py:179][0m |          -0.0339 |           1.1934 |           0.3487 |
[32m[20221208 14:38:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:38:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.71
[32m[20221208 14:38:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.91
[32m[20221208 14:38:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.21
[32m[20221208 14:38:07 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 73.21
[32m[20221208 14:38:07 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 73.21
[32m[20221208 14:38:07 @agent_ppo2.py:137][0m Total time:       1.55 min
[32m[20221208 14:38:07 @agent_ppo2.py:139][0m 122880 total steps have happened
[32m[20221208 14:38:07 @agent_ppo2.py:115][0m #------------------------ Iteration 60 --------------------------#
[32m[20221208 14:38:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |           0.0064 |           1.0429 |           0.3659 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0064 |           1.0322 |           0.3648 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0137 |           1.0249 |           0.3666 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0165 |           1.0185 |           0.3656 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0184 |           1.0162 |           0.3661 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0215 |           1.0259 |           0.3659 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0235 |           1.0134 |           0.3652 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0237 |           1.0089 |           0.3657 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0255 |           1.0067 |           0.3659 |
[32m[20221208 14:38:08 @agent_ppo2.py:179][0m |          -0.0258 |           1.0041 |           0.3672 |
[32m[20221208 14:38:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.44
[32m[20221208 14:38:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.15
[32m[20221208 14:38:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.09
[32m[20221208 14:38:09 @agent_ppo2.py:137][0m Total time:       1.58 min
[32m[20221208 14:38:09 @agent_ppo2.py:139][0m 124928 total steps have happened
[32m[20221208 14:38:09 @agent_ppo2.py:115][0m #------------------------ Iteration 61 --------------------------#
[32m[20221208 14:38:09 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:38:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:09 @agent_ppo2.py:179][0m |           0.0091 |           1.1116 |           0.3514 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0093 |           1.1009 |           0.3522 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0088 |           1.0939 |           0.3516 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0128 |           1.0882 |           0.3534 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0159 |           1.0860 |           0.3514 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0156 |           1.0890 |           0.3513 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0168 |           1.0730 |           0.3507 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0208 |           1.0818 |           0.3529 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0232 |           1.0748 |           0.3523 |
[32m[20221208 14:38:10 @agent_ppo2.py:179][0m |          -0.0248 |           1.0823 |           0.3521 |
[32m[20221208 14:38:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.97
[32m[20221208 14:38:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.57
[32m[20221208 14:38:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.78
[32m[20221208 14:38:10 @agent_ppo2.py:137][0m Total time:       1.60 min
[32m[20221208 14:38:10 @agent_ppo2.py:139][0m 126976 total steps have happened
[32m[20221208 14:38:10 @agent_ppo2.py:115][0m #------------------------ Iteration 62 --------------------------#
[32m[20221208 14:38:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |           0.0064 |           1.0118 |           0.3628 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0061 |           0.9234 |           0.3610 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0136 |           0.8998 |           0.3604 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0151 |           0.8931 |           0.3604 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0206 |           0.8974 |           0.3600 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0239 |           0.8863 |           0.3589 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0260 |           0.8866 |           0.3587 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0291 |           0.8785 |           0.3590 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0294 |           0.8749 |           0.3586 |
[32m[20221208 14:38:11 @agent_ppo2.py:179][0m |          -0.0326 |           0.8683 |           0.3577 |
[32m[20221208 14:38:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.98
[32m[20221208 14:38:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.55
[32m[20221208 14:38:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.51
[32m[20221208 14:38:12 @agent_ppo2.py:137][0m Total time:       1.63 min
[32m[20221208 14:38:12 @agent_ppo2.py:139][0m 129024 total steps have happened
[32m[20221208 14:38:12 @agent_ppo2.py:115][0m #------------------------ Iteration 63 --------------------------#
[32m[20221208 14:38:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:12 @agent_ppo2.py:179][0m |           0.0023 |           1.0955 |           0.3397 |
[32m[20221208 14:38:12 @agent_ppo2.py:179][0m |          -0.0140 |           1.0601 |           0.3383 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0191 |           1.0545 |           0.3379 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0243 |           1.0327 |           0.3380 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0272 |           1.0281 |           0.3384 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0296 |           1.0176 |           0.3374 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0317 |           1.0164 |           0.3381 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0350 |           1.0100 |           0.3384 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0370 |           1.0064 |           0.3385 |
[32m[20221208 14:38:13 @agent_ppo2.py:179][0m |          -0.0396 |           1.0061 |           0.3393 |
[32m[20221208 14:38:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.00
[32m[20221208 14:38:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.48
[32m[20221208 14:38:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.86
[32m[20221208 14:38:13 @agent_ppo2.py:137][0m Total time:       1.65 min
[32m[20221208 14:38:13 @agent_ppo2.py:139][0m 131072 total steps have happened
[32m[20221208 14:38:13 @agent_ppo2.py:115][0m #------------------------ Iteration 64 --------------------------#
[32m[20221208 14:38:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:38:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |           0.0100 |           1.6480 |           0.3327 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0101 |           1.6096 |           0.3315 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0128 |           1.5822 |           0.3291 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0196 |           1.5756 |           0.3313 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0232 |           1.5853 |           0.3295 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0239 |           1.5751 |           0.3298 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0285 |           1.5858 |           0.3299 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0313 |           1.5400 |           0.3302 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0338 |           1.5420 |           0.3308 |
[32m[20221208 14:38:14 @agent_ppo2.py:179][0m |          -0.0358 |           1.5411 |           0.3303 |
[32m[20221208 14:38:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.98
[32m[20221208 14:38:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.69
[32m[20221208 14:38:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.59
[32m[20221208 14:38:15 @agent_ppo2.py:137][0m Total time:       1.68 min
[32m[20221208 14:38:15 @agent_ppo2.py:139][0m 133120 total steps have happened
[32m[20221208 14:38:15 @agent_ppo2.py:115][0m #------------------------ Iteration 65 --------------------------#
[32m[20221208 14:38:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:38:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:15 @agent_ppo2.py:179][0m |           0.0073 |           0.7985 |           0.3293 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0101 |           0.7355 |           0.3271 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0136 |           0.7211 |           0.3259 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0153 |           0.7316 |           0.3261 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0161 |           0.7119 |           0.3243 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0241 |           0.6994 |           0.3245 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0257 |           0.6996 |           0.3235 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0272 |           0.7051 |           0.3243 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0268 |           0.7023 |           0.3236 |
[32m[20221208 14:38:16 @agent_ppo2.py:179][0m |          -0.0269 |           0.6947 |           0.3227 |
[32m[20221208 14:38:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.23
[32m[20221208 14:38:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.72
[32m[20221208 14:38:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.18
[32m[20221208 14:38:16 @agent_ppo2.py:137][0m Total time:       1.70 min
[32m[20221208 14:38:16 @agent_ppo2.py:139][0m 135168 total steps have happened
[32m[20221208 14:38:16 @agent_ppo2.py:115][0m #------------------------ Iteration 66 --------------------------#
[32m[20221208 14:38:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |           0.0015 |           0.8881 |           0.3351 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0089 |           0.8599 |           0.3341 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0124 |           0.8477 |           0.3336 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0166 |           0.8463 |           0.3321 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0207 |           0.8433 |           0.3314 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0213 |           0.8413 |           0.3319 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0254 |           0.8329 |           0.3317 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0261 |           0.8317 |           0.3318 |
[32m[20221208 14:38:17 @agent_ppo2.py:179][0m |          -0.0301 |           0.8338 |           0.3323 |
[32m[20221208 14:38:18 @agent_ppo2.py:179][0m |          -0.0309 |           0.8295 |           0.3330 |
[32m[20221208 14:38:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.13
[32m[20221208 14:38:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.20
[32m[20221208 14:38:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.45
[32m[20221208 14:38:18 @agent_ppo2.py:137][0m Total time:       1.73 min
[32m[20221208 14:38:18 @agent_ppo2.py:139][0m 137216 total steps have happened
[32m[20221208 14:38:18 @agent_ppo2.py:115][0m #------------------------ Iteration 67 --------------------------#
[32m[20221208 14:38:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:18 @agent_ppo2.py:179][0m |           0.0066 |           1.4026 |           0.3152 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0067 |           1.3679 |           0.3130 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0147 |           1.3134 |           0.3114 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0207 |           1.3266 |           0.3108 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0247 |           1.2890 |           0.3109 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0260 |           1.2701 |           0.3111 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0308 |           1.2646 |           0.3109 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0318 |           1.2600 |           0.3114 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0339 |           1.2453 |           0.3114 |
[32m[20221208 14:38:19 @agent_ppo2.py:179][0m |          -0.0350 |           1.2455 |           0.3113 |
[32m[20221208 14:38:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.42
[32m[20221208 14:38:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.17
[32m[20221208 14:38:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.57
[32m[20221208 14:38:19 @agent_ppo2.py:137][0m Total time:       1.75 min
[32m[20221208 14:38:19 @agent_ppo2.py:139][0m 139264 total steps have happened
[32m[20221208 14:38:19 @agent_ppo2.py:115][0m #------------------------ Iteration 68 --------------------------#
[32m[20221208 14:38:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |           0.0031 |           0.7161 |           0.3088 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0097 |           0.6824 |           0.3099 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0138 |           0.6728 |           0.3080 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0182 |           0.6646 |           0.3091 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0214 |           0.6573 |           0.3091 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0220 |           0.6541 |           0.3081 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0258 |           0.6512 |           0.3082 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0296 |           0.6550 |           0.3086 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0291 |           0.6426 |           0.3085 |
[32m[20221208 14:38:20 @agent_ppo2.py:179][0m |          -0.0296 |           0.6433 |           0.3077 |
[32m[20221208 14:38:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.04
[32m[20221208 14:38:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.29
[32m[20221208 14:38:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.21
[32m[20221208 14:38:21 @agent_ppo2.py:137][0m Total time:       1.78 min
[32m[20221208 14:38:21 @agent_ppo2.py:139][0m 141312 total steps have happened
[32m[20221208 14:38:21 @agent_ppo2.py:115][0m #------------------------ Iteration 69 --------------------------#
[32m[20221208 14:38:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:21 @agent_ppo2.py:179][0m |           0.0074 |           0.8401 |           0.3073 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0077 |           0.8090 |           0.3054 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0128 |           0.7909 |           0.3060 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0212 |           0.7789 |           0.3041 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0263 |           0.7754 |           0.3045 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0274 |           0.7616 |           0.3051 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0291 |           0.7578 |           0.3051 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0342 |           0.7496 |           0.3056 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0349 |           0.7381 |           0.3065 |
[32m[20221208 14:38:22 @agent_ppo2.py:179][0m |          -0.0378 |           0.7385 |           0.3064 |
[32m[20221208 14:38:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.15
[32m[20221208 14:38:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.55
[32m[20221208 14:38:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.35
[32m[20221208 14:38:22 @agent_ppo2.py:137][0m Total time:       1.80 min
[32m[20221208 14:38:22 @agent_ppo2.py:139][0m 143360 total steps have happened
[32m[20221208 14:38:22 @agent_ppo2.py:115][0m #------------------------ Iteration 70 --------------------------#
[32m[20221208 14:38:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |           0.0042 |           0.8529 |           0.3096 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0130 |           0.7944 |           0.3087 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0174 |           0.7771 |           0.3087 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0197 |           0.7649 |           0.3090 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0287 |           0.7497 |           0.3091 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0302 |           0.7409 |           0.3100 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0316 |           0.7330 |           0.3086 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0346 |           0.7323 |           0.3098 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0377 |           0.7154 |           0.3106 |
[32m[20221208 14:38:23 @agent_ppo2.py:179][0m |          -0.0378 |           0.7113 |           0.3103 |
[32m[20221208 14:38:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.95
[32m[20221208 14:38:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.42
[32m[20221208 14:38:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.86
[32m[20221208 14:38:24 @agent_ppo2.py:137][0m Total time:       1.83 min
[32m[20221208 14:38:24 @agent_ppo2.py:139][0m 145408 total steps have happened
[32m[20221208 14:38:24 @agent_ppo2.py:115][0m #------------------------ Iteration 71 --------------------------#
[32m[20221208 14:38:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:24 @agent_ppo2.py:179][0m |           0.0131 |           0.9565 |           0.3284 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0015 |           0.9066 |           0.3269 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0121 |           0.9152 |           0.3272 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0168 |           0.8975 |           0.3266 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0229 |           0.9013 |           0.3272 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0299 |           0.8938 |           0.3269 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0284 |           0.8988 |           0.3271 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0294 |           0.8907 |           0.3270 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0355 |           0.8840 |           0.3275 |
[32m[20221208 14:38:25 @agent_ppo2.py:179][0m |          -0.0338 |           0.8758 |           0.3275 |
[32m[20221208 14:38:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.10
[32m[20221208 14:38:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.21
[32m[20221208 14:38:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.59
[32m[20221208 14:38:25 @agent_ppo2.py:137][0m Total time:       1.85 min
[32m[20221208 14:38:25 @agent_ppo2.py:139][0m 147456 total steps have happened
[32m[20221208 14:38:25 @agent_ppo2.py:115][0m #------------------------ Iteration 72 --------------------------#
[32m[20221208 14:38:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |           0.0079 |           1.0705 |           0.3196 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0101 |           0.9782 |           0.3185 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0203 |           0.9598 |           0.3195 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0195 |           0.9460 |           0.3208 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0290 |           0.9010 |           0.3203 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0314 |           0.8895 |           0.3210 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0348 |           0.8829 |           0.3213 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0345 |           0.8712 |           0.3206 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0406 |           0.8633 |           0.3212 |
[32m[20221208 14:38:26 @agent_ppo2.py:179][0m |          -0.0402 |           0.8519 |           0.3219 |
[32m[20221208 14:38:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.24
[32m[20221208 14:38:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.03
[32m[20221208 14:38:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.26
[32m[20221208 14:38:27 @agent_ppo2.py:137][0m Total time:       1.88 min
[32m[20221208 14:38:27 @agent_ppo2.py:139][0m 149504 total steps have happened
[32m[20221208 14:38:27 @agent_ppo2.py:115][0m #------------------------ Iteration 73 --------------------------#
[32m[20221208 14:38:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:27 @agent_ppo2.py:179][0m |           0.0067 |           0.9424 |           0.3203 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0082 |           0.8927 |           0.3190 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0125 |           0.8764 |           0.3201 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0207 |           0.8550 |           0.3197 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0241 |           0.8489 |           0.3192 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0286 |           0.8395 |           0.3203 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0319 |           0.8371 |           0.3203 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0304 |           0.8347 |           0.3206 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0361 |           0.8296 |           0.3213 |
[32m[20221208 14:38:28 @agent_ppo2.py:179][0m |          -0.0367 |           0.8273 |           0.3220 |
[32m[20221208 14:38:28 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.81
[32m[20221208 14:38:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.99
[32m[20221208 14:38:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.08
[32m[20221208 14:38:28 @agent_ppo2.py:137][0m Total time:       1.90 min
[32m[20221208 14:38:28 @agent_ppo2.py:139][0m 151552 total steps have happened
[32m[20221208 14:38:28 @agent_ppo2.py:115][0m #------------------------ Iteration 74 --------------------------#
[32m[20221208 14:38:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |           0.0041 |           0.5525 |           0.3340 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0103 |           0.5386 |           0.3319 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0154 |           0.5268 |           0.3327 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0223 |           0.5270 |           0.3339 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0271 |           0.5212 |           0.3339 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0319 |           0.5094 |           0.3347 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0342 |           0.5131 |           0.3350 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0342 |           0.5052 |           0.3351 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0373 |           0.5053 |           0.3344 |
[32m[20221208 14:38:29 @agent_ppo2.py:179][0m |          -0.0393 |           0.5009 |           0.3356 |
[32m[20221208 14:38:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.76
[32m[20221208 14:38:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.53
[32m[20221208 14:38:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.46
[32m[20221208 14:38:30 @agent_ppo2.py:137][0m Total time:       1.93 min
[32m[20221208 14:38:30 @agent_ppo2.py:139][0m 153600 total steps have happened
[32m[20221208 14:38:30 @agent_ppo2.py:115][0m #------------------------ Iteration 75 --------------------------#
[32m[20221208 14:38:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:30 @agent_ppo2.py:179][0m |           0.0079 |           0.7709 |           0.3235 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0100 |           0.7236 |           0.3247 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0163 |           0.6921 |           0.3244 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0221 |           0.6731 |           0.3256 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0253 |           0.6667 |           0.3253 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0281 |           0.6631 |           0.3257 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0302 |           0.6584 |           0.3267 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0324 |           0.6494 |           0.3255 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0367 |           0.6472 |           0.3272 |
[32m[20221208 14:38:31 @agent_ppo2.py:179][0m |          -0.0383 |           0.6409 |           0.3258 |
[32m[20221208 14:38:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.92
[32m[20221208 14:38:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.67
[32m[20221208 14:38:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.23
[32m[20221208 14:38:31 @agent_ppo2.py:137][0m Total time:       1.95 min
[32m[20221208 14:38:31 @agent_ppo2.py:139][0m 155648 total steps have happened
[32m[20221208 14:38:31 @agent_ppo2.py:115][0m #------------------------ Iteration 76 --------------------------#
[32m[20221208 14:38:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |           0.0107 |           1.2156 |           0.3331 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0085 |           1.1577 |           0.3336 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0126 |           1.1260 |           0.3317 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0233 |           1.1125 |           0.3343 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0289 |           1.1011 |           0.3339 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0296 |           1.0907 |           0.3340 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0330 |           1.0847 |           0.3328 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0345 |           1.0781 |           0.3334 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0378 |           1.0796 |           0.3328 |
[32m[20221208 14:38:32 @agent_ppo2.py:179][0m |          -0.0408 |           1.0654 |           0.3343 |
[32m[20221208 14:38:32 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.64
[32m[20221208 14:38:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.67
[32m[20221208 14:38:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.63
[32m[20221208 14:38:33 @agent_ppo2.py:137][0m Total time:       1.98 min
[32m[20221208 14:38:33 @agent_ppo2.py:139][0m 157696 total steps have happened
[32m[20221208 14:38:33 @agent_ppo2.py:115][0m #------------------------ Iteration 77 --------------------------#
[32m[20221208 14:38:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:33 @agent_ppo2.py:179][0m |           0.0064 |           1.5606 |           0.3554 |
[32m[20221208 14:38:33 @agent_ppo2.py:179][0m |          -0.0124 |           1.4090 |           0.3562 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0202 |           1.3976 |           0.3556 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0254 |           1.3586 |           0.3575 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0307 |           1.3449 |           0.3592 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0347 |           1.3513 |           0.3593 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0375 |           1.3263 |           0.3597 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0406 |           1.3016 |           0.3612 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0429 |           1.3042 |           0.3611 |
[32m[20221208 14:38:34 @agent_ppo2.py:179][0m |          -0.0440 |           1.2893 |           0.3609 |
[32m[20221208 14:38:34 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.54
[32m[20221208 14:38:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.79
[32m[20221208 14:38:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.32
[32m[20221208 14:38:34 @agent_ppo2.py:137][0m Total time:       2.00 min
[32m[20221208 14:38:34 @agent_ppo2.py:139][0m 159744 total steps have happened
[32m[20221208 14:38:34 @agent_ppo2.py:115][0m #------------------------ Iteration 78 --------------------------#
[32m[20221208 14:38:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |           0.0000 |           1.3339 |           0.3449 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0137 |           1.2532 |           0.3440 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0240 |           1.2068 |           0.3433 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0280 |           1.1764 |           0.3440 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0313 |           1.1449 |           0.3438 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0358 |           1.1187 |           0.3447 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0376 |           1.0994 |           0.3452 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0397 |           1.0901 |           0.3464 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0404 |           1.0661 |           0.3460 |
[32m[20221208 14:38:35 @agent_ppo2.py:179][0m |          -0.0431 |           1.0501 |           0.3468 |
[32m[20221208 14:38:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.88
[32m[20221208 14:38:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.95
[32m[20221208 14:38:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.45
[32m[20221208 14:38:36 @agent_ppo2.py:137][0m Total time:       2.03 min
[32m[20221208 14:38:36 @agent_ppo2.py:139][0m 161792 total steps have happened
[32m[20221208 14:38:36 @agent_ppo2.py:115][0m #------------------------ Iteration 79 --------------------------#
[32m[20221208 14:38:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:36 @agent_ppo2.py:179][0m |           0.0067 |           1.3444 |           0.3601 |
[32m[20221208 14:38:36 @agent_ppo2.py:179][0m |          -0.0097 |           1.2535 |           0.3594 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0179 |           1.2489 |           0.3595 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0253 |           1.2187 |           0.3579 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0299 |           1.2142 |           0.3604 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0329 |           1.1987 |           0.3600 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0362 |           1.1866 |           0.3601 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0372 |           1.1998 |           0.3609 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0400 |           1.1852 |           0.3609 |
[32m[20221208 14:38:37 @agent_ppo2.py:179][0m |          -0.0436 |           1.1894 |           0.3611 |
[32m[20221208 14:38:37 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.41
[32m[20221208 14:38:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.25
[32m[20221208 14:38:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.94
[32m[20221208 14:38:37 @agent_ppo2.py:137][0m Total time:       2.05 min
[32m[20221208 14:38:37 @agent_ppo2.py:139][0m 163840 total steps have happened
[32m[20221208 14:38:37 @agent_ppo2.py:115][0m #------------------------ Iteration 80 --------------------------#
[32m[20221208 14:38:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |           0.0138 |           1.5701 |           0.3407 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0132 |           1.4850 |           0.3424 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0195 |           1.4436 |           0.3418 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0243 |           1.4128 |           0.3415 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0305 |           1.3919 |           0.3420 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0337 |           1.3686 |           0.3432 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0360 |           1.3649 |           0.3432 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0381 |           1.3701 |           0.3429 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0400 |           1.3322 |           0.3434 |
[32m[20221208 14:38:38 @agent_ppo2.py:179][0m |          -0.0430 |           1.3273 |           0.3430 |
[32m[20221208 14:38:38 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:38:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.26
[32m[20221208 14:38:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.97
[32m[20221208 14:38:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.19
[32m[20221208 14:38:39 @agent_ppo2.py:137][0m Total time:       2.08 min
[32m[20221208 14:38:39 @agent_ppo2.py:139][0m 165888 total steps have happened
[32m[20221208 14:38:39 @agent_ppo2.py:115][0m #------------------------ Iteration 81 --------------------------#
[32m[20221208 14:38:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:39 @agent_ppo2.py:179][0m |           0.0052 |           0.7734 |           0.3476 |
[32m[20221208 14:38:39 @agent_ppo2.py:179][0m |          -0.0060 |           0.7242 |           0.3457 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0152 |           0.7016 |           0.3451 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0164 |           0.6924 |           0.3438 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0223 |           0.6792 |           0.3446 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0240 |           0.6769 |           0.3443 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0270 |           0.6728 |           0.3439 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0282 |           0.6606 |           0.3435 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0287 |           0.6582 |           0.3432 |
[32m[20221208 14:38:40 @agent_ppo2.py:179][0m |          -0.0346 |           0.6539 |           0.3434 |
[32m[20221208 14:38:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.28
[32m[20221208 14:38:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.49
[32m[20221208 14:38:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.81
[32m[20221208 14:38:40 @agent_ppo2.py:137][0m Total time:       2.10 min
[32m[20221208 14:38:40 @agent_ppo2.py:139][0m 167936 total steps have happened
[32m[20221208 14:38:40 @agent_ppo2.py:115][0m #------------------------ Iteration 82 --------------------------#
[32m[20221208 14:38:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |           0.0058 |           1.7074 |           0.3380 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0121 |           1.5926 |           0.3378 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0216 |           1.5588 |           0.3368 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0284 |           1.5140 |           0.3380 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0302 |           1.4974 |           0.3380 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0343 |           1.4887 |           0.3377 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0372 |           1.4906 |           0.3379 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0394 |           1.4637 |           0.3396 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0425 |           1.4394 |           0.3400 |
[32m[20221208 14:38:41 @agent_ppo2.py:179][0m |          -0.0422 |           1.4447 |           0.3397 |
[32m[20221208 14:38:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.22
[32m[20221208 14:38:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.20
[32m[20221208 14:38:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.23
[32m[20221208 14:38:42 @agent_ppo2.py:137][0m Total time:       2.13 min
[32m[20221208 14:38:42 @agent_ppo2.py:139][0m 169984 total steps have happened
[32m[20221208 14:38:42 @agent_ppo2.py:115][0m #------------------------ Iteration 83 --------------------------#
[32m[20221208 14:38:42 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:38:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:42 @agent_ppo2.py:179][0m |           0.0062 |           1.0387 |           0.3536 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0024 |           1.0013 |           0.3518 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0130 |           0.9811 |           0.3521 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0153 |           0.9738 |           0.3539 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0178 |           0.9634 |           0.3532 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0227 |           0.9509 |           0.3539 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0179 |           0.9504 |           0.3537 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0222 |           0.9526 |           0.3544 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0269 |           0.9548 |           0.3546 |
[32m[20221208 14:38:43 @agent_ppo2.py:179][0m |          -0.0277 |           0.9303 |           0.3550 |
[32m[20221208 14:38:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.04
[32m[20221208 14:38:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.88
[32m[20221208 14:38:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.86
[32m[20221208 14:38:43 @agent_ppo2.py:137][0m Total time:       2.15 min
[32m[20221208 14:38:43 @agent_ppo2.py:139][0m 172032 total steps have happened
[32m[20221208 14:38:43 @agent_ppo2.py:115][0m #------------------------ Iteration 84 --------------------------#
[32m[20221208 14:38:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |           0.0070 |           0.7830 |           0.3390 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0103 |           0.7161 |           0.3394 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0191 |           0.6938 |           0.3366 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0260 |           0.6921 |           0.3371 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0304 |           0.6826 |           0.3365 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0333 |           0.6705 |           0.3369 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0383 |           0.6659 |           0.3364 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0385 |           0.6616 |           0.3361 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0407 |           0.6602 |           0.3359 |
[32m[20221208 14:38:44 @agent_ppo2.py:179][0m |          -0.0406 |           0.6521 |           0.3358 |
[32m[20221208 14:38:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.68
[32m[20221208 14:38:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.88
[32m[20221208 14:38:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.73
[32m[20221208 14:38:45 @agent_ppo2.py:137][0m Total time:       2.18 min
[32m[20221208 14:38:45 @agent_ppo2.py:139][0m 174080 total steps have happened
[32m[20221208 14:38:45 @agent_ppo2.py:115][0m #------------------------ Iteration 85 --------------------------#
[32m[20221208 14:38:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:45 @agent_ppo2.py:179][0m |           0.0056 |           1.4614 |           0.3454 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0148 |           1.4189 |           0.3471 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0171 |           1.3778 |           0.3464 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0235 |           1.3509 |           0.3481 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0269 |           1.3522 |           0.3477 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0309 |           1.3325 |           0.3485 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0328 |           1.3166 |           0.3505 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0334 |           1.3024 |           0.3503 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0346 |           1.3026 |           0.3507 |
[32m[20221208 14:38:46 @agent_ppo2.py:179][0m |          -0.0374 |           1.2894 |           0.3509 |
[32m[20221208 14:38:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.30
[32m[20221208 14:38:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.43
[32m[20221208 14:38:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.39
[32m[20221208 14:38:46 @agent_ppo2.py:137][0m Total time:       2.20 min
[32m[20221208 14:38:46 @agent_ppo2.py:139][0m 176128 total steps have happened
[32m[20221208 14:38:46 @agent_ppo2.py:115][0m #------------------------ Iteration 86 --------------------------#
[32m[20221208 14:38:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |           0.0086 |           0.6203 |           0.3473 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0047 |           0.5688 |           0.3456 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0199 |           0.5558 |           0.3457 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0209 |           0.5515 |           0.3447 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0248 |           0.5437 |           0.3461 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0284 |           0.5444 |           0.3450 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0304 |           0.5418 |           0.3466 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0357 |           0.5385 |           0.3465 |
[32m[20221208 14:38:47 @agent_ppo2.py:179][0m |          -0.0349 |           0.5264 |           0.3463 |
[32m[20221208 14:38:48 @agent_ppo2.py:179][0m |          -0.0360 |           0.5299 |           0.3468 |
[32m[20221208 14:38:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.12
[32m[20221208 14:38:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.30
[32m[20221208 14:38:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.69
[32m[20221208 14:38:48 @agent_ppo2.py:137][0m Total time:       2.23 min
[32m[20221208 14:38:48 @agent_ppo2.py:139][0m 178176 total steps have happened
[32m[20221208 14:38:48 @agent_ppo2.py:115][0m #------------------------ Iteration 87 --------------------------#
[32m[20221208 14:38:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:38:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:48 @agent_ppo2.py:179][0m |           0.0082 |           1.2193 |           0.3440 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0126 |           1.1479 |           0.3462 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0213 |           1.1181 |           0.3444 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0256 |           1.1247 |           0.3436 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0330 |           1.1013 |           0.3440 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0382 |           1.0794 |           0.3451 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0389 |           1.0724 |           0.3449 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0416 |           1.0762 |           0.3453 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0439 |           1.0720 |           0.3446 |
[32m[20221208 14:38:49 @agent_ppo2.py:179][0m |          -0.0474 |           1.0578 |           0.3469 |
[32m[20221208 14:38:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:38:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.63
[32m[20221208 14:38:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.63
[32m[20221208 14:38:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.85
[32m[20221208 14:38:49 @agent_ppo2.py:137][0m Total time:       2.25 min
[32m[20221208 14:38:49 @agent_ppo2.py:139][0m 180224 total steps have happened
[32m[20221208 14:38:49 @agent_ppo2.py:115][0m #------------------------ Iteration 88 --------------------------#
[32m[20221208 14:38:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |           0.0185 |           1.5363 |           0.3605 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0052 |           1.4658 |           0.3591 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0167 |           1.4246 |           0.3617 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0266 |           1.4055 |           0.3634 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0312 |           1.3805 |           0.3629 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0353 |           1.3554 |           0.3631 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0407 |           1.3398 |           0.3636 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0412 |           1.3199 |           0.3646 |
[32m[20221208 14:38:50 @agent_ppo2.py:179][0m |          -0.0433 |           1.3256 |           0.3639 |
[32m[20221208 14:38:51 @agent_ppo2.py:179][0m |          -0.0475 |           1.3048 |           0.3650 |
[32m[20221208 14:38:51 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:38:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.97
[32m[20221208 14:38:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.73
[32m[20221208 14:38:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.86
[32m[20221208 14:38:51 @agent_ppo2.py:137][0m Total time:       2.28 min
[32m[20221208 14:38:51 @agent_ppo2.py:139][0m 182272 total steps have happened
[32m[20221208 14:38:51 @agent_ppo2.py:115][0m #------------------------ Iteration 89 --------------------------#
[32m[20221208 14:38:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:38:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |           0.0085 |           1.1162 |           0.3455 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0068 |           1.0347 |           0.3432 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0228 |           1.0116 |           0.3447 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0202 |           0.9900 |           0.3440 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0283 |           0.9929 |           0.3435 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0324 |           0.9715 |           0.3460 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0376 |           0.9677 |           0.3464 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0385 |           0.9476 |           0.3461 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0392 |           0.9396 |           0.3466 |
[32m[20221208 14:38:52 @agent_ppo2.py:179][0m |          -0.0422 |           0.9477 |           0.3476 |
[32m[20221208 14:38:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.66
[32m[20221208 14:38:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.69
[32m[20221208 14:38:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.75
[32m[20221208 14:38:52 @agent_ppo2.py:137][0m Total time:       2.30 min
[32m[20221208 14:38:52 @agent_ppo2.py:139][0m 184320 total steps have happened
[32m[20221208 14:38:52 @agent_ppo2.py:115][0m #------------------------ Iteration 90 --------------------------#
[32m[20221208 14:38:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |           0.0072 |           1.1079 |           0.3714 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0124 |           1.0607 |           0.3692 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0235 |           1.0448 |           0.3699 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0290 |           1.0320 |           0.3707 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0308 |           1.0358 |           0.3698 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0345 |           1.0257 |           0.3700 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0374 |           1.0121 |           0.3702 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0410 |           1.0074 |           0.3709 |
[32m[20221208 14:38:53 @agent_ppo2.py:179][0m |          -0.0433 |           1.0094 |           0.3715 |
[32m[20221208 14:38:54 @agent_ppo2.py:179][0m |          -0.0449 |           0.9966 |           0.3719 |
[32m[20221208 14:38:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.55
[32m[20221208 14:38:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.91
[32m[20221208 14:38:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.46
[32m[20221208 14:38:54 @agent_ppo2.py:137][0m Total time:       2.33 min
[32m[20221208 14:38:54 @agent_ppo2.py:139][0m 186368 total steps have happened
[32m[20221208 14:38:54 @agent_ppo2.py:115][0m #------------------------ Iteration 91 --------------------------#
[32m[20221208 14:38:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:54 @agent_ppo2.py:179][0m |           0.0092 |           1.3604 |           0.3670 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0072 |           1.3234 |           0.3646 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0177 |           1.2980 |           0.3655 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0260 |           1.2918 |           0.3670 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0287 |           1.2864 |           0.3675 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0306 |           1.2654 |           0.3685 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0361 |           1.2573 |           0.3681 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0368 |           1.2589 |           0.3685 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0406 |           1.2510 |           0.3684 |
[32m[20221208 14:38:55 @agent_ppo2.py:179][0m |          -0.0424 |           1.2534 |           0.3688 |
[32m[20221208 14:38:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.74
[32m[20221208 14:38:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.70
[32m[20221208 14:38:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.96
[32m[20221208 14:38:55 @agent_ppo2.py:137][0m Total time:       2.35 min
[32m[20221208 14:38:55 @agent_ppo2.py:139][0m 188416 total steps have happened
[32m[20221208 14:38:55 @agent_ppo2.py:115][0m #------------------------ Iteration 92 --------------------------#
[32m[20221208 14:38:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:38:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |           0.0138 |           1.2265 |           0.3722 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0062 |           1.1824 |           0.3724 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0228 |           1.1573 |           0.3755 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0245 |           1.1541 |           0.3766 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0323 |           1.1264 |           0.3775 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0380 |           1.1117 |           0.3788 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0378 |           1.1026 |           0.3791 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0418 |           1.0889 |           0.3800 |
[32m[20221208 14:38:56 @agent_ppo2.py:179][0m |          -0.0410 |           1.0849 |           0.3805 |
[32m[20221208 14:38:57 @agent_ppo2.py:179][0m |          -0.0433 |           1.0790 |           0.3807 |
[32m[20221208 14:38:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:38:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.73
[32m[20221208 14:38:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.40
[32m[20221208 14:38:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.37
[32m[20221208 14:38:57 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 82.37
[32m[20221208 14:38:57 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 82.37
[32m[20221208 14:38:57 @agent_ppo2.py:137][0m Total time:       2.38 min
[32m[20221208 14:38:57 @agent_ppo2.py:139][0m 190464 total steps have happened
[32m[20221208 14:38:57 @agent_ppo2.py:115][0m #------------------------ Iteration 93 --------------------------#
[32m[20221208 14:38:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:38:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |           0.0093 |           0.8668 |           0.3869 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0109 |           0.8026 |           0.3862 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0192 |           0.7825 |           0.3879 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0214 |           0.7754 |           0.3866 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0249 |           0.7591 |           0.3866 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0288 |           0.7533 |           0.3866 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0317 |           0.7415 |           0.3857 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0329 |           0.7359 |           0.3851 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0356 |           0.7281 |           0.3867 |
[32m[20221208 14:38:58 @agent_ppo2.py:179][0m |          -0.0379 |           0.7233 |           0.3873 |
[32m[20221208 14:38:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:38:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.09
[32m[20221208 14:38:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.94
[32m[20221208 14:38:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.31
[32m[20221208 14:38:58 @agent_ppo2.py:137][0m Total time:       2.40 min
[32m[20221208 14:38:58 @agent_ppo2.py:139][0m 192512 total steps have happened
[32m[20221208 14:38:58 @agent_ppo2.py:115][0m #------------------------ Iteration 94 --------------------------#
[32m[20221208 14:38:59 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:38:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |           0.0096 |           0.7742 |           0.3777 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0092 |           0.6940 |           0.3773 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0200 |           0.6614 |           0.3753 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0266 |           0.6385 |           0.3757 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0288 |           0.6233 |           0.3748 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0324 |           0.6096 |           0.3744 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0358 |           0.6023 |           0.3751 |
[32m[20221208 14:38:59 @agent_ppo2.py:179][0m |          -0.0378 |           0.5867 |           0.3757 |
[32m[20221208 14:39:00 @agent_ppo2.py:179][0m |          -0.0400 |           0.5791 |           0.3769 |
[32m[20221208 14:39:00 @agent_ppo2.py:179][0m |          -0.0399 |           0.5686 |           0.3759 |
[32m[20221208 14:39:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.30
[32m[20221208 14:39:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.74
[32m[20221208 14:39:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.34
[32m[20221208 14:39:00 @agent_ppo2.py:137][0m Total time:       2.43 min
[32m[20221208 14:39:00 @agent_ppo2.py:139][0m 194560 total steps have happened
[32m[20221208 14:39:00 @agent_ppo2.py:115][0m #------------------------ Iteration 95 --------------------------#
[32m[20221208 14:39:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |           0.0138 |           1.0064 |           0.3588 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0117 |           0.9468 |           0.3556 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0197 |           0.9384 |           0.3537 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0243 |           0.9069 |           0.3545 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0302 |           0.8993 |           0.3544 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0356 |           0.8880 |           0.3549 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0403 |           0.8800 |           0.3557 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0427 |           0.8782 |           0.3549 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0420 |           0.8783 |           0.3544 |
[32m[20221208 14:39:01 @agent_ppo2.py:179][0m |          -0.0468 |           0.8559 |           0.3560 |
[32m[20221208 14:39:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.23
[32m[20221208 14:39:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.20
[32m[20221208 14:39:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.23
[32m[20221208 14:39:02 @agent_ppo2.py:137][0m Total time:       2.45 min
[32m[20221208 14:39:02 @agent_ppo2.py:139][0m 196608 total steps have happened
[32m[20221208 14:39:02 @agent_ppo2.py:115][0m #------------------------ Iteration 96 --------------------------#
[32m[20221208 14:39:02 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:39:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |           0.0062 |           0.6396 |           0.3686 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0087 |           0.6057 |           0.3688 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0133 |           0.5901 |           0.3690 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0196 |           0.5890 |           0.3678 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0221 |           0.5800 |           0.3687 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0283 |           0.5710 |           0.3689 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0291 |           0.5731 |           0.3694 |
[32m[20221208 14:39:02 @agent_ppo2.py:179][0m |          -0.0309 |           0.5634 |           0.3702 |
[32m[20221208 14:39:03 @agent_ppo2.py:179][0m |          -0.0333 |           0.5662 |           0.3709 |
[32m[20221208 14:39:03 @agent_ppo2.py:179][0m |          -0.0327 |           0.5594 |           0.3717 |
[32m[20221208 14:39:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.65
[32m[20221208 14:39:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.14
[32m[20221208 14:39:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.33
[32m[20221208 14:39:03 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 86.33
[32m[20221208 14:39:03 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 86.33
[32m[20221208 14:39:03 @agent_ppo2.py:137][0m Total time:       2.48 min
[32m[20221208 14:39:03 @agent_ppo2.py:139][0m 198656 total steps have happened
[32m[20221208 14:39:03 @agent_ppo2.py:115][0m #------------------------ Iteration 97 --------------------------#
[32m[20221208 14:39:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |           0.0080 |           0.8100 |           0.3714 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0099 |           0.7586 |           0.3684 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0201 |           0.7382 |           0.3709 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0276 |           0.7222 |           0.3704 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0332 |           0.7098 |           0.3698 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0359 |           0.6974 |           0.3715 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0373 |           0.6890 |           0.3715 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0404 |           0.6833 |           0.3716 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0436 |           0.6778 |           0.3712 |
[32m[20221208 14:39:04 @agent_ppo2.py:179][0m |          -0.0448 |           0.6740 |           0.3726 |
[32m[20221208 14:39:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.78
[32m[20221208 14:39:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.48
[32m[20221208 14:39:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.72
[32m[20221208 14:39:05 @agent_ppo2.py:137][0m Total time:       2.50 min
[32m[20221208 14:39:05 @agent_ppo2.py:139][0m 200704 total steps have happened
[32m[20221208 14:39:05 @agent_ppo2.py:115][0m #------------------------ Iteration 98 --------------------------#
[32m[20221208 14:39:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |           0.0084 |           1.6671 |           0.3830 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0154 |           1.5150 |           0.3821 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0215 |           1.4411 |           0.3828 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0285 |           1.4205 |           0.3839 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0311 |           1.3999 |           0.3863 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0362 |           1.3561 |           0.3855 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0394 |           1.3535 |           0.3850 |
[32m[20221208 14:39:05 @agent_ppo2.py:179][0m |          -0.0416 |           1.3244 |           0.3864 |
[32m[20221208 14:39:06 @agent_ppo2.py:179][0m |          -0.0421 |           1.3234 |           0.3868 |
[32m[20221208 14:39:06 @agent_ppo2.py:179][0m |          -0.0442 |           1.3033 |           0.3881 |
[32m[20221208 14:39:06 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.62
[32m[20221208 14:39:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.35
[32m[20221208 14:39:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.82
[32m[20221208 14:39:06 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 86.82
[32m[20221208 14:39:06 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 86.82
[32m[20221208 14:39:06 @agent_ppo2.py:137][0m Total time:       2.53 min
[32m[20221208 14:39:06 @agent_ppo2.py:139][0m 202752 total steps have happened
[32m[20221208 14:39:06 @agent_ppo2.py:115][0m #------------------------ Iteration 99 --------------------------#
[32m[20221208 14:39:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |           0.0075 |           0.4771 |           0.3955 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0120 |           0.4138 |           0.3938 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0220 |           0.3944 |           0.3924 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0314 |           0.3876 |           0.3917 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0327 |           0.3741 |           0.3927 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0362 |           0.3678 |           0.3912 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0379 |           0.3666 |           0.3896 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0429 |           0.3613 |           0.3905 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0470 |           0.3581 |           0.3902 |
[32m[20221208 14:39:07 @agent_ppo2.py:179][0m |          -0.0466 |           0.3591 |           0.3910 |
[32m[20221208 14:39:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:39:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 37.23
[32m[20221208 14:39:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.93
[32m[20221208 14:39:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.93
[32m[20221208 14:39:08 @agent_ppo2.py:137][0m Total time:       2.55 min
[32m[20221208 14:39:08 @agent_ppo2.py:139][0m 204800 total steps have happened
[32m[20221208 14:39:08 @agent_ppo2.py:115][0m #------------------------ Iteration 100 --------------------------#
[32m[20221208 14:39:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |           0.0067 |           1.9097 |           0.3734 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0145 |           1.8585 |           0.3753 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0215 |           1.8127 |           0.3763 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0282 |           1.8177 |           0.3770 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0332 |           1.7698 |           0.3792 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0366 |           1.7546 |           0.3798 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0383 |           1.7616 |           0.3803 |
[32m[20221208 14:39:08 @agent_ppo2.py:179][0m |          -0.0414 |           1.7324 |           0.3831 |
[32m[20221208 14:39:09 @agent_ppo2.py:179][0m |          -0.0444 |           1.7261 |           0.3833 |
[32m[20221208 14:39:09 @agent_ppo2.py:179][0m |          -0.0474 |           1.7113 |           0.3834 |
[32m[20221208 14:39:09 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.45
[32m[20221208 14:39:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.64
[32m[20221208 14:39:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.42
[32m[20221208 14:39:09 @agent_ppo2.py:137][0m Total time:       2.58 min
[32m[20221208 14:39:09 @agent_ppo2.py:139][0m 206848 total steps have happened
[32m[20221208 14:39:09 @agent_ppo2.py:115][0m #------------------------ Iteration 101 --------------------------#
[32m[20221208 14:39:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |           0.0127 |           2.0172 |           0.4063 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0132 |           1.9000 |           0.4056 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0231 |           1.8311 |           0.4089 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0292 |           1.8037 |           0.4083 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0306 |           1.7724 |           0.4112 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0383 |           1.7776 |           0.4106 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0384 |           1.7512 |           0.4126 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0413 |           1.7285 |           0.4120 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0437 |           1.7207 |           0.4134 |
[32m[20221208 14:39:10 @agent_ppo2.py:179][0m |          -0.0454 |           1.6926 |           0.4137 |
[32m[20221208 14:39:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.45
[32m[20221208 14:39:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.23
[32m[20221208 14:39:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.99
[32m[20221208 14:39:11 @agent_ppo2.py:137][0m Total time:       2.60 min
[32m[20221208 14:39:11 @agent_ppo2.py:139][0m 208896 total steps have happened
[32m[20221208 14:39:11 @agent_ppo2.py:115][0m #------------------------ Iteration 102 --------------------------#
[32m[20221208 14:39:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |           0.0065 |           1.3652 |           0.4156 |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |          -0.0158 |           1.2276 |           0.4175 |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |          -0.0265 |           1.1763 |           0.4169 |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |          -0.0329 |           1.1455 |           0.4177 |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |          -0.0388 |           1.1343 |           0.4193 |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |          -0.0438 |           1.1117 |           0.4196 |
[32m[20221208 14:39:11 @agent_ppo2.py:179][0m |          -0.0447 |           1.0987 |           0.4204 |
[32m[20221208 14:39:12 @agent_ppo2.py:179][0m |          -0.0455 |           1.0905 |           0.4204 |
[32m[20221208 14:39:12 @agent_ppo2.py:179][0m |          -0.0505 |           1.0856 |           0.4224 |
[32m[20221208 14:39:12 @agent_ppo2.py:179][0m |          -0.0527 |           1.0652 |           0.4231 |
[32m[20221208 14:39:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.87
[32m[20221208 14:39:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.73
[32m[20221208 14:39:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.14
[32m[20221208 14:39:12 @agent_ppo2.py:137][0m Total time:       2.63 min
[32m[20221208 14:39:12 @agent_ppo2.py:139][0m 210944 total steps have happened
[32m[20221208 14:39:12 @agent_ppo2.py:115][0m #------------------------ Iteration 103 --------------------------#
[32m[20221208 14:39:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |           0.0154 |           1.4429 |           0.4252 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0024 |           1.3834 |           0.4247 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0231 |           1.3497 |           0.4277 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0263 |           1.3410 |           0.4272 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0340 |           1.3241 |           0.4298 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0395 |           1.3184 |           0.4288 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0442 |           1.2985 |           0.4312 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0461 |           1.2862 |           0.4300 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0528 |           1.2831 |           0.4322 |
[32m[20221208 14:39:13 @agent_ppo2.py:179][0m |          -0.0540 |           1.2707 |           0.4321 |
[32m[20221208 14:39:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.37
[32m[20221208 14:39:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.44
[32m[20221208 14:39:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.59
[32m[20221208 14:39:14 @agent_ppo2.py:137][0m Total time:       2.65 min
[32m[20221208 14:39:14 @agent_ppo2.py:139][0m 212992 total steps have happened
[32m[20221208 14:39:14 @agent_ppo2.py:115][0m #------------------------ Iteration 104 --------------------------#
[32m[20221208 14:39:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |           0.0067 |           1.5586 |           0.4320 |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |          -0.0163 |           1.3341 |           0.4273 |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |          -0.0221 |           1.2573 |           0.4274 |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |          -0.0290 |           1.1898 |           0.4292 |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |          -0.0340 |           1.1570 |           0.4288 |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |          -0.0375 |           1.1254 |           0.4307 |
[32m[20221208 14:39:14 @agent_ppo2.py:179][0m |          -0.0386 |           1.1032 |           0.4289 |
[32m[20221208 14:39:15 @agent_ppo2.py:179][0m |          -0.0431 |           1.0649 |           0.4313 |
[32m[20221208 14:39:15 @agent_ppo2.py:179][0m |          -0.0449 |           1.0610 |           0.4321 |
[32m[20221208 14:39:15 @agent_ppo2.py:179][0m |          -0.0477 |           1.0292 |           0.4328 |
[32m[20221208 14:39:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.54
[32m[20221208 14:39:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.82
[32m[20221208 14:39:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.33
[32m[20221208 14:39:15 @agent_ppo2.py:137][0m Total time:       2.68 min
[32m[20221208 14:39:15 @agent_ppo2.py:139][0m 215040 total steps have happened
[32m[20221208 14:39:15 @agent_ppo2.py:115][0m #------------------------ Iteration 105 --------------------------#
[32m[20221208 14:39:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:39:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |           0.0103 |           1.7673 |           0.4283 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0087 |           1.7075 |           0.4260 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0195 |           1.6226 |           0.4252 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0294 |           1.6056 |           0.4277 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0330 |           1.5717 |           0.4289 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0381 |           1.5512 |           0.4296 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0403 |           1.5206 |           0.4298 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0438 |           1.5159 |           0.4286 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0463 |           1.5005 |           0.4319 |
[32m[20221208 14:39:16 @agent_ppo2.py:179][0m |          -0.0486 |           1.4806 |           0.4316 |
[32m[20221208 14:39:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.18
[32m[20221208 14:39:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.46
[32m[20221208 14:39:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.45
[32m[20221208 14:39:17 @agent_ppo2.py:137][0m Total time:       2.70 min
[32m[20221208 14:39:17 @agent_ppo2.py:139][0m 217088 total steps have happened
[32m[20221208 14:39:17 @agent_ppo2.py:115][0m #------------------------ Iteration 106 --------------------------#
[32m[20221208 14:39:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |           0.0059 |           1.8445 |           0.4413 |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |          -0.0122 |           1.7771 |           0.4376 |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |          -0.0231 |           1.7575 |           0.4376 |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |          -0.0302 |           1.7421 |           0.4355 |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |          -0.0309 |           1.7241 |           0.4360 |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |          -0.0356 |           1.7080 |           0.4388 |
[32m[20221208 14:39:17 @agent_ppo2.py:179][0m |          -0.0405 |           1.6969 |           0.4379 |
[32m[20221208 14:39:18 @agent_ppo2.py:179][0m |          -0.0414 |           1.6970 |           0.4393 |
[32m[20221208 14:39:18 @agent_ppo2.py:179][0m |          -0.0458 |           1.6830 |           0.4396 |
[32m[20221208 14:39:18 @agent_ppo2.py:179][0m |          -0.0481 |           1.6819 |           0.4398 |
[32m[20221208 14:39:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.18
[32m[20221208 14:39:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.64
[32m[20221208 14:39:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.56
[32m[20221208 14:39:18 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 114.56
[32m[20221208 14:39:18 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 114.56
[32m[20221208 14:39:18 @agent_ppo2.py:137][0m Total time:       2.73 min
[32m[20221208 14:39:18 @agent_ppo2.py:139][0m 219136 total steps have happened
[32m[20221208 14:39:18 @agent_ppo2.py:115][0m #------------------------ Iteration 107 --------------------------#
[32m[20221208 14:39:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |           0.0075 |           1.9504 |           0.4323 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0170 |           1.7996 |           0.4320 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0280 |           1.7661 |           0.4310 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0311 |           1.7466 |           0.4318 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0364 |           1.7168 |           0.4312 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0394 |           1.7123 |           0.4316 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0397 |           1.7218 |           0.4321 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0433 |           1.6772 |           0.4339 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0484 |           1.6587 |           0.4349 |
[32m[20221208 14:39:19 @agent_ppo2.py:179][0m |          -0.0496 |           1.6576 |           0.4360 |
[32m[20221208 14:39:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.87
[32m[20221208 14:39:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.09
[32m[20221208 14:39:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.51
[32m[20221208 14:39:20 @agent_ppo2.py:137][0m Total time:       2.76 min
[32m[20221208 14:39:20 @agent_ppo2.py:139][0m 221184 total steps have happened
[32m[20221208 14:39:20 @agent_ppo2.py:115][0m #------------------------ Iteration 108 --------------------------#
[32m[20221208 14:39:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |           0.0085 |           2.4265 |           0.4415 |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |          -0.0179 |           2.2900 |           0.4389 |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |          -0.0279 |           2.2313 |           0.4396 |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |          -0.0331 |           2.1963 |           0.4389 |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |          -0.0380 |           2.1883 |           0.4427 |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |          -0.0428 |           2.1542 |           0.4431 |
[32m[20221208 14:39:20 @agent_ppo2.py:179][0m |          -0.0447 |           2.1393 |           0.4437 |
[32m[20221208 14:39:21 @agent_ppo2.py:179][0m |          -0.0482 |           2.1293 |           0.4441 |
[32m[20221208 14:39:21 @agent_ppo2.py:179][0m |          -0.0511 |           2.1232 |           0.4456 |
[32m[20221208 14:39:21 @agent_ppo2.py:179][0m |          -0.0520 |           2.1012 |           0.4477 |
[32m[20221208 14:39:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.17
[32m[20221208 14:39:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.71
[32m[20221208 14:39:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.94
[32m[20221208 14:39:21 @agent_ppo2.py:137][0m Total time:       2.78 min
[32m[20221208 14:39:21 @agent_ppo2.py:139][0m 223232 total steps have happened
[32m[20221208 14:39:21 @agent_ppo2.py:115][0m #------------------------ Iteration 109 --------------------------#
[32m[20221208 14:39:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |           0.0070 |           1.5322 |           0.4523 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0174 |           1.4080 |           0.4522 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0282 |           1.3570 |           0.4541 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0329 |           1.3410 |           0.4549 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0371 |           1.3185 |           0.4537 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0426 |           1.2944 |           0.4558 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0416 |           1.3038 |           0.4573 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0449 |           1.2683 |           0.4575 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0478 |           1.2636 |           0.4598 |
[32m[20221208 14:39:22 @agent_ppo2.py:179][0m |          -0.0487 |           1.2526 |           0.4600 |
[32m[20221208 14:39:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.58
[32m[20221208 14:39:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.54
[32m[20221208 14:39:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.87
[32m[20221208 14:39:23 @agent_ppo2.py:137][0m Total time:       2.81 min
[32m[20221208 14:39:23 @agent_ppo2.py:139][0m 225280 total steps have happened
[32m[20221208 14:39:23 @agent_ppo2.py:115][0m #------------------------ Iteration 110 --------------------------#
[32m[20221208 14:39:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |           0.0106 |           1.2668 |           0.4634 |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |          -0.0117 |           1.1515 |           0.4590 |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |          -0.0233 |           1.1276 |           0.4589 |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |          -0.0280 |           1.0845 |           0.4603 |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |          -0.0344 |           1.0727 |           0.4599 |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |          -0.0420 |           1.0473 |           0.4606 |
[32m[20221208 14:39:23 @agent_ppo2.py:179][0m |          -0.0442 |           1.0598 |           0.4617 |
[32m[20221208 14:39:24 @agent_ppo2.py:179][0m |          -0.0469 |           1.0308 |           0.4632 |
[32m[20221208 14:39:24 @agent_ppo2.py:179][0m |          -0.0506 |           1.0265 |           0.4619 |
[32m[20221208 14:39:24 @agent_ppo2.py:179][0m |          -0.0520 |           1.0285 |           0.4644 |
[32m[20221208 14:39:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:39:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.41
[32m[20221208 14:39:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.82
[32m[20221208 14:39:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.81
[32m[20221208 14:39:24 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 115.81
[32m[20221208 14:39:24 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 115.81
[32m[20221208 14:39:24 @agent_ppo2.py:137][0m Total time:       2.83 min
[32m[20221208 14:39:24 @agent_ppo2.py:139][0m 227328 total steps have happened
[32m[20221208 14:39:24 @agent_ppo2.py:115][0m #------------------------ Iteration 111 --------------------------#
[32m[20221208 14:39:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |           0.0084 |           1.0893 |           0.4655 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0134 |           0.9905 |           0.4623 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0200 |           0.9699 |           0.4636 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0282 |           0.9351 |           0.4622 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0335 |           0.9162 |           0.4640 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0372 |           0.8946 |           0.4648 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0402 |           0.8895 |           0.4651 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0421 |           0.8755 |           0.4657 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0430 |           0.8646 |           0.4668 |
[32m[20221208 14:39:25 @agent_ppo2.py:179][0m |          -0.0479 |           0.8616 |           0.4661 |
[32m[20221208 14:39:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.43
[32m[20221208 14:39:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.35
[32m[20221208 14:39:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.74
[32m[20221208 14:39:26 @agent_ppo2.py:137][0m Total time:       2.86 min
[32m[20221208 14:39:26 @agent_ppo2.py:139][0m 229376 total steps have happened
[32m[20221208 14:39:26 @agent_ppo2.py:115][0m #------------------------ Iteration 112 --------------------------#
[32m[20221208 14:39:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:26 @agent_ppo2.py:179][0m |           0.0060 |           2.4187 |           0.4681 |
[32m[20221208 14:39:26 @agent_ppo2.py:179][0m |          -0.0153 |           2.2669 |           0.4638 |
[32m[20221208 14:39:26 @agent_ppo2.py:179][0m |          -0.0238 |           2.1912 |           0.4647 |
[32m[20221208 14:39:26 @agent_ppo2.py:179][0m |          -0.0329 |           2.1595 |           0.4630 |
[32m[20221208 14:39:26 @agent_ppo2.py:179][0m |          -0.0411 |           2.1190 |           0.4630 |
[32m[20221208 14:39:26 @agent_ppo2.py:179][0m |          -0.0451 |           2.0840 |           0.4638 |
[32m[20221208 14:39:27 @agent_ppo2.py:179][0m |          -0.0471 |           2.0453 |           0.4662 |
[32m[20221208 14:39:27 @agent_ppo2.py:179][0m |          -0.0510 |           2.0260 |           0.4665 |
[32m[20221208 14:39:27 @agent_ppo2.py:179][0m |          -0.0527 |           2.0337 |           0.4670 |
[32m[20221208 14:39:27 @agent_ppo2.py:179][0m |          -0.0561 |           1.9866 |           0.4683 |
[32m[20221208 14:39:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.11
[32m[20221208 14:39:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.46
[32m[20221208 14:39:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.51
[32m[20221208 14:39:27 @agent_ppo2.py:137][0m Total time:       2.88 min
[32m[20221208 14:39:27 @agent_ppo2.py:139][0m 231424 total steps have happened
[32m[20221208 14:39:27 @agent_ppo2.py:115][0m #------------------------ Iteration 113 --------------------------#
[32m[20221208 14:39:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |           0.0151 |           2.9943 |           0.4804 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0147 |           2.7819 |           0.4767 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0230 |           2.6922 |           0.4771 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0320 |           2.6388 |           0.4781 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0372 |           2.6244 |           0.4802 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0433 |           2.5761 |           0.4793 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0473 |           2.5374 |           0.4804 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0512 |           2.5304 |           0.4828 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0546 |           2.4897 |           0.4815 |
[32m[20221208 14:39:28 @agent_ppo2.py:179][0m |          -0.0568 |           2.4991 |           0.4840 |
[32m[20221208 14:39:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.79
[32m[20221208 14:39:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.64
[32m[20221208 14:39:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.40
[32m[20221208 14:39:29 @agent_ppo2.py:137][0m Total time:       2.91 min
[32m[20221208 14:39:29 @agent_ppo2.py:139][0m 233472 total steps have happened
[32m[20221208 14:39:29 @agent_ppo2.py:115][0m #------------------------ Iteration 114 --------------------------#
[32m[20221208 14:39:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:29 @agent_ppo2.py:179][0m |           0.0130 |           2.5953 |           0.4633 |
[32m[20221208 14:39:29 @agent_ppo2.py:179][0m |          -0.0151 |           2.3745 |           0.4607 |
[32m[20221208 14:39:29 @agent_ppo2.py:179][0m |          -0.0234 |           2.3074 |           0.4600 |
[32m[20221208 14:39:29 @agent_ppo2.py:179][0m |          -0.0323 |           2.2475 |           0.4617 |
[32m[20221208 14:39:29 @agent_ppo2.py:179][0m |          -0.0405 |           2.2281 |           0.4635 |
[32m[20221208 14:39:29 @agent_ppo2.py:179][0m |          -0.0427 |           2.2016 |           0.4644 |
[32m[20221208 14:39:30 @agent_ppo2.py:179][0m |          -0.0459 |           2.1783 |           0.4659 |
[32m[20221208 14:39:30 @agent_ppo2.py:179][0m |          -0.0469 |           2.1673 |           0.4660 |
[32m[20221208 14:39:30 @agent_ppo2.py:179][0m |          -0.0498 |           2.1386 |           0.4670 |
[32m[20221208 14:39:30 @agent_ppo2.py:179][0m |          -0.0522 |           2.1288 |           0.4672 |
[32m[20221208 14:39:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.66
[32m[20221208 14:39:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.94
[32m[20221208 14:39:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.26
[32m[20221208 14:39:30 @agent_ppo2.py:137][0m Total time:       2.93 min
[32m[20221208 14:39:30 @agent_ppo2.py:139][0m 235520 total steps have happened
[32m[20221208 14:39:30 @agent_ppo2.py:115][0m #------------------------ Iteration 115 --------------------------#
[32m[20221208 14:39:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |           0.0137 |           2.4545 |           0.4715 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0045 |           2.2524 |           0.4662 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0227 |           2.1885 |           0.4673 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0342 |           2.1314 |           0.4663 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0377 |           2.1411 |           0.4664 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0455 |           2.0755 |           0.4665 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0460 |           2.0473 |           0.4680 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0485 |           2.0201 |           0.4682 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0545 |           2.0101 |           0.4690 |
[32m[20221208 14:39:31 @agent_ppo2.py:179][0m |          -0.0563 |           1.9906 |           0.4693 |
[32m[20221208 14:39:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.34
[32m[20221208 14:39:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.26
[32m[20221208 14:39:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.05
[32m[20221208 14:39:32 @agent_ppo2.py:137][0m Total time:       2.96 min
[32m[20221208 14:39:32 @agent_ppo2.py:139][0m 237568 total steps have happened
[32m[20221208 14:39:32 @agent_ppo2.py:115][0m #------------------------ Iteration 116 --------------------------#
[32m[20221208 14:39:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:32 @agent_ppo2.py:179][0m |           0.0136 |           2.3574 |           0.4979 |
[32m[20221208 14:39:32 @agent_ppo2.py:179][0m |          -0.0061 |           2.2121 |           0.4914 |
[32m[20221208 14:39:32 @agent_ppo2.py:179][0m |          -0.0235 |           2.1999 |           0.4946 |
[32m[20221208 14:39:32 @agent_ppo2.py:179][0m |          -0.0345 |           2.1528 |           0.4989 |
[32m[20221208 14:39:32 @agent_ppo2.py:179][0m |          -0.0370 |           2.1591 |           0.4998 |
[32m[20221208 14:39:32 @agent_ppo2.py:179][0m |          -0.0407 |           2.1320 |           0.5019 |
[32m[20221208 14:39:33 @agent_ppo2.py:179][0m |          -0.0450 |           2.1192 |           0.5029 |
[32m[20221208 14:39:33 @agent_ppo2.py:179][0m |          -0.0471 |           2.1154 |           0.5041 |
[32m[20221208 14:39:33 @agent_ppo2.py:179][0m |          -0.0518 |           2.0996 |           0.5066 |
[32m[20221208 14:39:33 @agent_ppo2.py:179][0m |          -0.0508 |           2.0735 |           0.5067 |
[32m[20221208 14:39:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.29
[32m[20221208 14:39:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.13
[32m[20221208 14:39:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.35
[32m[20221208 14:39:33 @agent_ppo2.py:137][0m Total time:       2.98 min
[32m[20221208 14:39:33 @agent_ppo2.py:139][0m 239616 total steps have happened
[32m[20221208 14:39:33 @agent_ppo2.py:115][0m #------------------------ Iteration 117 --------------------------#
[32m[20221208 14:39:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |           0.0137 |           1.4796 |           0.4879 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0094 |           1.3106 |           0.4848 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0273 |           1.2449 |           0.4846 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0385 |           1.1963 |           0.4840 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0429 |           1.1492 |           0.4861 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0507 |           1.1115 |           0.4877 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0501 |           1.0753 |           0.4888 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0540 |           1.0489 |           0.4892 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0568 |           1.0324 |           0.4902 |
[32m[20221208 14:39:34 @agent_ppo2.py:179][0m |          -0.0616 |           1.0094 |           0.4910 |
[32m[20221208 14:39:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.40
[32m[20221208 14:39:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.07
[32m[20221208 14:39:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.80
[32m[20221208 14:39:35 @agent_ppo2.py:137][0m Total time:       3.01 min
[32m[20221208 14:39:35 @agent_ppo2.py:139][0m 241664 total steps have happened
[32m[20221208 14:39:35 @agent_ppo2.py:115][0m #------------------------ Iteration 118 --------------------------#
[32m[20221208 14:39:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:35 @agent_ppo2.py:179][0m |           0.0225 |           2.3646 |           0.4996 |
[32m[20221208 14:39:35 @agent_ppo2.py:179][0m |          -0.0044 |           2.2631 |           0.4982 |
[32m[20221208 14:39:35 @agent_ppo2.py:179][0m |          -0.0182 |           2.2127 |           0.5025 |
[32m[20221208 14:39:35 @agent_ppo2.py:179][0m |          -0.0246 |           2.1871 |           0.5033 |
[32m[20221208 14:39:35 @agent_ppo2.py:179][0m |          -0.0287 |           2.1583 |           0.5064 |
[32m[20221208 14:39:35 @agent_ppo2.py:179][0m |          -0.0318 |           2.1138 |           0.5068 |
[32m[20221208 14:39:36 @agent_ppo2.py:179][0m |          -0.0328 |           2.0907 |           0.5078 |
[32m[20221208 14:39:36 @agent_ppo2.py:179][0m |          -0.0384 |           2.0699 |           0.5105 |
[32m[20221208 14:39:36 @agent_ppo2.py:179][0m |          -0.0427 |           2.0760 |           0.5121 |
[32m[20221208 14:39:36 @agent_ppo2.py:179][0m |          -0.0412 |           2.0529 |           0.5121 |
[32m[20221208 14:39:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.69
[32m[20221208 14:39:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.54
[32m[20221208 14:39:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.13
[32m[20221208 14:39:36 @agent_ppo2.py:137][0m Total time:       3.03 min
[32m[20221208 14:39:36 @agent_ppo2.py:139][0m 243712 total steps have happened
[32m[20221208 14:39:36 @agent_ppo2.py:115][0m #------------------------ Iteration 119 --------------------------#
[32m[20221208 14:39:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:39:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |           0.0146 |           2.8097 |           0.5298 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0132 |           2.6858 |           0.5260 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0244 |           2.6256 |           0.5283 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0331 |           2.5794 |           0.5306 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0365 |           2.5453 |           0.5316 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0412 |           2.5027 |           0.5337 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0421 |           2.4683 |           0.5342 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0474 |           2.4538 |           0.5373 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0483 |           2.4375 |           0.5370 |
[32m[20221208 14:39:37 @agent_ppo2.py:179][0m |          -0.0495 |           2.4137 |           0.5385 |
[32m[20221208 14:39:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.13
[32m[20221208 14:39:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.77
[32m[20221208 14:39:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.70
[32m[20221208 14:39:38 @agent_ppo2.py:137][0m Total time:       3.06 min
[32m[20221208 14:39:38 @agent_ppo2.py:139][0m 245760 total steps have happened
[32m[20221208 14:39:38 @agent_ppo2.py:115][0m #------------------------ Iteration 120 --------------------------#
[32m[20221208 14:39:38 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:39:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:38 @agent_ppo2.py:179][0m |           0.0114 |           0.7676 |           0.5498 |
[32m[20221208 14:39:38 @agent_ppo2.py:179][0m |          -0.0053 |           0.6405 |           0.5468 |
[32m[20221208 14:39:38 @agent_ppo2.py:179][0m |          -0.0136 |           0.6130 |           0.5485 |
[32m[20221208 14:39:38 @agent_ppo2.py:179][0m |          -0.0205 |           0.5897 |           0.5476 |
[32m[20221208 14:39:38 @agent_ppo2.py:179][0m |          -0.0282 |           0.5762 |           0.5483 |
[32m[20221208 14:39:38 @agent_ppo2.py:179][0m |          -0.0292 |           0.5667 |           0.5477 |
[32m[20221208 14:39:39 @agent_ppo2.py:179][0m |          -0.0352 |           0.5643 |           0.5461 |
[32m[20221208 14:39:39 @agent_ppo2.py:179][0m |          -0.0350 |           0.5601 |           0.5491 |
[32m[20221208 14:39:39 @agent_ppo2.py:179][0m |          -0.0376 |           0.5542 |           0.5460 |
[32m[20221208 14:39:39 @agent_ppo2.py:179][0m |          -0.0398 |           0.5487 |           0.5474 |
[32m[20221208 14:39:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.64
[32m[20221208 14:39:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.02
[32m[20221208 14:39:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.98
[32m[20221208 14:39:39 @agent_ppo2.py:137][0m Total time:       3.08 min
[32m[20221208 14:39:39 @agent_ppo2.py:139][0m 247808 total steps have happened
[32m[20221208 14:39:39 @agent_ppo2.py:115][0m #------------------------ Iteration 121 --------------------------#
[32m[20221208 14:39:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |           0.0155 |           3.0714 |           0.5233 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0061 |           2.9392 |           0.5176 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0233 |           2.8873 |           0.5205 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0266 |           2.8910 |           0.5216 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0332 |           2.8335 |           0.5250 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0381 |           2.8444 |           0.5259 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0429 |           2.8410 |           0.5281 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0448 |           2.8255 |           0.5288 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0480 |           2.7997 |           0.5297 |
[32m[20221208 14:39:40 @agent_ppo2.py:179][0m |          -0.0504 |           2.7885 |           0.5325 |
[32m[20221208 14:39:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:39:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.24
[32m[20221208 14:39:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.94
[32m[20221208 14:39:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.25
[32m[20221208 14:39:41 @agent_ppo2.py:137][0m Total time:       3.11 min
[32m[20221208 14:39:41 @agent_ppo2.py:139][0m 249856 total steps have happened
[32m[20221208 14:39:41 @agent_ppo2.py:115][0m #------------------------ Iteration 122 --------------------------#
[32m[20221208 14:39:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:41 @agent_ppo2.py:179][0m |           0.0105 |           1.6336 |           0.5652 |
[32m[20221208 14:39:41 @agent_ppo2.py:179][0m |          -0.0153 |           1.5354 |           0.5596 |
[32m[20221208 14:39:41 @agent_ppo2.py:179][0m |          -0.0261 |           1.5013 |           0.5638 |
[32m[20221208 14:39:41 @agent_ppo2.py:179][0m |          -0.0331 |           1.4792 |           0.5670 |
[32m[20221208 14:39:41 @agent_ppo2.py:179][0m |          -0.0344 |           1.4409 |           0.5677 |
[32m[20221208 14:39:41 @agent_ppo2.py:179][0m |          -0.0412 |           1.4382 |           0.5664 |
[32m[20221208 14:39:42 @agent_ppo2.py:179][0m |          -0.0411 |           1.4154 |           0.5661 |
[32m[20221208 14:39:42 @agent_ppo2.py:179][0m |          -0.0436 |           1.3951 |           0.5670 |
[32m[20221208 14:39:42 @agent_ppo2.py:179][0m |          -0.0482 |           1.3813 |           0.5695 |
[32m[20221208 14:39:42 @agent_ppo2.py:179][0m |          -0.0504 |           1.3699 |           0.5705 |
[32m[20221208 14:39:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.88
[32m[20221208 14:39:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.48
[32m[20221208 14:39:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.09
[32m[20221208 14:39:42 @agent_ppo2.py:137][0m Total time:       3.13 min
[32m[20221208 14:39:42 @agent_ppo2.py:139][0m 251904 total steps have happened
[32m[20221208 14:39:42 @agent_ppo2.py:115][0m #------------------------ Iteration 123 --------------------------#
[32m[20221208 14:39:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |           0.0119 |           2.6228 |           0.5559 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0105 |           2.5346 |           0.5531 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0284 |           2.4489 |           0.5534 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0350 |           2.4023 |           0.5552 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0419 |           2.3378 |           0.5563 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0451 |           2.3066 |           0.5558 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0470 |           2.2458 |           0.5577 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0493 |           2.2354 |           0.5576 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0535 |           2.2132 |           0.5584 |
[32m[20221208 14:39:43 @agent_ppo2.py:179][0m |          -0.0526 |           2.1588 |           0.5588 |
[32m[20221208 14:39:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.00
[32m[20221208 14:39:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.45
[32m[20221208 14:39:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.06
[32m[20221208 14:39:44 @agent_ppo2.py:137][0m Total time:       3.16 min
[32m[20221208 14:39:44 @agent_ppo2.py:139][0m 253952 total steps have happened
[32m[20221208 14:39:44 @agent_ppo2.py:115][0m #------------------------ Iteration 124 --------------------------#
[32m[20221208 14:39:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:44 @agent_ppo2.py:179][0m |           0.0134 |           2.5657 |           0.5808 |
[32m[20221208 14:39:44 @agent_ppo2.py:179][0m |          -0.0114 |           2.4682 |           0.5799 |
[32m[20221208 14:39:44 @agent_ppo2.py:179][0m |          -0.0242 |           2.4255 |           0.5818 |
[32m[20221208 14:39:44 @agent_ppo2.py:179][0m |          -0.0321 |           2.4281 |           0.5844 |
[32m[20221208 14:39:44 @agent_ppo2.py:179][0m |          -0.0355 |           2.3944 |           0.5839 |
[32m[20221208 14:39:44 @agent_ppo2.py:179][0m |          -0.0427 |           2.3629 |           0.5855 |
[32m[20221208 14:39:45 @agent_ppo2.py:179][0m |          -0.0445 |           2.3414 |           0.5863 |
[32m[20221208 14:39:45 @agent_ppo2.py:179][0m |          -0.0492 |           2.3271 |           0.5870 |
[32m[20221208 14:39:45 @agent_ppo2.py:179][0m |          -0.0516 |           2.3049 |           0.5888 |
[32m[20221208 14:39:45 @agent_ppo2.py:179][0m |          -0.0548 |           2.3238 |           0.5907 |
[32m[20221208 14:39:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.58
[32m[20221208 14:39:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.98
[32m[20221208 14:39:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.12
[32m[20221208 14:39:45 @agent_ppo2.py:137][0m Total time:       3.18 min
[32m[20221208 14:39:45 @agent_ppo2.py:139][0m 256000 total steps have happened
[32m[20221208 14:39:45 @agent_ppo2.py:115][0m #------------------------ Iteration 125 --------------------------#
[32m[20221208 14:39:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |           0.0126 |           1.9553 |           0.5999 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0131 |           1.7785 |           0.5996 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0224 |           1.6913 |           0.6011 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0325 |           1.6393 |           0.6027 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0357 |           1.5866 |           0.6038 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0404 |           1.5490 |           0.6031 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0437 |           1.5241 |           0.6053 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0467 |           1.4898 |           0.6076 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0486 |           1.4754 |           0.6083 |
[32m[20221208 14:39:46 @agent_ppo2.py:179][0m |          -0.0512 |           1.4478 |           0.6079 |
[32m[20221208 14:39:46 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:39:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.81
[32m[20221208 14:39:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.58
[32m[20221208 14:39:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.23
[32m[20221208 14:39:47 @agent_ppo2.py:137][0m Total time:       3.21 min
[32m[20221208 14:39:47 @agent_ppo2.py:139][0m 258048 total steps have happened
[32m[20221208 14:39:47 @agent_ppo2.py:115][0m #------------------------ Iteration 126 --------------------------#
[32m[20221208 14:39:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:39:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:47 @agent_ppo2.py:179][0m |           0.0104 |           2.9249 |           0.6050 |
[32m[20221208 14:39:47 @agent_ppo2.py:179][0m |          -0.0150 |           2.5008 |           0.6033 |
[32m[20221208 14:39:47 @agent_ppo2.py:179][0m |          -0.0268 |           2.3419 |           0.6033 |
[32m[20221208 14:39:47 @agent_ppo2.py:179][0m |          -0.0320 |           2.2379 |           0.6064 |
[32m[20221208 14:39:47 @agent_ppo2.py:179][0m |          -0.0400 |           2.1517 |           0.6091 |
[32m[20221208 14:39:48 @agent_ppo2.py:179][0m |          -0.0443 |           2.0864 |           0.6098 |
[32m[20221208 14:39:48 @agent_ppo2.py:179][0m |          -0.0489 |           2.0457 |           0.6131 |
[32m[20221208 14:39:48 @agent_ppo2.py:179][0m |          -0.0491 |           1.9960 |           0.6138 |
[32m[20221208 14:39:48 @agent_ppo2.py:179][0m |          -0.0510 |           1.9699 |           0.6157 |
[32m[20221208 14:39:48 @agent_ppo2.py:179][0m |          -0.0550 |           1.9279 |           0.6186 |
[32m[20221208 14:39:48 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:39:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.02
[32m[20221208 14:39:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.42
[32m[20221208 14:39:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.80
[32m[20221208 14:39:48 @agent_ppo2.py:137][0m Total time:       3.23 min
[32m[20221208 14:39:48 @agent_ppo2.py:139][0m 260096 total steps have happened
[32m[20221208 14:39:48 @agent_ppo2.py:115][0m #------------------------ Iteration 127 --------------------------#
[32m[20221208 14:39:49 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:39:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |           0.0182 |           2.8304 |           0.6183 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0116 |           2.6234 |           0.6226 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0249 |           2.5648 |           0.6261 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0351 |           2.4931 |           0.6288 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0387 |           2.4588 |           0.6296 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0449 |           2.4730 |           0.6318 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0479 |           2.4164 |           0.6332 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0530 |           2.4055 |           0.6367 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0549 |           2.3839 |           0.6385 |
[32m[20221208 14:39:49 @agent_ppo2.py:179][0m |          -0.0571 |           2.3767 |           0.6372 |
[32m[20221208 14:39:49 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:39:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.00
[32m[20221208 14:39:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.34
[32m[20221208 14:39:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.86
[32m[20221208 14:39:50 @agent_ppo2.py:137][0m Total time:       3.26 min
[32m[20221208 14:39:50 @agent_ppo2.py:139][0m 262144 total steps have happened
[32m[20221208 14:39:50 @agent_ppo2.py:115][0m #------------------------ Iteration 128 --------------------------#
[32m[20221208 14:39:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:50 @agent_ppo2.py:179][0m |           0.0167 |           3.1309 |           0.6461 |
[32m[20221208 14:39:50 @agent_ppo2.py:179][0m |          -0.0108 |           2.9758 |           0.6450 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0245 |           2.9227 |           0.6422 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0305 |           2.8840 |           0.6437 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0332 |           2.8338 |           0.6444 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0411 |           2.8100 |           0.6449 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0439 |           2.8273 |           0.6476 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0442 |           2.7968 |           0.6458 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0490 |           2.7772 |           0.6478 |
[32m[20221208 14:39:51 @agent_ppo2.py:179][0m |          -0.0497 |           2.7785 |           0.6491 |
[32m[20221208 14:39:51 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:39:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.20
[32m[20221208 14:39:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.03
[32m[20221208 14:39:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.19
[32m[20221208 14:39:51 @agent_ppo2.py:137][0m Total time:       3.29 min
[32m[20221208 14:39:51 @agent_ppo2.py:139][0m 264192 total steps have happened
[32m[20221208 14:39:51 @agent_ppo2.py:115][0m #------------------------ Iteration 129 --------------------------#
[32m[20221208 14:39:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |           0.0175 |           2.6990 |           0.6438 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0119 |           2.5300 |           0.6425 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0244 |           2.4979 |           0.6469 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0290 |           2.4378 |           0.6491 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0368 |           2.4091 |           0.6524 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0407 |           2.3691 |           0.6530 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0460 |           2.3573 |           0.6550 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0528 |           2.3815 |           0.6583 |
[32m[20221208 14:39:52 @agent_ppo2.py:179][0m |          -0.0546 |           2.3200 |           0.6601 |
[32m[20221208 14:39:53 @agent_ppo2.py:179][0m |          -0.0533 |           2.3083 |           0.6610 |
[32m[20221208 14:39:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:39:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.74
[32m[20221208 14:39:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.21
[32m[20221208 14:39:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.96
[32m[20221208 14:39:53 @agent_ppo2.py:137][0m Total time:       3.31 min
[32m[20221208 14:39:53 @agent_ppo2.py:139][0m 266240 total steps have happened
[32m[20221208 14:39:53 @agent_ppo2.py:115][0m #------------------------ Iteration 130 --------------------------#
[32m[20221208 14:39:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:53 @agent_ppo2.py:179][0m |           0.0131 |           2.4930 |           0.6875 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0121 |           2.3743 |           0.6900 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0242 |           2.3106 |           0.6924 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0324 |           2.2855 |           0.6975 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0389 |           2.2596 |           0.6987 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0428 |           2.2193 |           0.7026 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0453 |           2.2064 |           0.7048 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0469 |           2.1812 |           0.7055 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0506 |           2.1406 |           0.7085 |
[32m[20221208 14:39:54 @agent_ppo2.py:179][0m |          -0.0537 |           2.1298 |           0.7086 |
[32m[20221208 14:39:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:39:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.28
[32m[20221208 14:39:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.60
[32m[20221208 14:39:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.16
[32m[20221208 14:39:54 @agent_ppo2.py:137][0m Total time:       3.34 min
[32m[20221208 14:39:54 @agent_ppo2.py:139][0m 268288 total steps have happened
[32m[20221208 14:39:54 @agent_ppo2.py:115][0m #------------------------ Iteration 131 --------------------------#
[32m[20221208 14:39:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |           0.0193 |           2.8439 |           0.7245 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0072 |           2.7340 |           0.7189 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0234 |           2.6919 |           0.7298 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0315 |           2.6672 |           0.7323 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0377 |           2.6598 |           0.7344 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0366 |           2.6215 |           0.7341 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0434 |           2.5959 |           0.7362 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0445 |           2.5823 |           0.7365 |
[32m[20221208 14:39:55 @agent_ppo2.py:179][0m |          -0.0467 |           2.5567 |           0.7380 |
[32m[20221208 14:39:56 @agent_ppo2.py:179][0m |          -0.0498 |           2.5333 |           0.7417 |
[32m[20221208 14:39:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.81
[32m[20221208 14:39:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.43
[32m[20221208 14:39:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.86
[32m[20221208 14:39:56 @agent_ppo2.py:137][0m Total time:       3.36 min
[32m[20221208 14:39:56 @agent_ppo2.py:139][0m 270336 total steps have happened
[32m[20221208 14:39:56 @agent_ppo2.py:115][0m #------------------------ Iteration 132 --------------------------#
[32m[20221208 14:39:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:39:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:56 @agent_ppo2.py:179][0m |           0.0114 |           3.3972 |           0.7324 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0056 |           3.1371 |           0.7357 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0243 |           3.0390 |           0.7378 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0314 |           2.9913 |           0.7418 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0398 |           2.9183 |           0.7427 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0429 |           2.9077 |           0.7459 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0482 |           2.8569 |           0.7477 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0493 |           2.8189 |           0.7480 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0530 |           2.8316 |           0.7512 |
[32m[20221208 14:39:57 @agent_ppo2.py:179][0m |          -0.0548 |           2.8027 |           0.7523 |
[32m[20221208 14:39:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:39:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.53
[32m[20221208 14:39:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.94
[32m[20221208 14:39:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.27
[32m[20221208 14:39:57 @agent_ppo2.py:137][0m Total time:       3.39 min
[32m[20221208 14:39:57 @agent_ppo2.py:139][0m 272384 total steps have happened
[32m[20221208 14:39:57 @agent_ppo2.py:115][0m #------------------------ Iteration 133 --------------------------#
[32m[20221208 14:39:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |           0.0072 |           3.2056 |           0.7413 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0163 |           3.0870 |           0.7421 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0257 |           2.9936 |           0.7408 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0328 |           2.9581 |           0.7431 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0397 |           2.9155 |           0.7457 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0441 |           2.9086 |           0.7480 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0459 |           2.9014 |           0.7491 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0469 |           2.8700 |           0.7507 |
[32m[20221208 14:39:58 @agent_ppo2.py:179][0m |          -0.0478 |           2.8455 |           0.7537 |
[32m[20221208 14:39:59 @agent_ppo2.py:179][0m |          -0.0514 |           2.8108 |           0.7565 |
[32m[20221208 14:39:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:39:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.88
[32m[20221208 14:39:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.56
[32m[20221208 14:39:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.17
[32m[20221208 14:39:59 @agent_ppo2.py:137][0m Total time:       3.41 min
[32m[20221208 14:39:59 @agent_ppo2.py:139][0m 274432 total steps have happened
[32m[20221208 14:39:59 @agent_ppo2.py:115][0m #------------------------ Iteration 134 --------------------------#
[32m[20221208 14:39:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:39:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:39:59 @agent_ppo2.py:179][0m |           0.0227 |           2.5831 |           0.7593 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0074 |           2.3951 |           0.7584 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0268 |           2.3023 |           0.7582 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0338 |           2.2387 |           0.7593 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0370 |           2.2335 |           0.7585 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0411 |           2.1834 |           0.7592 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0440 |           2.1500 |           0.7629 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0501 |           2.1154 |           0.7636 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0500 |           2.1451 |           0.7646 |
[32m[20221208 14:40:00 @agent_ppo2.py:179][0m |          -0.0539 |           2.1257 |           0.7682 |
[32m[20221208 14:40:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.75
[32m[20221208 14:40:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.50
[32m[20221208 14:40:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.94
[32m[20221208 14:40:00 @agent_ppo2.py:137][0m Total time:       3.44 min
[32m[20221208 14:40:00 @agent_ppo2.py:139][0m 276480 total steps have happened
[32m[20221208 14:40:00 @agent_ppo2.py:115][0m #------------------------ Iteration 135 --------------------------#
[32m[20221208 14:40:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |           0.0092 |           2.7450 |           0.8024 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0138 |           2.6748 |           0.7940 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0257 |           2.6411 |           0.8008 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0334 |           2.5976 |           0.8066 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0403 |           2.5740 |           0.8116 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0410 |           2.5748 |           0.8124 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0439 |           2.5255 |           0.8129 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0473 |           2.5382 |           0.8185 |
[32m[20221208 14:40:01 @agent_ppo2.py:179][0m |          -0.0482 |           2.5272 |           0.8195 |
[32m[20221208 14:40:02 @agent_ppo2.py:179][0m |          -0.0509 |           2.4810 |           0.8252 |
[32m[20221208 14:40:02 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:40:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.71
[32m[20221208 14:40:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.66
[32m[20221208 14:40:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.66
[32m[20221208 14:40:02 @agent_ppo2.py:137][0m Total time:       3.46 min
[32m[20221208 14:40:02 @agent_ppo2.py:139][0m 278528 total steps have happened
[32m[20221208 14:40:02 @agent_ppo2.py:115][0m #------------------------ Iteration 136 --------------------------#
[32m[20221208 14:40:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |           0.0081 |           1.9659 |           0.7966 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0161 |           1.8809 |           0.7883 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0277 |           1.8361 |           0.7991 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0320 |           1.8271 |           0.7968 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0384 |           1.8217 |           0.7972 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0420 |           1.7815 |           0.8001 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0414 |           1.7534 |           0.7964 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0475 |           1.7682 |           0.8027 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0509 |           1.7570 |           0.8038 |
[32m[20221208 14:40:03 @agent_ppo2.py:179][0m |          -0.0496 |           1.7333 |           0.8018 |
[32m[20221208 14:40:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.63
[32m[20221208 14:40:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.06
[32m[20221208 14:40:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.11
[32m[20221208 14:40:04 @agent_ppo2.py:137][0m Total time:       3.49 min
[32m[20221208 14:40:04 @agent_ppo2.py:139][0m 280576 total steps have happened
[32m[20221208 14:40:04 @agent_ppo2.py:115][0m #------------------------ Iteration 137 --------------------------#
[32m[20221208 14:40:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |           0.0120 |           1.7680 |           0.7918 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0064 |           1.6640 |           0.7880 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0161 |           1.6078 |           0.7799 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0269 |           1.5781 |           0.7899 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0371 |           1.5744 |           0.7880 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0381 |           1.5568 |           0.7903 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0432 |           1.5308 |           0.7908 |
[32m[20221208 14:40:04 @agent_ppo2.py:179][0m |          -0.0450 |           1.5064 |           0.7926 |
[32m[20221208 14:40:05 @agent_ppo2.py:179][0m |          -0.0469 |           1.4830 |           0.7939 |
[32m[20221208 14:40:05 @agent_ppo2.py:179][0m |          -0.0501 |           1.4703 |           0.7987 |
[32m[20221208 14:40:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.19
[32m[20221208 14:40:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.87
[32m[20221208 14:40:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.77
[32m[20221208 14:40:05 @agent_ppo2.py:137][0m Total time:       3.51 min
[32m[20221208 14:40:05 @agent_ppo2.py:139][0m 282624 total steps have happened
[32m[20221208 14:40:05 @agent_ppo2.py:115][0m #------------------------ Iteration 138 --------------------------#
[32m[20221208 14:40:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |           0.0249 |           1.7809 |           0.7875 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0037 |           1.7024 |           0.7948 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0191 |           1.6748 |           0.8003 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0264 |           1.6560 |           0.8016 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0320 |           1.6186 |           0.8033 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0368 |           1.6042 |           0.8085 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0394 |           1.5984 |           0.8094 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0424 |           1.5815 |           0.8126 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0450 |           1.5703 |           0.8163 |
[32m[20221208 14:40:06 @agent_ppo2.py:179][0m |          -0.0472 |           1.5641 |           0.8190 |
[32m[20221208 14:40:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.98
[32m[20221208 14:40:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.08
[32m[20221208 14:40:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.70
[32m[20221208 14:40:07 @agent_ppo2.py:137][0m Total time:       3.54 min
[32m[20221208 14:40:07 @agent_ppo2.py:139][0m 284672 total steps have happened
[32m[20221208 14:40:07 @agent_ppo2.py:115][0m #------------------------ Iteration 139 --------------------------#
[32m[20221208 14:40:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |           0.0130 |           2.4253 |           0.8068 |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |          -0.0117 |           2.2153 |           0.7998 |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |          -0.0242 |           2.1385 |           0.8069 |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |          -0.0282 |           2.0763 |           0.8047 |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |          -0.0375 |           2.0373 |           0.8120 |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |          -0.0402 |           2.0215 |           0.8135 |
[32m[20221208 14:40:07 @agent_ppo2.py:179][0m |          -0.0459 |           2.0048 |           0.8139 |
[32m[20221208 14:40:08 @agent_ppo2.py:179][0m |          -0.0468 |           1.9772 |           0.8145 |
[32m[20221208 14:40:08 @agent_ppo2.py:179][0m |          -0.0494 |           1.9696 |           0.8151 |
[32m[20221208 14:40:08 @agent_ppo2.py:179][0m |          -0.0511 |           1.9467 |           0.8139 |
[32m[20221208 14:40:08 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:40:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.17
[32m[20221208 14:40:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.97
[32m[20221208 14:40:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.74
[32m[20221208 14:40:08 @agent_ppo2.py:137][0m Total time:       3.56 min
[32m[20221208 14:40:08 @agent_ppo2.py:139][0m 286720 total steps have happened
[32m[20221208 14:40:08 @agent_ppo2.py:115][0m #------------------------ Iteration 140 --------------------------#
[32m[20221208 14:40:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |           0.0188 |           3.0474 |           0.8864 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0132 |           2.7101 |           0.8871 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0231 |           2.5122 |           0.8943 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0322 |           2.4159 |           0.8934 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0385 |           2.3092 |           0.8924 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0449 |           2.2341 |           0.8975 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0470 |           2.1815 |           0.8976 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0499 |           2.1661 |           0.9029 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0527 |           2.1389 |           0.9055 |
[32m[20221208 14:40:09 @agent_ppo2.py:179][0m |          -0.0548 |           2.1215 |           0.9060 |
[32m[20221208 14:40:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.68
[32m[20221208 14:40:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.49
[32m[20221208 14:40:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.69
[32m[20221208 14:40:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 121.69
[32m[20221208 14:40:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 121.69
[32m[20221208 14:40:10 @agent_ppo2.py:137][0m Total time:       3.59 min
[32m[20221208 14:40:10 @agent_ppo2.py:139][0m 288768 total steps have happened
[32m[20221208 14:40:10 @agent_ppo2.py:115][0m #------------------------ Iteration 141 --------------------------#
[32m[20221208 14:40:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |           0.0123 |           3.1420 |           0.9104 |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |          -0.0119 |           2.9205 |           0.9102 |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |          -0.0285 |           2.8619 |           0.9112 |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |          -0.0320 |           2.8323 |           0.9121 |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |          -0.0385 |           2.7870 |           0.9110 |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |          -0.0410 |           2.7757 |           0.9164 |
[32m[20221208 14:40:10 @agent_ppo2.py:179][0m |          -0.0432 |           2.8099 |           0.9155 |
[32m[20221208 14:40:11 @agent_ppo2.py:179][0m |          -0.0457 |           2.7853 |           0.9174 |
[32m[20221208 14:40:11 @agent_ppo2.py:179][0m |          -0.0490 |           2.7491 |           0.9201 |
[32m[20221208 14:40:11 @agent_ppo2.py:179][0m |          -0.0511 |           2.7267 |           0.9205 |
[32m[20221208 14:40:11 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.47
[32m[20221208 14:40:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.35
[32m[20221208 14:40:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.55
[32m[20221208 14:40:11 @agent_ppo2.py:137][0m Total time:       3.61 min
[32m[20221208 14:40:11 @agent_ppo2.py:139][0m 290816 total steps have happened
[32m[20221208 14:40:11 @agent_ppo2.py:115][0m #------------------------ Iteration 142 --------------------------#
[32m[20221208 14:40:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |           0.0119 |           0.9403 |           0.8709 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0162 |           0.8133 |           0.8636 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0235 |           0.7641 |           0.8655 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0341 |           0.7379 |           0.8659 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0403 |           0.7251 |           0.8712 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0465 |           0.7024 |           0.8710 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0454 |           0.6980 |           0.8674 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0520 |           0.7003 |           0.8750 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0532 |           0.6810 |           0.8770 |
[32m[20221208 14:40:12 @agent_ppo2.py:179][0m |          -0.0545 |           0.6647 |           0.8726 |
[32m[20221208 14:40:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:40:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.64
[32m[20221208 14:40:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.85
[32m[20221208 14:40:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.87
[32m[20221208 14:40:13 @agent_ppo2.py:137][0m Total time:       3.64 min
[32m[20221208 14:40:13 @agent_ppo2.py:139][0m 292864 total steps have happened
[32m[20221208 14:40:13 @agent_ppo2.py:115][0m #------------------------ Iteration 143 --------------------------#
[32m[20221208 14:40:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:13 @agent_ppo2.py:179][0m |           0.0108 |           3.1873 |           0.8948 |
[32m[20221208 14:40:13 @agent_ppo2.py:179][0m |          -0.0151 |           3.1225 |           0.8890 |
[32m[20221208 14:40:13 @agent_ppo2.py:179][0m |          -0.0280 |           3.0278 |           0.8912 |
[32m[20221208 14:40:13 @agent_ppo2.py:179][0m |          -0.0337 |           2.9583 |           0.8950 |
[32m[20221208 14:40:13 @agent_ppo2.py:179][0m |          -0.0393 |           2.9175 |           0.8954 |
[32m[20221208 14:40:13 @agent_ppo2.py:179][0m |          -0.0447 |           2.8889 |           0.8994 |
[32m[20221208 14:40:14 @agent_ppo2.py:179][0m |          -0.0468 |           2.8460 |           0.9016 |
[32m[20221208 14:40:14 @agent_ppo2.py:179][0m |          -0.0492 |           2.8110 |           0.9001 |
[32m[20221208 14:40:14 @agent_ppo2.py:179][0m |          -0.0509 |           2.7909 |           0.9040 |
[32m[20221208 14:40:14 @agent_ppo2.py:179][0m |          -0.0525 |           2.7883 |           0.9036 |
[32m[20221208 14:40:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.47
[32m[20221208 14:40:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.22
[32m[20221208 14:40:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.64
[32m[20221208 14:40:14 @agent_ppo2.py:137][0m Total time:       3.66 min
[32m[20221208 14:40:14 @agent_ppo2.py:139][0m 294912 total steps have happened
[32m[20221208 14:40:14 @agent_ppo2.py:115][0m #------------------------ Iteration 144 --------------------------#
[32m[20221208 14:40:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:40:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |           0.0072 |           2.8922 |           0.9114 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0134 |           2.7666 |           0.9150 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0234 |           2.7335 |           0.9161 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0345 |           2.6712 |           0.9224 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0363 |           2.6567 |           0.9276 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0418 |           2.6310 |           0.9303 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0457 |           2.6206 |           0.9334 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0480 |           2.6186 |           0.9349 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0515 |           2.6099 |           0.9349 |
[32m[20221208 14:40:15 @agent_ppo2.py:179][0m |          -0.0533 |           2.5844 |           0.9400 |
[32m[20221208 14:40:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.75
[32m[20221208 14:40:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.71
[32m[20221208 14:40:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.50
[32m[20221208 14:40:16 @agent_ppo2.py:137][0m Total time:       3.69 min
[32m[20221208 14:40:16 @agent_ppo2.py:139][0m 296960 total steps have happened
[32m[20221208 14:40:16 @agent_ppo2.py:115][0m #------------------------ Iteration 145 --------------------------#
[32m[20221208 14:40:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:16 @agent_ppo2.py:179][0m |           0.0237 |           2.0592 |           0.9241 |
[32m[20221208 14:40:16 @agent_ppo2.py:179][0m |          -0.0076 |           1.7586 |           0.9351 |
[32m[20221208 14:40:16 @agent_ppo2.py:179][0m |          -0.0185 |           1.6515 |           0.9394 |
[32m[20221208 14:40:16 @agent_ppo2.py:179][0m |          -0.0243 |           1.5558 |           0.9443 |
[32m[20221208 14:40:16 @agent_ppo2.py:179][0m |          -0.0329 |           1.5057 |           0.9520 |
[32m[20221208 14:40:16 @agent_ppo2.py:179][0m |          -0.0382 |           1.4589 |           0.9510 |
[32m[20221208 14:40:17 @agent_ppo2.py:179][0m |          -0.0426 |           1.4200 |           0.9542 |
[32m[20221208 14:40:17 @agent_ppo2.py:179][0m |          -0.0489 |           1.3842 |           0.9553 |
[32m[20221208 14:40:17 @agent_ppo2.py:179][0m |          -0.0502 |           1.3693 |           0.9564 |
[32m[20221208 14:40:17 @agent_ppo2.py:179][0m |          -0.0489 |           1.3589 |           0.9579 |
[32m[20221208 14:40:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.01
[32m[20221208 14:40:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.35
[32m[20221208 14:40:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.14
[32m[20221208 14:40:17 @agent_ppo2.py:137][0m Total time:       3.71 min
[32m[20221208 14:40:17 @agent_ppo2.py:139][0m 299008 total steps have happened
[32m[20221208 14:40:17 @agent_ppo2.py:115][0m #------------------------ Iteration 146 --------------------------#
[32m[20221208 14:40:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:40:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |           0.0190 |           1.4752 |           0.9719 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0084 |           1.2009 |           0.9691 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0262 |           1.0848 |           0.9760 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0319 |           1.0072 |           0.9788 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0398 |           0.9607 |           0.9833 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0461 |           0.9105 |           0.9872 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0496 |           0.8764 |           0.9892 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0498 |           0.8446 |           0.9898 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0498 |           0.8162 |           0.9926 |
[32m[20221208 14:40:18 @agent_ppo2.py:179][0m |          -0.0565 |           0.7893 |           0.9938 |
[32m[20221208 14:40:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.92
[32m[20221208 14:40:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.60
[32m[20221208 14:40:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.10
[32m[20221208 14:40:19 @agent_ppo2.py:137][0m Total time:       3.74 min
[32m[20221208 14:40:19 @agent_ppo2.py:139][0m 301056 total steps have happened
[32m[20221208 14:40:19 @agent_ppo2.py:115][0m #------------------------ Iteration 147 --------------------------#
[32m[20221208 14:40:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:19 @agent_ppo2.py:179][0m |           0.0105 |           2.1172 |           0.9971 |
[32m[20221208 14:40:19 @agent_ppo2.py:179][0m |          -0.0159 |           1.9274 |           0.9879 |
[32m[20221208 14:40:19 @agent_ppo2.py:179][0m |          -0.0237 |           1.7980 |           0.9902 |
[32m[20221208 14:40:19 @agent_ppo2.py:179][0m |          -0.0259 |           1.7297 |           0.9916 |
[32m[20221208 14:40:19 @agent_ppo2.py:179][0m |          -0.0331 |           1.6682 |           0.9918 |
[32m[20221208 14:40:20 @agent_ppo2.py:179][0m |          -0.0393 |           1.6411 |           0.9915 |
[32m[20221208 14:40:20 @agent_ppo2.py:179][0m |          -0.0429 |           1.5975 |           0.9929 |
[32m[20221208 14:40:20 @agent_ppo2.py:179][0m |          -0.0467 |           1.5612 |           0.9956 |
[32m[20221208 14:40:20 @agent_ppo2.py:179][0m |          -0.0475 |           1.5597 |           0.9942 |
[32m[20221208 14:40:20 @agent_ppo2.py:179][0m |          -0.0496 |           1.5084 |           0.9994 |
[32m[20221208 14:40:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:40:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.78
[32m[20221208 14:40:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.36
[32m[20221208 14:40:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.29
[32m[20221208 14:40:20 @agent_ppo2.py:137][0m Total time:       3.76 min
[32m[20221208 14:40:20 @agent_ppo2.py:139][0m 303104 total steps have happened
[32m[20221208 14:40:20 @agent_ppo2.py:115][0m #------------------------ Iteration 148 --------------------------#
[32m[20221208 14:40:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:40:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |           0.0296 |           4.1034 |           1.0142 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0032 |           3.7500 |           0.9935 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0266 |           3.6562 |           1.0062 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0404 |           3.6171 |           1.0125 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0423 |           3.5554 |           1.0161 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0512 |           3.5337 |           1.0207 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0547 |           3.5497 |           1.0205 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0581 |           3.4658 |           1.0225 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0601 |           3.4243 |           1.0226 |
[32m[20221208 14:40:21 @agent_ppo2.py:179][0m |          -0.0643 |           3.4210 |           1.0256 |
[32m[20221208 14:40:21 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.98
[32m[20221208 14:40:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.89
[32m[20221208 14:40:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.31
[32m[20221208 14:40:22 @agent_ppo2.py:137][0m Total time:       3.79 min
[32m[20221208 14:40:22 @agent_ppo2.py:139][0m 305152 total steps have happened
[32m[20221208 14:40:22 @agent_ppo2.py:115][0m #------------------------ Iteration 149 --------------------------#
[32m[20221208 14:40:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:22 @agent_ppo2.py:179][0m |           0.0151 |           3.5525 |           1.0102 |
[32m[20221208 14:40:22 @agent_ppo2.py:179][0m |          -0.0059 |           3.2978 |           1.0018 |
[32m[20221208 14:40:22 @agent_ppo2.py:179][0m |          -0.0201 |           3.2775 |           1.0108 |
[32m[20221208 14:40:22 @agent_ppo2.py:179][0m |          -0.0290 |           3.2315 |           1.0157 |
[32m[20221208 14:40:22 @agent_ppo2.py:179][0m |          -0.0341 |           3.1838 |           1.0158 |
[32m[20221208 14:40:23 @agent_ppo2.py:179][0m |          -0.0366 |           3.1897 |           1.0176 |
[32m[20221208 14:40:23 @agent_ppo2.py:179][0m |          -0.0437 |           3.1585 |           1.0206 |
[32m[20221208 14:40:23 @agent_ppo2.py:179][0m |          -0.0460 |           3.1581 |           1.0243 |
[32m[20221208 14:40:23 @agent_ppo2.py:179][0m |          -0.0503 |           3.1327 |           1.0302 |
[32m[20221208 14:40:23 @agent_ppo2.py:179][0m |          -0.0507 |           3.1217 |           1.0321 |
[32m[20221208 14:40:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:40:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.57
[32m[20221208 14:40:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.00
[32m[20221208 14:40:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.83
[32m[20221208 14:40:23 @agent_ppo2.py:137][0m Total time:       3.81 min
[32m[20221208 14:40:23 @agent_ppo2.py:139][0m 307200 total steps have happened
[32m[20221208 14:40:23 @agent_ppo2.py:115][0m #------------------------ Iteration 150 --------------------------#
[32m[20221208 14:40:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |           0.0218 |           3.3097 |           1.0514 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0129 |           2.9965 |           1.0586 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0288 |           2.9239 |           1.0672 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0321 |           2.9173 |           1.0660 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0386 |           2.8730 |           1.0688 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0432 |           2.8544 |           1.0700 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0431 |           2.8265 |           1.0692 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0477 |           2.7900 |           1.0739 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0489 |           2.7719 |           1.0684 |
[32m[20221208 14:40:24 @agent_ppo2.py:179][0m |          -0.0513 |           2.7735 |           1.0766 |
[32m[20221208 14:40:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.34
[32m[20221208 14:40:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.97
[32m[20221208 14:40:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.38
[32m[20221208 14:40:25 @agent_ppo2.py:137][0m Total time:       3.84 min
[32m[20221208 14:40:25 @agent_ppo2.py:139][0m 309248 total steps have happened
[32m[20221208 14:40:25 @agent_ppo2.py:115][0m #------------------------ Iteration 151 --------------------------#
[32m[20221208 14:40:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:25 @agent_ppo2.py:179][0m |           0.0107 |           2.6099 |           1.0758 |
[32m[20221208 14:40:25 @agent_ppo2.py:179][0m |          -0.0165 |           2.2168 |           1.0701 |
[32m[20221208 14:40:25 @agent_ppo2.py:179][0m |          -0.0313 |           2.0654 |           1.0763 |
[32m[20221208 14:40:25 @agent_ppo2.py:179][0m |          -0.0362 |           1.9833 |           1.0783 |
[32m[20221208 14:40:25 @agent_ppo2.py:179][0m |          -0.0425 |           1.9039 |           1.0800 |
[32m[20221208 14:40:26 @agent_ppo2.py:179][0m |          -0.0459 |           1.8398 |           1.0820 |
[32m[20221208 14:40:26 @agent_ppo2.py:179][0m |          -0.0500 |           1.8049 |           1.0869 |
[32m[20221208 14:40:26 @agent_ppo2.py:179][0m |          -0.0508 |           1.7689 |           1.0823 |
[32m[20221208 14:40:26 @agent_ppo2.py:179][0m |          -0.0541 |           1.7270 |           1.0885 |
[32m[20221208 14:40:26 @agent_ppo2.py:179][0m |          -0.0571 |           1.7008 |           1.0914 |
[32m[20221208 14:40:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.17
[32m[20221208 14:40:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.59
[32m[20221208 14:40:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.55
[32m[20221208 14:40:26 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 155.55
[32m[20221208 14:40:26 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 155.55
[32m[20221208 14:40:26 @agent_ppo2.py:137][0m Total time:       3.87 min
[32m[20221208 14:40:26 @agent_ppo2.py:139][0m 311296 total steps have happened
[32m[20221208 14:40:26 @agent_ppo2.py:115][0m #------------------------ Iteration 152 --------------------------#
[32m[20221208 14:40:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |           0.0149 |           2.5443 |           1.0632 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |           0.0097 |           2.2590 |           1.0559 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0065 |           2.1859 |           1.0483 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0245 |           2.1735 |           1.0568 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0231 |           2.1009 |           1.0544 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0328 |           2.0731 |           1.0616 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0406 |           2.0669 |           1.0673 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0430 |           2.0454 |           1.0690 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0413 |           2.0306 |           1.0738 |
[32m[20221208 14:40:27 @agent_ppo2.py:179][0m |          -0.0278 |           2.0313 |           1.0582 |
[32m[20221208 14:40:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.08
[32m[20221208 14:40:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.12
[32m[20221208 14:40:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.64
[32m[20221208 14:40:28 @agent_ppo2.py:137][0m Total time:       3.89 min
[32m[20221208 14:40:28 @agent_ppo2.py:139][0m 313344 total steps have happened
[32m[20221208 14:40:28 @agent_ppo2.py:115][0m #------------------------ Iteration 153 --------------------------#
[32m[20221208 14:40:28 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:40:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:28 @agent_ppo2.py:179][0m |           0.0143 |           1.8069 |           1.0788 |
[32m[20221208 14:40:28 @agent_ppo2.py:179][0m |          -0.0060 |           1.7389 |           1.0815 |
[32m[20221208 14:40:28 @agent_ppo2.py:179][0m |          -0.0194 |           1.7139 |           1.0838 |
[32m[20221208 14:40:28 @agent_ppo2.py:179][0m |          -0.0243 |           1.7030 |           1.0907 |
[32m[20221208 14:40:29 @agent_ppo2.py:179][0m |          -0.0283 |           1.6867 |           1.0971 |
[32m[20221208 14:40:29 @agent_ppo2.py:179][0m |          -0.0318 |           1.6645 |           1.0981 |
[32m[20221208 14:40:29 @agent_ppo2.py:179][0m |          -0.0357 |           1.6659 |           1.1026 |
[32m[20221208 14:40:29 @agent_ppo2.py:179][0m |          -0.0371 |           1.6428 |           1.1046 |
[32m[20221208 14:40:29 @agent_ppo2.py:179][0m |          -0.0367 |           1.6513 |           1.1073 |
[32m[20221208 14:40:29 @agent_ppo2.py:179][0m |          -0.0396 |           1.6226 |           1.1121 |
[32m[20221208 14:40:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.02
[32m[20221208 14:40:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.38
[32m[20221208 14:40:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.93
[32m[20221208 14:40:29 @agent_ppo2.py:137][0m Total time:       3.92 min
[32m[20221208 14:40:29 @agent_ppo2.py:139][0m 315392 total steps have happened
[32m[20221208 14:40:29 @agent_ppo2.py:115][0m #------------------------ Iteration 154 --------------------------#
[32m[20221208 14:40:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |           0.0163 |           2.4656 |           1.1420 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0058 |           2.2639 |           1.1405 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0220 |           2.1842 |           1.1465 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0306 |           2.1130 |           1.1541 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0336 |           2.0642 |           1.1607 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0397 |           2.0186 |           1.1618 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0425 |           2.0130 |           1.1684 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0421 |           1.9649 |           1.1677 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0469 |           1.9571 |           1.1725 |
[32m[20221208 14:40:30 @agent_ppo2.py:179][0m |          -0.0479 |           1.9363 |           1.1731 |
[32m[20221208 14:40:30 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.16
[32m[20221208 14:40:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.83
[32m[20221208 14:40:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.47
[32m[20221208 14:40:31 @agent_ppo2.py:137][0m Total time:       3.94 min
[32m[20221208 14:40:31 @agent_ppo2.py:139][0m 317440 total steps have happened
[32m[20221208 14:40:31 @agent_ppo2.py:115][0m #------------------------ Iteration 155 --------------------------#
[32m[20221208 14:40:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:31 @agent_ppo2.py:179][0m |           0.0145 |           2.5340 |           1.1390 |
[32m[20221208 14:40:31 @agent_ppo2.py:179][0m |           0.0002 |           2.2812 |           1.1331 |
[32m[20221208 14:40:31 @agent_ppo2.py:179][0m |          -0.0162 |           2.1481 |           1.1425 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0276 |           2.0925 |           1.1471 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0357 |           2.0374 |           1.1455 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0410 |           1.9750 |           1.1496 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0446 |           1.9715 |           1.1478 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0447 |           1.9424 |           1.1510 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0479 |           1.9340 |           1.1517 |
[32m[20221208 14:40:32 @agent_ppo2.py:179][0m |          -0.0499 |           1.9042 |           1.1539 |
[32m[20221208 14:40:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.90
[32m[20221208 14:40:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.03
[32m[20221208 14:40:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.55
[32m[20221208 14:40:32 @agent_ppo2.py:137][0m Total time:       3.97 min
[32m[20221208 14:40:32 @agent_ppo2.py:139][0m 319488 total steps have happened
[32m[20221208 14:40:32 @agent_ppo2.py:115][0m #------------------------ Iteration 156 --------------------------#
[32m[20221208 14:40:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:40:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |           0.0122 |           4.2854 |           1.1779 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0135 |           4.0303 |           1.1679 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0281 |           3.9322 |           1.1773 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0299 |           3.8563 |           1.1763 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0387 |           3.8148 |           1.1783 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0440 |           3.7543 |           1.1866 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0461 |           3.7207 |           1.1919 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0471 |           3.6699 |           1.1885 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0489 |           3.6242 |           1.1945 |
[32m[20221208 14:40:33 @agent_ppo2.py:179][0m |          -0.0506 |           3.5723 |           1.1990 |
[32m[20221208 14:40:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.73
[32m[20221208 14:40:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.62
[32m[20221208 14:40:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.07
[32m[20221208 14:40:34 @agent_ppo2.py:137][0m Total time:       3.99 min
[32m[20221208 14:40:34 @agent_ppo2.py:139][0m 321536 total steps have happened
[32m[20221208 14:40:34 @agent_ppo2.py:115][0m #------------------------ Iteration 157 --------------------------#
[32m[20221208 14:40:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:34 @agent_ppo2.py:179][0m |           0.0198 |           2.5502 |           1.1861 |
[32m[20221208 14:40:34 @agent_ppo2.py:179][0m |          -0.0111 |           2.2407 |           1.1758 |
[32m[20221208 14:40:34 @agent_ppo2.py:179][0m |          -0.0210 |           2.1527 |           1.1871 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0305 |           2.0908 |           1.1865 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0361 |           2.0543 |           1.1907 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0326 |           2.0289 |           1.1962 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0407 |           2.0095 |           1.1988 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0452 |           1.9891 |           1.2012 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0452 |           1.9785 |           1.2027 |
[32m[20221208 14:40:35 @agent_ppo2.py:179][0m |          -0.0481 |           1.9412 |           1.2048 |
[32m[20221208 14:40:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.42
[32m[20221208 14:40:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.66
[32m[20221208 14:40:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.33
[32m[20221208 14:40:35 @agent_ppo2.py:137][0m Total time:       4.02 min
[32m[20221208 14:40:35 @agent_ppo2.py:139][0m 323584 total steps have happened
[32m[20221208 14:40:35 @agent_ppo2.py:115][0m #------------------------ Iteration 158 --------------------------#
[32m[20221208 14:40:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |           0.0187 |           2.6253 |           1.2054 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |           0.0139 |           2.3347 |           1.1880 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0216 |           2.2305 |           1.2102 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0321 |           2.1636 |           1.2173 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0375 |           2.1262 |           1.2222 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0399 |           2.0729 |           1.2232 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0457 |           2.0329 |           1.2227 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0444 |           2.0115 |           1.2301 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0470 |           1.9392 |           1.2314 |
[32m[20221208 14:40:36 @agent_ppo2.py:179][0m |          -0.0485 |           1.8996 |           1.2293 |
[32m[20221208 14:40:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.24
[32m[20221208 14:40:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.16
[32m[20221208 14:40:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.86
[32m[20221208 14:40:37 @agent_ppo2.py:137][0m Total time:       4.04 min
[32m[20221208 14:40:37 @agent_ppo2.py:139][0m 325632 total steps have happened
[32m[20221208 14:40:37 @agent_ppo2.py:115][0m #------------------------ Iteration 159 --------------------------#
[32m[20221208 14:40:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:37 @agent_ppo2.py:179][0m |           0.0070 |           3.2389 |           1.2713 |
[32m[20221208 14:40:37 @agent_ppo2.py:179][0m |          -0.0023 |           2.9623 |           1.2653 |
[32m[20221208 14:40:37 @agent_ppo2.py:179][0m |          -0.0133 |           2.8301 |           1.2468 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0331 |           2.7594 |           1.2798 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0415 |           2.6742 |           1.2822 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0440 |           2.6246 |           1.2914 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0466 |           2.5858 |           1.2902 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0511 |           2.5358 |           1.2941 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0540 |           2.4993 |           1.2936 |
[32m[20221208 14:40:38 @agent_ppo2.py:179][0m |          -0.0557 |           2.4652 |           1.3016 |
[32m[20221208 14:40:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.42
[32m[20221208 14:40:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.79
[32m[20221208 14:40:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.91
[32m[20221208 14:40:38 @agent_ppo2.py:137][0m Total time:       4.07 min
[32m[20221208 14:40:38 @agent_ppo2.py:139][0m 327680 total steps have happened
[32m[20221208 14:40:38 @agent_ppo2.py:115][0m #------------------------ Iteration 160 --------------------------#
[32m[20221208 14:40:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |           0.0209 |           3.0251 |           1.3111 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0060 |           2.8050 |           1.3146 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0272 |           2.7255 |           1.3290 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0353 |           2.6509 |           1.3337 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0402 |           2.5931 |           1.3331 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0423 |           2.5154 |           1.3357 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0491 |           2.4925 |           1.3398 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0514 |           2.4519 |           1.3412 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0528 |           2.4287 |           1.3444 |
[32m[20221208 14:40:39 @agent_ppo2.py:179][0m |          -0.0550 |           2.3972 |           1.3413 |
[32m[20221208 14:40:39 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:40:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.80
[32m[20221208 14:40:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.07
[32m[20221208 14:40:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.56
[32m[20221208 14:40:40 @agent_ppo2.py:137][0m Total time:       4.09 min
[32m[20221208 14:40:40 @agent_ppo2.py:139][0m 329728 total steps have happened
[32m[20221208 14:40:40 @agent_ppo2.py:115][0m #------------------------ Iteration 161 --------------------------#
[32m[20221208 14:40:40 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:40:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:40 @agent_ppo2.py:179][0m |           0.0894 |           1.3465 |           1.3383 |
[32m[20221208 14:40:40 @agent_ppo2.py:179][0m |           0.0691 |           0.9786 |           1.1947 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |           0.0122 |           0.9015 |           1.2234 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0102 |           0.8221 |           1.2674 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0188 |           0.8015 |           1.2928 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0255 |           0.7824 |           1.3083 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0296 |           0.7582 |           1.3101 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0348 |           0.7490 |           1.3239 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0385 |           0.7317 |           1.3237 |
[32m[20221208 14:40:41 @agent_ppo2.py:179][0m |          -0.0430 |           0.7247 |           1.3356 |
[32m[20221208 14:40:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.12
[32m[20221208 14:40:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.16
[32m[20221208 14:40:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.59
[32m[20221208 14:40:41 @agent_ppo2.py:137][0m Total time:       4.12 min
[32m[20221208 14:40:41 @agent_ppo2.py:139][0m 331776 total steps have happened
[32m[20221208 14:40:41 @agent_ppo2.py:115][0m #------------------------ Iteration 162 --------------------------#
[32m[20221208 14:40:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |           0.0096 |           3.0640 |           1.3481 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0148 |           2.5506 |           1.3466 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0193 |           2.3777 |           1.3530 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0334 |           2.2161 |           1.3645 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0352 |           2.1397 |           1.3653 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0398 |           2.0499 |           1.3718 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0421 |           1.9850 |           1.3713 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0442 |           1.9308 |           1.3745 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0439 |           1.9114 |           1.3753 |
[32m[20221208 14:40:42 @agent_ppo2.py:179][0m |          -0.0455 |           1.8604 |           1.3829 |
[32m[20221208 14:40:42 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.70
[32m[20221208 14:40:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.49
[32m[20221208 14:40:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.12
[32m[20221208 14:40:43 @agent_ppo2.py:137][0m Total time:       4.14 min
[32m[20221208 14:40:43 @agent_ppo2.py:139][0m 333824 total steps have happened
[32m[20221208 14:40:43 @agent_ppo2.py:115][0m #------------------------ Iteration 163 --------------------------#
[32m[20221208 14:40:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:43 @agent_ppo2.py:179][0m |           0.0184 |           5.9949 |           1.4385 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0127 |           5.5579 |           1.4350 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0289 |           5.3659 |           1.4535 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0354 |           5.2654 |           1.4520 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0444 |           5.2188 |           1.4630 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0456 |           5.1508 |           1.4594 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0518 |           5.1389 |           1.4689 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0524 |           5.0912 |           1.4682 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0576 |           5.0263 |           1.4775 |
[32m[20221208 14:40:44 @agent_ppo2.py:179][0m |          -0.0587 |           5.0340 |           1.4782 |
[32m[20221208 14:40:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.08
[32m[20221208 14:40:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.83
[32m[20221208 14:40:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.09
[32m[20221208 14:40:44 @agent_ppo2.py:137][0m Total time:       4.17 min
[32m[20221208 14:40:44 @agent_ppo2.py:139][0m 335872 total steps have happened
[32m[20221208 14:40:44 @agent_ppo2.py:115][0m #------------------------ Iteration 164 --------------------------#
[32m[20221208 14:40:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |           0.0288 |           4.1671 |           1.4882 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |           0.0112 |           3.8425 |           1.4362 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0187 |           3.6998 |           1.4759 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0287 |           3.6273 |           1.4788 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0357 |           3.5642 |           1.4891 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0390 |           3.5540 |           1.4996 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0436 |           3.4957 |           1.5019 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0436 |           3.4637 |           1.4987 |
[32m[20221208 14:40:45 @agent_ppo2.py:179][0m |          -0.0455 |           3.4035 |           1.5044 |
[32m[20221208 14:40:46 @agent_ppo2.py:179][0m |          -0.0519 |           3.4165 |           1.5152 |
[32m[20221208 14:40:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.56
[32m[20221208 14:40:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.63
[32m[20221208 14:40:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.13
[32m[20221208 14:40:46 @agent_ppo2.py:137][0m Total time:       4.19 min
[32m[20221208 14:40:46 @agent_ppo2.py:139][0m 337920 total steps have happened
[32m[20221208 14:40:46 @agent_ppo2.py:115][0m #------------------------ Iteration 165 --------------------------#
[32m[20221208 14:40:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:46 @agent_ppo2.py:179][0m |           0.0038 |           2.4133 |           1.5277 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0191 |           2.3011 |           1.5256 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0290 |           2.2498 |           1.5369 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0342 |           2.1957 |           1.5373 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0361 |           2.1488 |           1.5353 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0408 |           2.1129 |           1.5412 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0444 |           2.1026 |           1.5513 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0458 |           2.0747 |           1.5515 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0478 |           2.0398 |           1.5532 |
[32m[20221208 14:40:47 @agent_ppo2.py:179][0m |          -0.0504 |           2.0247 |           1.5613 |
[32m[20221208 14:40:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.98
[32m[20221208 14:40:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.15
[32m[20221208 14:40:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.67
[32m[20221208 14:40:47 @agent_ppo2.py:137][0m Total time:       4.22 min
[32m[20221208 14:40:47 @agent_ppo2.py:139][0m 339968 total steps have happened
[32m[20221208 14:40:47 @agent_ppo2.py:115][0m #------------------------ Iteration 166 --------------------------#
[32m[20221208 14:40:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:40:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |           0.0336 |           5.8955 |           1.5768 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |           0.0014 |           5.5986 |           1.5523 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0281 |           5.4429 |           1.5685 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0391 |           5.2998 |           1.5892 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0451 |           5.2149 |           1.5900 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0483 |           5.1323 |           1.5944 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0536 |           5.0406 |           1.5970 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0569 |           4.9923 |           1.6057 |
[32m[20221208 14:40:48 @agent_ppo2.py:179][0m |          -0.0573 |           4.9355 |           1.5975 |
[32m[20221208 14:40:49 @agent_ppo2.py:179][0m |          -0.0609 |           4.8872 |           1.6047 |
[32m[20221208 14:40:49 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:40:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.75
[32m[20221208 14:40:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.33
[32m[20221208 14:40:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.87
[32m[20221208 14:40:49 @agent_ppo2.py:137][0m Total time:       4.24 min
[32m[20221208 14:40:49 @agent_ppo2.py:139][0m 342016 total steps have happened
[32m[20221208 14:40:49 @agent_ppo2.py:115][0m #------------------------ Iteration 167 --------------------------#
[32m[20221208 14:40:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:49 @agent_ppo2.py:179][0m |           0.0110 |           2.7092 |           1.6376 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0205 |           2.4156 |           1.6317 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0248 |           2.3140 |           1.6269 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0373 |           2.2388 |           1.6446 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0432 |           2.2142 |           1.6458 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0505 |           2.1639 |           1.6595 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0486 |           2.1328 |           1.6568 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0562 |           2.1125 |           1.6629 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0603 |           2.0819 |           1.6676 |
[32m[20221208 14:40:50 @agent_ppo2.py:179][0m |          -0.0618 |           2.0781 |           1.6729 |
[32m[20221208 14:40:50 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:40:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.38
[32m[20221208 14:40:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.16
[32m[20221208 14:40:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.81
[32m[20221208 14:40:50 @agent_ppo2.py:137][0m Total time:       4.27 min
[32m[20221208 14:40:50 @agent_ppo2.py:139][0m 344064 total steps have happened
[32m[20221208 14:40:50 @agent_ppo2.py:115][0m #------------------------ Iteration 168 --------------------------#
[32m[20221208 14:40:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |           0.0141 |           3.9717 |           1.6499 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0127 |           3.7365 |           1.6493 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0303 |           3.5776 |           1.6519 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0333 |           3.4929 |           1.6656 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0390 |           3.3569 |           1.6679 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0433 |           3.2840 |           1.6676 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0481 |           3.2288 |           1.6797 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0497 |           3.2180 |           1.6843 |
[32m[20221208 14:40:51 @agent_ppo2.py:179][0m |          -0.0524 |           3.0862 |           1.6895 |
[32m[20221208 14:40:52 @agent_ppo2.py:179][0m |          -0.0538 |           3.0493 |           1.6924 |
[32m[20221208 14:40:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.57
[32m[20221208 14:40:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.00
[32m[20221208 14:40:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.95
[32m[20221208 14:40:52 @agent_ppo2.py:137][0m Total time:       4.30 min
[32m[20221208 14:40:52 @agent_ppo2.py:139][0m 346112 total steps have happened
[32m[20221208 14:40:52 @agent_ppo2.py:115][0m #------------------------ Iteration 169 --------------------------#
[32m[20221208 14:40:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |           0.0174 |           2.1923 |           1.7130 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0055 |           1.9583 |           1.7030 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0234 |           1.8913 |           1.7219 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0347 |           1.8462 |           1.7325 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0384 |           1.8168 |           1.7314 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0400 |           1.7948 |           1.7415 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0442 |           1.7734 |           1.7427 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0471 |           1.7477 |           1.7452 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0482 |           1.7439 |           1.7499 |
[32m[20221208 14:40:53 @agent_ppo2.py:179][0m |          -0.0513 |           1.7182 |           1.7507 |
[32m[20221208 14:40:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:40:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.41
[32m[20221208 14:40:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.45
[32m[20221208 14:40:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.37
[32m[20221208 14:40:54 @agent_ppo2.py:137][0m Total time:       4.32 min
[32m[20221208 14:40:54 @agent_ppo2.py:139][0m 348160 total steps have happened
[32m[20221208 14:40:54 @agent_ppo2.py:115][0m #------------------------ Iteration 170 --------------------------#
[32m[20221208 14:40:54 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:40:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |           0.0291 |           2.3435 |           1.7535 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0068 |           1.9904 |           1.7382 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0237 |           1.8550 |           1.7474 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0289 |           1.7728 |           1.7474 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0333 |           1.7228 |           1.7423 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0370 |           1.6973 |           1.7458 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0372 |           1.6552 |           1.7402 |
[32m[20221208 14:40:54 @agent_ppo2.py:179][0m |          -0.0421 |           1.6269 |           1.7471 |
[32m[20221208 14:40:55 @agent_ppo2.py:179][0m |          -0.0431 |           1.6224 |           1.7477 |
[32m[20221208 14:40:55 @agent_ppo2.py:179][0m |          -0.0461 |           1.6113 |           1.7544 |
[32m[20221208 14:40:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:40:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.67
[32m[20221208 14:40:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.01
[32m[20221208 14:40:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.01
[32m[20221208 14:40:55 @agent_ppo2.py:137][0m Total time:       4.35 min
[32m[20221208 14:40:55 @agent_ppo2.py:139][0m 350208 total steps have happened
[32m[20221208 14:40:55 @agent_ppo2.py:115][0m #------------------------ Iteration 171 --------------------------#
[32m[20221208 14:40:55 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:40:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |           0.0134 |           2.6981 |           1.7132 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |           0.0012 |           2.4179 |           1.6958 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0189 |           2.3260 |           1.7154 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0294 |           2.2424 |           1.7290 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0375 |           2.1795 |           1.7356 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0393 |           2.1024 |           1.7405 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0402 |           2.0506 |           1.7485 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0420 |           2.0307 |           1.7412 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0442 |           1.9246 |           1.7579 |
[32m[20221208 14:40:56 @agent_ppo2.py:179][0m |          -0.0471 |           1.8891 |           1.7613 |
[32m[20221208 14:40:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:40:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.58
[32m[20221208 14:40:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.40
[32m[20221208 14:40:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.20
[32m[20221208 14:40:57 @agent_ppo2.py:137][0m Total time:       4.37 min
[32m[20221208 14:40:57 @agent_ppo2.py:139][0m 352256 total steps have happened
[32m[20221208 14:40:57 @agent_ppo2.py:115][0m #------------------------ Iteration 172 --------------------------#
[32m[20221208 14:40:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |           0.0289 |           3.8383 |           1.7439 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0059 |           3.1043 |           1.7497 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0221 |           2.8747 |           1.7542 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0324 |           2.7363 |           1.7507 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0371 |           2.6650 |           1.7614 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0458 |           2.5704 |           1.7586 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0547 |           2.5128 |           1.7730 |
[32m[20221208 14:40:57 @agent_ppo2.py:179][0m |          -0.0555 |           2.4417 |           1.7747 |
[32m[20221208 14:40:58 @agent_ppo2.py:179][0m |          -0.0603 |           2.4399 |           1.7785 |
[32m[20221208 14:40:58 @agent_ppo2.py:179][0m |          -0.0646 |           2.3796 |           1.7848 |
[32m[20221208 14:40:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:40:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.67
[32m[20221208 14:40:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.97
[32m[20221208 14:40:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.45
[32m[20221208 14:40:58 @agent_ppo2.py:137][0m Total time:       4.40 min
[32m[20221208 14:40:58 @agent_ppo2.py:139][0m 354304 total steps have happened
[32m[20221208 14:40:58 @agent_ppo2.py:115][0m #------------------------ Iteration 173 --------------------------#
[32m[20221208 14:40:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:40:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |           0.0194 |           4.3860 |           1.8548 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0076 |           4.1207 |           1.8689 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0311 |           4.0109 |           1.8847 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0382 |           3.9525 |           1.8802 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0419 |           3.9058 |           1.8879 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0454 |           3.8060 |           1.8840 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0516 |           3.8065 |           1.8892 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0525 |           3.7547 |           1.8911 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0540 |           3.7324 |           1.9040 |
[32m[20221208 14:40:59 @agent_ppo2.py:179][0m |          -0.0581 |           3.6938 |           1.9041 |
[32m[20221208 14:40:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.42
[32m[20221208 14:41:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.36
[32m[20221208 14:41:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.19
[32m[20221208 14:41:00 @agent_ppo2.py:137][0m Total time:       4.42 min
[32m[20221208 14:41:00 @agent_ppo2.py:139][0m 356352 total steps have happened
[32m[20221208 14:41:00 @agent_ppo2.py:115][0m #------------------------ Iteration 174 --------------------------#
[32m[20221208 14:41:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |           0.0248 |           3.6519 |           1.8516 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0098 |           3.1088 |           1.8398 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0275 |           2.9186 |           1.8411 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0370 |           2.8145 |           1.8456 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0459 |           2.6949 |           1.8650 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0458 |           2.6169 |           1.8711 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0504 |           2.5431 |           1.8680 |
[32m[20221208 14:41:00 @agent_ppo2.py:179][0m |          -0.0539 |           2.4966 |           1.8750 |
[32m[20221208 14:41:01 @agent_ppo2.py:179][0m |          -0.0557 |           2.4706 |           1.8771 |
[32m[20221208 14:41:01 @agent_ppo2.py:179][0m |          -0.0565 |           2.4380 |           1.8797 |
[32m[20221208 14:41:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.70
[32m[20221208 14:41:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.39
[32m[20221208 14:41:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.27
[32m[20221208 14:41:01 @agent_ppo2.py:137][0m Total time:       4.45 min
[32m[20221208 14:41:01 @agent_ppo2.py:139][0m 358400 total steps have happened
[32m[20221208 14:41:01 @agent_ppo2.py:115][0m #------------------------ Iteration 175 --------------------------#
[32m[20221208 14:41:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |           0.0285 |           5.0622 |           1.8791 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0036 |           4.6575 |           1.8916 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0245 |           4.5011 |           1.8934 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0354 |           4.4113 |           1.9114 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0387 |           4.3526 |           1.9126 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0435 |           4.2658 |           1.9176 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0469 |           4.2384 |           1.9288 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0489 |           4.2179 |           1.9324 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0523 |           4.1462 |           1.9380 |
[32m[20221208 14:41:02 @agent_ppo2.py:179][0m |          -0.0541 |           4.1311 |           1.9416 |
[32m[20221208 14:41:02 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:41:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.73
[32m[20221208 14:41:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.87
[32m[20221208 14:41:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.96
[32m[20221208 14:41:03 @agent_ppo2.py:137][0m Total time:       4.47 min
[32m[20221208 14:41:03 @agent_ppo2.py:139][0m 360448 total steps have happened
[32m[20221208 14:41:03 @agent_ppo2.py:115][0m #------------------------ Iteration 176 --------------------------#
[32m[20221208 14:41:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |           0.0212 |           4.7573 |           1.9869 |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |          -0.0109 |           4.4994 |           1.9687 |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |          -0.0213 |           4.3834 |           1.9684 |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |          -0.0354 |           4.3096 |           1.9843 |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |          -0.0415 |           4.2342 |           1.9874 |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |          -0.0487 |           4.1741 |           2.0061 |
[32m[20221208 14:41:03 @agent_ppo2.py:179][0m |          -0.0513 |           4.1616 |           2.0046 |
[32m[20221208 14:41:04 @agent_ppo2.py:179][0m |          -0.0557 |           4.1139 |           2.0125 |
[32m[20221208 14:41:04 @agent_ppo2.py:179][0m |          -0.0574 |           4.0840 |           2.0144 |
[32m[20221208 14:41:04 @agent_ppo2.py:179][0m |          -0.0590 |           4.0721 |           2.0219 |
[32m[20221208 14:41:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.03
[32m[20221208 14:41:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.16
[32m[20221208 14:41:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.68
[32m[20221208 14:41:04 @agent_ppo2.py:137][0m Total time:       4.50 min
[32m[20221208 14:41:04 @agent_ppo2.py:139][0m 362496 total steps have happened
[32m[20221208 14:41:04 @agent_ppo2.py:115][0m #------------------------ Iteration 177 --------------------------#
[32m[20221208 14:41:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |           0.0163 |           4.2138 |           2.0375 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0026 |           4.0553 |           2.0169 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0177 |           3.9794 |           2.0381 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0297 |           3.8968 |           2.0352 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0391 |           3.8947 |           2.0480 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0416 |           3.8411 |           2.0450 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0442 |           3.7829 |           2.0508 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0490 |           3.7713 |           2.0622 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0505 |           3.7232 |           2.0607 |
[32m[20221208 14:41:05 @agent_ppo2.py:179][0m |          -0.0525 |           3.6916 |           2.0695 |
[32m[20221208 14:41:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.37
[32m[20221208 14:41:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.30
[32m[20221208 14:41:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.31
[32m[20221208 14:41:06 @agent_ppo2.py:137][0m Total time:       4.52 min
[32m[20221208 14:41:06 @agent_ppo2.py:139][0m 364544 total steps have happened
[32m[20221208 14:41:06 @agent_ppo2.py:115][0m #------------------------ Iteration 178 --------------------------#
[32m[20221208 14:41:06 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:06 @agent_ppo2.py:179][0m |           0.0178 |           0.3987 |           2.0747 |
[32m[20221208 14:41:06 @agent_ppo2.py:179][0m |          -0.0038 |           0.3318 |           2.0656 |
[32m[20221208 14:41:06 @agent_ppo2.py:179][0m |          -0.0102 |           0.3216 |           2.0550 |
[32m[20221208 14:41:06 @agent_ppo2.py:179][0m |          -0.0129 |           0.3183 |           2.0502 |
[32m[20221208 14:41:06 @agent_ppo2.py:179][0m |          -0.0147 |           0.3132 |           2.0501 |
[32m[20221208 14:41:06 @agent_ppo2.py:179][0m |          -0.0191 |           0.3128 |           2.0506 |
[32m[20221208 14:41:07 @agent_ppo2.py:179][0m |          -0.0164 |           0.3096 |           2.0553 |
[32m[20221208 14:41:07 @agent_ppo2.py:179][0m |          -0.0153 |           0.3086 |           2.0412 |
[32m[20221208 14:41:07 @agent_ppo2.py:179][0m |          -0.0172 |           0.3065 |           2.0581 |
[32m[20221208 14:41:07 @agent_ppo2.py:179][0m |          -0.0180 |           0.3048 |           2.0657 |
[32m[20221208 14:41:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.66
[32m[20221208 14:41:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.03
[32m[20221208 14:41:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.73
[32m[20221208 14:41:07 @agent_ppo2.py:137][0m Total time:       4.55 min
[32m[20221208 14:41:07 @agent_ppo2.py:139][0m 366592 total steps have happened
[32m[20221208 14:41:07 @agent_ppo2.py:115][0m #------------------------ Iteration 179 --------------------------#
[32m[20221208 14:41:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |           0.0161 |           5.8338 |           2.0384 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0133 |           5.4417 |           2.0188 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0335 |           5.3078 |           2.0249 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0374 |           5.0919 |           2.0280 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0454 |           5.0024 |           2.0387 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0481 |           4.9453 |           2.0367 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0522 |           4.8934 |           2.0437 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0555 |           4.8304 |           2.0486 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0558 |           4.7591 |           2.0483 |
[32m[20221208 14:41:08 @agent_ppo2.py:179][0m |          -0.0597 |           4.7388 |           2.0561 |
[32m[20221208 14:41:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.26
[32m[20221208 14:41:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.16
[32m[20221208 14:41:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.56
[32m[20221208 14:41:09 @agent_ppo2.py:137][0m Total time:       4.57 min
[32m[20221208 14:41:09 @agent_ppo2.py:139][0m 368640 total steps have happened
[32m[20221208 14:41:09 @agent_ppo2.py:115][0m #------------------------ Iteration 180 --------------------------#
[32m[20221208 14:41:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:09 @agent_ppo2.py:179][0m |           0.0235 |           2.4512 |           2.0847 |
[32m[20221208 14:41:09 @agent_ppo2.py:179][0m |          -0.0173 |           2.1354 |           2.0971 |
[32m[20221208 14:41:09 @agent_ppo2.py:179][0m |          -0.0265 |           2.0233 |           2.0949 |
[32m[20221208 14:41:09 @agent_ppo2.py:179][0m |          -0.0351 |           1.9208 |           2.0901 |
[32m[20221208 14:41:09 @agent_ppo2.py:179][0m |          -0.0412 |           1.8628 |           2.0895 |
[32m[20221208 14:41:09 @agent_ppo2.py:179][0m |          -0.0464 |           1.8121 |           2.1006 |
[32m[20221208 14:41:10 @agent_ppo2.py:179][0m |          -0.0479 |           1.7518 |           2.1005 |
[32m[20221208 14:41:10 @agent_ppo2.py:179][0m |          -0.0510 |           1.7147 |           2.1108 |
[32m[20221208 14:41:10 @agent_ppo2.py:179][0m |          -0.0539 |           1.6628 |           2.1073 |
[32m[20221208 14:41:10 @agent_ppo2.py:179][0m |          -0.0538 |           1.6369 |           2.1172 |
[32m[20221208 14:41:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.75
[32m[20221208 14:41:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.93
[32m[20221208 14:41:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.95
[32m[20221208 14:41:10 @agent_ppo2.py:137][0m Total time:       4.60 min
[32m[20221208 14:41:10 @agent_ppo2.py:139][0m 370688 total steps have happened
[32m[20221208 14:41:10 @agent_ppo2.py:115][0m #------------------------ Iteration 181 --------------------------#
[32m[20221208 14:41:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |           0.0288 |           4.7933 |           2.0750 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0097 |           4.2922 |           2.1003 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0314 |           4.1298 |           2.1002 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0395 |           4.0622 |           2.1076 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0447 |           4.0046 |           2.1091 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0518 |           3.9504 |           2.1189 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0580 |           3.8936 |           2.1283 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0601 |           3.8800 |           2.1369 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0608 |           3.8500 |           2.1342 |
[32m[20221208 14:41:11 @agent_ppo2.py:179][0m |          -0.0647 |           3.8150 |           2.1411 |
[32m[20221208 14:41:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:41:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.03
[32m[20221208 14:41:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.51
[32m[20221208 14:41:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.99
[32m[20221208 14:41:12 @agent_ppo2.py:137][0m Total time:       4.62 min
[32m[20221208 14:41:12 @agent_ppo2.py:139][0m 372736 total steps have happened
[32m[20221208 14:41:12 @agent_ppo2.py:115][0m #------------------------ Iteration 182 --------------------------#
[32m[20221208 14:41:12 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:12 @agent_ppo2.py:179][0m |           0.0071 |           0.3431 |           2.1298 |
[32m[20221208 14:41:12 @agent_ppo2.py:179][0m |          -0.0082 |           0.2746 |           2.1068 |
[32m[20221208 14:41:12 @agent_ppo2.py:179][0m |          -0.0163 |           0.2579 |           2.1077 |
[32m[20221208 14:41:12 @agent_ppo2.py:179][0m |          -0.0207 |           0.2511 |           2.1096 |
[32m[20221208 14:41:12 @agent_ppo2.py:179][0m |          -0.0202 |           0.2443 |           2.1131 |
[32m[20221208 14:41:12 @agent_ppo2.py:179][0m |          -0.0226 |           0.2402 |           2.1051 |
[32m[20221208 14:41:13 @agent_ppo2.py:179][0m |          -0.0238 |           0.2348 |           2.1076 |
[32m[20221208 14:41:13 @agent_ppo2.py:179][0m |          -0.0229 |           0.2360 |           2.1040 |
[32m[20221208 14:41:13 @agent_ppo2.py:179][0m |          -0.0284 |           0.2311 |           2.1088 |
[32m[20221208 14:41:13 @agent_ppo2.py:179][0m |          -0.0262 |           0.2307 |           2.1115 |
[32m[20221208 14:41:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:41:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.15
[32m[20221208 14:41:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.36
[32m[20221208 14:41:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.36
[32m[20221208 14:41:13 @agent_ppo2.py:137][0m Total time:       4.65 min
[32m[20221208 14:41:13 @agent_ppo2.py:139][0m 374784 total steps have happened
[32m[20221208 14:41:13 @agent_ppo2.py:115][0m #------------------------ Iteration 183 --------------------------#
[32m[20221208 14:41:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |           0.0122 |           2.8309 |           2.1169 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0206 |           2.5404 |           2.1017 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0331 |           2.4178 |           2.1122 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0428 |           2.3251 |           2.1231 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0489 |           2.2739 |           2.1290 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0525 |           2.2379 |           2.1255 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0553 |           2.2086 |           2.1351 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0597 |           2.1542 |           2.1314 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0607 |           2.1470 |           2.1428 |
[32m[20221208 14:41:14 @agent_ppo2.py:179][0m |          -0.0635 |           2.1132 |           2.1377 |
[32m[20221208 14:41:14 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:41:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.79
[32m[20221208 14:41:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.41
[32m[20221208 14:41:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.62
[32m[20221208 14:41:15 @agent_ppo2.py:137][0m Total time:       4.67 min
[32m[20221208 14:41:15 @agent_ppo2.py:139][0m 376832 total steps have happened
[32m[20221208 14:41:15 @agent_ppo2.py:115][0m #------------------------ Iteration 184 --------------------------#
[32m[20221208 14:41:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:15 @agent_ppo2.py:179][0m |           0.0044 |           0.3596 |           2.2008 |
[32m[20221208 14:41:15 @agent_ppo2.py:179][0m |          -0.0078 |           0.2731 |           2.1886 |
[32m[20221208 14:41:15 @agent_ppo2.py:179][0m |          -0.0142 |           0.2541 |           2.1769 |
[32m[20221208 14:41:15 @agent_ppo2.py:179][0m |          -0.0180 |           0.2439 |           2.1895 |
[32m[20221208 14:41:15 @agent_ppo2.py:179][0m |          -0.0201 |           0.2370 |           2.1905 |
[32m[20221208 14:41:15 @agent_ppo2.py:179][0m |          -0.0233 |           0.2339 |           2.1876 |
[32m[20221208 14:41:16 @agent_ppo2.py:179][0m |          -0.0179 |           0.2326 |           2.1697 |
[32m[20221208 14:41:16 @agent_ppo2.py:179][0m |          -0.0240 |           0.2276 |           2.1865 |
[32m[20221208 14:41:16 @agent_ppo2.py:179][0m |          -0.0238 |           0.2267 |           2.1907 |
[32m[20221208 14:41:16 @agent_ppo2.py:179][0m |          -0.0211 |           0.2217 |           2.1754 |
[32m[20221208 14:41:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:41:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.26
[32m[20221208 14:41:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.70
[32m[20221208 14:41:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.41
[32m[20221208 14:41:16 @agent_ppo2.py:137][0m Total time:       4.70 min
[32m[20221208 14:41:16 @agent_ppo2.py:139][0m 378880 total steps have happened
[32m[20221208 14:41:16 @agent_ppo2.py:115][0m #------------------------ Iteration 185 --------------------------#
[32m[20221208 14:41:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |           0.0317 |           3.6631 |           2.1403 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0072 |           3.2733 |           2.1308 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0269 |           3.1577 |           2.1491 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0360 |           3.0284 |           2.1588 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0432 |           2.9555 |           2.1679 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0461 |           2.8700 |           2.1765 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0484 |           2.8145 |           2.1770 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0517 |           2.7731 |           2.1817 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0561 |           2.7014 |           2.1880 |
[32m[20221208 14:41:17 @agent_ppo2.py:179][0m |          -0.0567 |           2.6142 |           2.1930 |
[32m[20221208 14:41:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.48
[32m[20221208 14:41:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.47
[32m[20221208 14:41:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.55
[32m[20221208 14:41:18 @agent_ppo2.py:137][0m Total time:       4.72 min
[32m[20221208 14:41:18 @agent_ppo2.py:139][0m 380928 total steps have happened
[32m[20221208 14:41:18 @agent_ppo2.py:115][0m #------------------------ Iteration 186 --------------------------#
[32m[20221208 14:41:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:18 @agent_ppo2.py:179][0m |           0.0270 |           3.3545 |           2.2162 |
[32m[20221208 14:41:18 @agent_ppo2.py:179][0m |          -0.0041 |           2.8913 |           2.1972 |
[32m[20221208 14:41:18 @agent_ppo2.py:179][0m |          -0.0209 |           2.7411 |           2.2284 |
[32m[20221208 14:41:18 @agent_ppo2.py:179][0m |          -0.0298 |           2.6084 |           2.2246 |
[32m[20221208 14:41:18 @agent_ppo2.py:179][0m |          -0.0399 |           2.5519 |           2.2384 |
[32m[20221208 14:41:18 @agent_ppo2.py:179][0m |          -0.0424 |           2.4903 |           2.2331 |
[32m[20221208 14:41:19 @agent_ppo2.py:179][0m |          -0.0479 |           2.4465 |           2.2446 |
[32m[20221208 14:41:19 @agent_ppo2.py:179][0m |          -0.0545 |           2.3968 |           2.2559 |
[32m[20221208 14:41:19 @agent_ppo2.py:179][0m |          -0.0564 |           2.3713 |           2.2585 |
[32m[20221208 14:41:19 @agent_ppo2.py:179][0m |          -0.0575 |           2.3282 |           2.2603 |
[32m[20221208 14:41:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.69
[32m[20221208 14:41:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.10
[32m[20221208 14:41:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.03
[32m[20221208 14:41:19 @agent_ppo2.py:137][0m Total time:       4.75 min
[32m[20221208 14:41:19 @agent_ppo2.py:139][0m 382976 total steps have happened
[32m[20221208 14:41:19 @agent_ppo2.py:115][0m #------------------------ Iteration 187 --------------------------#
[32m[20221208 14:41:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |           0.0213 |           4.0112 |           2.1766 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0147 |           3.0815 |           2.1537 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0316 |           2.6832 |           2.1671 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0427 |           2.4976 |           2.1685 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0484 |           2.3839 |           2.1771 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0537 |           2.3202 |           2.1729 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0571 |           2.2809 |           2.1773 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0595 |           2.2273 |           2.1821 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0611 |           2.1838 |           2.1733 |
[32m[20221208 14:41:20 @agent_ppo2.py:179][0m |          -0.0633 |           2.1149 |           2.1847 |
[32m[20221208 14:41:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.42
[32m[20221208 14:41:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.64
[32m[20221208 14:41:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.20
[32m[20221208 14:41:21 @agent_ppo2.py:137][0m Total time:       4.77 min
[32m[20221208 14:41:21 @agent_ppo2.py:139][0m 385024 total steps have happened
[32m[20221208 14:41:21 @agent_ppo2.py:115][0m #------------------------ Iteration 188 --------------------------#
[32m[20221208 14:41:21 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:41:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:21 @agent_ppo2.py:179][0m |           0.0369 |           1.0276 |           2.1740 |
[32m[20221208 14:41:21 @agent_ppo2.py:179][0m |           0.0049 |           0.6344 |           2.1589 |
[32m[20221208 14:41:21 @agent_ppo2.py:179][0m |          -0.0228 |           0.5519 |           2.1818 |
[32m[20221208 14:41:21 @agent_ppo2.py:179][0m |          -0.0355 |           0.5139 |           2.1843 |
[32m[20221208 14:41:21 @agent_ppo2.py:179][0m |          -0.0388 |           0.4939 |           2.1955 |
[32m[20221208 14:41:21 @agent_ppo2.py:179][0m |          -0.0414 |           0.4735 |           2.1956 |
[32m[20221208 14:41:22 @agent_ppo2.py:179][0m |          -0.0458 |           0.4623 |           2.2103 |
[32m[20221208 14:41:22 @agent_ppo2.py:179][0m |          -0.0461 |           0.4508 |           2.2084 |
[32m[20221208 14:41:22 @agent_ppo2.py:179][0m |          -0.0488 |           0.4425 |           2.2140 |
[32m[20221208 14:41:22 @agent_ppo2.py:179][0m |          -0.0484 |           0.4361 |           2.2191 |
[32m[20221208 14:41:22 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:41:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.77
[32m[20221208 14:41:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.23
[32m[20221208 14:41:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.02
[32m[20221208 14:41:22 @agent_ppo2.py:137][0m Total time:       4.80 min
[32m[20221208 14:41:22 @agent_ppo2.py:139][0m 387072 total steps have happened
[32m[20221208 14:41:22 @agent_ppo2.py:115][0m #------------------------ Iteration 189 --------------------------#
[32m[20221208 14:41:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |           0.0069 |           2.1490 |           2.2368 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0201 |           1.4710 |           2.2154 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0297 |           1.2841 |           2.2344 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0433 |           1.2018 |           2.2532 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0513 |           1.1496 |           2.2586 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0538 |           1.1041 |           2.2506 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0581 |           1.0841 |           2.2564 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0591 |           1.0449 |           2.2530 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0595 |           1.0273 |           2.2619 |
[32m[20221208 14:41:23 @agent_ppo2.py:179][0m |          -0.0626 |           1.0246 |           2.2695 |
[32m[20221208 14:41:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:41:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.04
[32m[20221208 14:41:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.08
[32m[20221208 14:41:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.92
[32m[20221208 14:41:24 @agent_ppo2.py:137][0m Total time:       4.82 min
[32m[20221208 14:41:24 @agent_ppo2.py:139][0m 389120 total steps have happened
[32m[20221208 14:41:24 @agent_ppo2.py:115][0m #------------------------ Iteration 190 --------------------------#
[32m[20221208 14:41:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:24 @agent_ppo2.py:179][0m |           0.0273 |           1.3078 |           2.3103 |
[32m[20221208 14:41:24 @agent_ppo2.py:179][0m |          -0.0159 |           0.9963 |           2.2615 |
[32m[20221208 14:41:24 @agent_ppo2.py:179][0m |          -0.0341 |           0.9133 |           2.2866 |
[32m[20221208 14:41:24 @agent_ppo2.py:179][0m |          -0.0424 |           0.8720 |           2.2981 |
[32m[20221208 14:41:24 @agent_ppo2.py:179][0m |          -0.0468 |           0.8477 |           2.3019 |
[32m[20221208 14:41:24 @agent_ppo2.py:179][0m |          -0.0499 |           0.8284 |           2.3070 |
[32m[20221208 14:41:25 @agent_ppo2.py:179][0m |          -0.0534 |           0.8112 |           2.3095 |
[32m[20221208 14:41:25 @agent_ppo2.py:179][0m |          -0.0565 |           0.7884 |           2.3186 |
[32m[20221208 14:41:25 @agent_ppo2.py:179][0m |          -0.0580 |           0.7803 |           2.3210 |
[32m[20221208 14:41:25 @agent_ppo2.py:179][0m |          -0.0603 |           0.7702 |           2.3255 |
[32m[20221208 14:41:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.94
[32m[20221208 14:41:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.72
[32m[20221208 14:41:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.11
[32m[20221208 14:41:25 @agent_ppo2.py:137][0m Total time:       4.85 min
[32m[20221208 14:41:25 @agent_ppo2.py:139][0m 391168 total steps have happened
[32m[20221208 14:41:25 @agent_ppo2.py:115][0m #------------------------ Iteration 191 --------------------------#
[32m[20221208 14:41:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |           0.0149 |           4.5860 |           2.3550 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0029 |           4.0280 |           2.3158 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0266 |           3.9130 |           2.3502 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0382 |           3.7957 |           2.3436 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0430 |           3.6789 |           2.3607 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0474 |           3.6591 |           2.3577 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0536 |           3.5610 |           2.3540 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0569 |           3.5577 |           2.3713 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0623 |           3.4466 |           2.3804 |
[32m[20221208 14:41:26 @agent_ppo2.py:179][0m |          -0.0638 |           3.4169 |           2.3782 |
[32m[20221208 14:41:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.73
[32m[20221208 14:41:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.14
[32m[20221208 14:41:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.24
[32m[20221208 14:41:27 @agent_ppo2.py:137][0m Total time:       4.87 min
[32m[20221208 14:41:27 @agent_ppo2.py:139][0m 393216 total steps have happened
[32m[20221208 14:41:27 @agent_ppo2.py:115][0m #------------------------ Iteration 192 --------------------------#
[32m[20221208 14:41:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:27 @agent_ppo2.py:179][0m |           0.0267 |           4.8533 |           2.3118 |
[32m[20221208 14:41:27 @agent_ppo2.py:179][0m |           0.0305 |           4.2958 |           2.2376 |
[32m[20221208 14:41:27 @agent_ppo2.py:179][0m |           0.0010 |           4.1389 |           2.2743 |
[32m[20221208 14:41:27 @agent_ppo2.py:179][0m |          -0.0261 |           4.0340 |           2.3247 |
[32m[20221208 14:41:27 @agent_ppo2.py:179][0m |          -0.0337 |           3.9520 |           2.3446 |
[32m[20221208 14:41:27 @agent_ppo2.py:179][0m |          -0.0398 |           3.9022 |           2.3428 |
[32m[20221208 14:41:28 @agent_ppo2.py:179][0m |          -0.0415 |           3.8698 |           2.3481 |
[32m[20221208 14:41:28 @agent_ppo2.py:179][0m |          -0.0487 |           3.8497 |           2.3547 |
[32m[20221208 14:41:28 @agent_ppo2.py:179][0m |          -0.0474 |           3.7970 |           2.3616 |
[32m[20221208 14:41:28 @agent_ppo2.py:179][0m |          -0.0501 |           3.7664 |           2.3655 |
[32m[20221208 14:41:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.33
[32m[20221208 14:41:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.55
[32m[20221208 14:41:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.04
[32m[20221208 14:41:28 @agent_ppo2.py:137][0m Total time:       4.90 min
[32m[20221208 14:41:28 @agent_ppo2.py:139][0m 395264 total steps have happened
[32m[20221208 14:41:28 @agent_ppo2.py:115][0m #------------------------ Iteration 193 --------------------------#
[32m[20221208 14:41:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |           0.0118 |           4.5681 |           2.4511 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0143 |           4.1402 |           2.4411 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0308 |           3.9702 |           2.4696 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0379 |           3.8542 |           2.4886 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0433 |           3.7785 |           2.4908 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0486 |           3.7097 |           2.5051 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0491 |           3.6631 |           2.5130 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0521 |           3.6279 |           2.5180 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0556 |           3.5857 |           2.5211 |
[32m[20221208 14:41:29 @agent_ppo2.py:179][0m |          -0.0565 |           3.5660 |           2.5297 |
[32m[20221208 14:41:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:41:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.84
[32m[20221208 14:41:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.87
[32m[20221208 14:41:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.68
[32m[20221208 14:41:30 @agent_ppo2.py:137][0m Total time:       4.92 min
[32m[20221208 14:41:30 @agent_ppo2.py:139][0m 397312 total steps have happened
[32m[20221208 14:41:30 @agent_ppo2.py:115][0m #------------------------ Iteration 194 --------------------------#
[32m[20221208 14:41:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:30 @agent_ppo2.py:179][0m |           0.0310 |           2.9048 |           2.4843 |
[32m[20221208 14:41:30 @agent_ppo2.py:179][0m |          -0.0052 |           2.7382 |           2.4714 |
[32m[20221208 14:41:30 @agent_ppo2.py:179][0m |          -0.0294 |           2.6591 |           2.5114 |
[32m[20221208 14:41:30 @agent_ppo2.py:179][0m |          -0.0386 |           2.5960 |           2.5144 |
[32m[20221208 14:41:30 @agent_ppo2.py:179][0m |          -0.0426 |           2.5406 |           2.5272 |
[32m[20221208 14:41:31 @agent_ppo2.py:179][0m |          -0.0456 |           2.5185 |           2.5214 |
[32m[20221208 14:41:31 @agent_ppo2.py:179][0m |          -0.0487 |           2.4842 |           2.5318 |
[32m[20221208 14:41:31 @agent_ppo2.py:179][0m |          -0.0534 |           2.4617 |           2.5348 |
[32m[20221208 14:41:31 @agent_ppo2.py:179][0m |          -0.0537 |           2.4507 |           2.5360 |
[32m[20221208 14:41:31 @agent_ppo2.py:179][0m |          -0.0563 |           2.4147 |           2.5413 |
[32m[20221208 14:41:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.68
[32m[20221208 14:41:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.89
[32m[20221208 14:41:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.02
[32m[20221208 14:41:31 @agent_ppo2.py:137][0m Total time:       4.95 min
[32m[20221208 14:41:31 @agent_ppo2.py:139][0m 399360 total steps have happened
[32m[20221208 14:41:31 @agent_ppo2.py:115][0m #------------------------ Iteration 195 --------------------------#
[32m[20221208 14:41:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |           0.0254 |           5.8034 |           2.4424 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0065 |           5.3365 |           2.4171 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0204 |           5.2183 |           2.4336 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0386 |           5.1506 |           2.4721 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0421 |           5.1226 |           2.4672 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0478 |           5.0648 |           2.4778 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0495 |           5.0354 |           2.4815 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0523 |           5.0174 |           2.4827 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0572 |           4.9968 |           2.4863 |
[32m[20221208 14:41:32 @agent_ppo2.py:179][0m |          -0.0574 |           4.9911 |           2.4852 |
[32m[20221208 14:41:32 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:41:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.54
[32m[20221208 14:41:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.00
[32m[20221208 14:41:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.40
[32m[20221208 14:41:33 @agent_ppo2.py:137][0m Total time:       4.97 min
[32m[20221208 14:41:33 @agent_ppo2.py:139][0m 401408 total steps have happened
[32m[20221208 14:41:33 @agent_ppo2.py:115][0m #------------------------ Iteration 196 --------------------------#
[32m[20221208 14:41:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:33 @agent_ppo2.py:179][0m |           0.0258 |           4.3780 |           2.5021 |
[32m[20221208 14:41:33 @agent_ppo2.py:179][0m |           0.0227 |           3.9742 |           2.4567 |
[32m[20221208 14:41:33 @agent_ppo2.py:179][0m |          -0.0054 |           3.8486 |           2.4630 |
[32m[20221208 14:41:33 @agent_ppo2.py:179][0m |          -0.0299 |           3.7597 |           2.5061 |
[32m[20221208 14:41:33 @agent_ppo2.py:179][0m |          -0.0399 |           3.7218 |           2.5122 |
[32m[20221208 14:41:34 @agent_ppo2.py:179][0m |          -0.0445 |           3.6735 |           2.5370 |
[32m[20221208 14:41:34 @agent_ppo2.py:179][0m |          -0.0455 |           3.6362 |           2.5460 |
[32m[20221208 14:41:34 @agent_ppo2.py:179][0m |          -0.0511 |           3.6287 |           2.5449 |
[32m[20221208 14:41:34 @agent_ppo2.py:179][0m |          -0.0543 |           3.5951 |           2.5638 |
[32m[20221208 14:41:34 @agent_ppo2.py:179][0m |          -0.0563 |           3.5613 |           2.5598 |
[32m[20221208 14:41:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.19
[32m[20221208 14:41:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.95
[32m[20221208 14:41:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.62
[32m[20221208 14:41:34 @agent_ppo2.py:137][0m Total time:       5.00 min
[32m[20221208 14:41:34 @agent_ppo2.py:139][0m 403456 total steps have happened
[32m[20221208 14:41:34 @agent_ppo2.py:115][0m #------------------------ Iteration 197 --------------------------#
[32m[20221208 14:41:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |           0.0308 |           2.6569 |           2.5949 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0092 |           2.3040 |           2.5990 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0278 |           2.2392 |           2.6104 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0304 |           2.2098 |           2.6323 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0398 |           2.1672 |           2.6350 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0378 |           2.1512 |           2.6315 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0442 |           2.1119 |           2.6415 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0435 |           2.0931 |           2.6468 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0485 |           2.0894 |           2.6579 |
[32m[20221208 14:41:35 @agent_ppo2.py:179][0m |          -0.0492 |           2.0932 |           2.6626 |
[32m[20221208 14:41:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.82
[32m[20221208 14:41:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.95
[32m[20221208 14:41:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.90
[32m[20221208 14:41:36 @agent_ppo2.py:137][0m Total time:       5.02 min
[32m[20221208 14:41:36 @agent_ppo2.py:139][0m 405504 total steps have happened
[32m[20221208 14:41:36 @agent_ppo2.py:115][0m #------------------------ Iteration 198 --------------------------#
[32m[20221208 14:41:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:36 @agent_ppo2.py:179][0m |           0.0341 |           4.6970 |           2.6318 |
[32m[20221208 14:41:36 @agent_ppo2.py:179][0m |           0.0076 |           4.3479 |           2.6515 |
[32m[20221208 14:41:36 @agent_ppo2.py:179][0m |          -0.0226 |           4.2377 |           2.6769 |
[32m[20221208 14:41:36 @agent_ppo2.py:179][0m |          -0.0340 |           4.1238 |           2.7021 |
[32m[20221208 14:41:36 @agent_ppo2.py:179][0m |          -0.0375 |           4.0465 |           2.7103 |
[32m[20221208 14:41:37 @agent_ppo2.py:179][0m |          -0.0436 |           3.9838 |           2.7193 |
[32m[20221208 14:41:37 @agent_ppo2.py:179][0m |          -0.0434 |           3.9353 |           2.7206 |
[32m[20221208 14:41:37 @agent_ppo2.py:179][0m |          -0.0474 |           3.9025 |           2.7221 |
[32m[20221208 14:41:37 @agent_ppo2.py:179][0m |          -0.0509 |           3.8412 |           2.7316 |
[32m[20221208 14:41:37 @agent_ppo2.py:179][0m |          -0.0528 |           3.7355 |           2.7332 |
[32m[20221208 14:41:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.25
[32m[20221208 14:41:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.59
[32m[20221208 14:41:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.26
[32m[20221208 14:41:37 @agent_ppo2.py:137][0m Total time:       5.05 min
[32m[20221208 14:41:37 @agent_ppo2.py:139][0m 407552 total steps have happened
[32m[20221208 14:41:37 @agent_ppo2.py:115][0m #------------------------ Iteration 199 --------------------------#
[32m[20221208 14:41:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |           0.0177 |           0.3782 |           2.7112 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |           0.0055 |           0.3044 |           2.6575 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |           0.0001 |           0.2995 |           2.6597 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0081 |           0.3048 |           2.6683 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0101 |           0.2904 |           2.6956 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0128 |           0.2871 |           2.6343 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0172 |           0.2852 |           2.6917 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0202 |           0.2845 |           2.7073 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0076 |           0.2844 |           2.6836 |
[32m[20221208 14:41:38 @agent_ppo2.py:179][0m |          -0.0228 |           0.2854 |           2.7079 |
[32m[20221208 14:41:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.84
[32m[20221208 14:41:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.96
[32m[20221208 14:41:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.25
[32m[20221208 14:41:39 @agent_ppo2.py:137][0m Total time:       5.07 min
[32m[20221208 14:41:39 @agent_ppo2.py:139][0m 409600 total steps have happened
[32m[20221208 14:41:39 @agent_ppo2.py:115][0m #------------------------ Iteration 200 --------------------------#
[32m[20221208 14:41:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:39 @agent_ppo2.py:179][0m |           0.0274 |           3.3632 |           2.6367 |
[32m[20221208 14:41:39 @agent_ppo2.py:179][0m |           0.0399 |           3.0231 |           2.5359 |
[32m[20221208 14:41:39 @agent_ppo2.py:179][0m |           0.0033 |           2.8430 |           2.5770 |
[32m[20221208 14:41:39 @agent_ppo2.py:179][0m |          -0.0252 |           2.7505 |           2.6375 |
[32m[20221208 14:41:40 @agent_ppo2.py:179][0m |          -0.0399 |           2.6576 |           2.6529 |
[32m[20221208 14:41:40 @agent_ppo2.py:179][0m |          -0.0435 |           2.6590 |           2.6659 |
[32m[20221208 14:41:40 @agent_ppo2.py:179][0m |          -0.0463 |           2.6086 |           2.6655 |
[32m[20221208 14:41:40 @agent_ppo2.py:179][0m |          -0.0523 |           2.5242 |           2.6742 |
[32m[20221208 14:41:40 @agent_ppo2.py:179][0m |          -0.0564 |           2.4788 |           2.6862 |
[32m[20221208 14:41:40 @agent_ppo2.py:179][0m |          -0.0595 |           2.4223 |           2.6982 |
[32m[20221208 14:41:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.94
[32m[20221208 14:41:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.80
[32m[20221208 14:41:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.35
[32m[20221208 14:41:40 @agent_ppo2.py:137][0m Total time:       5.10 min
[32m[20221208 14:41:40 @agent_ppo2.py:139][0m 411648 total steps have happened
[32m[20221208 14:41:40 @agent_ppo2.py:115][0m #------------------------ Iteration 201 --------------------------#
[32m[20221208 14:41:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |           0.0251 |           4.6858 |           2.6959 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0085 |           4.1224 |           2.6904 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0292 |           3.9995 |           2.7017 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0377 |           3.9523 |           2.7125 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0401 |           3.8679 |           2.7224 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0412 |           3.8099 |           2.7316 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0453 |           3.7774 |           2.7318 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0492 |           3.7618 |           2.7516 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0503 |           3.7080 |           2.7529 |
[32m[20221208 14:41:41 @agent_ppo2.py:179][0m |          -0.0503 |           3.6669 |           2.7620 |
[32m[20221208 14:41:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:41:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.63
[32m[20221208 14:41:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.98
[32m[20221208 14:41:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.21
[32m[20221208 14:41:42 @agent_ppo2.py:137][0m Total time:       5.12 min
[32m[20221208 14:41:42 @agent_ppo2.py:139][0m 413696 total steps have happened
[32m[20221208 14:41:42 @agent_ppo2.py:115][0m #------------------------ Iteration 202 --------------------------#
[32m[20221208 14:41:42 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:42 @agent_ppo2.py:179][0m |           0.0318 |           6.1041 |           2.8198 |
[32m[20221208 14:41:42 @agent_ppo2.py:179][0m |           0.0146 |           5.7872 |           2.7382 |
[32m[20221208 14:41:42 @agent_ppo2.py:179][0m |          -0.0235 |           5.7006 |           2.8161 |
[32m[20221208 14:41:42 @agent_ppo2.py:179][0m |          -0.0305 |           5.6629 |           2.8316 |
[32m[20221208 14:41:43 @agent_ppo2.py:179][0m |          -0.0409 |           5.5838 |           2.8346 |
[32m[20221208 14:41:43 @agent_ppo2.py:179][0m |          -0.0446 |           5.5636 |           2.8569 |
[32m[20221208 14:41:43 @agent_ppo2.py:179][0m |          -0.0498 |           5.5777 |           2.8559 |
[32m[20221208 14:41:43 @agent_ppo2.py:179][0m |          -0.0522 |           5.4774 |           2.8654 |
[32m[20221208 14:41:43 @agent_ppo2.py:179][0m |          -0.0525 |           5.4876 |           2.8632 |
[32m[20221208 14:41:43 @agent_ppo2.py:179][0m |          -0.0533 |           5.4666 |           2.8656 |
[32m[20221208 14:41:43 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.57
[32m[20221208 14:41:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.60
[32m[20221208 14:41:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.37
[32m[20221208 14:41:43 @agent_ppo2.py:137][0m Total time:       5.15 min
[32m[20221208 14:41:43 @agent_ppo2.py:139][0m 415744 total steps have happened
[32m[20221208 14:41:43 @agent_ppo2.py:115][0m #------------------------ Iteration 203 --------------------------#
[32m[20221208 14:41:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |           0.0338 |           7.1302 |           2.7344 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |           0.0036 |           6.6741 |           2.4286 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0171 |           6.5251 |           2.4403 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0317 |           6.5048 |           2.4581 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0385 |           6.3896 |           2.4578 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0436 |           6.3521 |           2.4791 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0441 |           6.3172 |           2.4645 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0495 |           6.2413 |           2.4797 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0522 |           6.1969 |           2.4868 |
[32m[20221208 14:41:44 @agent_ppo2.py:179][0m |          -0.0551 |           6.1620 |           2.4977 |
[32m[20221208 14:41:44 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:41:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.85
[32m[20221208 14:41:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.62
[32m[20221208 14:41:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.57
[32m[20221208 14:41:45 @agent_ppo2.py:137][0m Total time:       5.18 min
[32m[20221208 14:41:45 @agent_ppo2.py:139][0m 417792 total steps have happened
[32m[20221208 14:41:45 @agent_ppo2.py:115][0m #------------------------ Iteration 204 --------------------------#
[32m[20221208 14:41:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:45 @agent_ppo2.py:179][0m |           0.0192 |           4.6011 |           2.9501 |
[32m[20221208 14:41:45 @agent_ppo2.py:179][0m |          -0.0057 |           4.0827 |           2.9378 |
[32m[20221208 14:41:45 @agent_ppo2.py:179][0m |          -0.0278 |           3.8698 |           2.9684 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0347 |           3.7190 |           2.9596 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0433 |           3.5992 |           2.9898 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0467 |           3.5202 |           2.9951 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0480 |           3.4503 |           3.0241 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0499 |           3.3725 |           3.0121 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0533 |           3.2886 |           3.0347 |
[32m[20221208 14:41:46 @agent_ppo2.py:179][0m |          -0.0551 |           3.2427 |           3.0268 |
[32m[20221208 14:41:46 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:41:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.02
[32m[20221208 14:41:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.45
[32m[20221208 14:41:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.49
[32m[20221208 14:41:46 @agent_ppo2.py:137][0m Total time:       5.20 min
[32m[20221208 14:41:46 @agent_ppo2.py:139][0m 419840 total steps have happened
[32m[20221208 14:41:46 @agent_ppo2.py:115][0m #------------------------ Iteration 205 --------------------------#
[32m[20221208 14:41:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:41:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |           0.0165 |           5.1131 |           2.9929 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0152 |           4.6431 |           2.9842 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0273 |           4.4735 |           2.9932 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0418 |           4.3503 |           3.0129 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0472 |           4.3312 |           3.0384 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0528 |           4.2804 |           3.0432 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0524 |           4.1860 |           3.0495 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0551 |           4.1481 |           3.0502 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0599 |           4.0586 |           3.0525 |
[32m[20221208 14:41:47 @agent_ppo2.py:179][0m |          -0.0618 |           4.0265 |           3.0770 |
[32m[20221208 14:41:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.35
[32m[20221208 14:41:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.67
[32m[20221208 14:41:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.18
[32m[20221208 14:41:48 @agent_ppo2.py:137][0m Total time:       5.23 min
[32m[20221208 14:41:48 @agent_ppo2.py:139][0m 421888 total steps have happened
[32m[20221208 14:41:48 @agent_ppo2.py:115][0m #------------------------ Iteration 206 --------------------------#
[32m[20221208 14:41:48 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:48 @agent_ppo2.py:179][0m |           0.0099 |           4.6344 |           3.1409 |
[32m[20221208 14:41:48 @agent_ppo2.py:179][0m |          -0.0071 |           4.3682 |           3.0787 |
[32m[20221208 14:41:48 @agent_ppo2.py:179][0m |          -0.0273 |           4.2780 |           3.1018 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0361 |           4.1991 |           3.1204 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0424 |           4.1480 |           3.1414 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0459 |           4.1210 |           3.1558 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0486 |           4.0899 |           3.1583 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0516 |           4.0229 |           3.1742 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0545 |           4.0174 |           3.1761 |
[32m[20221208 14:41:49 @agent_ppo2.py:179][0m |          -0.0576 |           3.9773 |           3.1982 |
[32m[20221208 14:41:49 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:41:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.04
[32m[20221208 14:41:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.34
[32m[20221208 14:41:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.83
[32m[20221208 14:41:49 @agent_ppo2.py:137][0m Total time:       5.25 min
[32m[20221208 14:41:49 @agent_ppo2.py:139][0m 423936 total steps have happened
[32m[20221208 14:41:49 @agent_ppo2.py:115][0m #------------------------ Iteration 207 --------------------------#
[32m[20221208 14:41:50 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |           0.0167 |           2.5136 |           3.1709 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |           0.0051 |           2.2394 |           3.1604 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0195 |           2.1405 |           3.2282 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0256 |           2.0752 |           3.2609 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0314 |           2.0266 |           3.2792 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0354 |           1.9825 |           3.3030 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0382 |           1.9488 |           3.3234 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0374 |           1.9670 |           3.3499 |
[32m[20221208 14:41:50 @agent_ppo2.py:179][0m |          -0.0348 |           1.8998 |           3.3212 |
[32m[20221208 14:41:51 @agent_ppo2.py:179][0m |          -0.0391 |           1.8874 |           3.3520 |
[32m[20221208 14:41:51 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:41:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.13
[32m[20221208 14:41:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.44
[32m[20221208 14:41:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.77
[32m[20221208 14:41:51 @agent_ppo2.py:137][0m Total time:       5.28 min
[32m[20221208 14:41:51 @agent_ppo2.py:139][0m 425984 total steps have happened
[32m[20221208 14:41:51 @agent_ppo2.py:115][0m #------------------------ Iteration 208 --------------------------#
[32m[20221208 14:41:51 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |           0.0223 |           5.0329 |           3.3003 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |           0.0470 |           4.5226 |           3.0245 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0169 |           4.3449 |           3.1778 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0266 |           4.2743 |           3.2471 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0349 |           4.2082 |           3.2851 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0395 |           4.1511 |           3.3066 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0428 |           4.1062 |           3.3216 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0466 |           4.0610 |           3.3284 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0482 |           4.0392 |           3.3445 |
[32m[20221208 14:41:52 @agent_ppo2.py:179][0m |          -0.0497 |           4.0065 |           3.3420 |
[32m[20221208 14:41:52 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:41:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.58
[32m[20221208 14:41:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.15
[32m[20221208 14:41:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.05
[32m[20221208 14:41:52 @agent_ppo2.py:137][0m Total time:       5.30 min
[32m[20221208 14:41:52 @agent_ppo2.py:139][0m 428032 total steps have happened
[32m[20221208 14:41:52 @agent_ppo2.py:115][0m #------------------------ Iteration 209 --------------------------#
[32m[20221208 14:41:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |           0.0504 |           2.6397 |           3.2703 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |           0.0225 |           2.1163 |           3.2314 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |          -0.0156 |           1.9669 |           3.2715 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |          -0.0294 |           1.8386 |           3.2933 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |          -0.0361 |           1.7889 |           3.3146 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |          -0.0410 |           1.7552 |           3.3215 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |          -0.0477 |           1.6901 |           3.3467 |
[32m[20221208 14:41:53 @agent_ppo2.py:179][0m |          -0.0501 |           1.6458 |           3.3546 |
[32m[20221208 14:41:54 @agent_ppo2.py:179][0m |          -0.0515 |           1.5886 |           3.3666 |
[32m[20221208 14:41:54 @agent_ppo2.py:179][0m |          -0.0542 |           1.5617 |           3.3811 |
[32m[20221208 14:41:54 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:41:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.74
[32m[20221208 14:41:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.96
[32m[20221208 14:41:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 166.06
[32m[20221208 14:41:54 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 166.06
[32m[20221208 14:41:54 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 166.06
[32m[20221208 14:41:54 @agent_ppo2.py:137][0m Total time:       5.33 min
[32m[20221208 14:41:54 @agent_ppo2.py:139][0m 430080 total steps have happened
[32m[20221208 14:41:54 @agent_ppo2.py:115][0m #------------------------ Iteration 210 --------------------------#
[32m[20221208 14:41:54 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:41:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |           0.0143 |           3.6220 |           3.3983 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0020 |           3.1983 |           3.3094 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0238 |           3.0391 |           3.4115 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0317 |           2.9536 |           3.4310 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0417 |           2.8901 |           3.4483 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0455 |           2.8383 |           3.4495 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0476 |           2.7944 |           3.4637 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0519 |           2.7545 |           3.4821 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0524 |           2.7152 |           3.4807 |
[32m[20221208 14:41:55 @agent_ppo2.py:179][0m |          -0.0554 |           2.7108 |           3.4905 |
[32m[20221208 14:41:55 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:41:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.98
[32m[20221208 14:41:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.60
[32m[20221208 14:41:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.23
[32m[20221208 14:41:56 @agent_ppo2.py:137][0m Total time:       5.36 min
[32m[20221208 14:41:56 @agent_ppo2.py:139][0m 432128 total steps have happened
[32m[20221208 14:41:56 @agent_ppo2.py:115][0m #------------------------ Iteration 211 --------------------------#
[32m[20221208 14:41:56 @agent_ppo2.py:121][0m Sampling time: 0.49 s by 1 slaves
[32m[20221208 14:41:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:56 @agent_ppo2.py:179][0m |           0.0518 |           2.2799 |           3.3629 |
[32m[20221208 14:41:56 @agent_ppo2.py:179][0m |           0.0070 |           1.5650 |           3.2736 |
[32m[20221208 14:41:56 @agent_ppo2.py:179][0m |          -0.0161 |           1.2623 |           3.3140 |
[32m[20221208 14:41:56 @agent_ppo2.py:179][0m |          -0.0354 |           1.1205 |           3.3646 |
[32m[20221208 14:41:57 @agent_ppo2.py:179][0m |          -0.0403 |           1.0275 |           3.3757 |
[32m[20221208 14:41:57 @agent_ppo2.py:179][0m |          -0.0434 |           0.9722 |           3.3772 |
[32m[20221208 14:41:57 @agent_ppo2.py:179][0m |          -0.0468 |           0.9376 |           3.3974 |
[32m[20221208 14:41:57 @agent_ppo2.py:179][0m |          -0.0501 |           0.9063 |           3.3842 |
[32m[20221208 14:41:57 @agent_ppo2.py:179][0m |          -0.0506 |           0.8903 |           3.4051 |
[32m[20221208 14:41:57 @agent_ppo2.py:179][0m |          -0.0539 |           0.8628 |           3.4156 |
[32m[20221208 14:41:57 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:41:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.57
[32m[20221208 14:41:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.76
[32m[20221208 14:41:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.16
[32m[20221208 14:41:57 @agent_ppo2.py:137][0m Total time:       5.38 min
[32m[20221208 14:41:57 @agent_ppo2.py:139][0m 434176 total steps have happened
[32m[20221208 14:41:57 @agent_ppo2.py:115][0m #------------------------ Iteration 212 --------------------------#
[32m[20221208 14:41:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |           0.0160 |           3.4963 |           3.3457 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0080 |           2.9627 |           3.3093 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0297 |           2.7245 |           3.3261 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0413 |           2.6084 |           3.3211 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0496 |           2.4511 |           3.3262 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0521 |           2.3959 |           3.3390 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0557 |           2.3582 |           3.3406 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0592 |           2.2535 |           3.3621 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0604 |           2.1557 |           3.3701 |
[32m[20221208 14:41:58 @agent_ppo2.py:179][0m |          -0.0628 |           2.1046 |           3.3711 |
[32m[20221208 14:41:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:41:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.00
[32m[20221208 14:41:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.43
[32m[20221208 14:41:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.59
[32m[20221208 14:41:59 @agent_ppo2.py:137][0m Total time:       5.41 min
[32m[20221208 14:41:59 @agent_ppo2.py:139][0m 436224 total steps have happened
[32m[20221208 14:41:59 @agent_ppo2.py:115][0m #------------------------ Iteration 213 --------------------------#
[32m[20221208 14:41:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:41:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:41:59 @agent_ppo2.py:179][0m |           0.0391 |           3.3153 |           3.4386 |
[32m[20221208 14:41:59 @agent_ppo2.py:179][0m |           0.0202 |           2.9847 |           3.3780 |
[32m[20221208 14:41:59 @agent_ppo2.py:179][0m |          -0.0088 |           2.8709 |           3.4179 |
[32m[20221208 14:41:59 @agent_ppo2.py:179][0m |          -0.0211 |           2.7816 |           3.4410 |
[32m[20221208 14:42:00 @agent_ppo2.py:179][0m |          -0.0298 |           2.7409 |           3.4644 |
[32m[20221208 14:42:00 @agent_ppo2.py:179][0m |          -0.0347 |           2.7022 |           3.4747 |
[32m[20221208 14:42:00 @agent_ppo2.py:179][0m |          -0.0396 |           2.6738 |           3.4819 |
[32m[20221208 14:42:00 @agent_ppo2.py:179][0m |          -0.0414 |           2.6347 |           3.4963 |
[32m[20221208 14:42:00 @agent_ppo2.py:179][0m |          -0.0423 |           2.6022 |           3.5032 |
[32m[20221208 14:42:00 @agent_ppo2.py:179][0m |          -0.0434 |           2.5706 |           3.5042 |
[32m[20221208 14:42:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.30
[32m[20221208 14:42:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.47
[32m[20221208 14:42:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.80
[32m[20221208 14:42:00 @agent_ppo2.py:137][0m Total time:       5.43 min
[32m[20221208 14:42:00 @agent_ppo2.py:139][0m 438272 total steps have happened
[32m[20221208 14:42:00 @agent_ppo2.py:115][0m #------------------------ Iteration 214 --------------------------#
[32m[20221208 14:42:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |           0.0143 |           6.3907 |           3.4179 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0074 |           6.0184 |           3.4018 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0267 |           5.8664 |           3.4212 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0272 |           5.7240 |           3.4240 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0425 |           5.6254 |           3.4540 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0467 |           5.5617 |           3.4690 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0482 |           5.4484 |           3.4631 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0542 |           5.3590 |           3.4756 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0568 |           5.2821 |           3.5019 |
[32m[20221208 14:42:01 @agent_ppo2.py:179][0m |          -0.0592 |           5.2235 |           3.5057 |
[32m[20221208 14:42:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.41
[32m[20221208 14:42:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.44
[32m[20221208 14:42:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.24
[32m[20221208 14:42:02 @agent_ppo2.py:137][0m Total time:       5.46 min
[32m[20221208 14:42:02 @agent_ppo2.py:139][0m 440320 total steps have happened
[32m[20221208 14:42:02 @agent_ppo2.py:115][0m #------------------------ Iteration 215 --------------------------#
[32m[20221208 14:42:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:02 @agent_ppo2.py:179][0m |           0.0345 |           2.4874 |           3.7009 |
[32m[20221208 14:42:02 @agent_ppo2.py:179][0m |           0.0061 |           1.6763 |           3.6262 |
[32m[20221208 14:42:02 @agent_ppo2.py:179][0m |          -0.0141 |           1.4723 |           3.6625 |
[32m[20221208 14:42:02 @agent_ppo2.py:179][0m |          -0.0313 |           1.3794 |           3.6765 |
[32m[20221208 14:42:03 @agent_ppo2.py:179][0m |          -0.0437 |           1.3304 |           3.7098 |
[32m[20221208 14:42:03 @agent_ppo2.py:179][0m |          -0.0486 |           1.2790 |           3.7137 |
[32m[20221208 14:42:03 @agent_ppo2.py:179][0m |          -0.0526 |           1.2236 |           3.7248 |
[32m[20221208 14:42:03 @agent_ppo2.py:179][0m |          -0.0555 |           1.1967 |           3.7298 |
[32m[20221208 14:42:03 @agent_ppo2.py:179][0m |          -0.0593 |           1.1622 |           3.7619 |
[32m[20221208 14:42:03 @agent_ppo2.py:179][0m |          -0.0597 |           1.1405 |           3.7492 |
[32m[20221208 14:42:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.30
[32m[20221208 14:42:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.57
[32m[20221208 14:42:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.84
[32m[20221208 14:42:03 @agent_ppo2.py:137][0m Total time:       5.48 min
[32m[20221208 14:42:03 @agent_ppo2.py:139][0m 442368 total steps have happened
[32m[20221208 14:42:03 @agent_ppo2.py:115][0m #------------------------ Iteration 216 --------------------------#
[32m[20221208 14:42:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |           0.0252 |           5.9500 |           3.5817 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0026 |           4.9399 |           3.5324 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0172 |           4.7359 |           3.5782 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0331 |           4.6795 |           3.6302 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0392 |           4.5547 |           3.6468 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0396 |           4.4936 |           3.6463 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0456 |           4.4793 |           3.6657 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0467 |           4.4032 |           3.6746 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0453 |           4.3675 |           3.6904 |
[32m[20221208 14:42:04 @agent_ppo2.py:179][0m |          -0.0482 |           4.3378 |           3.6791 |
[32m[20221208 14:42:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.33
[32m[20221208 14:42:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.95
[32m[20221208 14:42:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 153.55
[32m[20221208 14:42:05 @agent_ppo2.py:137][0m Total time:       5.51 min
[32m[20221208 14:42:05 @agent_ppo2.py:139][0m 444416 total steps have happened
[32m[20221208 14:42:05 @agent_ppo2.py:115][0m #------------------------ Iteration 217 --------------------------#
[32m[20221208 14:42:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:05 @agent_ppo2.py:179][0m |           0.0252 |           5.1176 |           3.6212 |
[32m[20221208 14:42:05 @agent_ppo2.py:179][0m |           0.0003 |           4.8498 |           3.5764 |
[32m[20221208 14:42:05 @agent_ppo2.py:179][0m |          -0.0239 |           4.7426 |           3.6782 |
[32m[20221208 14:42:05 @agent_ppo2.py:179][0m |          -0.0309 |           4.6527 |           3.7072 |
[32m[20221208 14:42:06 @agent_ppo2.py:179][0m |          -0.0363 |           4.5502 |           3.7119 |
[32m[20221208 14:42:06 @agent_ppo2.py:179][0m |          -0.0397 |           4.4951 |           3.7326 |
[32m[20221208 14:42:06 @agent_ppo2.py:179][0m |          -0.0376 |           4.4584 |           3.7204 |
[32m[20221208 14:42:06 @agent_ppo2.py:179][0m |          -0.0385 |           4.4322 |           3.7180 |
[32m[20221208 14:42:06 @agent_ppo2.py:179][0m |          -0.0428 |           4.3835 |           3.7422 |
[32m[20221208 14:42:06 @agent_ppo2.py:179][0m |          -0.0451 |           4.3254 |           3.7554 |
[32m[20221208 14:42:06 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:42:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.71
[32m[20221208 14:42:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.06
[32m[20221208 14:42:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.22
[32m[20221208 14:42:06 @agent_ppo2.py:137][0m Total time:       5.53 min
[32m[20221208 14:42:06 @agent_ppo2.py:139][0m 446464 total steps have happened
[32m[20221208 14:42:06 @agent_ppo2.py:115][0m #------------------------ Iteration 218 --------------------------#
[32m[20221208 14:42:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |           0.0396 |           1.2293 |           3.8112 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |           0.0104 |           0.9805 |           3.7519 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0260 |           0.8729 |           3.8045 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0365 |           0.8177 |           3.8236 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0418 |           0.7801 |           3.8251 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0475 |           0.7502 |           3.8434 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0501 |           0.7271 |           3.8555 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0543 |           0.7029 |           3.8698 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0550 |           0.6862 |           3.8771 |
[32m[20221208 14:42:07 @agent_ppo2.py:179][0m |          -0.0597 |           0.6733 |           3.9082 |
[32m[20221208 14:42:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.80
[32m[20221208 14:42:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.05
[32m[20221208 14:42:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.52
[32m[20221208 14:42:08 @agent_ppo2.py:137][0m Total time:       5.56 min
[32m[20221208 14:42:08 @agent_ppo2.py:139][0m 448512 total steps have happened
[32m[20221208 14:42:08 @agent_ppo2.py:115][0m #------------------------ Iteration 219 --------------------------#
[32m[20221208 14:42:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:08 @agent_ppo2.py:179][0m |           0.0247 |           3.9081 |           3.9053 |
[32m[20221208 14:42:08 @agent_ppo2.py:179][0m |           0.0079 |           3.3049 |           3.8428 |
[32m[20221208 14:42:08 @agent_ppo2.py:179][0m |          -0.0213 |           3.1127 |           3.9204 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0373 |           2.9690 |           3.9498 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0457 |           2.8595 |           3.9614 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0496 |           2.7723 |           3.9806 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0507 |           2.7389 |           3.9855 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0544 |           2.6315 |           3.9958 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0575 |           2.5937 |           4.0144 |
[32m[20221208 14:42:09 @agent_ppo2.py:179][0m |          -0.0588 |           2.5445 |           4.0206 |
[32m[20221208 14:42:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.49
[32m[20221208 14:42:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.73
[32m[20221208 14:42:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.50
[32m[20221208 14:42:09 @agent_ppo2.py:137][0m Total time:       5.58 min
[32m[20221208 14:42:09 @agent_ppo2.py:139][0m 450560 total steps have happened
[32m[20221208 14:42:09 @agent_ppo2.py:115][0m #------------------------ Iteration 220 --------------------------#
[32m[20221208 14:42:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |           0.0097 |           5.6905 |           3.9866 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0161 |           5.0812 |           3.9927 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0313 |           4.9164 |           4.0179 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0392 |           4.8045 |           4.0627 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0458 |           4.6528 |           4.0859 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0477 |           4.5811 |           4.1095 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0493 |           4.4502 |           4.1130 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0523 |           4.3618 |           4.1232 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0538 |           4.2796 |           4.1387 |
[32m[20221208 14:42:10 @agent_ppo2.py:179][0m |          -0.0538 |           4.2512 |           4.1460 |
[32m[20221208 14:42:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.19
[32m[20221208 14:42:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.39
[32m[20221208 14:42:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.50
[32m[20221208 14:42:11 @agent_ppo2.py:137][0m Total time:       5.61 min
[32m[20221208 14:42:11 @agent_ppo2.py:139][0m 452608 total steps have happened
[32m[20221208 14:42:11 @agent_ppo2.py:115][0m #------------------------ Iteration 221 --------------------------#
[32m[20221208 14:42:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:11 @agent_ppo2.py:179][0m |           0.0335 |           4.4042 |           4.1939 |
[32m[20221208 14:42:11 @agent_ppo2.py:179][0m |          -0.0022 |           3.7069 |           4.1552 |
[32m[20221208 14:42:11 @agent_ppo2.py:179][0m |          -0.0188 |           3.4891 |           4.1752 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0297 |           3.2972 |           4.2468 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0430 |           3.2086 |           4.2551 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0496 |           3.1693 |           4.2600 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0528 |           3.0746 |           4.2949 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0571 |           3.0178 |           4.2947 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0571 |           2.9375 |           4.2905 |
[32m[20221208 14:42:12 @agent_ppo2.py:179][0m |          -0.0596 |           2.8857 |           4.3042 |
[32m[20221208 14:42:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.96
[32m[20221208 14:42:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.28
[32m[20221208 14:42:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.77
[32m[20221208 14:42:12 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 169.77
[32m[20221208 14:42:12 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 169.77
[32m[20221208 14:42:12 @agent_ppo2.py:137][0m Total time:       5.63 min
[32m[20221208 14:42:12 @agent_ppo2.py:139][0m 454656 total steps have happened
[32m[20221208 14:42:12 @agent_ppo2.py:115][0m #------------------------ Iteration 222 --------------------------#
[32m[20221208 14:42:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |           0.0399 |           3.5208 |           4.1807 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |           0.0479 |           2.9047 |           4.0501 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0050 |           2.7306 |           4.2013 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0206 |           2.6476 |           4.3074 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0292 |           2.5914 |           4.3557 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0362 |           2.5741 |           4.3716 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0414 |           2.4977 |           4.4040 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0396 |           2.4600 |           4.4086 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0453 |           2.4008 |           4.4278 |
[32m[20221208 14:42:13 @agent_ppo2.py:179][0m |          -0.0465 |           2.3853 |           4.4318 |
[32m[20221208 14:42:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.24
[32m[20221208 14:42:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.63
[32m[20221208 14:42:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.23
[32m[20221208 14:42:14 @agent_ppo2.py:137][0m Total time:       5.66 min
[32m[20221208 14:42:14 @agent_ppo2.py:139][0m 456704 total steps have happened
[32m[20221208 14:42:14 @agent_ppo2.py:115][0m #------------------------ Iteration 223 --------------------------#
[32m[20221208 14:42:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:14 @agent_ppo2.py:179][0m |           0.0247 |           5.0065 |           4.3293 |
[32m[20221208 14:42:14 @agent_ppo2.py:179][0m |           0.0229 |           4.2344 |           4.2555 |
[32m[20221208 14:42:14 @agent_ppo2.py:179][0m |          -0.0176 |           3.9646 |           4.3250 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0196 |           3.8167 |           4.3426 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0369 |           3.7581 |           4.3611 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0423 |           3.6406 |           4.4024 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0493 |           3.5462 |           4.4215 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0515 |           3.5314 |           4.4386 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0536 |           3.4759 |           4.4553 |
[32m[20221208 14:42:15 @agent_ppo2.py:179][0m |          -0.0564 |           3.4584 |           4.4773 |
[32m[20221208 14:42:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.58
[32m[20221208 14:42:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.08
[32m[20221208 14:42:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.19
[32m[20221208 14:42:15 @agent_ppo2.py:137][0m Total time:       5.68 min
[32m[20221208 14:42:15 @agent_ppo2.py:139][0m 458752 total steps have happened
[32m[20221208 14:42:15 @agent_ppo2.py:115][0m #------------------------ Iteration 224 --------------------------#
[32m[20221208 14:42:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |           0.0253 |           4.4774 |           4.3758 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |           0.0005 |           3.9468 |           4.3482 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0213 |           3.7337 |           4.4413 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0348 |           3.5653 |           4.4608 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0414 |           3.4418 |           4.4878 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0468 |           3.3351 |           4.4923 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0456 |           3.2606 |           4.5016 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0478 |           3.2014 |           4.4973 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0534 |           3.1421 |           4.4971 |
[32m[20221208 14:42:16 @agent_ppo2.py:179][0m |          -0.0553 |           3.0776 |           4.5294 |
[32m[20221208 14:42:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.06
[32m[20221208 14:42:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.03
[32m[20221208 14:42:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 153.02
[32m[20221208 14:42:17 @agent_ppo2.py:137][0m Total time:       5.71 min
[32m[20221208 14:42:17 @agent_ppo2.py:139][0m 460800 total steps have happened
[32m[20221208 14:42:17 @agent_ppo2.py:115][0m #------------------------ Iteration 225 --------------------------#
[32m[20221208 14:42:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:17 @agent_ppo2.py:179][0m |           0.0078 |           1.2309 |           4.4447 |
[32m[20221208 14:42:17 @agent_ppo2.py:179][0m |          -0.0152 |           0.8758 |           4.4252 |
[32m[20221208 14:42:17 @agent_ppo2.py:179][0m |          -0.0245 |           0.7436 |           4.4237 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0333 |           0.6663 |           4.4435 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0370 |           0.6108 |           4.4683 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0368 |           0.5685 |           4.4756 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0386 |           0.5374 |           4.4902 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0401 |           0.5095 |           4.4753 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0422 |           0.4912 |           4.4789 |
[32m[20221208 14:42:18 @agent_ppo2.py:179][0m |          -0.0433 |           0.4731 |           4.4795 |
[32m[20221208 14:42:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.30
[32m[20221208 14:42:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.32
[32m[20221208 14:42:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.86
[32m[20221208 14:42:18 @agent_ppo2.py:137][0m Total time:       5.73 min
[32m[20221208 14:42:18 @agent_ppo2.py:139][0m 462848 total steps have happened
[32m[20221208 14:42:18 @agent_ppo2.py:115][0m #------------------------ Iteration 226 --------------------------#
[32m[20221208 14:42:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |           0.0274 |           9.5150 |           4.5873 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |           0.0227 |           8.4778 |           4.3785 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0048 |           8.2520 |           4.4384 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0336 |           8.1708 |           4.5388 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0442 |           7.9951 |           4.5747 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0521 |           7.9596 |           4.6036 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0488 |           7.9257 |           4.5985 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0584 |           7.8658 |           4.6216 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0595 |           7.8454 |           4.6115 |
[32m[20221208 14:42:19 @agent_ppo2.py:179][0m |          -0.0611 |           7.7754 |           4.6290 |
[32m[20221208 14:42:19 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:42:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.02
[32m[20221208 14:42:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.98
[32m[20221208 14:42:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.54
[32m[20221208 14:42:20 @agent_ppo2.py:137][0m Total time:       5.76 min
[32m[20221208 14:42:20 @agent_ppo2.py:139][0m 464896 total steps have happened
[32m[20221208 14:42:20 @agent_ppo2.py:115][0m #------------------------ Iteration 227 --------------------------#
[32m[20221208 14:42:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:20 @agent_ppo2.py:179][0m |           0.0448 |           3.9471 |           4.4198 |
[32m[20221208 14:42:20 @agent_ppo2.py:179][0m |           0.0095 |           3.3593 |           4.2500 |
[32m[20221208 14:42:20 @agent_ppo2.py:179][0m |          -0.0209 |           3.1543 |           4.3785 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0307 |           3.0477 |           4.4291 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0368 |           2.9473 |           4.4427 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0353 |           2.8926 |           4.4104 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0451 |           2.8732 |           4.4580 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0502 |           2.7875 |           4.5007 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0489 |           2.7449 |           4.4985 |
[32m[20221208 14:42:21 @agent_ppo2.py:179][0m |          -0.0546 |           2.7163 |           4.5157 |
[32m[20221208 14:42:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.55
[32m[20221208 14:42:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.54
[32m[20221208 14:42:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.77
[32m[20221208 14:42:21 @agent_ppo2.py:137][0m Total time:       5.78 min
[32m[20221208 14:42:21 @agent_ppo2.py:139][0m 466944 total steps have happened
[32m[20221208 14:42:21 @agent_ppo2.py:115][0m #------------------------ Iteration 228 --------------------------#
[32m[20221208 14:42:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |           0.0275 |           7.3433 |           4.7212 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |           0.0316 |           6.9687 |           4.5031 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |           0.0084 |           6.7949 |           4.5327 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0204 |           6.5919 |           4.6691 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0368 |           6.4428 |           4.7577 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0436 |           6.3552 |           4.7959 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0448 |           6.2560 |           4.8194 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0480 |           6.1993 |           4.8430 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0521 |           6.0842 |           4.8592 |
[32m[20221208 14:42:22 @agent_ppo2.py:179][0m |          -0.0547 |           6.0429 |           4.8731 |
[32m[20221208 14:42:22 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:42:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.46
[32m[20221208 14:42:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.24
[32m[20221208 14:42:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.35
[32m[20221208 14:42:23 @agent_ppo2.py:137][0m Total time:       5.81 min
[32m[20221208 14:42:23 @agent_ppo2.py:139][0m 468992 total steps have happened
[32m[20221208 14:42:23 @agent_ppo2.py:115][0m #------------------------ Iteration 229 --------------------------#
[32m[20221208 14:42:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:23 @agent_ppo2.py:179][0m |           0.0201 |           5.8574 |           4.7888 |
[32m[20221208 14:42:23 @agent_ppo2.py:179][0m |           0.0001 |           5.7409 |           4.7227 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0004 |           5.6445 |           4.7291 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0191 |           5.5955 |           4.7491 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0331 |           5.5760 |           4.8098 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0383 |           5.5305 |           4.8505 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0424 |           5.5361 |           4.8627 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0444 |           5.5106 |           4.8703 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0465 |           5.5006 |           4.8893 |
[32m[20221208 14:42:24 @agent_ppo2.py:179][0m |          -0.0436 |           5.5484 |           4.9030 |
[32m[20221208 14:42:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.16
[32m[20221208 14:42:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.86
[32m[20221208 14:42:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.91
[32m[20221208 14:42:24 @agent_ppo2.py:137][0m Total time:       5.83 min
[32m[20221208 14:42:24 @agent_ppo2.py:139][0m 471040 total steps have happened
[32m[20221208 14:42:24 @agent_ppo2.py:115][0m #------------------------ Iteration 230 --------------------------#
[32m[20221208 14:42:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |           0.0343 |           7.2270 |           4.9623 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0116 |           6.7660 |           4.9852 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0246 |           6.6144 |           5.0066 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0331 |           6.4216 |           5.0314 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0418 |           6.2870 |           5.0596 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0487 |           6.1811 |           5.0816 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0508 |           6.1872 |           5.0747 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0518 |           6.1008 |           5.0878 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0546 |           6.0173 |           5.0895 |
[32m[20221208 14:42:25 @agent_ppo2.py:179][0m |          -0.0584 |           5.9728 |           5.1368 |
[32m[20221208 14:42:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.48
[32m[20221208 14:42:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.70
[32m[20221208 14:42:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.24
[32m[20221208 14:42:26 @agent_ppo2.py:137][0m Total time:       5.86 min
[32m[20221208 14:42:26 @agent_ppo2.py:139][0m 473088 total steps have happened
[32m[20221208 14:42:26 @agent_ppo2.py:115][0m #------------------------ Iteration 231 --------------------------#
[32m[20221208 14:42:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:26 @agent_ppo2.py:179][0m |           0.0419 |           5.7752 |           4.9078 |
[32m[20221208 14:42:26 @agent_ppo2.py:179][0m |           0.0040 |           5.2417 |           4.8019 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0223 |           5.1366 |           4.8982 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0313 |           5.0184 |           4.9450 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0407 |           4.9830 |           4.9836 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0463 |           4.9045 |           5.0244 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0491 |           4.8888 |           5.0504 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0524 |           4.8294 |           5.0607 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0481 |           4.8315 |           5.0662 |
[32m[20221208 14:42:27 @agent_ppo2.py:179][0m |          -0.0521 |           4.8215 |           5.0642 |
[32m[20221208 14:42:27 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.33
[32m[20221208 14:42:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.60
[32m[20221208 14:42:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.49
[32m[20221208 14:42:27 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 179.49
[32m[20221208 14:42:27 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 179.49
[32m[20221208 14:42:27 @agent_ppo2.py:137][0m Total time:       5.89 min
[32m[20221208 14:42:27 @agent_ppo2.py:139][0m 475136 total steps have happened
[32m[20221208 14:42:27 @agent_ppo2.py:115][0m #------------------------ Iteration 232 --------------------------#
[32m[20221208 14:42:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |           0.0442 |           5.4991 |           5.0964 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |           0.0149 |           5.3273 |           4.8703 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0159 |           5.2129 |           5.0083 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0295 |           5.1614 |           5.0876 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0357 |           5.0904 |           5.0958 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0419 |           5.0703 |           5.1241 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0417 |           5.0566 |           5.1376 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0447 |           5.0431 |           5.1375 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0483 |           5.0278 |           5.1700 |
[32m[20221208 14:42:28 @agent_ppo2.py:179][0m |          -0.0520 |           5.0241 |           5.1989 |
[32m[20221208 14:42:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.45
[32m[20221208 14:42:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.05
[32m[20221208 14:42:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.74
[32m[20221208 14:42:29 @agent_ppo2.py:137][0m Total time:       5.91 min
[32m[20221208 14:42:29 @agent_ppo2.py:139][0m 477184 total steps have happened
[32m[20221208 14:42:29 @agent_ppo2.py:115][0m #------------------------ Iteration 233 --------------------------#
[32m[20221208 14:42:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:29 @agent_ppo2.py:179][0m |           0.0275 |           6.0342 |           5.0252 |
[32m[20221208 14:42:29 @agent_ppo2.py:179][0m |          -0.0063 |           5.4711 |           5.0216 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0188 |           5.3282 |           5.0563 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0331 |           5.2244 |           5.1002 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0391 |           5.2038 |           5.1073 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0437 |           5.1265 |           5.1425 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0422 |           5.1108 |           5.1255 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0475 |           5.0442 |           5.1393 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0489 |           5.0671 |           5.1673 |
[32m[20221208 14:42:30 @agent_ppo2.py:179][0m |          -0.0497 |           5.0191 |           5.1819 |
[32m[20221208 14:42:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.91
[32m[20221208 14:42:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.45
[32m[20221208 14:42:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.45
[32m[20221208 14:42:30 @agent_ppo2.py:137][0m Total time:       5.93 min
[32m[20221208 14:42:30 @agent_ppo2.py:139][0m 479232 total steps have happened
[32m[20221208 14:42:30 @agent_ppo2.py:115][0m #------------------------ Iteration 234 --------------------------#
[32m[20221208 14:42:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |           0.0320 |           4.2181 |           5.0608 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0026 |           3.6956 |           5.0156 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0215 |           3.5279 |           5.0822 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0336 |           3.4125 |           5.0867 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0388 |           3.3375 |           5.1168 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0420 |           3.2808 |           5.1234 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0460 |           3.2358 |           5.1515 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0464 |           3.1988 |           5.1565 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0476 |           3.1748 |           5.1664 |
[32m[20221208 14:42:31 @agent_ppo2.py:179][0m |          -0.0511 |           3.1466 |           5.1964 |
[32m[20221208 14:42:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.09
[32m[20221208 14:42:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.78
[32m[20221208 14:42:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 176.09
[32m[20221208 14:42:32 @agent_ppo2.py:137][0m Total time:       5.96 min
[32m[20221208 14:42:32 @agent_ppo2.py:139][0m 481280 total steps have happened
[32m[20221208 14:42:32 @agent_ppo2.py:115][0m #------------------------ Iteration 235 --------------------------#
[32m[20221208 14:42:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:32 @agent_ppo2.py:179][0m |           0.0296 |           5.9870 |           5.1210 |
[32m[20221208 14:42:32 @agent_ppo2.py:179][0m |           0.0092 |           5.7309 |           5.0471 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0142 |           5.5732 |           5.0974 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0297 |           5.5311 |           5.1403 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0291 |           5.4800 |           5.1415 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0374 |           5.3809 |           5.1602 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0398 |           5.3850 |           5.1919 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0452 |           5.3569 |           5.2317 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0485 |           5.2535 |           5.2252 |
[32m[20221208 14:42:33 @agent_ppo2.py:179][0m |          -0.0503 |           5.2352 |           5.2742 |
[32m[20221208 14:42:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.39
[32m[20221208 14:42:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.17
[32m[20221208 14:42:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.00
[32m[20221208 14:42:33 @agent_ppo2.py:137][0m Total time:       5.98 min
[32m[20221208 14:42:33 @agent_ppo2.py:139][0m 483328 total steps have happened
[32m[20221208 14:42:33 @agent_ppo2.py:115][0m #------------------------ Iteration 236 --------------------------#
[32m[20221208 14:42:34 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:42:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |           0.0407 |           5.6507 |           5.1891 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |           0.0008 |           5.3077 |           5.1994 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0217 |           5.1626 |           5.2857 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0313 |           5.1048 |           5.2973 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0385 |           5.0470 |           5.3211 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0405 |           4.9944 |           5.3204 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0441 |           4.9876 |           5.3379 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0485 |           4.9379 |           5.3461 |
[32m[20221208 14:42:34 @agent_ppo2.py:179][0m |          -0.0517 |           4.8727 |           5.3833 |
[32m[20221208 14:42:35 @agent_ppo2.py:179][0m |          -0.0516 |           4.8986 |           5.3702 |
[32m[20221208 14:42:35 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:42:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.84
[32m[20221208 14:42:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.30
[32m[20221208 14:42:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.16
[32m[20221208 14:42:35 @agent_ppo2.py:137][0m Total time:       6.01 min
[32m[20221208 14:42:35 @agent_ppo2.py:139][0m 485376 total steps have happened
[32m[20221208 14:42:35 @agent_ppo2.py:115][0m #------------------------ Iteration 237 --------------------------#
[32m[20221208 14:42:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:35 @agent_ppo2.py:179][0m |           0.0348 |           7.6458 |           5.1978 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |           0.0153 |           7.3340 |           4.9722 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0173 |           7.1144 |           5.1876 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0305 |           7.0569 |           5.2292 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0392 |           6.9866 |           5.2420 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0380 |           6.9828 |           5.2575 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0444 |           6.9406 |           5.3010 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0470 |           6.8239 |           5.2875 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0500 |           6.8383 |           5.3036 |
[32m[20221208 14:42:36 @agent_ppo2.py:179][0m |          -0.0521 |           6.7778 |           5.3205 |
[32m[20221208 14:42:36 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:42:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.20
[32m[20221208 14:42:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.40
[32m[20221208 14:42:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.01
[32m[20221208 14:42:36 @agent_ppo2.py:137][0m Total time:       6.04 min
[32m[20221208 14:42:36 @agent_ppo2.py:139][0m 487424 total steps have happened
[32m[20221208 14:42:36 @agent_ppo2.py:115][0m #------------------------ Iteration 238 --------------------------#
[32m[20221208 14:42:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |           0.0413 |           4.0949 |           5.0953 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |           0.0095 |           3.7526 |           5.0315 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0096 |           3.5765 |           5.0720 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0266 |           3.3824 |           5.1873 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0285 |           3.2848 |           5.2041 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0391 |           3.1264 |           5.2350 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0420 |           2.9637 |           5.2407 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0430 |           2.8295 |           5.2577 |
[32m[20221208 14:42:37 @agent_ppo2.py:179][0m |          -0.0470 |           2.7074 |           5.2948 |
[32m[20221208 14:42:38 @agent_ppo2.py:179][0m |          -0.0507 |           2.5875 |           5.3265 |
[32m[20221208 14:42:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.84
[32m[20221208 14:42:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.94
[32m[20221208 14:42:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.98
[32m[20221208 14:42:38 @agent_ppo2.py:137][0m Total time:       6.06 min
[32m[20221208 14:42:38 @agent_ppo2.py:139][0m 489472 total steps have happened
[32m[20221208 14:42:38 @agent_ppo2.py:115][0m #------------------------ Iteration 239 --------------------------#
[32m[20221208 14:42:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:38 @agent_ppo2.py:179][0m |           0.0217 |           5.7564 |           5.3734 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0110 |           5.2069 |           5.3115 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0217 |           5.0353 |           5.3687 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0356 |           4.9696 |           5.4226 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0421 |           4.8986 |           5.4663 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0460 |           4.8489 |           5.4689 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0502 |           4.7927 |           5.5080 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0515 |           4.7748 |           5.5255 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0491 |           4.7516 |           5.5348 |
[32m[20221208 14:42:39 @agent_ppo2.py:179][0m |          -0.0509 |           4.7356 |           5.5381 |
[32m[20221208 14:42:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.91
[32m[20221208 14:42:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.97
[32m[20221208 14:42:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 185.82
[32m[20221208 14:42:39 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 185.82
[32m[20221208 14:42:39 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 185.82
[32m[20221208 14:42:39 @agent_ppo2.py:137][0m Total time:       6.09 min
[32m[20221208 14:42:39 @agent_ppo2.py:139][0m 491520 total steps have happened
[32m[20221208 14:42:39 @agent_ppo2.py:115][0m #------------------------ Iteration 240 --------------------------#
[32m[20221208 14:42:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |           0.0235 |           3.3393 |           5.5350 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |           0.0026 |           3.1372 |           5.1553 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0191 |           3.0609 |           5.5224 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0271 |           3.0220 |           5.6256 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0335 |           2.9904 |           5.5165 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0386 |           2.9692 |           5.6733 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0413 |           2.9412 |           5.7095 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0422 |           2.9366 |           5.7303 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0433 |           2.9327 |           5.6652 |
[32m[20221208 14:42:40 @agent_ppo2.py:179][0m |          -0.0466 |           2.8955 |           5.7193 |
[32m[20221208 14:42:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.70
[32m[20221208 14:42:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.47
[32m[20221208 14:42:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.88
[32m[20221208 14:42:41 @agent_ppo2.py:137][0m Total time:       6.11 min
[32m[20221208 14:42:41 @agent_ppo2.py:139][0m 493568 total steps have happened
[32m[20221208 14:42:41 @agent_ppo2.py:115][0m #------------------------ Iteration 241 --------------------------#
[32m[20221208 14:42:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:41 @agent_ppo2.py:179][0m |           0.0410 |           5.9624 |           5.8105 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |           0.0118 |           5.1495 |           5.6799 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0149 |           4.9396 |           5.7330 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0341 |           4.7694 |           5.8002 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0430 |           4.6868 |           5.8289 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0493 |           4.5810 |           5.8641 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0520 |           4.5274 |           5.8859 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0539 |           4.4620 |           5.8906 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0587 |           4.3808 |           5.9155 |
[32m[20221208 14:42:42 @agent_ppo2.py:179][0m |          -0.0595 |           4.3490 |           5.9224 |
[32m[20221208 14:42:42 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.31
[32m[20221208 14:42:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.79
[32m[20221208 14:42:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.71
[32m[20221208 14:42:42 @agent_ppo2.py:137][0m Total time:       6.14 min
[32m[20221208 14:42:42 @agent_ppo2.py:139][0m 495616 total steps have happened
[32m[20221208 14:42:42 @agent_ppo2.py:115][0m #------------------------ Iteration 242 --------------------------#
[32m[20221208 14:42:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |           0.0342 |          10.8623 |           5.8030 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0133 |           9.0826 |           5.6912 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0333 |           8.4956 |           5.7719 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0452 |           8.0371 |           5.8178 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0469 |           7.7499 |           5.8188 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0533 |           7.4527 |           5.8506 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0542 |           7.3320 |           5.8611 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0565 |           7.1300 |           5.8670 |
[32m[20221208 14:42:43 @agent_ppo2.py:179][0m |          -0.0586 |           7.0498 |           5.8902 |
[32m[20221208 14:42:44 @agent_ppo2.py:179][0m |          -0.0618 |           6.9007 |           5.8988 |
[32m[20221208 14:42:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.92
[32m[20221208 14:42:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.72
[32m[20221208 14:42:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.23
[32m[20221208 14:42:44 @agent_ppo2.py:137][0m Total time:       6.16 min
[32m[20221208 14:42:44 @agent_ppo2.py:139][0m 497664 total steps have happened
[32m[20221208 14:42:44 @agent_ppo2.py:115][0m #------------------------ Iteration 243 --------------------------#
[32m[20221208 14:42:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:44 @agent_ppo2.py:179][0m |           0.0372 |           6.6588 |           5.9054 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |           0.0042 |           5.9902 |           5.8566 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0213 |           5.7231 |           5.9345 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0269 |           5.5077 |           6.0046 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0378 |           5.3680 |           6.0500 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0312 |           5.2399 |           6.0518 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0415 |           5.0902 |           6.0808 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0421 |           5.0005 |           6.0886 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0510 |           4.9204 |           6.1407 |
[32m[20221208 14:42:45 @agent_ppo2.py:179][0m |          -0.0500 |           4.8040 |           6.1291 |
[32m[20221208 14:42:45 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.48
[32m[20221208 14:42:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.06
[32m[20221208 14:42:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 177.03
[32m[20221208 14:42:45 @agent_ppo2.py:137][0m Total time:       6.19 min
[32m[20221208 14:42:45 @agent_ppo2.py:139][0m 499712 total steps have happened
[32m[20221208 14:42:45 @agent_ppo2.py:115][0m #------------------------ Iteration 244 --------------------------#
[32m[20221208 14:42:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |           0.0196 |           8.8575 |           5.8043 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |           0.0017 |           8.2930 |           5.6470 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0236 |           8.1345 |           5.7276 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0363 |           8.0270 |           5.8064 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0422 |           8.0313 |           5.7980 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0476 |           7.9101 |           5.8471 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0509 |           7.8790 |           5.8715 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0537 |           7.8170 |           5.8910 |
[32m[20221208 14:42:46 @agent_ppo2.py:179][0m |          -0.0552 |           7.8375 |           5.9149 |
[32m[20221208 14:42:47 @agent_ppo2.py:179][0m |          -0.0553 |           7.7855 |           5.8955 |
[32m[20221208 14:42:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.83
[32m[20221208 14:42:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.94
[32m[20221208 14:42:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.89
[32m[20221208 14:42:47 @agent_ppo2.py:137][0m Total time:       6.21 min
[32m[20221208 14:42:47 @agent_ppo2.py:139][0m 501760 total steps have happened
[32m[20221208 14:42:47 @agent_ppo2.py:115][0m #------------------------ Iteration 245 --------------------------#
[32m[20221208 14:42:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |           0.0365 |           6.4028 |           5.9362 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |           0.0138 |           6.0253 |           5.9243 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0148 |           5.9210 |           5.9974 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0270 |           5.8852 |           6.0895 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0336 |           5.8119 |           6.0924 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0363 |           5.7666 |           6.1085 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0414 |           5.7120 |           6.1146 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0423 |           5.7153 |           6.1325 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0451 |           5.6835 |           6.1847 |
[32m[20221208 14:42:48 @agent_ppo2.py:179][0m |          -0.0478 |           5.6955 |           6.1731 |
[32m[20221208 14:42:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:42:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.22
[32m[20221208 14:42:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.12
[32m[20221208 14:42:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.97
[32m[20221208 14:42:48 @agent_ppo2.py:137][0m Total time:       6.24 min
[32m[20221208 14:42:48 @agent_ppo2.py:139][0m 503808 total steps have happened
[32m[20221208 14:42:48 @agent_ppo2.py:115][0m #------------------------ Iteration 246 --------------------------#
[32m[20221208 14:42:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |           0.0973 |           3.6622 |           6.0838 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |           0.0312 |           3.4357 |           5.9179 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |          -0.0070 |           3.3691 |           6.1835 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |          -0.0052 |           3.3349 |           6.0831 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |          -0.0143 |           3.3201 |           5.9561 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |           0.0057 |           3.3002 |           5.9300 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |          -0.0261 |           3.2854 |           6.2776 |
[32m[20221208 14:42:49 @agent_ppo2.py:179][0m |          -0.0203 |           3.2648 |           6.1273 |
[32m[20221208 14:42:50 @agent_ppo2.py:179][0m |          -0.0335 |           3.2666 |           6.2613 |
[32m[20221208 14:42:50 @agent_ppo2.py:179][0m |          -0.0329 |           3.2514 |           6.3127 |
[32m[20221208 14:42:50 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:42:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.88
[32m[20221208 14:42:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.72
[32m[20221208 14:42:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.75
[32m[20221208 14:42:50 @agent_ppo2.py:137][0m Total time:       6.26 min
[32m[20221208 14:42:50 @agent_ppo2.py:139][0m 505856 total steps have happened
[32m[20221208 14:42:50 @agent_ppo2.py:115][0m #------------------------ Iteration 247 --------------------------#
[32m[20221208 14:42:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |           0.0425 |           6.1151 |           6.1057 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |           0.0173 |           5.5230 |           5.9252 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0132 |           5.2843 |           6.1286 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0238 |           5.0716 |           6.1993 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0297 |           4.9106 |           6.2526 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0379 |           4.7561 |           6.2824 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0408 |           4.7070 |           6.3022 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0444 |           4.5789 |           6.3295 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0480 |           4.4525 |           6.3559 |
[32m[20221208 14:42:51 @agent_ppo2.py:179][0m |          -0.0486 |           4.3739 |           6.3663 |
[32m[20221208 14:42:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:42:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.13
[32m[20221208 14:42:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.62
[32m[20221208 14:42:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.85
[32m[20221208 14:42:51 @agent_ppo2.py:137][0m Total time:       6.29 min
[32m[20221208 14:42:51 @agent_ppo2.py:139][0m 507904 total steps have happened
[32m[20221208 14:42:51 @agent_ppo2.py:115][0m #------------------------ Iteration 248 --------------------------#
[32m[20221208 14:42:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |           0.0247 |           9.2094 |           6.0739 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0066 |           8.4383 |           5.9510 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0264 |           8.1110 |           6.0792 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0396 |           8.1126 |           6.1536 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0442 |           7.9994 |           6.1550 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0481 |           8.0232 |           6.1698 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0510 |           7.9667 |           6.2029 |
[32m[20221208 14:42:52 @agent_ppo2.py:179][0m |          -0.0558 |           7.8742 |           6.2161 |
[32m[20221208 14:42:53 @agent_ppo2.py:179][0m |          -0.0570 |           7.8110 |           6.2226 |
[32m[20221208 14:42:53 @agent_ppo2.py:179][0m |          -0.0580 |           7.7565 |           6.2315 |
[32m[20221208 14:42:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.24
[32m[20221208 14:42:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.27
[32m[20221208 14:42:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.67
[32m[20221208 14:42:53 @agent_ppo2.py:137][0m Total time:       6.31 min
[32m[20221208 14:42:53 @agent_ppo2.py:139][0m 509952 total steps have happened
[32m[20221208 14:42:53 @agent_ppo2.py:115][0m #------------------------ Iteration 249 --------------------------#
[32m[20221208 14:42:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |           0.0461 |           4.5679 |           6.2468 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |           0.0085 |           4.2231 |           6.2830 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |           0.0023 |           4.0314 |           6.2752 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0108 |           3.8029 |           6.2805 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0249 |           3.6801 |           6.4012 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0241 |           3.5495 |           6.3649 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0299 |           3.4656 |           6.3975 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0386 |           3.4227 |           6.4086 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0384 |           3.3594 |           6.4727 |
[32m[20221208 14:42:54 @agent_ppo2.py:179][0m |          -0.0376 |           3.3055 |           6.5219 |
[32m[20221208 14:42:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:42:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.95
[32m[20221208 14:42:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.00
[32m[20221208 14:42:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.41
[32m[20221208 14:42:55 @agent_ppo2.py:137][0m Total time:       6.34 min
[32m[20221208 14:42:55 @agent_ppo2.py:139][0m 512000 total steps have happened
[32m[20221208 14:42:55 @agent_ppo2.py:115][0m #------------------------ Iteration 250 --------------------------#
[32m[20221208 14:42:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:42:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |           0.0144 |           8.1410 |           6.5515 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0165 |           7.1521 |           6.4834 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0319 |           6.8268 |           6.5408 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0365 |           6.6034 |           6.5447 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0452 |           6.4248 |           6.5765 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0468 |           6.3327 |           6.5825 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0512 |           6.2266 |           6.5908 |
[32m[20221208 14:42:55 @agent_ppo2.py:179][0m |          -0.0538 |           6.1146 |           6.6149 |
[32m[20221208 14:42:56 @agent_ppo2.py:179][0m |          -0.0573 |           6.0392 |           6.6197 |
[32m[20221208 14:42:56 @agent_ppo2.py:179][0m |          -0.0595 |           6.0001 |           6.6414 |
[32m[20221208 14:42:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.47
[32m[20221208 14:42:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.06
[32m[20221208 14:42:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.84
[32m[20221208 14:42:56 @agent_ppo2.py:137][0m Total time:       6.36 min
[32m[20221208 14:42:56 @agent_ppo2.py:139][0m 514048 total steps have happened
[32m[20221208 14:42:56 @agent_ppo2.py:115][0m #------------------------ Iteration 251 --------------------------#
[32m[20221208 14:42:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |           0.0289 |           6.9411 |           6.3604 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |           0.0118 |           6.2289 |           6.2279 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0203 |           5.8554 |           6.4538 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0305 |           5.7213 |           6.4791 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0347 |           5.4950 |           6.5197 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0400 |           5.4043 |           6.5873 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0459 |           5.3375 |           6.6068 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0465 |           5.2762 |           6.6256 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0447 |           5.1907 |           6.6544 |
[32m[20221208 14:42:57 @agent_ppo2.py:179][0m |          -0.0511 |           5.1549 |           6.6912 |
[32m[20221208 14:42:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:42:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.41
[32m[20221208 14:42:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.95
[32m[20221208 14:42:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.56
[32m[20221208 14:42:58 @agent_ppo2.py:137][0m Total time:       6.39 min
[32m[20221208 14:42:58 @agent_ppo2.py:139][0m 516096 total steps have happened
[32m[20221208 14:42:58 @agent_ppo2.py:115][0m #------------------------ Iteration 252 --------------------------#
[32m[20221208 14:42:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:42:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |           0.0367 |           8.3808 |           6.5834 |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |           0.0025 |           7.9918 |           6.2808 |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |          -0.0257 |           7.8114 |           6.5998 |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |          -0.0342 |           7.7435 |           6.6618 |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |          -0.0377 |           7.7127 |           6.6511 |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |          -0.0441 |           7.6769 |           6.7093 |
[32m[20221208 14:42:58 @agent_ppo2.py:179][0m |          -0.0484 |           7.5984 |           6.7076 |
[32m[20221208 14:42:59 @agent_ppo2.py:179][0m |          -0.0510 |           7.6146 |           6.7688 |
[32m[20221208 14:42:59 @agent_ppo2.py:179][0m |          -0.0487 |           7.5339 |           6.7631 |
[32m[20221208 14:42:59 @agent_ppo2.py:179][0m |          -0.0552 |           7.5125 |           6.7939 |
[32m[20221208 14:42:59 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:42:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.92
[32m[20221208 14:42:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.05
[32m[20221208 14:42:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.16
[32m[20221208 14:42:59 @agent_ppo2.py:137][0m Total time:       6.41 min
[32m[20221208 14:42:59 @agent_ppo2.py:139][0m 518144 total steps have happened
[32m[20221208 14:42:59 @agent_ppo2.py:115][0m #------------------------ Iteration 253 --------------------------#
[32m[20221208 14:43:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |           0.0332 |           8.3901 |           6.7812 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |           0.0074 |           8.1354 |           6.7133 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0133 |           8.0056 |           6.8257 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0283 |           7.9864 |           6.8619 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0335 |           7.9299 |           6.9043 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0380 |           7.9123 |           6.9241 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0424 |           7.8547 |           6.9477 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0448 |           7.8326 |           6.9674 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0459 |           7.8073 |           6.9853 |
[32m[20221208 14:43:00 @agent_ppo2.py:179][0m |          -0.0483 |           7.8390 |           6.9840 |
[32m[20221208 14:43:00 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:43:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.76
[32m[20221208 14:43:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.64
[32m[20221208 14:43:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.36
[32m[20221208 14:43:01 @agent_ppo2.py:137][0m Total time:       6.44 min
[32m[20221208 14:43:01 @agent_ppo2.py:139][0m 520192 total steps have happened
[32m[20221208 14:43:01 @agent_ppo2.py:115][0m #------------------------ Iteration 254 --------------------------#
[32m[20221208 14:43:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:01 @agent_ppo2.py:179][0m |           0.0462 |           8.9117 |           6.8067 |
[32m[20221208 14:43:01 @agent_ppo2.py:179][0m |           0.0218 |           8.5327 |           6.5434 |
[32m[20221208 14:43:01 @agent_ppo2.py:179][0m |          -0.0101 |           8.4196 |           6.7665 |
[32m[20221208 14:43:01 @agent_ppo2.py:179][0m |          -0.0269 |           8.4446 |           6.9464 |
[32m[20221208 14:43:01 @agent_ppo2.py:179][0m |          -0.0352 |           8.3094 |           7.0131 |
[32m[20221208 14:43:01 @agent_ppo2.py:179][0m |          -0.0390 |           8.3023 |           7.0595 |
[32m[20221208 14:43:02 @agent_ppo2.py:179][0m |          -0.0431 |           8.3004 |           7.0722 |
[32m[20221208 14:43:02 @agent_ppo2.py:179][0m |          -0.0463 |           8.2433 |           7.1147 |
[32m[20221208 14:43:02 @agent_ppo2.py:179][0m |          -0.0464 |           8.2428 |           7.1334 |
[32m[20221208 14:43:02 @agent_ppo2.py:179][0m |          -0.0514 |           8.1726 |           7.1528 |
[32m[20221208 14:43:02 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.30
[32m[20221208 14:43:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.42
[32m[20221208 14:43:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.64
[32m[20221208 14:43:02 @agent_ppo2.py:137][0m Total time:       6.46 min
[32m[20221208 14:43:02 @agent_ppo2.py:139][0m 522240 total steps have happened
[32m[20221208 14:43:02 @agent_ppo2.py:115][0m #------------------------ Iteration 255 --------------------------#
[32m[20221208 14:43:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |           0.0369 |           6.9186 |           7.1178 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |           0.0275 |           6.3076 |           6.7642 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |           0.0064 |           6.1226 |           6.6601 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0178 |           6.0708 |           6.8451 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0334 |           5.9298 |           7.0306 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0387 |           5.8887 |           7.1102 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0420 |           5.8035 |           7.1624 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0460 |           5.7502 |           7.1982 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0462 |           5.7501 |           7.1545 |
[32m[20221208 14:43:03 @agent_ppo2.py:179][0m |          -0.0464 |           5.6784 |           7.1954 |
[32m[20221208 14:43:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.72
[32m[20221208 14:43:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.65
[32m[20221208 14:43:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.48
[32m[20221208 14:43:04 @agent_ppo2.py:137][0m Total time:       6.49 min
[32m[20221208 14:43:04 @agent_ppo2.py:139][0m 524288 total steps have happened
[32m[20221208 14:43:04 @agent_ppo2.py:115][0m #------------------------ Iteration 256 --------------------------#
[32m[20221208 14:43:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:04 @agent_ppo2.py:179][0m |           0.0305 |           8.3203 |           7.1649 |
[32m[20221208 14:43:04 @agent_ppo2.py:179][0m |          -0.0101 |           7.9638 |           7.0897 |
[32m[20221208 14:43:04 @agent_ppo2.py:179][0m |          -0.0278 |           7.8514 |           7.2055 |
[32m[20221208 14:43:04 @agent_ppo2.py:179][0m |          -0.0356 |           7.8238 |           7.2542 |
[32m[20221208 14:43:04 @agent_ppo2.py:179][0m |          -0.0420 |           7.7385 |           7.2567 |
[32m[20221208 14:43:05 @agent_ppo2.py:179][0m |          -0.0459 |           7.6784 |           7.3096 |
[32m[20221208 14:43:05 @agent_ppo2.py:179][0m |          -0.0493 |           7.6187 |           7.3005 |
[32m[20221208 14:43:05 @agent_ppo2.py:179][0m |          -0.0509 |           7.5895 |           7.3300 |
[32m[20221208 14:43:05 @agent_ppo2.py:179][0m |          -0.0547 |           7.5520 |           7.3553 |
[32m[20221208 14:43:05 @agent_ppo2.py:179][0m |          -0.0545 |           7.5271 |           7.3492 |
[32m[20221208 14:43:05 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:43:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.83
[32m[20221208 14:43:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.46
[32m[20221208 14:43:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.73
[32m[20221208 14:43:05 @agent_ppo2.py:137][0m Total time:       6.51 min
[32m[20221208 14:43:05 @agent_ppo2.py:139][0m 526336 total steps have happened
[32m[20221208 14:43:05 @agent_ppo2.py:115][0m #------------------------ Iteration 257 --------------------------#
[32m[20221208 14:43:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |           0.0429 |           6.3642 |           6.9543 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |           0.0168 |           6.0233 |           6.7821 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0176 |           5.8787 |           7.0032 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0261 |           5.8318 |           7.0416 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0357 |           5.6716 |           7.0719 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0411 |           5.5816 |           7.0903 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0429 |           5.4920 |           7.1390 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0478 |           5.4103 |           7.1630 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0426 |           5.3628 |           7.1776 |
[32m[20221208 14:43:06 @agent_ppo2.py:179][0m |          -0.0494 |           5.2301 |           7.2094 |
[32m[20221208 14:43:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.03
[32m[20221208 14:43:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.75
[32m[20221208 14:43:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.31
[32m[20221208 14:43:07 @agent_ppo2.py:137][0m Total time:       6.54 min
[32m[20221208 14:43:07 @agent_ppo2.py:139][0m 528384 total steps have happened
[32m[20221208 14:43:07 @agent_ppo2.py:115][0m #------------------------ Iteration 258 --------------------------#
[32m[20221208 14:43:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:07 @agent_ppo2.py:179][0m |           0.0351 |           4.9679 |           7.2109 |
[32m[20221208 14:43:07 @agent_ppo2.py:179][0m |           0.0085 |           4.5179 |           7.0279 |
[32m[20221208 14:43:07 @agent_ppo2.py:179][0m |          -0.0217 |           4.3605 |           7.2002 |
[32m[20221208 14:43:07 @agent_ppo2.py:179][0m |          -0.0328 |           4.2557 |           7.2585 |
[32m[20221208 14:43:07 @agent_ppo2.py:179][0m |          -0.0379 |           4.1715 |           7.2722 |
[32m[20221208 14:43:08 @agent_ppo2.py:179][0m |          -0.0416 |           4.0755 |           7.3234 |
[32m[20221208 14:43:08 @agent_ppo2.py:179][0m |          -0.0506 |           4.0022 |           7.3266 |
[32m[20221208 14:43:08 @agent_ppo2.py:179][0m |          -0.0480 |           4.0208 |           7.3450 |
[32m[20221208 14:43:08 @agent_ppo2.py:179][0m |          -0.0541 |           3.9282 |           7.3718 |
[32m[20221208 14:43:08 @agent_ppo2.py:179][0m |          -0.0559 |           3.8846 |           7.3979 |
[32m[20221208 14:43:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.09
[32m[20221208 14:43:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.63
[32m[20221208 14:43:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.54
[32m[20221208 14:43:08 @agent_ppo2.py:137][0m Total time:       6.57 min
[32m[20221208 14:43:08 @agent_ppo2.py:139][0m 530432 total steps have happened
[32m[20221208 14:43:08 @agent_ppo2.py:115][0m #------------------------ Iteration 259 --------------------------#
[32m[20221208 14:43:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |           0.0203 |           4.3319 |           7.2259 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0049 |           4.0275 |           7.1910 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0114 |           3.8298 |           7.1302 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0219 |           3.7513 |           7.2829 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0272 |           3.6427 |           7.1605 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0337 |           3.6021 |           7.2553 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0368 |           3.5735 |           7.3612 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0363 |           3.5156 |           7.3079 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0391 |           3.4355 |           7.3859 |
[32m[20221208 14:43:09 @agent_ppo2.py:179][0m |          -0.0400 |           3.4377 |           7.3318 |
[32m[20221208 14:43:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.40
[32m[20221208 14:43:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.24
[32m[20221208 14:43:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.68
[32m[20221208 14:43:10 @agent_ppo2.py:137][0m Total time:       6.59 min
[32m[20221208 14:43:10 @agent_ppo2.py:139][0m 532480 total steps have happened
[32m[20221208 14:43:10 @agent_ppo2.py:115][0m #------------------------ Iteration 260 --------------------------#
[32m[20221208 14:43:10 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:43:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:10 @agent_ppo2.py:179][0m |           0.0370 |           6.4909 |           7.2623 |
[32m[20221208 14:43:10 @agent_ppo2.py:179][0m |           0.0449 |           5.9735 |           7.2918 |
[32m[20221208 14:43:10 @agent_ppo2.py:179][0m |          -0.0035 |           5.7946 |           7.4131 |
[32m[20221208 14:43:10 @agent_ppo2.py:179][0m |          -0.0238 |           5.6924 |           7.5299 |
[32m[20221208 14:43:11 @agent_ppo2.py:179][0m |          -0.0326 |           5.5976 |           7.5887 |
[32m[20221208 14:43:11 @agent_ppo2.py:179][0m |          -0.0375 |           5.4900 |           7.6457 |
[32m[20221208 14:43:11 @agent_ppo2.py:179][0m |          -0.0419 |           5.4325 |           7.6495 |
[32m[20221208 14:43:11 @agent_ppo2.py:179][0m |          -0.0451 |           5.3162 |           7.6855 |
[32m[20221208 14:43:11 @agent_ppo2.py:179][0m |          -0.0481 |           5.2946 |           7.7166 |
[32m[20221208 14:43:11 @agent_ppo2.py:179][0m |          -0.0491 |           5.2362 |           7.7418 |
[32m[20221208 14:43:11 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.75
[32m[20221208 14:43:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.50
[32m[20221208 14:43:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.85
[32m[20221208 14:43:11 @agent_ppo2.py:137][0m Total time:       6.62 min
[32m[20221208 14:43:11 @agent_ppo2.py:139][0m 534528 total steps have happened
[32m[20221208 14:43:11 @agent_ppo2.py:115][0m #------------------------ Iteration 261 --------------------------#
[32m[20221208 14:43:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |           0.0315 |           5.8958 |           7.3324 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0016 |           4.9645 |           7.3163 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0173 |           4.5833 |           7.5211 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0270 |           4.4714 |           7.5770 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0318 |           4.2750 |           7.5576 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0398 |           4.1233 |           7.6422 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0439 |           4.0249 |           7.6620 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0452 |           3.9326 |           7.6725 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0472 |           3.8623 |           7.7125 |
[32m[20221208 14:43:12 @agent_ppo2.py:179][0m |          -0.0487 |           3.8191 |           7.7115 |
[32m[20221208 14:43:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:43:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.10
[32m[20221208 14:43:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.75
[32m[20221208 14:43:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.35
[32m[20221208 14:43:13 @agent_ppo2.py:137][0m Total time:       6.64 min
[32m[20221208 14:43:13 @agent_ppo2.py:139][0m 536576 total steps have happened
[32m[20221208 14:43:13 @agent_ppo2.py:115][0m #------------------------ Iteration 262 --------------------------#
[32m[20221208 14:43:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:13 @agent_ppo2.py:179][0m |           0.0246 |           9.0788 |           7.4172 |
[32m[20221208 14:43:13 @agent_ppo2.py:179][0m |          -0.0060 |           8.5795 |           7.3935 |
[32m[20221208 14:43:13 @agent_ppo2.py:179][0m |          -0.0255 |           8.4030 |           7.4518 |
[32m[20221208 14:43:13 @agent_ppo2.py:179][0m |          -0.0343 |           8.3202 |           7.4846 |
[32m[20221208 14:43:14 @agent_ppo2.py:179][0m |          -0.0366 |           8.1702 |           7.5182 |
[32m[20221208 14:43:14 @agent_ppo2.py:179][0m |          -0.0424 |           8.0392 |           7.5316 |
[32m[20221208 14:43:14 @agent_ppo2.py:179][0m |          -0.0438 |           7.9477 |           7.5521 |
[32m[20221208 14:43:14 @agent_ppo2.py:179][0m |          -0.0472 |           7.8906 |           7.5792 |
[32m[20221208 14:43:14 @agent_ppo2.py:179][0m |          -0.0458 |           7.8161 |           7.5789 |
[32m[20221208 14:43:14 @agent_ppo2.py:179][0m |          -0.0509 |           7.7075 |           7.5717 |
[32m[20221208 14:43:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.04
[32m[20221208 14:43:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.23
[32m[20221208 14:43:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.76
[32m[20221208 14:43:14 @agent_ppo2.py:137][0m Total time:       6.67 min
[32m[20221208 14:43:14 @agent_ppo2.py:139][0m 538624 total steps have happened
[32m[20221208 14:43:14 @agent_ppo2.py:115][0m #------------------------ Iteration 263 --------------------------#
[32m[20221208 14:43:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |           0.0483 |           9.1469 |           7.5958 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |           0.0023 |           8.7286 |           7.6107 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0178 |           8.5073 |           7.6570 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0336 |           8.4172 |           7.7645 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0386 |           8.1646 |           7.7818 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0426 |           8.0595 |           7.8122 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0461 |           8.0106 |           7.8304 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0492 |           7.8946 |           7.8681 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0532 |           7.7658 |           7.8969 |
[32m[20221208 14:43:15 @agent_ppo2.py:179][0m |          -0.0564 |           7.6788 |           7.9193 |
[32m[20221208 14:43:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.55
[32m[20221208 14:43:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.90
[32m[20221208 14:43:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.98
[32m[20221208 14:43:16 @agent_ppo2.py:137][0m Total time:       6.69 min
[32m[20221208 14:43:16 @agent_ppo2.py:139][0m 540672 total steps have happened
[32m[20221208 14:43:16 @agent_ppo2.py:115][0m #------------------------ Iteration 264 --------------------------#
[32m[20221208 14:43:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:16 @agent_ppo2.py:179][0m |           0.0322 |           9.0242 |           7.6789 |
[32m[20221208 14:43:16 @agent_ppo2.py:179][0m |           0.0215 |           8.0411 |           7.3713 |
[32m[20221208 14:43:16 @agent_ppo2.py:179][0m |          -0.0176 |           7.7306 |           7.5830 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0338 |           7.5252 |           7.6545 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0437 |           7.3862 |           7.7270 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0511 |           7.2566 |           7.7809 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0542 |           7.1205 |           7.8277 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0566 |           7.0029 |           7.8512 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0616 |           6.8571 |           7.8457 |
[32m[20221208 14:43:17 @agent_ppo2.py:179][0m |          -0.0649 |           6.7843 |           7.8832 |
[32m[20221208 14:43:17 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:43:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.42
[32m[20221208 14:43:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.59
[32m[20221208 14:43:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 159.33
[32m[20221208 14:43:17 @agent_ppo2.py:137][0m Total time:       6.72 min
[32m[20221208 14:43:17 @agent_ppo2.py:139][0m 542720 total steps have happened
[32m[20221208 14:43:17 @agent_ppo2.py:115][0m #------------------------ Iteration 265 --------------------------#
[32m[20221208 14:43:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |           0.0281 |           5.2900 |           7.7496 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |           0.0218 |           4.7515 |           7.3855 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0154 |           4.5059 |           7.6448 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0293 |           4.3505 |           7.7809 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0396 |           4.2556 |           7.8668 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0464 |           4.2047 |           7.8725 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0490 |           4.1670 |           7.9410 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0484 |           4.1285 |           7.9077 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0499 |           4.0665 |           7.9594 |
[32m[20221208 14:43:18 @agent_ppo2.py:179][0m |          -0.0580 |           4.0672 |           7.9636 |
[32m[20221208 14:43:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.49
[32m[20221208 14:43:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.14
[32m[20221208 14:43:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.47
[32m[20221208 14:43:19 @agent_ppo2.py:137][0m Total time:       6.74 min
[32m[20221208 14:43:19 @agent_ppo2.py:139][0m 544768 total steps have happened
[32m[20221208 14:43:19 @agent_ppo2.py:115][0m #------------------------ Iteration 266 --------------------------#
[32m[20221208 14:43:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:19 @agent_ppo2.py:179][0m |           0.0195 |           4.6057 |           7.8529 |
[32m[20221208 14:43:19 @agent_ppo2.py:179][0m |           0.0106 |           4.0873 |           7.4555 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |           0.0040 |           3.8544 |           7.4471 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0299 |           3.7043 |           7.8168 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0378 |           3.5887 |           7.9525 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0452 |           3.5316 |           7.9121 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0497 |           3.4098 |           7.9492 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0522 |           3.3721 |           7.9821 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0557 |           3.2837 |           7.9905 |
[32m[20221208 14:43:20 @agent_ppo2.py:179][0m |          -0.0541 |           3.2478 |           8.0116 |
[32m[20221208 14:43:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.28
[32m[20221208 14:43:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.53
[32m[20221208 14:43:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.61
[32m[20221208 14:43:20 @agent_ppo2.py:137][0m Total time:       6.77 min
[32m[20221208 14:43:20 @agent_ppo2.py:139][0m 546816 total steps have happened
[32m[20221208 14:43:20 @agent_ppo2.py:115][0m #------------------------ Iteration 267 --------------------------#
[32m[20221208 14:43:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |           0.0329 |           9.6542 |           7.9039 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |           0.0130 |           9.2026 |           7.5557 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0054 |           9.1103 |           7.6822 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0285 |           8.9857 |           7.9218 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0400 |           8.9176 |           7.9841 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0458 |           8.8438 |           8.0381 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0515 |           8.7849 |           8.0647 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0523 |           8.7438 |           8.0934 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0533 |           8.6735 |           8.0883 |
[32m[20221208 14:43:21 @agent_ppo2.py:179][0m |          -0.0565 |           8.6443 |           8.1199 |
[32m[20221208 14:43:21 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.52
[32m[20221208 14:43:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.88
[32m[20221208 14:43:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.02
[32m[20221208 14:43:22 @agent_ppo2.py:137][0m Total time:       6.79 min
[32m[20221208 14:43:22 @agent_ppo2.py:139][0m 548864 total steps have happened
[32m[20221208 14:43:22 @agent_ppo2.py:115][0m #------------------------ Iteration 268 --------------------------#
[32m[20221208 14:43:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:22 @agent_ppo2.py:179][0m |           0.0428 |           2.5947 |           8.2816 |
[32m[20221208 14:43:22 @agent_ppo2.py:179][0m |           0.0345 |           1.9582 |           8.1894 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0030 |           1.8670 |           8.0100 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0151 |           1.7812 |           8.1448 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0324 |           1.7652 |           8.1964 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0346 |           1.6979 |           8.1633 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0398 |           1.6559 |           8.2319 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0454 |           1.6205 |           8.2689 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0454 |           1.6049 |           8.2160 |
[32m[20221208 14:43:23 @agent_ppo2.py:179][0m |          -0.0494 |           1.5903 |           8.2513 |
[32m[20221208 14:43:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.21
[32m[20221208 14:43:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.97
[32m[20221208 14:43:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.75
[32m[20221208 14:43:23 @agent_ppo2.py:137][0m Total time:       6.82 min
[32m[20221208 14:43:23 @agent_ppo2.py:139][0m 550912 total steps have happened
[32m[20221208 14:43:23 @agent_ppo2.py:115][0m #------------------------ Iteration 269 --------------------------#
[32m[20221208 14:43:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |           0.0279 |           8.4891 |           8.2774 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0027 |           7.9111 |           8.1976 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0267 |           7.7160 |           8.2864 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0368 |           7.5935 |           8.3202 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0411 |           7.4810 |           8.3607 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0459 |           7.3490 |           8.3673 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0455 |           7.3094 |           8.3335 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0506 |           7.1891 |           8.3660 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0553 |           7.0475 |           8.4144 |
[32m[20221208 14:43:24 @agent_ppo2.py:179][0m |          -0.0592 |           6.9415 |           8.4148 |
[32m[20221208 14:43:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:43:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 175.72
[32m[20221208 14:43:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.36
[32m[20221208 14:43:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.16
[32m[20221208 14:43:25 @agent_ppo2.py:137][0m Total time:       6.84 min
[32m[20221208 14:43:25 @agent_ppo2.py:139][0m 552960 total steps have happened
[32m[20221208 14:43:25 @agent_ppo2.py:115][0m #------------------------ Iteration 270 --------------------------#
[32m[20221208 14:43:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:25 @agent_ppo2.py:179][0m |           0.0284 |           8.5470 |           8.1890 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0113 |           8.0493 |           8.2641 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0272 |           7.7538 |           8.3246 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0412 |           7.6535 |           8.3656 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0402 |           7.4264 |           8.3839 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0444 |           7.3796 |           8.3929 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0517 |           7.2362 |           8.4149 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0554 |           7.1179 |           8.4437 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0556 |           7.0589 |           8.4493 |
[32m[20221208 14:43:26 @agent_ppo2.py:179][0m |          -0.0582 |           6.9788 |           8.4794 |
[32m[20221208 14:43:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.41
[32m[20221208 14:43:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.76
[32m[20221208 14:43:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.34
[32m[20221208 14:43:26 @agent_ppo2.py:137][0m Total time:       6.87 min
[32m[20221208 14:43:26 @agent_ppo2.py:139][0m 555008 total steps have happened
[32m[20221208 14:43:26 @agent_ppo2.py:115][0m #------------------------ Iteration 271 --------------------------#
[32m[20221208 14:43:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:43:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |           0.0390 |           6.4018 |           8.1649 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |           0.0306 |           5.8800 |           7.9474 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0035 |           5.7678 |           7.9494 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0238 |           5.6703 |           8.1567 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0347 |           5.5701 |           8.2491 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0400 |           5.5647 |           8.3003 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0449 |           5.4973 |           8.3286 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0452 |           5.4147 |           8.3386 |
[32m[20221208 14:43:27 @agent_ppo2.py:179][0m |          -0.0469 |           5.4032 |           8.3566 |
[32m[20221208 14:43:28 @agent_ppo2.py:179][0m |          -0.0484 |           5.4191 |           8.3760 |
[32m[20221208 14:43:28 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.41
[32m[20221208 14:43:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.28
[32m[20221208 14:43:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.70
[32m[20221208 14:43:28 @agent_ppo2.py:137][0m Total time:       6.89 min
[32m[20221208 14:43:28 @agent_ppo2.py:139][0m 557056 total steps have happened
[32m[20221208 14:43:28 @agent_ppo2.py:115][0m #------------------------ Iteration 272 --------------------------#
[32m[20221208 14:43:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:28 @agent_ppo2.py:179][0m |           0.0482 |           9.0558 |           8.5638 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |           0.0297 |           8.3065 |           8.1963 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0099 |           8.0834 |           8.5792 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0247 |           7.9726 |           8.6060 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0407 |           7.9017 |           8.7168 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0477 |           7.8257 |           8.7528 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0473 |           7.7733 |           8.7807 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0544 |           7.7039 |           8.7836 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0551 |           7.6467 |           8.7979 |
[32m[20221208 14:43:29 @agent_ppo2.py:179][0m |          -0.0605 |           7.5942 |           8.8344 |
[32m[20221208 14:43:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.81
[32m[20221208 14:43:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.29
[32m[20221208 14:43:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 164.98
[32m[20221208 14:43:29 @agent_ppo2.py:137][0m Total time:       6.92 min
[32m[20221208 14:43:29 @agent_ppo2.py:139][0m 559104 total steps have happened
[32m[20221208 14:43:29 @agent_ppo2.py:115][0m #------------------------ Iteration 273 --------------------------#
[32m[20221208 14:43:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |           0.0305 |           3.9218 |           8.5213 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0045 |           3.6287 |           8.5801 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0119 |           3.5147 |           8.3840 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0273 |           3.4460 |           8.5283 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0315 |           3.3729 |           8.5858 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0379 |           3.3254 |           8.7262 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0410 |           3.2687 |           8.7080 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0440 |           3.2395 |           8.7878 |
[32m[20221208 14:43:30 @agent_ppo2.py:179][0m |          -0.0459 |           3.1819 |           8.7839 |
[32m[20221208 14:43:31 @agent_ppo2.py:179][0m |          -0.0476 |           3.1601 |           8.7941 |
[32m[20221208 14:43:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.69
[32m[20221208 14:43:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.54
[32m[20221208 14:43:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.84
[32m[20221208 14:43:31 @agent_ppo2.py:137][0m Total time:       6.94 min
[32m[20221208 14:43:31 @agent_ppo2.py:139][0m 561152 total steps have happened
[32m[20221208 14:43:31 @agent_ppo2.py:115][0m #------------------------ Iteration 274 --------------------------#
[32m[20221208 14:43:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:31 @agent_ppo2.py:179][0m |           0.0375 |           8.5953 |           8.3728 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |           0.0218 |           8.2199 |           7.9028 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0132 |           8.0626 |           8.2643 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0249 |           7.9872 |           8.3254 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0381 |           7.8916 |           8.5018 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0405 |           7.7998 |           8.5092 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0471 |           7.8550 |           8.5597 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0501 |           7.7461 |           8.5670 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0530 |           7.6630 |           8.5903 |
[32m[20221208 14:43:32 @agent_ppo2.py:179][0m |          -0.0531 |           7.6677 |           8.6231 |
[32m[20221208 14:43:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.14
[32m[20221208 14:43:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.21
[32m[20221208 14:43:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.23
[32m[20221208 14:43:32 @agent_ppo2.py:137][0m Total time:       6.97 min
[32m[20221208 14:43:32 @agent_ppo2.py:139][0m 563200 total steps have happened
[32m[20221208 14:43:32 @agent_ppo2.py:115][0m #------------------------ Iteration 275 --------------------------#
[32m[20221208 14:43:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |           0.0251 |           6.2547 |           8.6856 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0115 |           5.7805 |           8.6440 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0289 |           5.5529 |           8.8030 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0388 |           5.4151 |           8.8156 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0464 |           5.2969 |           8.8509 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0444 |           5.1986 |           8.8661 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0509 |           5.0882 |           8.8827 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0525 |           5.0112 |           8.8992 |
[32m[20221208 14:43:33 @agent_ppo2.py:179][0m |          -0.0560 |           4.9341 |           8.9247 |
[32m[20221208 14:43:34 @agent_ppo2.py:179][0m |          -0.0555 |           4.8239 |           8.9346 |
[32m[20221208 14:43:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:43:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.98
[32m[20221208 14:43:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.47
[32m[20221208 14:43:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.28
[32m[20221208 14:43:34 @agent_ppo2.py:137][0m Total time:       7.00 min
[32m[20221208 14:43:34 @agent_ppo2.py:139][0m 565248 total steps have happened
[32m[20221208 14:43:34 @agent_ppo2.py:115][0m #------------------------ Iteration 276 --------------------------#
[32m[20221208 14:43:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |           0.0195 |           3.9432 |           8.5939 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0002 |           3.6246 |           8.5637 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0127 |           3.5460 |           8.5895 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0229 |           3.4895 |           8.5725 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0358 |           3.4080 |           8.6922 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0390 |           3.3996 |           8.7596 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0425 |           3.3607 |           8.7343 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0427 |           3.3430 |           8.7439 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0424 |           3.3451 |           8.7842 |
[32m[20221208 14:43:35 @agent_ppo2.py:179][0m |          -0.0445 |           3.2775 |           8.7840 |
[32m[20221208 14:43:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.87
[32m[20221208 14:43:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.42
[32m[20221208 14:43:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 189.46
[32m[20221208 14:43:35 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 189.46
[32m[20221208 14:43:35 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 189.46
[32m[20221208 14:43:35 @agent_ppo2.py:137][0m Total time:       7.02 min
[32m[20221208 14:43:35 @agent_ppo2.py:139][0m 567296 total steps have happened
[32m[20221208 14:43:35 @agent_ppo2.py:115][0m #------------------------ Iteration 277 --------------------------#
[32m[20221208 14:43:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |           0.0562 |           6.3894 |           8.4471 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |           0.0229 |           5.8265 |           8.3770 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |          -0.0070 |           5.5236 |           8.7020 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |          -0.0249 |           5.3340 |           8.6987 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |          -0.0334 |           5.1974 |           8.8129 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |          -0.0406 |           5.0509 |           8.8480 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |          -0.0435 |           4.9409 |           8.8714 |
[32m[20221208 14:43:36 @agent_ppo2.py:179][0m |          -0.0480 |           4.8122 |           8.8806 |
[32m[20221208 14:43:37 @agent_ppo2.py:179][0m |          -0.0441 |           4.7200 |           8.8928 |
[32m[20221208 14:43:37 @agent_ppo2.py:179][0m |          -0.0497 |           4.6710 |           8.8959 |
[32m[20221208 14:43:37 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.10
[32m[20221208 14:43:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.68
[32m[20221208 14:43:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.92
[32m[20221208 14:43:37 @agent_ppo2.py:137][0m Total time:       7.05 min
[32m[20221208 14:43:37 @agent_ppo2.py:139][0m 569344 total steps have happened
[32m[20221208 14:43:37 @agent_ppo2.py:115][0m #------------------------ Iteration 278 --------------------------#
[32m[20221208 14:43:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |           0.0300 |           9.0277 |           8.7798 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0016 |           8.0242 |           8.5898 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0278 |           7.6689 |           8.8117 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0368 |           7.4454 |           8.8547 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0441 |           7.3208 |           8.8798 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0479 |           7.1649 |           8.9304 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0484 |           6.9967 |           8.9156 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0522 |           6.9045 |           8.9184 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0550 |           6.8717 |           8.9402 |
[32m[20221208 14:43:38 @agent_ppo2.py:179][0m |          -0.0549 |           6.7799 |           8.9685 |
[32m[20221208 14:43:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.44
[32m[20221208 14:43:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.29
[32m[20221208 14:43:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.03
[32m[20221208 14:43:39 @agent_ppo2.py:137][0m Total time:       7.07 min
[32m[20221208 14:43:39 @agent_ppo2.py:139][0m 571392 total steps have happened
[32m[20221208 14:43:39 @agent_ppo2.py:115][0m #------------------------ Iteration 279 --------------------------#
[32m[20221208 14:43:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |           0.0409 |           7.6502 |           8.8754 |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |           0.0226 |           6.9223 |           8.7203 |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |          -0.0158 |           6.6220 |           8.8632 |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |          -0.0303 |           6.4634 |           8.9661 |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |          -0.0426 |           6.2841 |           9.0349 |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |          -0.0491 |           6.1597 |           9.0524 |
[32m[20221208 14:43:39 @agent_ppo2.py:179][0m |          -0.0520 |           6.1017 |           9.0978 |
[32m[20221208 14:43:40 @agent_ppo2.py:179][0m |          -0.0566 |           5.9561 |           9.1065 |
[32m[20221208 14:43:40 @agent_ppo2.py:179][0m |          -0.0602 |           5.8449 |           9.1417 |
[32m[20221208 14:43:40 @agent_ppo2.py:179][0m |          -0.0613 |           5.7761 |           9.1851 |
[32m[20221208 14:43:40 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:43:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.86
[32m[20221208 14:43:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.68
[32m[20221208 14:43:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.98
[32m[20221208 14:43:40 @agent_ppo2.py:137][0m Total time:       7.10 min
[32m[20221208 14:43:40 @agent_ppo2.py:139][0m 573440 total steps have happened
[32m[20221208 14:43:40 @agent_ppo2.py:115][0m #------------------------ Iteration 280 --------------------------#
[32m[20221208 14:43:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |           0.0169 |           8.3746 |           8.8308 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |           0.0390 |           7.7480 |           8.5071 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |           0.0088 |           7.5073 |           8.2128 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0222 |           7.3120 |           8.5999 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0354 |           7.1745 |           8.7849 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0430 |           7.0821 |           8.8623 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0441 |           6.9706 |           8.8678 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0515 |           6.9026 |           8.9423 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0540 |           6.8301 |           8.9610 |
[32m[20221208 14:43:41 @agent_ppo2.py:179][0m |          -0.0575 |           6.7727 |           8.9948 |
[32m[20221208 14:43:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.23
[32m[20221208 14:43:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.69
[32m[20221208 14:43:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.49
[32m[20221208 14:43:42 @agent_ppo2.py:137][0m Total time:       7.12 min
[32m[20221208 14:43:42 @agent_ppo2.py:139][0m 575488 total steps have happened
[32m[20221208 14:43:42 @agent_ppo2.py:115][0m #------------------------ Iteration 281 --------------------------#
[32m[20221208 14:43:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |           0.0367 |           6.5280 |           8.9796 |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |           0.0043 |           5.9656 |           8.8775 |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |          -0.0229 |           5.8705 |           9.0546 |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |          -0.0327 |           5.7384 |           9.1670 |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |          -0.0423 |           5.6614 |           9.1834 |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |          -0.0448 |           5.6454 |           9.2722 |
[32m[20221208 14:43:42 @agent_ppo2.py:179][0m |          -0.0483 |           5.6127 |           9.2695 |
[32m[20221208 14:43:43 @agent_ppo2.py:179][0m |          -0.0510 |           5.5630 |           9.2532 |
[32m[20221208 14:43:43 @agent_ppo2.py:179][0m |          -0.0487 |           5.5231 |           9.2905 |
[32m[20221208 14:43:43 @agent_ppo2.py:179][0m |          -0.0519 |           5.5024 |           9.2515 |
[32m[20221208 14:43:43 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.46
[32m[20221208 14:43:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.49
[32m[20221208 14:43:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.14
[32m[20221208 14:43:43 @agent_ppo2.py:137][0m Total time:       7.15 min
[32m[20221208 14:43:43 @agent_ppo2.py:139][0m 577536 total steps have happened
[32m[20221208 14:43:43 @agent_ppo2.py:115][0m #------------------------ Iteration 282 --------------------------#
[32m[20221208 14:43:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |           0.0272 |           8.7182 |           9.0151 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |           0.0045 |           8.1456 |           8.8251 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0188 |           7.8076 |           8.9341 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0330 |           7.5494 |           9.0644 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0397 |           7.3835 |           9.0985 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0447 |           7.2812 |           9.1669 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0481 |           7.1562 |           9.1625 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0511 |           7.0134 |           9.1911 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0536 |           6.9604 |           9.2017 |
[32m[20221208 14:43:44 @agent_ppo2.py:179][0m |          -0.0551 |           6.8616 |           9.2295 |
[32m[20221208 14:43:44 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.55
[32m[20221208 14:43:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.77
[32m[20221208 14:43:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.04
[32m[20221208 14:43:45 @agent_ppo2.py:137][0m Total time:       7.17 min
[32m[20221208 14:43:45 @agent_ppo2.py:139][0m 579584 total steps have happened
[32m[20221208 14:43:45 @agent_ppo2.py:115][0m #------------------------ Iteration 283 --------------------------#
[32m[20221208 14:43:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:45 @agent_ppo2.py:179][0m |           0.0543 |           6.5862 |           8.9552 |
[32m[20221208 14:43:45 @agent_ppo2.py:179][0m |           0.0168 |           6.0750 |           8.8543 |
[32m[20221208 14:43:45 @agent_ppo2.py:179][0m |          -0.0127 |           5.8449 |           9.1371 |
[32m[20221208 14:43:45 @agent_ppo2.py:179][0m |          -0.0271 |           5.7116 |           9.2396 |
[32m[20221208 14:43:45 @agent_ppo2.py:179][0m |          -0.0390 |           5.6474 |           9.2879 |
[32m[20221208 14:43:45 @agent_ppo2.py:179][0m |          -0.0437 |           5.5620 |           9.3578 |
[32m[20221208 14:43:46 @agent_ppo2.py:179][0m |          -0.0481 |           5.5031 |           9.4074 |
[32m[20221208 14:43:46 @agent_ppo2.py:179][0m |          -0.0518 |           5.5010 |           9.4241 |
[32m[20221208 14:43:46 @agent_ppo2.py:179][0m |          -0.0503 |           5.4160 |           9.4431 |
[32m[20221208 14:43:46 @agent_ppo2.py:179][0m |          -0.0535 |           5.4152 |           9.4604 |
[32m[20221208 14:43:46 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.24
[32m[20221208 14:43:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.27
[32m[20221208 14:43:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.78
[32m[20221208 14:43:46 @agent_ppo2.py:137][0m Total time:       7.20 min
[32m[20221208 14:43:46 @agent_ppo2.py:139][0m 581632 total steps have happened
[32m[20221208 14:43:46 @agent_ppo2.py:115][0m #------------------------ Iteration 284 --------------------------#
[32m[20221208 14:43:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |           0.0331 |           9.1580 |           9.4803 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |           0.0104 |           8.0644 |           9.2709 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0077 |           7.4612 |           9.4527 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |           0.0185 |           6.9869 |           8.9190 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0177 |           6.8087 |           9.3172 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0356 |           6.5012 |           9.5199 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0431 |           6.3329 |           9.6077 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0464 |           6.1695 |           9.6652 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0525 |           5.9861 |           9.6948 |
[32m[20221208 14:43:47 @agent_ppo2.py:179][0m |          -0.0551 |           5.9414 |           9.7307 |
[32m[20221208 14:43:47 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:43:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.56
[32m[20221208 14:43:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.12
[32m[20221208 14:43:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.81
[32m[20221208 14:43:48 @agent_ppo2.py:137][0m Total time:       7.22 min
[32m[20221208 14:43:48 @agent_ppo2.py:139][0m 583680 total steps have happened
[32m[20221208 14:43:48 @agent_ppo2.py:115][0m #------------------------ Iteration 285 --------------------------#
[32m[20221208 14:43:48 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:43:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:48 @agent_ppo2.py:179][0m |           0.0299 |           8.9721 |           9.6280 |
[32m[20221208 14:43:48 @agent_ppo2.py:179][0m |           0.0131 |           7.9948 |           9.4355 |
[32m[20221208 14:43:48 @agent_ppo2.py:179][0m |           0.0121 |           7.7090 |           9.0658 |
[32m[20221208 14:43:48 @agent_ppo2.py:179][0m |          -0.0248 |           7.4240 |           9.4787 |
[32m[20221208 14:43:48 @agent_ppo2.py:179][0m |          -0.0378 |           7.2543 |           9.5889 |
[32m[20221208 14:43:49 @agent_ppo2.py:179][0m |          -0.0462 |           7.1506 |           9.6740 |
[32m[20221208 14:43:49 @agent_ppo2.py:179][0m |          -0.0532 |           7.0449 |           9.7345 |
[32m[20221208 14:43:49 @agent_ppo2.py:179][0m |          -0.0520 |           6.9631 |           9.7299 |
[32m[20221208 14:43:49 @agent_ppo2.py:179][0m |          -0.0548 |           6.8349 |           9.7437 |
[32m[20221208 14:43:49 @agent_ppo2.py:179][0m |          -0.0592 |           6.7325 |           9.7755 |
[32m[20221208 14:43:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:43:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.89
[32m[20221208 14:43:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.55
[32m[20221208 14:43:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.37
[32m[20221208 14:43:49 @agent_ppo2.py:137][0m Total time:       7.25 min
[32m[20221208 14:43:49 @agent_ppo2.py:139][0m 585728 total steps have happened
[32m[20221208 14:43:49 @agent_ppo2.py:115][0m #------------------------ Iteration 286 --------------------------#
[32m[20221208 14:43:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:43:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |           0.0593 |           6.8688 |           9.4413 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |           0.0159 |           5.9401 |           9.2444 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |           0.0001 |           5.6735 |           9.4250 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0213 |           5.5090 |           9.5954 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0348 |           5.3914 |           9.6658 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0380 |           5.3118 |           9.6128 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0484 |           5.2435 |           9.6814 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0529 |           5.1598 |           9.7511 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0558 |           5.1004 |           9.7878 |
[32m[20221208 14:43:50 @agent_ppo2.py:179][0m |          -0.0539 |           5.0890 |           9.8090 |
[32m[20221208 14:43:50 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:43:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.35
[32m[20221208 14:43:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.22
[32m[20221208 14:43:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.36
[32m[20221208 14:43:51 @agent_ppo2.py:137][0m Total time:       7.27 min
[32m[20221208 14:43:51 @agent_ppo2.py:139][0m 587776 total steps have happened
[32m[20221208 14:43:51 @agent_ppo2.py:115][0m #------------------------ Iteration 287 --------------------------#
[32m[20221208 14:43:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:51 @agent_ppo2.py:179][0m |           0.0411 |           7.0135 |           9.6598 |
[32m[20221208 14:43:51 @agent_ppo2.py:179][0m |           0.0316 |           6.4495 |           9.2872 |
[32m[20221208 14:43:51 @agent_ppo2.py:179][0m |          -0.0060 |           6.3245 |           9.4894 |
[32m[20221208 14:43:51 @agent_ppo2.py:179][0m |          -0.0249 |           6.1367 |           9.6727 |
[32m[20221208 14:43:52 @agent_ppo2.py:179][0m |          -0.0349 |           6.0888 |           9.8075 |
[32m[20221208 14:43:52 @agent_ppo2.py:179][0m |          -0.0414 |           5.9888 |           9.8510 |
[32m[20221208 14:43:52 @agent_ppo2.py:179][0m |          -0.0490 |           5.9356 |           9.9089 |
[32m[20221208 14:43:52 @agent_ppo2.py:179][0m |          -0.0532 |           5.8726 |           9.8941 |
[32m[20221208 14:43:52 @agent_ppo2.py:179][0m |          -0.0542 |           5.8140 |           9.9342 |
[32m[20221208 14:43:52 @agent_ppo2.py:179][0m |          -0.0594 |           5.7911 |           9.9611 |
[32m[20221208 14:43:52 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:43:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.90
[32m[20221208 14:43:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.57
[32m[20221208 14:43:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.13
[32m[20221208 14:43:52 @agent_ppo2.py:137][0m Total time:       7.30 min
[32m[20221208 14:43:52 @agent_ppo2.py:139][0m 589824 total steps have happened
[32m[20221208 14:43:52 @agent_ppo2.py:115][0m #------------------------ Iteration 288 --------------------------#
[32m[20221208 14:43:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |           0.0270 |           8.3191 |           9.9975 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0068 |           7.8942 |           9.8551 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0216 |           7.6790 |          10.0460 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0307 |           7.5560 |          10.0576 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0390 |           7.4472 |          10.1832 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0434 |           7.4018 |          10.1872 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0471 |           7.3029 |          10.2602 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0489 |           7.2455 |          10.1987 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0549 |           7.1552 |          10.2680 |
[32m[20221208 14:43:53 @agent_ppo2.py:179][0m |          -0.0545 |           7.1161 |          10.2967 |
[32m[20221208 14:43:53 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:43:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.29
[32m[20221208 14:43:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.78
[32m[20221208 14:43:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.74
[32m[20221208 14:43:54 @agent_ppo2.py:137][0m Total time:       7.33 min
[32m[20221208 14:43:54 @agent_ppo2.py:139][0m 591872 total steps have happened
[32m[20221208 14:43:54 @agent_ppo2.py:115][0m #------------------------ Iteration 289 --------------------------#
[32m[20221208 14:43:54 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:43:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:54 @agent_ppo2.py:179][0m |           0.0766 |           6.7182 |           9.4992 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |           0.0125 |           6.1537 |           7.1185 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0134 |           6.0371 |           6.9706 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0330 |           5.9177 |           7.2350 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0481 |           5.8602 |           7.3656 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0550 |           5.8428 |           7.4754 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0585 |           5.7856 |           7.5275 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0651 |           5.7428 |           7.5910 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0693 |           5.7137 |           7.6286 |
[32m[20221208 14:43:55 @agent_ppo2.py:179][0m |          -0.0705 |           5.7634 |           7.6613 |
[32m[20221208 14:43:55 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:43:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.79
[32m[20221208 14:43:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.27
[32m[20221208 14:43:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.46
[32m[20221208 14:43:55 @agent_ppo2.py:137][0m Total time:       7.35 min
[32m[20221208 14:43:55 @agent_ppo2.py:139][0m 593920 total steps have happened
[32m[20221208 14:43:55 @agent_ppo2.py:115][0m #------------------------ Iteration 290 --------------------------#
[32m[20221208 14:43:56 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:43:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |           0.0534 |           8.9878 |          10.1734 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |           0.0219 |           7.9284 |          10.1184 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |          -0.0067 |           7.4862 |          10.1771 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |          -0.0312 |           7.1963 |          10.4247 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |          -0.0356 |           7.0257 |          10.4582 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |          -0.0399 |           6.8921 |          10.4916 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |          -0.0517 |           6.7402 |          10.6133 |
[32m[20221208 14:43:56 @agent_ppo2.py:179][0m |          -0.0527 |           6.5880 |          10.5990 |
[32m[20221208 14:43:57 @agent_ppo2.py:179][0m |          -0.0564 |           6.4673 |          10.6266 |
[32m[20221208 14:43:57 @agent_ppo2.py:179][0m |          -0.0587 |           6.4031 |          10.6603 |
[32m[20221208 14:43:57 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:43:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.99
[32m[20221208 14:43:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.01
[32m[20221208 14:43:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.05
[32m[20221208 14:43:57 @agent_ppo2.py:137][0m Total time:       7.38 min
[32m[20221208 14:43:57 @agent_ppo2.py:139][0m 595968 total steps have happened
[32m[20221208 14:43:57 @agent_ppo2.py:115][0m #------------------------ Iteration 291 --------------------------#
[32m[20221208 14:43:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:43:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |           0.0202 |           9.7445 |          10.6727 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0038 |           8.8977 |          10.4486 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0261 |           8.7014 |          10.6500 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0365 |           8.5984 |          10.6890 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0465 |           8.4591 |          10.7680 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0519 |           8.3528 |          10.7926 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0527 |           8.3594 |          10.7971 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0563 |           8.2658 |          10.7968 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0577 |           8.2228 |          10.8169 |
[32m[20221208 14:43:58 @agent_ppo2.py:179][0m |          -0.0592 |           8.1611 |          10.8366 |
[32m[20221208 14:43:58 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:43:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 179.80
[32m[20221208 14:43:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.91
[32m[20221208 14:43:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.71
[32m[20221208 14:43:59 @agent_ppo2.py:137][0m Total time:       7.41 min
[32m[20221208 14:43:59 @agent_ppo2.py:139][0m 598016 total steps have happened
[32m[20221208 14:43:59 @agent_ppo2.py:115][0m #------------------------ Iteration 292 --------------------------#
[32m[20221208 14:43:59 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:43:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:43:59 @agent_ppo2.py:179][0m |           0.0221 |           7.1937 |          10.6599 |
[32m[20221208 14:43:59 @agent_ppo2.py:179][0m |           0.0215 |           6.4752 |          10.4799 |
[32m[20221208 14:43:59 @agent_ppo2.py:179][0m |          -0.0087 |           6.2498 |          10.6244 |
[32m[20221208 14:43:59 @agent_ppo2.py:179][0m |          -0.0238 |           6.0963 |          10.7963 |
[32m[20221208 14:43:59 @agent_ppo2.py:179][0m |          -0.0346 |           5.9748 |          10.8020 |
[32m[20221208 14:43:59 @agent_ppo2.py:179][0m |          -0.0469 |           5.9237 |          10.9095 |
[32m[20221208 14:44:00 @agent_ppo2.py:179][0m |          -0.0492 |           5.8476 |          10.9703 |
[32m[20221208 14:44:00 @agent_ppo2.py:179][0m |          -0.0457 |           5.8206 |          10.9447 |
[32m[20221208 14:44:00 @agent_ppo2.py:179][0m |          -0.0509 |           5.7328 |          10.9839 |
[32m[20221208 14:44:00 @agent_ppo2.py:179][0m |          -0.0551 |           5.7227 |          10.9935 |
[32m[20221208 14:44:00 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:44:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.38
[32m[20221208 14:44:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.38
[32m[20221208 14:44:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.40
[32m[20221208 14:44:00 @agent_ppo2.py:137][0m Total time:       7.43 min
[32m[20221208 14:44:00 @agent_ppo2.py:139][0m 600064 total steps have happened
[32m[20221208 14:44:00 @agent_ppo2.py:115][0m #------------------------ Iteration 293 --------------------------#
[32m[20221208 14:44:01 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:44:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |           0.0345 |           6.2756 |          10.9563 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |           0.0025 |           5.8269 |          10.8848 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0208 |           5.5835 |          11.0472 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0266 |           5.4662 |          11.1192 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0243 |           5.4057 |          11.0796 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0388 |           5.2655 |          11.1637 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0447 |           5.1743 |          11.1845 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0490 |           5.1024 |          11.2166 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0502 |           5.0178 |          11.2275 |
[32m[20221208 14:44:01 @agent_ppo2.py:179][0m |          -0.0489 |           4.9359 |          11.2244 |
[32m[20221208 14:44:01 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:44:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.73
[32m[20221208 14:44:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.13
[32m[20221208 14:44:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.74
[32m[20221208 14:44:02 @agent_ppo2.py:137][0m Total time:       7.46 min
[32m[20221208 14:44:02 @agent_ppo2.py:139][0m 602112 total steps have happened
[32m[20221208 14:44:02 @agent_ppo2.py:115][0m #------------------------ Iteration 294 --------------------------#
[32m[20221208 14:44:02 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:02 @agent_ppo2.py:179][0m |           0.0654 |           9.3020 |          10.7869 |
[32m[20221208 14:44:02 @agent_ppo2.py:179][0m |           0.0104 |           8.8621 |          10.9190 |
[32m[20221208 14:44:02 @agent_ppo2.py:179][0m |          -0.0100 |           8.6270 |          11.1144 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0261 |           8.4921 |          11.2299 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0344 |           8.3795 |          11.2945 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0324 |           8.2718 |          11.2716 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0378 |           8.3108 |          11.3057 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0424 |           8.2112 |          11.3593 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0450 |           8.1212 |          11.3429 |
[32m[20221208 14:44:03 @agent_ppo2.py:179][0m |          -0.0494 |           8.0931 |          11.3630 |
[32m[20221208 14:44:03 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:44:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.57
[32m[20221208 14:44:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.11
[32m[20221208 14:44:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.18
[32m[20221208 14:44:03 @agent_ppo2.py:137][0m Total time:       7.48 min
[32m[20221208 14:44:03 @agent_ppo2.py:139][0m 604160 total steps have happened
[32m[20221208 14:44:03 @agent_ppo2.py:115][0m #------------------------ Iteration 295 --------------------------#
[32m[20221208 14:44:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |           0.0489 |           9.0846 |          10.8590 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |           0.0180 |           8.7772 |          10.5129 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0184 |           8.6096 |          10.9730 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0310 |           8.4482 |          11.1335 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0414 |           8.4175 |          11.2571 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0404 |           8.2769 |          11.2204 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0452 |           8.2347 |          11.2905 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0471 |           8.1792 |          11.3145 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0492 |           8.0878 |          11.3586 |
[32m[20221208 14:44:04 @agent_ppo2.py:179][0m |          -0.0538 |           8.1044 |          11.3900 |
[32m[20221208 14:44:04 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:44:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.94
[32m[20221208 14:44:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.04
[32m[20221208 14:44:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.95
[32m[20221208 14:44:05 @agent_ppo2.py:137][0m Total time:       7.51 min
[32m[20221208 14:44:05 @agent_ppo2.py:139][0m 606208 total steps have happened
[32m[20221208 14:44:05 @agent_ppo2.py:115][0m #------------------------ Iteration 296 --------------------------#
[32m[20221208 14:44:05 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:05 @agent_ppo2.py:179][0m |           0.0446 |           6.5959 |          11.0557 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |           0.0208 |           6.1696 |          10.8902 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0216 |           5.9362 |          11.2344 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0328 |           5.7487 |          11.3069 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0389 |           5.6072 |          11.3808 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0420 |           5.4867 |          11.3835 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0490 |           5.3411 |          11.4484 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0475 |           5.2433 |          11.4491 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0514 |           5.1617 |          11.4259 |
[32m[20221208 14:44:06 @agent_ppo2.py:179][0m |          -0.0536 |           5.0969 |          11.4855 |
[32m[20221208 14:44:06 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:44:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.59
[32m[20221208 14:44:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.61
[32m[20221208 14:44:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.35
[32m[20221208 14:44:06 @agent_ppo2.py:137][0m Total time:       7.54 min
[32m[20221208 14:44:06 @agent_ppo2.py:139][0m 608256 total steps have happened
[32m[20221208 14:44:06 @agent_ppo2.py:115][0m #------------------------ Iteration 297 --------------------------#
[32m[20221208 14:44:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |           0.0329 |           6.8696 |          11.3388 |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |           0.0033 |           6.3116 |          11.2809 |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |          -0.0262 |           6.1768 |          11.5429 |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |          -0.0328 |           6.0338 |          11.4780 |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |          -0.0382 |           6.0167 |          11.5388 |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |          -0.0456 |           5.8960 |          11.5902 |
[32m[20221208 14:44:07 @agent_ppo2.py:179][0m |          -0.0482 |           5.9022 |          11.6124 |
[32m[20221208 14:44:08 @agent_ppo2.py:179][0m |          -0.0500 |           5.8381 |          11.6009 |
[32m[20221208 14:44:08 @agent_ppo2.py:179][0m |          -0.0523 |           5.8236 |          11.6384 |
[32m[20221208 14:44:08 @agent_ppo2.py:179][0m |          -0.0533 |           5.7565 |          11.6218 |
[32m[20221208 14:44:08 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:44:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.50
[32m[20221208 14:44:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.58
[32m[20221208 14:44:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.03
[32m[20221208 14:44:08 @agent_ppo2.py:137][0m Total time:       7.56 min
[32m[20221208 14:44:08 @agent_ppo2.py:139][0m 610304 total steps have happened
[32m[20221208 14:44:08 @agent_ppo2.py:115][0m #------------------------ Iteration 298 --------------------------#
[32m[20221208 14:44:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |           0.0237 |           3.9856 |          11.4559 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |           0.0463 |           3.5460 |          10.6200 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0145 |           3.4347 |          11.0231 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0321 |           3.3272 |          11.2234 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0391 |           3.2575 |          11.3373 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0447 |           3.1926 |          11.3307 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0498 |           3.1229 |          11.4079 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0518 |           3.0334 |          11.4200 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0566 |           2.9534 |          11.4784 |
[32m[20221208 14:44:09 @agent_ppo2.py:179][0m |          -0.0568 |           2.8963 |          11.4871 |
[32m[20221208 14:44:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:44:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.33
[32m[20221208 14:44:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.53
[32m[20221208 14:44:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.08
[32m[20221208 14:44:10 @agent_ppo2.py:137][0m Total time:       7.59 min
[32m[20221208 14:44:10 @agent_ppo2.py:139][0m 612352 total steps have happened
[32m[20221208 14:44:10 @agent_ppo2.py:115][0m #------------------------ Iteration 299 --------------------------#
[32m[20221208 14:44:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:10 @agent_ppo2.py:179][0m |           0.0281 |           6.6962 |          11.8386 |
[32m[20221208 14:44:10 @agent_ppo2.py:179][0m |           0.0602 |           6.3999 |          11.0639 |
[32m[20221208 14:44:10 @agent_ppo2.py:179][0m |           0.0179 |           6.2068 |          11.3433 |
[32m[20221208 14:44:10 @agent_ppo2.py:179][0m |          -0.0059 |           6.0928 |          11.6603 |
[32m[20221208 14:44:10 @agent_ppo2.py:179][0m |          -0.0183 |           6.0218 |          11.8063 |
[32m[20221208 14:44:11 @agent_ppo2.py:179][0m |          -0.0308 |           5.9614 |          11.9427 |
[32m[20221208 14:44:11 @agent_ppo2.py:179][0m |          -0.0355 |           5.9791 |          11.9583 |
[32m[20221208 14:44:11 @agent_ppo2.py:179][0m |          -0.0428 |           5.8510 |          12.0169 |
[32m[20221208 14:44:11 @agent_ppo2.py:179][0m |          -0.0481 |           5.8127 |          12.0771 |
[32m[20221208 14:44:11 @agent_ppo2.py:179][0m |          -0.0503 |           5.7789 |          12.0419 |
[32m[20221208 14:44:11 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:44:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.47
[32m[20221208 14:44:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.55
[32m[20221208 14:44:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.76
[32m[20221208 14:44:11 @agent_ppo2.py:137][0m Total time:       7.62 min
[32m[20221208 14:44:11 @agent_ppo2.py:139][0m 614400 total steps have happened
[32m[20221208 14:44:11 @agent_ppo2.py:115][0m #------------------------ Iteration 300 --------------------------#
[32m[20221208 14:44:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |           0.0546 |           7.0697 |          11.4658 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |           0.0403 |           6.5950 |          10.5536 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0034 |           6.4244 |          11.3733 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0249 |           6.2968 |          11.6012 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0361 |           6.1949 |          11.7071 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0391 |           6.1304 |          11.7351 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0439 |           6.0658 |          11.7015 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0498 |           6.0189 |          11.7760 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0518 |           5.9979 |          11.7793 |
[32m[20221208 14:44:12 @agent_ppo2.py:179][0m |          -0.0570 |           5.9515 |          11.8380 |
[32m[20221208 14:44:12 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:44:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.02
[32m[20221208 14:44:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.69
[32m[20221208 14:44:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.96
[32m[20221208 14:44:13 @agent_ppo2.py:137][0m Total time:       7.64 min
[32m[20221208 14:44:13 @agent_ppo2.py:139][0m 616448 total steps have happened
[32m[20221208 14:44:13 @agent_ppo2.py:115][0m #------------------------ Iteration 301 --------------------------#
[32m[20221208 14:44:13 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:13 @agent_ppo2.py:179][0m |           0.0151 |           9.3477 |          11.7750 |
[32m[20221208 14:44:13 @agent_ppo2.py:179][0m |          -0.0063 |           8.8521 |          11.6120 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0266 |           8.6905 |          11.8460 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0329 |           8.6084 |          11.8039 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0421 |           8.5400 |          11.9211 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0472 |           8.4804 |          11.9735 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0461 |           8.4491 |          11.8939 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0526 |           8.4696 |          11.9636 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0553 |           8.3665 |          12.0050 |
[32m[20221208 14:44:14 @agent_ppo2.py:179][0m |          -0.0566 |           8.3582 |          11.9944 |
[32m[20221208 14:44:14 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:44:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.86
[32m[20221208 14:44:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.22
[32m[20221208 14:44:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 171.14
[32m[20221208 14:44:14 @agent_ppo2.py:137][0m Total time:       7.67 min
[32m[20221208 14:44:14 @agent_ppo2.py:139][0m 618496 total steps have happened
[32m[20221208 14:44:14 @agent_ppo2.py:115][0m #------------------------ Iteration 302 --------------------------#
[32m[20221208 14:44:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |           0.0501 |           6.5388 |          11.2765 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |           0.0205 |           6.3556 |          11.2808 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0010 |           6.2394 |          11.4915 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0241 |           6.1968 |          11.6754 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0366 |           6.0934 |          11.8193 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0428 |           6.0578 |          11.8979 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0453 |           6.0669 |          11.8991 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0488 |           5.9993 |          11.9362 |
[32m[20221208 14:44:15 @agent_ppo2.py:179][0m |          -0.0523 |           5.9839 |          11.9593 |
[32m[20221208 14:44:16 @agent_ppo2.py:179][0m |          -0.0558 |           5.9099 |          11.9585 |
[32m[20221208 14:44:16 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:44:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.57
[32m[20221208 14:44:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.19
[32m[20221208 14:44:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.15
[32m[20221208 14:44:16 @agent_ppo2.py:137][0m Total time:       7.69 min
[32m[20221208 14:44:16 @agent_ppo2.py:139][0m 620544 total steps have happened
[32m[20221208 14:44:16 @agent_ppo2.py:115][0m #------------------------ Iteration 303 --------------------------#
[32m[20221208 14:44:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |           0.0420 |           6.5151 |          11.4499 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |           0.0200 |           6.0242 |          11.0670 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0137 |           5.7989 |          11.6432 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0314 |           5.6034 |          11.7367 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0404 |           5.5182 |          11.8139 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0448 |           5.4201 |          11.8810 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0499 |           5.2660 |          11.9102 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0525 |           5.2174 |          11.9387 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0551 |           5.1588 |          11.9770 |
[32m[20221208 14:44:17 @agent_ppo2.py:179][0m |          -0.0574 |           4.9894 |          11.9948 |
[32m[20221208 14:44:17 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:44:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.49
[32m[20221208 14:44:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.75
[32m[20221208 14:44:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.70
[32m[20221208 14:44:18 @agent_ppo2.py:137][0m Total time:       7.72 min
[32m[20221208 14:44:18 @agent_ppo2.py:139][0m 622592 total steps have happened
[32m[20221208 14:44:18 @agent_ppo2.py:115][0m #------------------------ Iteration 304 --------------------------#
[32m[20221208 14:44:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |           0.0699 |           7.9682 |          11.6237 |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |           0.0566 |           6.8258 |          10.1295 |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |           0.0296 |           6.3410 |          10.1941 |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |          -0.0085 |           6.0790 |          10.8660 |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |          -0.0223 |           5.8712 |          11.1024 |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |          -0.0228 |           5.7584 |          11.1016 |
[32m[20221208 14:44:18 @agent_ppo2.py:179][0m |          -0.0312 |           5.7545 |          11.1572 |
[32m[20221208 14:44:19 @agent_ppo2.py:179][0m |          -0.0397 |           5.5465 |          11.2757 |
[32m[20221208 14:44:19 @agent_ppo2.py:179][0m |          -0.0473 |           5.5345 |          11.3028 |
[32m[20221208 14:44:19 @agent_ppo2.py:179][0m |          -0.0436 |           5.4422 |          11.2557 |
[32m[20221208 14:44:19 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:44:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.75
[32m[20221208 14:44:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.15
[32m[20221208 14:44:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.64
[32m[20221208 14:44:19 @agent_ppo2.py:137][0m Total time:       7.75 min
[32m[20221208 14:44:19 @agent_ppo2.py:139][0m 624640 total steps have happened
[32m[20221208 14:44:19 @agent_ppo2.py:115][0m #------------------------ Iteration 305 --------------------------#
[32m[20221208 14:44:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |           0.0660 |           8.3519 |          11.7073 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |           0.0990 |           7.3248 |          10.3570 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |           0.0022 |           7.0355 |          10.4605 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0194 |           6.7948 |          10.7501 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0346 |           6.6779 |          10.9661 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0443 |           6.5470 |          11.1617 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0510 |           6.4874 |          11.1954 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0546 |           6.3796 |          11.2819 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0577 |           6.3268 |          11.2774 |
[32m[20221208 14:44:20 @agent_ppo2.py:179][0m |          -0.0603 |           6.2300 |          11.3273 |
[32m[20221208 14:44:20 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:44:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.29
[32m[20221208 14:44:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.92
[32m[20221208 14:44:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.10
[32m[20221208 14:44:21 @agent_ppo2.py:137][0m Total time:       7.77 min
[32m[20221208 14:44:21 @agent_ppo2.py:139][0m 626688 total steps have happened
[32m[20221208 14:44:21 @agent_ppo2.py:115][0m #------------------------ Iteration 306 --------------------------#
[32m[20221208 14:44:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:21 @agent_ppo2.py:179][0m |           0.0497 |           9.9540 |          11.9928 |
[32m[20221208 14:44:21 @agent_ppo2.py:179][0m |           0.0610 |           9.2546 |          11.1021 |
[32m[20221208 14:44:21 @agent_ppo2.py:179][0m |           0.0176 |           9.0884 |          11.3285 |
[32m[20221208 14:44:21 @agent_ppo2.py:179][0m |          -0.0026 |           8.9823 |          11.5708 |
[32m[20221208 14:44:22 @agent_ppo2.py:179][0m |          -0.0212 |           8.8521 |          11.9233 |
[32m[20221208 14:44:22 @agent_ppo2.py:179][0m |          -0.0287 |           8.7348 |          12.0664 |
[32m[20221208 14:44:22 @agent_ppo2.py:179][0m |          -0.0355 |           8.6837 |          12.1944 |
[32m[20221208 14:44:22 @agent_ppo2.py:179][0m |          -0.0438 |           8.6275 |          12.2870 |
[32m[20221208 14:44:22 @agent_ppo2.py:179][0m |          -0.0473 |           8.4654 |          12.2910 |
[32m[20221208 14:44:22 @agent_ppo2.py:179][0m |          -0.0493 |           8.4187 |          12.3204 |
[32m[20221208 14:44:22 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:44:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.87
[32m[20221208 14:44:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.06
[32m[20221208 14:44:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.80
[32m[20221208 14:44:22 @agent_ppo2.py:137][0m Total time:       7.80 min
[32m[20221208 14:44:22 @agent_ppo2.py:139][0m 628736 total steps have happened
[32m[20221208 14:44:22 @agent_ppo2.py:115][0m #------------------------ Iteration 307 --------------------------#
[32m[20221208 14:44:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |           0.0481 |          10.0502 |          12.0006 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |           0.0385 |           9.3247 |          11.5259 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0101 |           9.0612 |          12.0351 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0301 |           8.9691 |          12.1753 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0373 |           8.9269 |          12.2302 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0436 |           8.8485 |          12.3055 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0457 |           8.7843 |          12.2472 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0477 |           8.7200 |          12.2260 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0515 |           8.6469 |          12.2874 |
[32m[20221208 14:44:23 @agent_ppo2.py:179][0m |          -0.0557 |           8.6270 |          12.3093 |
[32m[20221208 14:44:23 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:44:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.96
[32m[20221208 14:44:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.68
[32m[20221208 14:44:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 177.81
[32m[20221208 14:44:24 @agent_ppo2.py:137][0m Total time:       7.83 min
[32m[20221208 14:44:24 @agent_ppo2.py:139][0m 630784 total steps have happened
[32m[20221208 14:44:24 @agent_ppo2.py:115][0m #------------------------ Iteration 308 --------------------------#
[32m[20221208 14:44:24 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:24 @agent_ppo2.py:179][0m |           0.0576 |           8.0174 |          11.7670 |
[32m[20221208 14:44:24 @agent_ppo2.py:179][0m |           0.0319 |           7.6088 |          11.9224 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |           0.0151 |           7.4581 |          11.6594 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0129 |           7.3151 |          10.9833 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0443 |           7.2641 |           9.3061 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0535 |           7.1740 |           9.2082 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0580 |           7.0882 |           9.2225 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0617 |           7.0662 |           9.2343 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0642 |           7.0427 |           9.2423 |
[32m[20221208 14:44:25 @agent_ppo2.py:179][0m |          -0.0669 |           6.9261 |           9.2391 |
[32m[20221208 14:44:25 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:44:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.82
[32m[20221208 14:44:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.86
[32m[20221208 14:44:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.20
[32m[20221208 14:44:25 @agent_ppo2.py:137][0m Total time:       7.85 min
[32m[20221208 14:44:25 @agent_ppo2.py:139][0m 632832 total steps have happened
[32m[20221208 14:44:25 @agent_ppo2.py:115][0m #------------------------ Iteration 309 --------------------------#
[32m[20221208 14:44:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |           0.0240 |           7.7920 |          12.5945 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0078 |           6.9179 |          12.5917 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0231 |           6.4693 |          12.5351 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0379 |           6.1858 |          12.7051 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0426 |           5.9293 |          12.7031 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0441 |           5.7186 |          12.6575 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0452 |           5.5624 |          12.6881 |
[32m[20221208 14:44:26 @agent_ppo2.py:179][0m |          -0.0523 |           5.3934 |          12.7063 |
[32m[20221208 14:44:27 @agent_ppo2.py:179][0m |          -0.0557 |           5.2559 |          12.7079 |
[32m[20221208 14:44:27 @agent_ppo2.py:179][0m |          -0.0579 |           5.0736 |          12.6503 |
[32m[20221208 14:44:27 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:44:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.58
[32m[20221208 14:44:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.49
[32m[20221208 14:44:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.05
[32m[20221208 14:44:27 @agent_ppo2.py:137][0m Total time:       7.88 min
[32m[20221208 14:44:27 @agent_ppo2.py:139][0m 634880 total steps have happened
[32m[20221208 14:44:27 @agent_ppo2.py:115][0m #------------------------ Iteration 310 --------------------------#
[32m[20221208 14:44:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |           0.0403 |           6.4130 |          12.4062 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |           0.0207 |           5.3865 |          11.8900 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |           0.0030 |           4.9933 |          12.1883 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0240 |           4.7456 |          12.2967 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0377 |           4.5738 |          12.4353 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0433 |           4.4522 |          12.4753 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0472 |           4.3729 |          12.4735 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0545 |           4.2863 |          12.5512 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0558 |           4.2089 |          12.5425 |
[32m[20221208 14:44:28 @agent_ppo2.py:179][0m |          -0.0579 |           4.1507 |          12.5497 |
[32m[20221208 14:44:28 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:44:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.10
[32m[20221208 14:44:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.78
[32m[20221208 14:44:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.05
[32m[20221208 14:44:29 @agent_ppo2.py:137][0m Total time:       7.91 min
[32m[20221208 14:44:29 @agent_ppo2.py:139][0m 636928 total steps have happened
[32m[20221208 14:44:29 @agent_ppo2.py:115][0m #------------------------ Iteration 311 --------------------------#
[32m[20221208 14:44:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:29 @agent_ppo2.py:179][0m |           0.0630 |           7.2424 |          12.5266 |
[32m[20221208 14:44:29 @agent_ppo2.py:179][0m |           0.0166 |           6.2585 |          12.3867 |
[32m[20221208 14:44:29 @agent_ppo2.py:179][0m |          -0.0143 |           5.8970 |          12.7732 |
[32m[20221208 14:44:29 @agent_ppo2.py:179][0m |          -0.0369 |           5.6254 |          12.8158 |
[32m[20221208 14:44:29 @agent_ppo2.py:179][0m |          -0.0400 |           5.5179 |          12.7632 |
[32m[20221208 14:44:29 @agent_ppo2.py:179][0m |          -0.0421 |           5.3550 |          12.7210 |
[32m[20221208 14:44:30 @agent_ppo2.py:179][0m |          -0.0519 |           5.2400 |          12.7393 |
[32m[20221208 14:44:30 @agent_ppo2.py:179][0m |          -0.0617 |           5.1647 |          12.7362 |
[32m[20221208 14:44:30 @agent_ppo2.py:179][0m |          -0.0673 |           5.0911 |          12.8591 |
[32m[20221208 14:44:30 @agent_ppo2.py:179][0m |          -0.0699 |           5.0481 |          12.8677 |
[32m[20221208 14:44:30 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:44:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.26
[32m[20221208 14:44:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.57
[32m[20221208 14:44:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.04
[32m[20221208 14:44:30 @agent_ppo2.py:137][0m Total time:       7.93 min
[32m[20221208 14:44:30 @agent_ppo2.py:139][0m 638976 total steps have happened
[32m[20221208 14:44:30 @agent_ppo2.py:115][0m #------------------------ Iteration 312 --------------------------#
[32m[20221208 14:44:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:44:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |           0.0386 |           7.6662 |          12.5471 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |           0.0006 |           7.3641 |          12.4931 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0085 |           7.2354 |          12.5955 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0267 |           7.1234 |          12.6287 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0329 |           7.0744 |          12.6284 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0380 |           7.0379 |          12.7003 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0425 |           6.9750 |          12.6970 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0462 |           6.9766 |          12.6923 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0468 |           6.9299 |          12.7231 |
[32m[20221208 14:44:31 @agent_ppo2.py:179][0m |          -0.0540 |           6.8804 |          12.7032 |
[32m[20221208 14:44:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:44:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.46
[32m[20221208 14:44:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.61
[32m[20221208 14:44:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.22
[32m[20221208 14:44:32 @agent_ppo2.py:137][0m Total time:       7.96 min
[32m[20221208 14:44:32 @agent_ppo2.py:139][0m 641024 total steps have happened
[32m[20221208 14:44:32 @agent_ppo2.py:115][0m #------------------------ Iteration 313 --------------------------#
[32m[20221208 14:44:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:44:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:32 @agent_ppo2.py:179][0m |           0.0304 |           4.6131 |          12.6237 |
[32m[20221208 14:44:32 @agent_ppo2.py:179][0m |          -0.0016 |           3.9443 |          12.3972 |
[32m[20221208 14:44:32 @agent_ppo2.py:179][0m |          -0.0250 |           3.8560 |          12.6841 |
[32m[20221208 14:44:32 @agent_ppo2.py:179][0m |          -0.0302 |           3.7158 |          12.5447 |
[32m[20221208 14:44:32 @agent_ppo2.py:179][0m |          -0.0319 |           3.6057 |          12.5250 |
[32m[20221208 14:44:33 @agent_ppo2.py:179][0m |          -0.0309 |           3.5631 |          12.3981 |
[32m[20221208 14:44:33 @agent_ppo2.py:179][0m |          -0.0375 |           3.4902 |          12.3967 |
[32m[20221208 14:44:33 @agent_ppo2.py:179][0m |          -0.0383 |           3.4487 |          12.2413 |
[32m[20221208 14:44:33 @agent_ppo2.py:179][0m |          -0.0439 |           3.3729 |          12.2267 |
[32m[20221208 14:44:33 @agent_ppo2.py:179][0m |          -0.0471 |           3.3377 |          12.3243 |
[32m[20221208 14:44:33 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:44:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.22
[32m[20221208 14:44:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.39
[32m[20221208 14:44:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.33
[32m[20221208 14:44:33 @agent_ppo2.py:137][0m Total time:       7.98 min
[32m[20221208 14:44:33 @agent_ppo2.py:139][0m 643072 total steps have happened
[32m[20221208 14:44:33 @agent_ppo2.py:115][0m #------------------------ Iteration 314 --------------------------#
[32m[20221208 14:44:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:44:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |           0.0362 |           2.1525 |          12.7224 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |           0.0015 |           1.6161 |          12.5746 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |          -0.0029 |           1.4907 |          12.6492 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |           0.0229 |           1.4103 |          12.2581 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |           0.0031 |           1.3459 |          12.4794 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |          -0.0179 |           1.3035 |          12.6474 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |          -0.0225 |           1.2730 |          12.6557 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |          -0.0183 |           1.2566 |          12.6562 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |          -0.0155 |           1.2341 |          12.6747 |
[32m[20221208 14:44:34 @agent_ppo2.py:179][0m |          -0.0261 |           1.2205 |          12.7497 |
[32m[20221208 14:44:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:44:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.95
[32m[20221208 14:44:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 28.89
[32m[20221208 14:44:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.38
[32m[20221208 14:44:35 @agent_ppo2.py:137][0m Total time:       8.01 min
[32m[20221208 14:44:35 @agent_ppo2.py:139][0m 645120 total steps have happened
[32m[20221208 14:44:35 @agent_ppo2.py:115][0m #------------------------ Iteration 315 --------------------------#
[32m[20221208 14:44:35 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:35 @agent_ppo2.py:179][0m |           0.0498 |           8.4462 |          12.7885 |
[32m[20221208 14:44:35 @agent_ppo2.py:179][0m |           0.0299 |           7.5560 |          12.3388 |
[32m[20221208 14:44:35 @agent_ppo2.py:179][0m |          -0.0197 |           7.2793 |          12.7743 |
[32m[20221208 14:44:35 @agent_ppo2.py:179][0m |          -0.0428 |           7.0853 |          12.9818 |
[32m[20221208 14:44:36 @agent_ppo2.py:179][0m |          -0.0436 |           6.8984 |          12.9792 |
[32m[20221208 14:44:36 @agent_ppo2.py:179][0m |          -0.0552 |           6.7467 |          13.0439 |
[32m[20221208 14:44:36 @agent_ppo2.py:179][0m |          -0.0573 |           6.6318 |          12.9926 |
[32m[20221208 14:44:36 @agent_ppo2.py:179][0m |          -0.0615 |           6.4866 |          12.9784 |
[32m[20221208 14:44:36 @agent_ppo2.py:179][0m |          -0.0677 |           6.4374 |          12.9709 |
[32m[20221208 14:44:36 @agent_ppo2.py:179][0m |          -0.0693 |           6.3099 |          13.0520 |
[32m[20221208 14:44:36 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:44:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.94
[32m[20221208 14:44:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.06
[32m[20221208 14:44:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 177.73
[32m[20221208 14:44:36 @agent_ppo2.py:137][0m Total time:       8.03 min
[32m[20221208 14:44:36 @agent_ppo2.py:139][0m 647168 total steps have happened
[32m[20221208 14:44:36 @agent_ppo2.py:115][0m #------------------------ Iteration 316 --------------------------#
[32m[20221208 14:44:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |           0.0379 |           7.0769 |          12.9725 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |           0.0063 |           6.4991 |          12.8070 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0169 |           6.2739 |          12.9110 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0374 |           6.1836 |          13.0474 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0468 |           6.0936 |          13.0889 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0545 |           5.9663 |          13.1276 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0576 |           5.9481 |          13.1166 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0593 |           5.8875 |          13.0773 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0609 |           5.8297 |          13.0728 |
[32m[20221208 14:44:37 @agent_ppo2.py:179][0m |          -0.0652 |           5.8283 |          13.1182 |
[32m[20221208 14:44:37 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:44:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.84
[32m[20221208 14:44:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.96
[32m[20221208 14:44:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.32
[32m[20221208 14:44:38 @agent_ppo2.py:137][0m Total time:       8.06 min
[32m[20221208 14:44:38 @agent_ppo2.py:139][0m 649216 total steps have happened
[32m[20221208 14:44:38 @agent_ppo2.py:115][0m #------------------------ Iteration 317 --------------------------#
[32m[20221208 14:44:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:38 @agent_ppo2.py:179][0m |           0.0436 |           3.8745 |          12.8233 |
[32m[20221208 14:44:38 @agent_ppo2.py:179][0m |           0.0247 |           3.5188 |          12.4300 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0063 |           3.3385 |          12.6656 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0342 |           3.1862 |          13.0479 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0360 |           3.0876 |          13.0111 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0436 |           3.0152 |          12.9937 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0451 |           2.8751 |          12.9455 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0494 |           2.8127 |          12.9855 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0563 |           2.7314 |          13.0182 |
[32m[20221208 14:44:39 @agent_ppo2.py:179][0m |          -0.0511 |           2.6666 |          12.9181 |
[32m[20221208 14:44:39 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:44:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.16
[32m[20221208 14:44:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.24
[32m[20221208 14:44:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.18
[32m[20221208 14:44:39 @agent_ppo2.py:137][0m Total time:       8.09 min
[32m[20221208 14:44:39 @agent_ppo2.py:139][0m 651264 total steps have happened
[32m[20221208 14:44:39 @agent_ppo2.py:115][0m #------------------------ Iteration 318 --------------------------#
[32m[20221208 14:44:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |           0.0448 |           9.1488 |          12.9755 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0019 |           8.4499 |          13.0083 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0265 |           8.0852 |          13.1470 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0342 |           7.9061 |          13.1450 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0465 |           7.8307 |          13.2337 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0491 |           7.5973 |          13.2094 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0552 |           7.4848 |          13.1686 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0606 |           7.3936 |          13.1964 |
[32m[20221208 14:44:40 @agent_ppo2.py:179][0m |          -0.0644 |           7.2898 |          13.2012 |
[32m[20221208 14:44:41 @agent_ppo2.py:179][0m |          -0.0669 |           7.1979 |          13.2062 |
[32m[20221208 14:44:41 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:44:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.77
[32m[20221208 14:44:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.61
[32m[20221208 14:44:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.23
[32m[20221208 14:44:41 @agent_ppo2.py:137][0m Total time:       8.11 min
[32m[20221208 14:44:41 @agent_ppo2.py:139][0m 653312 total steps have happened
[32m[20221208 14:44:41 @agent_ppo2.py:115][0m #------------------------ Iteration 319 --------------------------#
[32m[20221208 14:44:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |           0.0346 |           6.5491 |          13.0764 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |           0.0018 |           5.9814 |          13.0662 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0184 |           5.8050 |          13.2575 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0307 |           5.6142 |          13.2906 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0385 |           5.5137 |          13.3490 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0470 |           5.3811 |          13.3384 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0505 |           5.2906 |          13.3382 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0550 |           5.2282 |          13.2647 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0567 |           5.1265 |          13.3490 |
[32m[20221208 14:44:42 @agent_ppo2.py:179][0m |          -0.0603 |           5.0633 |          13.3169 |
[32m[20221208 14:44:42 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:44:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.43
[32m[20221208 14:44:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.29
[32m[20221208 14:44:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.80
[32m[20221208 14:44:43 @agent_ppo2.py:137][0m Total time:       8.14 min
[32m[20221208 14:44:43 @agent_ppo2.py:139][0m 655360 total steps have happened
[32m[20221208 14:44:43 @agent_ppo2.py:115][0m #------------------------ Iteration 320 --------------------------#
[32m[20221208 14:44:43 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:43 @agent_ppo2.py:179][0m |           0.0501 |           8.4132 |          12.8241 |
[32m[20221208 14:44:43 @agent_ppo2.py:179][0m |           0.0499 |           7.4446 |          12.1636 |
[32m[20221208 14:44:43 @agent_ppo2.py:179][0m |          -0.0074 |           7.1207 |          12.4939 |
[32m[20221208 14:44:43 @agent_ppo2.py:179][0m |          -0.0298 |           7.0161 |          12.7720 |
[32m[20221208 14:44:43 @agent_ppo2.py:179][0m |          -0.0402 |           6.8577 |          12.8673 |
[32m[20221208 14:44:43 @agent_ppo2.py:179][0m |          -0.0481 |           6.7267 |          12.8525 |
[32m[20221208 14:44:44 @agent_ppo2.py:179][0m |          -0.0521 |           6.6118 |          12.8898 |
[32m[20221208 14:44:44 @agent_ppo2.py:179][0m |          -0.0538 |           6.6105 |          12.8772 |
[32m[20221208 14:44:44 @agent_ppo2.py:179][0m |          -0.0598 |           6.5374 |          12.8900 |
[32m[20221208 14:44:44 @agent_ppo2.py:179][0m |          -0.0635 |           6.4282 |          12.8777 |
[32m[20221208 14:44:44 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:44:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.18
[32m[20221208 14:44:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.19
[32m[20221208 14:44:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 176.34
[32m[20221208 14:44:44 @agent_ppo2.py:137][0m Total time:       8.16 min
[32m[20221208 14:44:44 @agent_ppo2.py:139][0m 657408 total steps have happened
[32m[20221208 14:44:44 @agent_ppo2.py:115][0m #------------------------ Iteration 321 --------------------------#
[32m[20221208 14:44:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |           0.0608 |           9.5605 |          12.8683 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |           0.0263 |           8.9196 |          12.4773 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |           0.0276 |           8.6638 |          12.0184 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0156 |           8.5725 |          12.6188 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0328 |           8.4071 |          12.8443 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0433 |           8.3463 |          12.8582 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0520 |           8.2676 |          12.9155 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0548 |           8.1490 |          12.9115 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0595 |           8.1253 |          12.8742 |
[32m[20221208 14:44:45 @agent_ppo2.py:179][0m |          -0.0572 |           7.9937 |          12.8008 |
[32m[20221208 14:44:45 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:44:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.33
[32m[20221208 14:44:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.02
[32m[20221208 14:44:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.13
[32m[20221208 14:44:46 @agent_ppo2.py:137][0m Total time:       8.19 min
[32m[20221208 14:44:46 @agent_ppo2.py:139][0m 659456 total steps have happened
[32m[20221208 14:44:46 @agent_ppo2.py:115][0m #------------------------ Iteration 322 --------------------------#
[32m[20221208 14:44:46 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:44:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:46 @agent_ppo2.py:179][0m |           0.1112 |           6.3942 |          12.4866 |
[32m[20221208 14:44:46 @agent_ppo2.py:179][0m |           0.0436 |           5.6835 |          12.0556 |
[32m[20221208 14:44:46 @agent_ppo2.py:179][0m |          -0.0007 |           5.6263 |          12.1864 |
[32m[20221208 14:44:46 @agent_ppo2.py:179][0m |          -0.0200 |           5.3475 |          12.4805 |
[32m[20221208 14:44:47 @agent_ppo2.py:179][0m |          -0.0261 |           5.2865 |          12.5462 |
[32m[20221208 14:44:47 @agent_ppo2.py:179][0m |          -0.0347 |           5.1513 |          12.5931 |
[32m[20221208 14:44:47 @agent_ppo2.py:179][0m |          -0.0378 |           5.0953 |          12.5674 |
[32m[20221208 14:44:47 @agent_ppo2.py:179][0m |          -0.0416 |           4.9721 |          12.6324 |
[32m[20221208 14:44:47 @agent_ppo2.py:179][0m |          -0.0500 |           4.9317 |          12.6080 |
[32m[20221208 14:44:47 @agent_ppo2.py:179][0m |          -0.0489 |           4.8744 |          12.6111 |
[32m[20221208 14:44:47 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:44:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.85
[32m[20221208 14:44:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.46
[32m[20221208 14:44:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.69
[32m[20221208 14:44:47 @agent_ppo2.py:137][0m Total time:       8.22 min
[32m[20221208 14:44:47 @agent_ppo2.py:139][0m 661504 total steps have happened
[32m[20221208 14:44:47 @agent_ppo2.py:115][0m #------------------------ Iteration 323 --------------------------#
[32m[20221208 14:44:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |           0.0684 |           8.9693 |          12.5296 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |           0.0661 |           8.3641 |          11.6240 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |           0.0036 |           8.1085 |          12.5606 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0269 |           7.8535 |          12.7824 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0381 |           7.6697 |          12.9022 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0420 |           7.5141 |          12.8627 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0479 |           7.3602 |          12.8983 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0500 |           7.2626 |          12.9081 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0534 |           7.1435 |          12.9159 |
[32m[20221208 14:44:48 @agent_ppo2.py:179][0m |          -0.0584 |           7.0206 |          12.8919 |
[32m[20221208 14:44:48 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:44:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.17
[32m[20221208 14:44:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.05
[32m[20221208 14:44:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.46
[32m[20221208 14:44:49 @agent_ppo2.py:137][0m Total time:       8.24 min
[32m[20221208 14:44:49 @agent_ppo2.py:139][0m 663552 total steps have happened
[32m[20221208 14:44:49 @agent_ppo2.py:115][0m #------------------------ Iteration 324 --------------------------#
[32m[20221208 14:44:49 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:49 @agent_ppo2.py:179][0m |           0.0498 |           8.4660 |          12.4285 |
[32m[20221208 14:44:49 @agent_ppo2.py:179][0m |           0.0182 |           7.5414 |          12.1227 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0259 |           7.2459 |          12.3271 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0423 |           7.0786 |          12.5098 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0513 |           6.9162 |          12.4997 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0591 |           6.7793 |          12.5555 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0620 |           6.6888 |          12.5149 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0656 |           6.5888 |          12.5080 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0651 |           6.4970 |          12.5156 |
[32m[20221208 14:44:50 @agent_ppo2.py:179][0m |          -0.0713 |           6.4428 |          12.4869 |
[32m[20221208 14:44:50 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:44:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.57
[32m[20221208 14:44:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.40
[32m[20221208 14:44:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.89
[32m[20221208 14:44:50 @agent_ppo2.py:137][0m Total time:       8.27 min
[32m[20221208 14:44:50 @agent_ppo2.py:139][0m 665600 total steps have happened
[32m[20221208 14:44:50 @agent_ppo2.py:115][0m #------------------------ Iteration 325 --------------------------#
[32m[20221208 14:44:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |           0.0352 |           7.1077 |          12.8212 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |           0.0043 |           6.2739 |          12.8254 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |          -0.0256 |           5.9191 |          12.8908 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |          -0.0373 |           5.7210 |          13.0324 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |          -0.0494 |           5.5264 |          12.9925 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |          -0.0494 |           5.3914 |          13.0357 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |          -0.0524 |           5.3088 |          12.9516 |
[32m[20221208 14:44:51 @agent_ppo2.py:179][0m |          -0.0571 |           5.2299 |          12.9339 |
[32m[20221208 14:44:52 @agent_ppo2.py:179][0m |          -0.0581 |           5.1304 |          12.9098 |
[32m[20221208 14:44:52 @agent_ppo2.py:179][0m |          -0.0609 |           5.0791 |          12.9145 |
[32m[20221208 14:44:52 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:44:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.03
[32m[20221208 14:44:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.66
[32m[20221208 14:44:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 187.69
[32m[20221208 14:44:52 @agent_ppo2.py:137][0m Total time:       8.30 min
[32m[20221208 14:44:52 @agent_ppo2.py:139][0m 667648 total steps have happened
[32m[20221208 14:44:52 @agent_ppo2.py:115][0m #------------------------ Iteration 326 --------------------------#
[32m[20221208 14:44:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |           0.0590 |          10.1116 |          12.7979 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |           0.0222 |           9.6647 |          12.4270 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |           0.0004 |           9.3337 |          12.8552 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0225 |           9.1768 |          13.0180 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0381 |           8.9944 |          13.0605 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0464 |           8.9076 |          13.0760 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0500 |           8.8155 |          13.0490 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0564 |           8.7650 |          13.0934 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0587 |           8.6839 |          13.0528 |
[32m[20221208 14:44:53 @agent_ppo2.py:179][0m |          -0.0621 |           8.6041 |          12.9418 |
[32m[20221208 14:44:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:44:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.90
[32m[20221208 14:44:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.57
[32m[20221208 14:44:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.24
[32m[20221208 14:44:54 @agent_ppo2.py:137][0m Total time:       8.32 min
[32m[20221208 14:44:54 @agent_ppo2.py:139][0m 669696 total steps have happened
[32m[20221208 14:44:54 @agent_ppo2.py:115][0m #------------------------ Iteration 327 --------------------------#
[32m[20221208 14:44:54 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:54 @agent_ppo2.py:179][0m |           0.0423 |           9.7081 |          12.8269 |
[32m[20221208 14:44:54 @agent_ppo2.py:179][0m |           0.0171 |           9.0558 |          12.8452 |
[32m[20221208 14:44:54 @agent_ppo2.py:179][0m |          -0.0075 |           8.8527 |          12.9496 |
[32m[20221208 14:44:54 @agent_ppo2.py:179][0m |          -0.0261 |           8.6256 |          12.9636 |
[32m[20221208 14:44:54 @agent_ppo2.py:179][0m |          -0.0405 |           8.4630 |          13.1275 |
[32m[20221208 14:44:55 @agent_ppo2.py:179][0m |          -0.0445 |           8.4594 |          13.0503 |
[32m[20221208 14:44:55 @agent_ppo2.py:179][0m |          -0.0522 |           8.2766 |          13.0467 |
[32m[20221208 14:44:55 @agent_ppo2.py:179][0m |          -0.0575 |           8.2257 |          13.0501 |
[32m[20221208 14:44:55 @agent_ppo2.py:179][0m |          -0.0612 |           8.1560 |          13.0386 |
[32m[20221208 14:44:55 @agent_ppo2.py:179][0m |          -0.0613 |           8.0849 |          13.0260 |
[32m[20221208 14:44:55 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:44:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 161.28
[32m[20221208 14:44:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.51
[32m[20221208 14:44:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.25
[32m[20221208 14:44:55 @agent_ppo2.py:137][0m Total time:       8.35 min
[32m[20221208 14:44:55 @agent_ppo2.py:139][0m 671744 total steps have happened
[32m[20221208 14:44:55 @agent_ppo2.py:115][0m #------------------------ Iteration 328 --------------------------#
[32m[20221208 14:44:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |           0.0241 |           9.3790 |          12.5028 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |           0.0120 |           8.9133 |          12.3121 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0156 |           8.6083 |          12.3851 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0323 |           8.4468 |          12.4963 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0375 |           8.3360 |          12.3988 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0417 |           8.1716 |          12.4215 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0508 |           8.1111 |          12.2966 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0511 |           8.0248 |          12.2085 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0530 |           7.9635 |          12.2118 |
[32m[20221208 14:44:56 @agent_ppo2.py:179][0m |          -0.0554 |           7.8161 |          12.1057 |
[32m[20221208 14:44:56 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:44:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 175.65
[32m[20221208 14:44:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.17
[32m[20221208 14:44:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.17
[32m[20221208 14:44:57 @agent_ppo2.py:137][0m Total time:       8.38 min
[32m[20221208 14:44:57 @agent_ppo2.py:139][0m 673792 total steps have happened
[32m[20221208 14:44:57 @agent_ppo2.py:115][0m #------------------------ Iteration 329 --------------------------#
[32m[20221208 14:44:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:44:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:57 @agent_ppo2.py:179][0m |           0.0393 |           8.2143 |          12.4322 |
[32m[20221208 14:44:57 @agent_ppo2.py:179][0m |           0.0251 |           7.3005 |          11.8864 |
[32m[20221208 14:44:57 @agent_ppo2.py:179][0m |          -0.0053 |           6.9266 |          11.9261 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0247 |           6.6845 |          12.2890 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0356 |           6.5390 |          12.3872 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0433 |           6.4532 |          12.4546 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0426 |           6.3045 |          12.3166 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0436 |           6.1913 |          12.4024 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0466 |           6.0956 |          12.2644 |
[32m[20221208 14:44:58 @agent_ppo2.py:179][0m |          -0.0500 |           6.0612 |          12.3311 |
[32m[20221208 14:44:58 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:44:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.46
[32m[20221208 14:44:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.19
[32m[20221208 14:44:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.19
[32m[20221208 14:44:58 @agent_ppo2.py:137][0m Total time:       8.40 min
[32m[20221208 14:44:58 @agent_ppo2.py:139][0m 675840 total steps have happened
[32m[20221208 14:44:58 @agent_ppo2.py:115][0m #------------------------ Iteration 330 --------------------------#
[32m[20221208 14:44:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:44:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |           0.0682 |           9.2073 |          11.8385 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |           0.0524 |           8.6111 |          11.0355 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |           0.0255 |           8.2002 |          11.6302 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |          -0.0039 |           7.9737 |          11.8183 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |          -0.0166 |           7.6667 |          12.0507 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |          -0.0339 |           7.4175 |          12.0365 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |          -0.0412 |           7.2966 |          12.0283 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |          -0.0451 |           7.0016 |          12.0527 |
[32m[20221208 14:44:59 @agent_ppo2.py:179][0m |          -0.0525 |           6.7910 |          12.0788 |
[32m[20221208 14:45:00 @agent_ppo2.py:179][0m |          -0.0513 |           6.6606 |          11.9667 |
[32m[20221208 14:45:00 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:45:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 181.84
[32m[20221208 14:45:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.47
[32m[20221208 14:45:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 170.49
[32m[20221208 14:45:00 @agent_ppo2.py:137][0m Total time:       8.43 min
[32m[20221208 14:45:00 @agent_ppo2.py:139][0m 677888 total steps have happened
[32m[20221208 14:45:00 @agent_ppo2.py:115][0m #------------------------ Iteration 331 --------------------------#
[32m[20221208 14:45:00 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |           0.0505 |           6.8948 |          11.9934 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |           0.0367 |           6.5143 |          11.6173 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0067 |           6.3607 |          11.9512 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0239 |           6.2288 |          12.1296 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0366 |           6.1582 |          12.2004 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0333 |           6.1087 |          12.2400 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0435 |           6.0885 |          12.2070 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0509 |           5.9501 |          12.2480 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0544 |           5.8932 |          12.2173 |
[32m[20221208 14:45:01 @agent_ppo2.py:179][0m |          -0.0509 |           5.8485 |          11.9155 |
[32m[20221208 14:45:01 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:45:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.16
[32m[20221208 14:45:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.51
[32m[20221208 14:45:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.40
[32m[20221208 14:45:02 @agent_ppo2.py:137][0m Total time:       8.45 min
[32m[20221208 14:45:02 @agent_ppo2.py:139][0m 679936 total steps have happened
[32m[20221208 14:45:02 @agent_ppo2.py:115][0m #------------------------ Iteration 332 --------------------------#
[32m[20221208 14:45:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |           0.0858 |          10.3287 |          11.7125 |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |           0.0586 |           9.0042 |          11.0381 |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |           0.0157 |           8.5790 |          11.0751 |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |          -0.0165 |           8.2609 |          11.5280 |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |          -0.0290 |           7.9837 |          11.7297 |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |          -0.0385 |           7.8008 |          11.7928 |
[32m[20221208 14:45:02 @agent_ppo2.py:179][0m |          -0.0468 |           7.7195 |          11.7997 |
[32m[20221208 14:45:03 @agent_ppo2.py:179][0m |          -0.0491 |           7.5588 |          11.8144 |
[32m[20221208 14:45:03 @agent_ppo2.py:179][0m |          -0.0523 |           7.4653 |          11.7351 |
[32m[20221208 14:45:03 @agent_ppo2.py:179][0m |          -0.0562 |           7.3785 |          11.6684 |
[32m[20221208 14:45:03 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:45:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.49
[32m[20221208 14:45:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.80
[32m[20221208 14:45:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.48
[32m[20221208 14:45:03 @agent_ppo2.py:137][0m Total time:       8.48 min
[32m[20221208 14:45:03 @agent_ppo2.py:139][0m 681984 total steps have happened
[32m[20221208 14:45:03 @agent_ppo2.py:115][0m #------------------------ Iteration 333 --------------------------#
[32m[20221208 14:45:04 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |           0.0281 |           4.8653 |          11.7482 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |           0.0071 |           3.8508 |          11.5870 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0210 |           3.5883 |          11.7600 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0304 |           3.3851 |          11.6673 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0426 |           3.2291 |          11.5873 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0435 |           3.1314 |          11.7093 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0459 |           3.0314 |          11.6864 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0486 |           2.9476 |          11.5028 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0501 |           2.8871 |          11.4583 |
[32m[20221208 14:45:04 @agent_ppo2.py:179][0m |          -0.0544 |           2.8479 |          11.3999 |
[32m[20221208 14:45:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.95
[32m[20221208 14:45:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.77
[32m[20221208 14:45:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.22
[32m[20221208 14:45:05 @agent_ppo2.py:137][0m Total time:       8.51 min
[32m[20221208 14:45:05 @agent_ppo2.py:139][0m 684032 total steps have happened
[32m[20221208 14:45:05 @agent_ppo2.py:115][0m #------------------------ Iteration 334 --------------------------#
[32m[20221208 14:45:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:05 @agent_ppo2.py:179][0m |           0.0336 |           6.7937 |          12.0067 |
[32m[20221208 14:45:05 @agent_ppo2.py:179][0m |          -0.0006 |           6.3520 |          11.9341 |
[32m[20221208 14:45:05 @agent_ppo2.py:179][0m |          -0.0297 |           6.0129 |          12.1519 |
[32m[20221208 14:45:05 @agent_ppo2.py:179][0m |          -0.0427 |           5.8383 |          12.0676 |
[32m[20221208 14:45:05 @agent_ppo2.py:179][0m |          -0.0458 |           5.6735 |          11.9235 |
[32m[20221208 14:45:06 @agent_ppo2.py:179][0m |          -0.0468 |           5.5379 |          11.9160 |
[32m[20221208 14:45:06 @agent_ppo2.py:179][0m |          -0.0553 |           5.4262 |          11.8410 |
[32m[20221208 14:45:06 @agent_ppo2.py:179][0m |          -0.0605 |           5.2966 |          11.6887 |
[32m[20221208 14:45:06 @agent_ppo2.py:179][0m |          -0.0592 |           5.2294 |          11.6330 |
[32m[20221208 14:45:06 @agent_ppo2.py:179][0m |          -0.0614 |           5.1724 |          11.5722 |
[32m[20221208 14:45:06 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:45:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.74
[32m[20221208 14:45:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.31
[32m[20221208 14:45:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.67
[32m[20221208 14:45:06 @agent_ppo2.py:137][0m Total time:       8.53 min
[32m[20221208 14:45:06 @agent_ppo2.py:139][0m 686080 total steps have happened
[32m[20221208 14:45:06 @agent_ppo2.py:115][0m #------------------------ Iteration 335 --------------------------#
[32m[20221208 14:45:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |           0.0594 |           9.7731 |          11.3026 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |           0.0508 |           9.0553 |          10.9729 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |           0.0190 |           8.7432 |          11.0352 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0203 |           8.4835 |          11.3412 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0324 |           8.1829 |          11.4810 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0423 |           8.0185 |          11.4838 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0463 |           7.7610 |          11.4694 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0529 |           7.6094 |          11.4939 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0534 |           7.4694 |          11.3723 |
[32m[20221208 14:45:07 @agent_ppo2.py:179][0m |          -0.0600 |           7.3835 |          11.1854 |
[32m[20221208 14:45:07 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:45:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.22
[32m[20221208 14:45:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.13
[32m[20221208 14:45:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.29
[32m[20221208 14:45:08 @agent_ppo2.py:137][0m Total time:       8.56 min
[32m[20221208 14:45:08 @agent_ppo2.py:139][0m 688128 total steps have happened
[32m[20221208 14:45:08 @agent_ppo2.py:115][0m #------------------------ Iteration 336 --------------------------#
[32m[20221208 14:45:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:08 @agent_ppo2.py:179][0m |           0.0527 |          10.6994 |          11.1524 |
[32m[20221208 14:45:08 @agent_ppo2.py:179][0m |           0.0868 |           9.9115 |          10.8521 |
[32m[20221208 14:45:08 @agent_ppo2.py:179][0m |           0.0139 |           9.5736 |          10.8182 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0139 |           9.4427 |          11.2876 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0362 |           9.2483 |          11.3605 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0440 |           9.1422 |          11.2347 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0472 |           9.0541 |          11.3340 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0513 |           9.0232 |          11.2941 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0577 |           8.8877 |          11.1922 |
[32m[20221208 14:45:09 @agent_ppo2.py:179][0m |          -0.0582 |           8.8100 |          11.2702 |
[32m[20221208 14:45:09 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:45:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 190.13
[32m[20221208 14:45:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 205.32
[32m[20221208 14:45:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.06
[32m[20221208 14:45:09 @agent_ppo2.py:137][0m Total time:       8.59 min
[32m[20221208 14:45:09 @agent_ppo2.py:139][0m 690176 total steps have happened
[32m[20221208 14:45:09 @agent_ppo2.py:115][0m #------------------------ Iteration 337 --------------------------#
[32m[20221208 14:45:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |           0.0541 |          11.3741 |          11.2930 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |           0.0213 |          10.6732 |          10.9702 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0073 |          10.4633 |          10.9662 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0206 |          10.3009 |          11.0153 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0321 |          10.1260 |          11.0031 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0426 |           9.9418 |          10.9055 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0495 |           9.9073 |          10.8609 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0520 |           9.8718 |          10.8294 |
[32m[20221208 14:45:10 @agent_ppo2.py:179][0m |          -0.0565 |           9.7477 |          10.7932 |
[32m[20221208 14:45:11 @agent_ppo2.py:179][0m |          -0.0594 |           9.7506 |          10.6882 |
[32m[20221208 14:45:11 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:45:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.83
[32m[20221208 14:45:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.69
[32m[20221208 14:45:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.21
[32m[20221208 14:45:11 @agent_ppo2.py:137][0m Total time:       8.61 min
[32m[20221208 14:45:11 @agent_ppo2.py:139][0m 692224 total steps have happened
[32m[20221208 14:45:11 @agent_ppo2.py:115][0m #------------------------ Iteration 338 --------------------------#
[32m[20221208 14:45:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |           0.0580 |           9.8144 |          11.1516 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |           0.0233 |           9.1686 |          10.8091 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0070 |           8.8634 |          10.9321 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0285 |           8.6074 |          10.9400 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0401 |           8.2966 |          10.9710 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0459 |           8.0877 |          10.7903 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0495 |           7.9291 |          10.7029 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0530 |           7.6876 |          10.6582 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0570 |           7.4810 |          10.5450 |
[32m[20221208 14:45:12 @agent_ppo2.py:179][0m |          -0.0586 |           7.3134 |          10.4875 |
[32m[20221208 14:45:12 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:45:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.44
[32m[20221208 14:45:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.68
[32m[20221208 14:45:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.85
[32m[20221208 14:45:13 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 190.85
[32m[20221208 14:45:13 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 190.85
[32m[20221208 14:45:13 @agent_ppo2.py:137][0m Total time:       8.64 min
[32m[20221208 14:45:13 @agent_ppo2.py:139][0m 694272 total steps have happened
[32m[20221208 14:45:13 @agent_ppo2.py:115][0m #------------------------ Iteration 339 --------------------------#
[32m[20221208 14:45:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |           0.0422 |           8.1082 |          10.8694 |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |           0.0147 |           7.4724 |          11.2282 |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |          -0.0092 |           7.2392 |          10.8367 |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |          -0.0321 |           7.1287 |          10.8898 |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |          -0.0361 |           7.0135 |          10.4651 |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |          -0.0409 |           6.9151 |          10.4939 |
[32m[20221208 14:45:13 @agent_ppo2.py:179][0m |          -0.0426 |           6.8523 |          10.3915 |
[32m[20221208 14:45:14 @agent_ppo2.py:179][0m |          -0.0467 |           6.7577 |          10.3134 |
[32m[20221208 14:45:14 @agent_ppo2.py:179][0m |          -0.0540 |           6.7443 |          10.1725 |
[32m[20221208 14:45:14 @agent_ppo2.py:179][0m |          -0.0544 |           6.6866 |          10.1316 |
[32m[20221208 14:45:14 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:45:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.14
[32m[20221208 14:45:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.30
[32m[20221208 14:45:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.78
[32m[20221208 14:45:14 @agent_ppo2.py:137][0m Total time:       8.66 min
[32m[20221208 14:45:14 @agent_ppo2.py:139][0m 696320 total steps have happened
[32m[20221208 14:45:14 @agent_ppo2.py:115][0m #------------------------ Iteration 340 --------------------------#
[32m[20221208 14:45:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |           0.0582 |           8.3362 |          10.0200 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |           0.0509 |           7.5327 |          10.1945 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0029 |           7.1382 |          10.1877 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0208 |           6.8668 |          10.1751 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0369 |           6.6903 |          10.1915 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0396 |           6.5553 |           9.8992 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0418 |           6.4328 |           9.9523 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0475 |           6.3110 |           9.9341 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0551 |           6.1741 |           9.7029 |
[32m[20221208 14:45:15 @agent_ppo2.py:179][0m |          -0.0557 |           6.1316 |           9.7381 |
[32m[20221208 14:45:15 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:45:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.83
[32m[20221208 14:45:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.23
[32m[20221208 14:45:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.21
[32m[20221208 14:45:16 @agent_ppo2.py:137][0m Total time:       8.69 min
[32m[20221208 14:45:16 @agent_ppo2.py:139][0m 698368 total steps have happened
[32m[20221208 14:45:16 @agent_ppo2.py:115][0m #------------------------ Iteration 341 --------------------------#
[32m[20221208 14:45:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:16 @agent_ppo2.py:179][0m |           0.0594 |           9.8825 |           9.9337 |
[32m[20221208 14:45:16 @agent_ppo2.py:179][0m |           0.0494 |           9.3817 |           9.6048 |
[32m[20221208 14:45:16 @agent_ppo2.py:179][0m |           0.0044 |           9.1998 |           9.2032 |
[32m[20221208 14:45:16 @agent_ppo2.py:179][0m |          -0.0201 |           9.0348 |           9.3149 |
[32m[20221208 14:45:17 @agent_ppo2.py:179][0m |          -0.0287 |           8.9898 |           9.4082 |
[32m[20221208 14:45:17 @agent_ppo2.py:179][0m |          -0.0352 |           8.8706 |           9.2904 |
[32m[20221208 14:45:17 @agent_ppo2.py:179][0m |          -0.0344 |           8.7644 |           9.0911 |
[32m[20221208 14:45:17 @agent_ppo2.py:179][0m |          -0.0479 |           8.7278 |           8.8564 |
[32m[20221208 14:45:17 @agent_ppo2.py:179][0m |          -0.0503 |           8.6708 |           8.6640 |
[32m[20221208 14:45:17 @agent_ppo2.py:179][0m |          -0.0503 |           8.6292 |           8.7667 |
[32m[20221208 14:45:17 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:45:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.89
[32m[20221208 14:45:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.80
[32m[20221208 14:45:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.85
[32m[20221208 14:45:17 @agent_ppo2.py:137][0m Total time:       8.72 min
[32m[20221208 14:45:17 @agent_ppo2.py:139][0m 700416 total steps have happened
[32m[20221208 14:45:17 @agent_ppo2.py:115][0m #------------------------ Iteration 342 --------------------------#
[32m[20221208 14:45:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |           0.0346 |           7.5632 |          10.0466 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |           0.0027 |           7.1000 |          10.0185 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0217 |           6.8435 |           9.9974 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0346 |           6.7153 |           9.9053 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0374 |           6.6087 |           9.8491 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0407 |           6.5609 |           9.8296 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0479 |           6.4462 |           9.7022 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0484 |           6.3390 |           9.4932 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0484 |           6.2886 |           9.4532 |
[32m[20221208 14:45:18 @agent_ppo2.py:179][0m |          -0.0503 |           6.1969 |           9.1607 |
[32m[20221208 14:45:18 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:45:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.75
[32m[20221208 14:45:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.86
[32m[20221208 14:45:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.11
[32m[20221208 14:45:19 @agent_ppo2.py:137][0m Total time:       8.74 min
[32m[20221208 14:45:19 @agent_ppo2.py:139][0m 702464 total steps have happened
[32m[20221208 14:45:19 @agent_ppo2.py:115][0m #------------------------ Iteration 343 --------------------------#
[32m[20221208 14:45:19 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:45:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:19 @agent_ppo2.py:179][0m |           0.0385 |           9.8585 |           9.6245 |
[32m[20221208 14:45:19 @agent_ppo2.py:179][0m |           0.0224 |           9.2036 |           9.5504 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0111 |           8.7695 |           9.7314 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0277 |           8.4140 |           9.8363 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0403 |           8.2082 |           9.7859 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0435 |           7.9887 |           9.9497 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0487 |           7.8873 |           9.7645 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0508 |           7.6686 |           9.7444 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0543 |           7.5527 |           9.4926 |
[32m[20221208 14:45:20 @agent_ppo2.py:179][0m |          -0.0550 |           7.4464 |           9.5190 |
[32m[20221208 14:45:20 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:45:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 201.93
[32m[20221208 14:45:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 215.66
[32m[20221208 14:45:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.24
[32m[20221208 14:45:20 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 200.24
[32m[20221208 14:45:20 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 200.24
[32m[20221208 14:45:20 @agent_ppo2.py:137][0m Total time:       8.77 min
[32m[20221208 14:45:20 @agent_ppo2.py:139][0m 704512 total steps have happened
[32m[20221208 14:45:20 @agent_ppo2.py:115][0m #------------------------ Iteration 344 --------------------------#
[32m[20221208 14:45:21 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |           0.0608 |           5.0249 |          10.0283 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |           0.0151 |           4.3300 |          10.1018 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |          -0.0018 |           4.1533 |          10.1844 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |          -0.0154 |           4.0984 |          10.1202 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |          -0.0326 |           4.0016 |          10.0459 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |          -0.0387 |           3.9560 |           9.7639 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |          -0.0402 |           3.9136 |          10.0040 |
[32m[20221208 14:45:21 @agent_ppo2.py:179][0m |          -0.0480 |           3.8658 |           9.5953 |
[32m[20221208 14:45:22 @agent_ppo2.py:179][0m |          -0.0497 |           3.8396 |           9.6293 |
[32m[20221208 14:45:22 @agent_ppo2.py:179][0m |          -0.0470 |           3.8129 |           9.1095 |
[32m[20221208 14:45:22 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:45:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.51
[32m[20221208 14:45:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.80
[32m[20221208 14:45:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.61
[32m[20221208 14:45:22 @agent_ppo2.py:137][0m Total time:       8.80 min
[32m[20221208 14:45:22 @agent_ppo2.py:139][0m 706560 total steps have happened
[32m[20221208 14:45:22 @agent_ppo2.py:115][0m #------------------------ Iteration 345 --------------------------#
[32m[20221208 14:45:22 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |           0.0444 |           4.7006 |           9.4219 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |           0.0241 |           4.2405 |           9.3046 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0077 |           4.0813 |           9.4300 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0247 |           3.9570 |           9.4106 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0332 |           3.8490 |           9.1735 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0407 |           3.7250 |           9.1877 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0406 |           3.6709 |           9.0901 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0428 |           3.6224 |           9.0335 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0412 |           3.5405 |           9.2811 |
[32m[20221208 14:45:23 @agent_ppo2.py:179][0m |          -0.0480 |           3.4743 |           8.7292 |
[32m[20221208 14:45:23 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:45:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.03
[32m[20221208 14:45:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.98
[32m[20221208 14:45:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.72
[32m[20221208 14:45:24 @agent_ppo2.py:137][0m Total time:       8.82 min
[32m[20221208 14:45:24 @agent_ppo2.py:139][0m 708608 total steps have happened
[32m[20221208 14:45:24 @agent_ppo2.py:115][0m #------------------------ Iteration 346 --------------------------#
[32m[20221208 14:45:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:24 @agent_ppo2.py:179][0m |           0.0513 |           4.3013 |           9.1951 |
[32m[20221208 14:45:24 @agent_ppo2.py:179][0m |           0.0164 |           3.8587 |           8.9535 |
[32m[20221208 14:45:24 @agent_ppo2.py:179][0m |          -0.0118 |           3.7318 |           8.8848 |
[32m[20221208 14:45:24 @agent_ppo2.py:179][0m |          -0.0212 |           3.6064 |           8.7739 |
[32m[20221208 14:45:24 @agent_ppo2.py:179][0m |          -0.0223 |           3.4651 |           8.5602 |
[32m[20221208 14:45:24 @agent_ppo2.py:179][0m |          -0.0336 |           3.3626 |           8.0956 |
[32m[20221208 14:45:25 @agent_ppo2.py:179][0m |          -0.0349 |           3.3084 |           8.0650 |
[32m[20221208 14:45:25 @agent_ppo2.py:179][0m |          -0.0380 |           3.1934 |           7.8123 |
[32m[20221208 14:45:25 @agent_ppo2.py:179][0m |          -0.0428 |           3.1009 |           7.7204 |
[32m[20221208 14:45:25 @agent_ppo2.py:179][0m |          -0.0406 |           3.0768 |           7.4158 |
[32m[20221208 14:45:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.03
[32m[20221208 14:45:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.39
[32m[20221208 14:45:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.58
[32m[20221208 14:45:25 @agent_ppo2.py:137][0m Total time:       8.85 min
[32m[20221208 14:45:25 @agent_ppo2.py:139][0m 710656 total steps have happened
[32m[20221208 14:45:25 @agent_ppo2.py:115][0m #------------------------ Iteration 347 --------------------------#
[32m[20221208 14:45:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |           0.0838 |           4.9206 |           7.7292 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |           0.0494 |           4.2707 |           7.9982 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |           0.0183 |           4.0621 |           7.9197 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0116 |           3.9221 |           7.1562 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0281 |           3.8539 |           7.3255 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0333 |           3.7437 |           7.0076 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0366 |           3.6824 |           7.1055 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0430 |           3.5808 |           6.8433 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0483 |           3.5396 |           6.7083 |
[32m[20221208 14:45:26 @agent_ppo2.py:179][0m |          -0.0493 |           3.4622 |           6.5527 |
[32m[20221208 14:45:26 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.82
[32m[20221208 14:45:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.37
[32m[20221208 14:45:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.29
[32m[20221208 14:45:27 @agent_ppo2.py:137][0m Total time:       8.87 min
[32m[20221208 14:45:27 @agent_ppo2.py:139][0m 712704 total steps have happened
[32m[20221208 14:45:27 @agent_ppo2.py:115][0m #------------------------ Iteration 348 --------------------------#
[32m[20221208 14:45:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:27 @agent_ppo2.py:179][0m |           0.0862 |           8.5034 |           6.1125 |
[32m[20221208 14:45:27 @agent_ppo2.py:179][0m |           0.0523 |           7.6215 |           6.6942 |
[32m[20221208 14:45:27 @agent_ppo2.py:179][0m |          -0.0028 |           7.2812 |           6.2613 |
[32m[20221208 14:45:27 @agent_ppo2.py:179][0m |          -0.0202 |           7.0473 |           6.1024 |
[32m[20221208 14:45:27 @agent_ppo2.py:179][0m |          -0.0266 |           6.9646 |           6.3312 |
[32m[20221208 14:45:28 @agent_ppo2.py:179][0m |          -0.0327 |           6.8102 |           6.0636 |
[32m[20221208 14:45:28 @agent_ppo2.py:179][0m |          -0.0377 |           6.7512 |           5.7348 |
[32m[20221208 14:45:28 @agent_ppo2.py:179][0m |          -0.0437 |           6.6801 |           5.6864 |
[32m[20221208 14:45:28 @agent_ppo2.py:179][0m |          -0.0473 |           6.6153 |           5.5507 |
[32m[20221208 14:45:28 @agent_ppo2.py:179][0m |          -0.0527 |           6.5680 |           5.5366 |
[32m[20221208 14:45:28 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.24
[32m[20221208 14:45:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.74
[32m[20221208 14:45:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 186.65
[32m[20221208 14:45:28 @agent_ppo2.py:137][0m Total time:       8.90 min
[32m[20221208 14:45:28 @agent_ppo2.py:139][0m 714752 total steps have happened
[32m[20221208 14:45:28 @agent_ppo2.py:115][0m #------------------------ Iteration 349 --------------------------#
[32m[20221208 14:45:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |           0.0339 |           8.9996 |           6.2980 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |           0.0001 |           8.1285 |           6.3888 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0205 |           7.8994 |           6.2279 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0345 |           7.6467 |           6.1173 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0411 |           7.5179 |           5.9235 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0481 |           7.4260 |           5.5061 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0563 |           7.3380 |           5.2618 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0562 |           7.2693 |           5.0258 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0604 |           7.1995 |           4.8160 |
[32m[20221208 14:45:29 @agent_ppo2.py:179][0m |          -0.0608 |           7.1390 |           4.4987 |
[32m[20221208 14:45:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.50
[32m[20221208 14:45:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.79
[32m[20221208 14:45:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.19
[32m[20221208 14:45:30 @agent_ppo2.py:137][0m Total time:       8.92 min
[32m[20221208 14:45:30 @agent_ppo2.py:139][0m 716800 total steps have happened
[32m[20221208 14:45:30 @agent_ppo2.py:115][0m #------------------------ Iteration 350 --------------------------#
[32m[20221208 14:45:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:30 @agent_ppo2.py:179][0m |           0.0657 |           3.3042 |           6.2477 |
[32m[20221208 14:45:30 @agent_ppo2.py:179][0m |           0.0417 |           2.2451 |           7.2680 |
[32m[20221208 14:45:30 @agent_ppo2.py:179][0m |           0.0108 |           1.8128 |           7.0702 |
[32m[20221208 14:45:30 @agent_ppo2.py:179][0m |          -0.0140 |           1.5604 |           6.8894 |
[32m[20221208 14:45:30 @agent_ppo2.py:179][0m |          -0.0240 |           1.4050 |           6.8265 |
[32m[20221208 14:45:31 @agent_ppo2.py:179][0m |          -0.0315 |           1.3038 |           6.5944 |
[32m[20221208 14:45:31 @agent_ppo2.py:179][0m |          -0.0393 |           1.2350 |           6.4878 |
[32m[20221208 14:45:31 @agent_ppo2.py:179][0m |          -0.0413 |           1.1907 |           6.2555 |
[32m[20221208 14:45:31 @agent_ppo2.py:179][0m |          -0.0444 |           1.1516 |           5.9021 |
[32m[20221208 14:45:31 @agent_ppo2.py:179][0m |          -0.0479 |           1.1352 |           5.6188 |
[32m[20221208 14:45:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:45:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.11
[32m[20221208 14:45:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.39
[32m[20221208 14:45:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.76
[32m[20221208 14:45:31 @agent_ppo2.py:137][0m Total time:       8.95 min
[32m[20221208 14:45:31 @agent_ppo2.py:139][0m 718848 total steps have happened
[32m[20221208 14:45:31 @agent_ppo2.py:115][0m #------------------------ Iteration 351 --------------------------#
[32m[20221208 14:45:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |           0.0588 |          12.1827 |           3.8716 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |           0.0313 |          11.2087 |           4.5726 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |           0.0053 |          10.7903 |           4.4101 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0254 |          10.5120 |           3.6921 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0383 |          10.3260 |           3.2704 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0483 |          10.1390 |           2.8529 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0515 |          10.0311 |           2.4211 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0558 |           9.8956 |           2.5136 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0605 |           9.8233 |           2.1562 |
[32m[20221208 14:45:32 @agent_ppo2.py:179][0m |          -0.0618 |           9.6881 |           2.2172 |
[32m[20221208 14:45:32 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.98
[32m[20221208 14:45:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.08
[32m[20221208 14:45:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.81
[32m[20221208 14:45:33 @agent_ppo2.py:137][0m Total time:       8.97 min
[32m[20221208 14:45:33 @agent_ppo2.py:139][0m 720896 total steps have happened
[32m[20221208 14:45:33 @agent_ppo2.py:115][0m #------------------------ Iteration 352 --------------------------#
[32m[20221208 14:45:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:33 @agent_ppo2.py:179][0m |           0.0653 |           8.3049 |           5.1270 |
[32m[20221208 14:45:33 @agent_ppo2.py:179][0m |           0.0449 |           7.8274 |           4.9903 |
[32m[20221208 14:45:33 @agent_ppo2.py:179][0m |           0.0022 |           7.6170 |           4.8604 |
[32m[20221208 14:45:33 @agent_ppo2.py:179][0m |          -0.0204 |           7.4202 |           4.7569 |
[32m[20221208 14:45:34 @agent_ppo2.py:179][0m |          -0.0323 |           7.3252 |           4.4590 |
[32m[20221208 14:45:34 @agent_ppo2.py:179][0m |          -0.0399 |           7.2179 |           4.1204 |
[32m[20221208 14:45:34 @agent_ppo2.py:179][0m |          -0.0461 |           7.1380 |           3.9338 |
[32m[20221208 14:45:34 @agent_ppo2.py:179][0m |          -0.0500 |           7.0949 |           4.0141 |
[32m[20221208 14:45:34 @agent_ppo2.py:179][0m |          -0.0505 |           6.9827 |           3.5877 |
[32m[20221208 14:45:34 @agent_ppo2.py:179][0m |          -0.0471 |           6.8811 |           3.5856 |
[32m[20221208 14:45:34 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.00
[32m[20221208 14:45:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.22
[32m[20221208 14:45:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.16
[32m[20221208 14:45:34 @agent_ppo2.py:137][0m Total time:       9.00 min
[32m[20221208 14:45:34 @agent_ppo2.py:139][0m 722944 total steps have happened
[32m[20221208 14:45:34 @agent_ppo2.py:115][0m #------------------------ Iteration 353 --------------------------#
[32m[20221208 14:45:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |           0.0482 |           8.9450 |           5.0791 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |           0.0333 |           7.9218 |           4.5405 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0054 |           7.5145 |           4.8608 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0218 |           7.1913 |           4.6811 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0163 |           7.0011 |           5.0345 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0244 |           6.8705 |           4.4380 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0408 |           6.6881 |           4.1766 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0476 |           6.5813 |           4.2842 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0503 |           6.4436 |           4.0529 |
[32m[20221208 14:45:35 @agent_ppo2.py:179][0m |          -0.0552 |           6.3736 |           3.7305 |
[32m[20221208 14:45:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.98
[32m[20221208 14:45:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 206.14
[32m[20221208 14:45:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.12
[32m[20221208 14:45:36 @agent_ppo2.py:137][0m Total time:       9.02 min
[32m[20221208 14:45:36 @agent_ppo2.py:139][0m 724992 total steps have happened
[32m[20221208 14:45:36 @agent_ppo2.py:115][0m #------------------------ Iteration 354 --------------------------#
[32m[20221208 14:45:36 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:36 @agent_ppo2.py:179][0m |           0.1529 |           7.6997 |           4.1688 |
[32m[20221208 14:45:36 @agent_ppo2.py:179][0m |           0.0586 |           7.0232 |           5.3351 |
[32m[20221208 14:45:36 @agent_ppo2.py:179][0m |           0.0107 |           6.6494 |           4.4677 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0119 |           6.3869 |           4.0311 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0280 |           6.1609 |           3.5880 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0325 |           5.9680 |           3.7516 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0385 |           5.7909 |           3.5024 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0450 |           5.6093 |           3.3880 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0495 |           5.4849 |           3.1051 |
[32m[20221208 14:45:37 @agent_ppo2.py:179][0m |          -0.0542 |           5.3638 |           3.1041 |
[32m[20221208 14:45:37 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:45:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.10
[32m[20221208 14:45:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.18
[32m[20221208 14:45:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 204.95
[32m[20221208 14:45:37 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 204.95
[32m[20221208 14:45:37 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 204.95
[32m[20221208 14:45:37 @agent_ppo2.py:137][0m Total time:       9.05 min
[32m[20221208 14:45:37 @agent_ppo2.py:139][0m 727040 total steps have happened
[32m[20221208 14:45:37 @agent_ppo2.py:115][0m #------------------------ Iteration 355 --------------------------#
[32m[20221208 14:45:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |           0.0661 |          10.9981 |           4.2480 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |           0.0550 |          10.4351 |           5.4413 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |           0.0125 |          10.3238 |           3.9629 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0155 |          10.0259 |           3.3638 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0277 |           9.8873 |           3.2063 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0359 |           9.7910 |           2.8386 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0432 |           9.7108 |           2.4331 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0382 |           9.6474 |           2.4912 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0429 |           9.5630 |           2.2561 |
[32m[20221208 14:45:38 @agent_ppo2.py:179][0m |          -0.0478 |           9.4359 |           1.8617 |
[32m[20221208 14:45:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.87
[32m[20221208 14:45:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 200.76
[32m[20221208 14:45:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.86
[32m[20221208 14:45:39 @agent_ppo2.py:137][0m Total time:       9.08 min
[32m[20221208 14:45:39 @agent_ppo2.py:139][0m 729088 total steps have happened
[32m[20221208 14:45:39 @agent_ppo2.py:115][0m #------------------------ Iteration 356 --------------------------#
[32m[20221208 14:45:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:39 @agent_ppo2.py:179][0m |           0.0445 |          11.4102 |           3.6494 |
[32m[20221208 14:45:39 @agent_ppo2.py:179][0m |           0.0100 |          10.7123 |           4.0696 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0141 |          10.3380 |           3.9434 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0216 |          10.1285 |           3.5499 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0395 |           9.9179 |           3.2395 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0411 |           9.7282 |           2.9253 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0489 |           9.5452 |           2.9946 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0494 |           9.3988 |           2.8664 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0502 |           9.3226 |           2.7006 |
[32m[20221208 14:45:40 @agent_ppo2.py:179][0m |          -0.0537 |           9.2583 |           2.3550 |
[32m[20221208 14:45:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.55
[32m[20221208 14:45:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.57
[32m[20221208 14:45:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 195.53
[32m[20221208 14:45:40 @agent_ppo2.py:137][0m Total time:       9.10 min
[32m[20221208 14:45:40 @agent_ppo2.py:139][0m 731136 total steps have happened
[32m[20221208 14:45:40 @agent_ppo2.py:115][0m #------------------------ Iteration 357 --------------------------#
[32m[20221208 14:45:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |           0.0359 |           7.1795 |           1.9231 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |           0.0510 |           6.5987 |           2.5780 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0079 |           6.4345 |           2.0112 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0265 |           6.2641 |           1.0403 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0361 |           6.1267 |           0.9645 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0445 |           6.0418 |           0.3428 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0490 |           5.9392 |          -0.0737 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0485 |           5.8645 |          -0.0761 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0517 |           5.8073 |          -0.3056 |
[32m[20221208 14:45:41 @agent_ppo2.py:179][0m |          -0.0539 |           5.7356 |          -0.4150 |
[32m[20221208 14:45:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.03
[32m[20221208 14:45:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 200.26
[32m[20221208 14:45:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.11
[32m[20221208 14:45:42 @agent_ppo2.py:137][0m Total time:       9.13 min
[32m[20221208 14:45:42 @agent_ppo2.py:139][0m 733184 total steps have happened
[32m[20221208 14:45:42 @agent_ppo2.py:115][0m #------------------------ Iteration 358 --------------------------#
[32m[20221208 14:45:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:42 @agent_ppo2.py:179][0m |           0.1060 |          10.5939 |           1.5946 |
[32m[20221208 14:45:42 @agent_ppo2.py:179][0m |           0.0962 |          10.1899 |           4.1108 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |           0.0582 |           9.9496 |           3.6127 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |           0.0194 |           9.8770 |           3.0126 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |           0.0020 |           9.7134 |           2.5212 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |          -0.0108 |           9.7083 |           1.9431 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |          -0.0190 |           9.5444 |           1.2933 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |          -0.0284 |           9.4680 |           0.7768 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |          -0.0366 |           9.3769 |           0.4329 |
[32m[20221208 14:45:43 @agent_ppo2.py:179][0m |          -0.0406 |           9.3324 |           0.1352 |
[32m[20221208 14:45:43 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:45:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 208.89
[32m[20221208 14:45:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 211.34
[32m[20221208 14:45:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.14
[32m[20221208 14:45:43 @agent_ppo2.py:137][0m Total time:       9.15 min
[32m[20221208 14:45:43 @agent_ppo2.py:139][0m 735232 total steps have happened
[32m[20221208 14:45:43 @agent_ppo2.py:115][0m #------------------------ Iteration 359 --------------------------#
[32m[20221208 14:45:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |           0.0599 |           5.8379 |           0.7611 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |           0.0289 |           5.2636 |           1.4503 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0055 |           5.0621 |           0.7569 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0267 |           4.9094 |           0.4839 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0355 |           4.8606 |           0.3759 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0431 |           4.7545 |           0.2936 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0488 |           4.7075 |          -0.2197 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0509 |           4.7966 |          -0.3424 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0528 |           4.6137 |          -0.9609 |
[32m[20221208 14:45:44 @agent_ppo2.py:179][0m |          -0.0605 |           4.5314 |          -1.1097 |
[32m[20221208 14:45:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.70
[32m[20221208 14:45:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.97
[32m[20221208 14:45:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 197.18
[32m[20221208 14:45:45 @agent_ppo2.py:137][0m Total time:       9.18 min
[32m[20221208 14:45:45 @agent_ppo2.py:139][0m 737280 total steps have happened
[32m[20221208 14:45:45 @agent_ppo2.py:115][0m #------------------------ Iteration 360 --------------------------#
[32m[20221208 14:45:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:45 @agent_ppo2.py:179][0m |           0.0387 |           7.0511 |           0.9049 |
[32m[20221208 14:45:45 @agent_ppo2.py:179][0m |           0.0081 |           6.0369 |           1.2395 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0119 |           5.5661 |           0.9956 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0299 |           5.2479 |           0.4467 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0383 |           5.0359 |           0.0012 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0437 |           4.8395 |          -0.2938 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0430 |           4.6807 |          -0.4533 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0486 |           4.5799 |          -0.7969 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0494 |           4.5066 |          -1.0681 |
[32m[20221208 14:45:46 @agent_ppo2.py:179][0m |          -0.0546 |           4.4220 |          -1.6132 |
[32m[20221208 14:45:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.14
[32m[20221208 14:45:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 213.74
[32m[20221208 14:45:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.48
[32m[20221208 14:45:46 @agent_ppo2.py:137][0m Total time:       9.20 min
[32m[20221208 14:45:46 @agent_ppo2.py:139][0m 739328 total steps have happened
[32m[20221208 14:45:46 @agent_ppo2.py:115][0m #------------------------ Iteration 361 --------------------------#
[32m[20221208 14:45:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |           0.0540 |           1.3576 |           0.9034 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |           0.0015 |           0.8899 |           1.0139 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0066 |           0.8429 |           1.0471 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0082 |           0.8139 |           1.4409 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0020 |           0.7948 |           1.1569 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0127 |           0.7857 |           1.0724 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |           0.0004 |           0.7771 |           1.0056 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0094 |           0.7697 |           1.0171 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0155 |           0.7696 |           0.8143 |
[32m[20221208 14:45:47 @agent_ppo2.py:179][0m |          -0.0119 |           0.7637 |           1.1048 |
[32m[20221208 14:45:47 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:45:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.42
[32m[20221208 14:45:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.62
[32m[20221208 14:45:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 183.02
[32m[20221208 14:45:48 @agent_ppo2.py:137][0m Total time:       9.23 min
[32m[20221208 14:45:48 @agent_ppo2.py:139][0m 741376 total steps have happened
[32m[20221208 14:45:48 @agent_ppo2.py:115][0m #------------------------ Iteration 362 --------------------------#
[32m[20221208 14:45:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:48 @agent_ppo2.py:179][0m |           0.0604 |           8.1692 |           1.1473 |
[32m[20221208 14:45:48 @agent_ppo2.py:179][0m |           0.0452 |           7.3726 |           3.0719 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |           0.0038 |           7.1098 |           2.0493 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0222 |           6.9640 |           1.7131 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0257 |           6.9761 |           1.2150 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0421 |           6.8137 |           1.0544 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0465 |           6.7409 |           0.6419 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0452 |           6.7153 |           0.5787 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0520 |           6.6529 |           0.4072 |
[32m[20221208 14:45:49 @agent_ppo2.py:179][0m |          -0.0525 |           6.5597 |           0.0951 |
[32m[20221208 14:45:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:45:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.48
[32m[20221208 14:45:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.63
[32m[20221208 14:45:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.88
[32m[20221208 14:45:49 @agent_ppo2.py:137][0m Total time:       9.25 min
[32m[20221208 14:45:49 @agent_ppo2.py:139][0m 743424 total steps have happened
[32m[20221208 14:45:49 @agent_ppo2.py:115][0m #------------------------ Iteration 363 --------------------------#
[32m[20221208 14:45:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |           0.0973 |           8.2310 |           2.6249 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |           0.0438 |           7.6465 |           2.9677 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0013 |           7.3537 |           1.6548 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0158 |           7.1591 |           1.8016 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0284 |           7.0106 |           1.2409 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0370 |           6.8865 |           1.0083 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0427 |           6.7502 |           0.5925 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0410 |           6.6262 |           0.1824 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0427 |           6.5656 |          -0.0088 |
[32m[20221208 14:45:50 @agent_ppo2.py:179][0m |          -0.0482 |           6.5447 |          -0.1803 |
[32m[20221208 14:45:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.15
[32m[20221208 14:45:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.59
[32m[20221208 14:45:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.44
[32m[20221208 14:45:51 @agent_ppo2.py:137][0m Total time:       9.28 min
[32m[20221208 14:45:51 @agent_ppo2.py:139][0m 745472 total steps have happened
[32m[20221208 14:45:51 @agent_ppo2.py:115][0m #------------------------ Iteration 364 --------------------------#
[32m[20221208 14:45:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:51 @agent_ppo2.py:179][0m |           0.0669 |           7.0344 |           2.3325 |
[32m[20221208 14:45:51 @agent_ppo2.py:179][0m |           0.0350 |           6.4521 |           1.6290 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0015 |           6.0979 |           1.6479 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0164 |           5.8298 |           0.9851 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0254 |           5.6256 |           0.5436 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0324 |           5.4921 |           0.6161 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0383 |           5.2287 |           0.1805 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0379 |           5.0347 |           0.0292 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0435 |           4.8120 |          -0.0182 |
[32m[20221208 14:45:52 @agent_ppo2.py:179][0m |          -0.0478 |           4.7195 |          -0.4324 |
[32m[20221208 14:45:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.18
[32m[20221208 14:45:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.69
[32m[20221208 14:45:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 195.44
[32m[20221208 14:45:52 @agent_ppo2.py:137][0m Total time:       9.30 min
[32m[20221208 14:45:52 @agent_ppo2.py:139][0m 747520 total steps have happened
[32m[20221208 14:45:52 @agent_ppo2.py:115][0m #------------------------ Iteration 365 --------------------------#
[32m[20221208 14:45:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |           0.0735 |          11.9026 |           1.7078 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |           0.0338 |          10.7437 |           3.2598 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0069 |          10.2268 |           2.1669 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0309 |           9.8868 |           1.7678 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0382 |           9.5444 |           1.2807 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0462 |           9.3389 |           1.4087 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0502 |           9.1763 |           1.3774 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0544 |           8.9083 |           0.7207 |
[32m[20221208 14:45:53 @agent_ppo2.py:179][0m |          -0.0600 |           8.7229 |           0.5215 |
[32m[20221208 14:45:54 @agent_ppo2.py:179][0m |          -0.0597 |           8.6071 |          -0.0047 |
[32m[20221208 14:45:54 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:45:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 182.24
[32m[20221208 14:45:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.40
[32m[20221208 14:45:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.36
[32m[20221208 14:45:54 @agent_ppo2.py:137][0m Total time:       9.33 min
[32m[20221208 14:45:54 @agent_ppo2.py:139][0m 749568 total steps have happened
[32m[20221208 14:45:54 @agent_ppo2.py:115][0m #------------------------ Iteration 366 --------------------------#
[32m[20221208 14:45:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:54 @agent_ppo2.py:179][0m |           0.0861 |          13.1728 |           0.9289 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |           0.0440 |          12.2489 |           1.8615 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0011 |          11.8855 |           1.3767 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0197 |          11.7077 |           0.7172 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0312 |          11.5349 |          -0.0635 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0317 |          11.4342 |           0.0590 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0306 |          11.2994 |           0.1508 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0466 |          11.2467 |          -0.3446 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0547 |          11.0927 |          -0.3058 |
[32m[20221208 14:45:55 @agent_ppo2.py:179][0m |          -0.0576 |          11.0809 |          -0.9061 |
[32m[20221208 14:45:55 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:45:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.56
[32m[20221208 14:45:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.35
[32m[20221208 14:45:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.65
[32m[20221208 14:45:55 @agent_ppo2.py:137][0m Total time:       9.35 min
[32m[20221208 14:45:55 @agent_ppo2.py:139][0m 751616 total steps have happened
[32m[20221208 14:45:55 @agent_ppo2.py:115][0m #------------------------ Iteration 367 --------------------------#
[32m[20221208 14:45:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:45:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |           0.0365 |           4.5641 |           0.7844 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |           0.0171 |           4.1527 |           1.0758 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0079 |           4.0442 |           1.1720 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0198 |           3.9744 |           0.9531 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0338 |           3.9324 |           0.9685 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0377 |           3.8427 |           0.4498 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0411 |           3.8070 |           0.4732 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0420 |           3.7391 |           0.4680 |
[32m[20221208 14:45:56 @agent_ppo2.py:179][0m |          -0.0468 |           3.6993 |           0.2326 |
[32m[20221208 14:45:57 @agent_ppo2.py:179][0m |          -0.0476 |           3.6561 |          -0.1808 |
[32m[20221208 14:45:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.12
[32m[20221208 14:45:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.16
[32m[20221208 14:45:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.43
[32m[20221208 14:45:57 @agent_ppo2.py:137][0m Total time:       9.38 min
[32m[20221208 14:45:57 @agent_ppo2.py:139][0m 753664 total steps have happened
[32m[20221208 14:45:57 @agent_ppo2.py:115][0m #------------------------ Iteration 368 --------------------------#
[32m[20221208 14:45:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:45:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |           0.0452 |           4.9163 |          -0.1251 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |           0.0124 |           4.4570 |          -0.3056 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0123 |           4.3007 |          -0.5155 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0256 |           4.1545 |          -0.1319 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0291 |           4.0334 |          -0.2052 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0314 |           3.9513 |          -0.2355 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0424 |           3.8960 |          -0.4939 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0474 |           3.8363 |          -0.8161 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0450 |           3.7996 |          -0.5535 |
[32m[20221208 14:45:58 @agent_ppo2.py:179][0m |          -0.0484 |           3.7714 |          -0.7767 |
[32m[20221208 14:45:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:45:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.55
[32m[20221208 14:45:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.80
[32m[20221208 14:45:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.92
[32m[20221208 14:45:58 @agent_ppo2.py:137][0m Total time:       9.40 min
[32m[20221208 14:45:58 @agent_ppo2.py:139][0m 755712 total steps have happened
[32m[20221208 14:45:58 @agent_ppo2.py:115][0m #------------------------ Iteration 369 --------------------------#
[32m[20221208 14:45:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:45:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |           0.0300 |           6.9429 |           0.4997 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |           0.0099 |           5.8229 |           0.8982 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |           0.0013 |           5.4014 |           1.2277 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |          -0.0229 |           5.1036 |           0.5009 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |          -0.0315 |           4.9220 |           0.2259 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |          -0.0394 |           4.7588 |           0.1701 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |          -0.0420 |           4.6141 |          -0.1374 |
[32m[20221208 14:45:59 @agent_ppo2.py:179][0m |          -0.0490 |           4.4616 |          -0.2196 |
[32m[20221208 14:46:00 @agent_ppo2.py:179][0m |          -0.0500 |           4.3648 |          -0.8918 |
[32m[20221208 14:46:00 @agent_ppo2.py:179][0m |          -0.0518 |           4.2988 |          -1.0039 |
[32m[20221208 14:46:00 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.85
[32m[20221208 14:46:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.38
[32m[20221208 14:46:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.72
[32m[20221208 14:46:00 @agent_ppo2.py:137][0m Total time:       9.43 min
[32m[20221208 14:46:00 @agent_ppo2.py:139][0m 757760 total steps have happened
[32m[20221208 14:46:00 @agent_ppo2.py:115][0m #------------------------ Iteration 370 --------------------------#
[32m[20221208 14:46:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |           0.1082 |           8.5221 |           0.4288 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |           0.1112 |           6.9971 |           0.9105 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |           0.0463 |           6.5607 |           1.2698 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |           0.0056 |           6.1661 |           0.5154 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |          -0.0167 |           5.9251 |           0.0919 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |          -0.0312 |           5.7454 |          -0.4059 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |          -0.0346 |           5.5845 |          -0.1059 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |          -0.0462 |           5.4712 |          -0.6250 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |          -0.0539 |           5.3282 |          -0.7730 |
[32m[20221208 14:46:01 @agent_ppo2.py:179][0m |          -0.0529 |           5.2227 |          -1.1987 |
[32m[20221208 14:46:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.51
[32m[20221208 14:46:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.28
[32m[20221208 14:46:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.55
[32m[20221208 14:46:02 @agent_ppo2.py:137][0m Total time:       9.45 min
[32m[20221208 14:46:02 @agent_ppo2.py:139][0m 759808 total steps have happened
[32m[20221208 14:46:02 @agent_ppo2.py:115][0m #------------------------ Iteration 371 --------------------------#
[32m[20221208 14:46:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |           0.0710 |           7.7736 |           1.3257 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |           0.0767 |           7.4682 |           3.2777 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |           0.0277 |           7.3509 |           3.0783 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |           0.0095 |           7.2001 |           1.9666 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |          -0.0156 |           7.0676 |           1.0131 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |          -0.0229 |           7.0037 |           1.2611 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |          -0.0316 |           6.8963 |           1.1188 |
[32m[20221208 14:46:02 @agent_ppo2.py:179][0m |          -0.0386 |           6.8534 |           0.5949 |
[32m[20221208 14:46:03 @agent_ppo2.py:179][0m |          -0.0444 |           6.7658 |           0.3052 |
[32m[20221208 14:46:03 @agent_ppo2.py:179][0m |          -0.0477 |           6.6822 |           0.0341 |
[32m[20221208 14:46:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.09
[32m[20221208 14:46:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.02
[32m[20221208 14:46:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.24
[32m[20221208 14:46:03 @agent_ppo2.py:137][0m Total time:       9.48 min
[32m[20221208 14:46:03 @agent_ppo2.py:139][0m 761856 total steps have happened
[32m[20221208 14:46:03 @agent_ppo2.py:115][0m #------------------------ Iteration 372 --------------------------#
[32m[20221208 14:46:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |           0.0326 |           9.1317 |           2.0305 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0021 |           8.2348 |           0.5312 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0202 |           7.9001 |           0.2737 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0310 |           7.6957 |          -0.1143 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0357 |           7.5158 |          -0.3542 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0461 |           7.4433 |          -0.8813 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0507 |           7.3221 |          -0.7802 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0510 |           7.2712 |          -1.4175 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0531 |           7.1893 |          -1.7879 |
[32m[20221208 14:46:04 @agent_ppo2.py:179][0m |          -0.0569 |           7.1576 |          -2.0170 |
[32m[20221208 14:46:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.50
[32m[20221208 14:46:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.82
[32m[20221208 14:46:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.32
[32m[20221208 14:46:05 @agent_ppo2.py:137][0m Total time:       9.50 min
[32m[20221208 14:46:05 @agent_ppo2.py:139][0m 763904 total steps have happened
[32m[20221208 14:46:05 @agent_ppo2.py:115][0m #------------------------ Iteration 373 --------------------------#
[32m[20221208 14:46:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |           0.0510 |           5.3122 |          -1.1118 |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |           0.0163 |           4.8544 |           0.1744 |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |          -0.0095 |           4.5550 |          -0.4873 |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |          -0.0270 |           4.3325 |          -0.7923 |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |          -0.0359 |           4.1948 |          -1.0179 |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |          -0.0390 |           4.1048 |          -1.5733 |
[32m[20221208 14:46:05 @agent_ppo2.py:179][0m |          -0.0441 |           4.0241 |          -1.5772 |
[32m[20221208 14:46:06 @agent_ppo2.py:179][0m |          -0.0478 |           3.9311 |          -2.0842 |
[32m[20221208 14:46:06 @agent_ppo2.py:179][0m |          -0.0450 |           3.9006 |          -2.5031 |
[32m[20221208 14:46:06 @agent_ppo2.py:179][0m |          -0.0514 |           3.8144 |          -2.7249 |
[32m[20221208 14:46:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.76
[32m[20221208 14:46:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.52
[32m[20221208 14:46:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.41
[32m[20221208 14:46:06 @agent_ppo2.py:137][0m Total time:       9.53 min
[32m[20221208 14:46:06 @agent_ppo2.py:139][0m 765952 total steps have happened
[32m[20221208 14:46:06 @agent_ppo2.py:115][0m #------------------------ Iteration 374 --------------------------#
[32m[20221208 14:46:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |           0.0834 |          10.8588 |          -4.0384 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |           0.0823 |           9.9934 |          -0.4101 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |           0.0163 |           9.5459 |          -1.9476 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0064 |           9.2009 |          -3.2491 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0192 |           9.0279 |          -3.1885 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0327 |           8.8563 |          -3.9225 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0409 |           8.6655 |          -4.3053 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0442 |           8.5509 |          -5.3439 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0470 |           8.4388 |          -5.2612 |
[32m[20221208 14:46:07 @agent_ppo2.py:179][0m |          -0.0540 |           8.3090 |          -5.6142 |
[32m[20221208 14:46:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.47
[32m[20221208 14:46:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.02
[32m[20221208 14:46:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.97
[32m[20221208 14:46:08 @agent_ppo2.py:137][0m Total time:       9.55 min
[32m[20221208 14:46:08 @agent_ppo2.py:139][0m 768000 total steps have happened
[32m[20221208 14:46:08 @agent_ppo2.py:115][0m #------------------------ Iteration 375 --------------------------#
[32m[20221208 14:46:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |           0.0884 |           8.1302 |          -3.5590 |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |           0.0598 |           7.3570 |          -0.4916 |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |           0.0165 |           7.0988 |          -1.3644 |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |          -0.0178 |           6.8758 |          -3.0052 |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |          -0.0324 |           6.6869 |          -3.1974 |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |          -0.0399 |           6.5595 |          -3.5160 |
[32m[20221208 14:46:08 @agent_ppo2.py:179][0m |          -0.0423 |           6.5578 |          -3.5585 |
[32m[20221208 14:46:09 @agent_ppo2.py:179][0m |          -0.0506 |           6.4226 |          -4.0571 |
[32m[20221208 14:46:09 @agent_ppo2.py:179][0m |          -0.0562 |           6.3051 |          -4.7245 |
[32m[20221208 14:46:09 @agent_ppo2.py:179][0m |          -0.0576 |           6.2372 |          -5.4667 |
[32m[20221208 14:46:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.06
[32m[20221208 14:46:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.14
[32m[20221208 14:46:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.02
[32m[20221208 14:46:09 @agent_ppo2.py:137][0m Total time:       9.58 min
[32m[20221208 14:46:09 @agent_ppo2.py:139][0m 770048 total steps have happened
[32m[20221208 14:46:09 @agent_ppo2.py:115][0m #------------------------ Iteration 376 --------------------------#
[32m[20221208 14:46:10 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:46:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |           0.0374 |           4.2152 |          -3.0225 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |           0.0137 |           3.7754 |          -1.9976 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0009 |           3.6279 |          -2.9609 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0059 |           3.5333 |          -2.8965 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0255 |           3.4505 |          -3.4996 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0351 |           3.3992 |          -4.3256 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0399 |           3.3520 |          -4.2050 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0424 |           3.3047 |          -5.0203 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0434 |           3.2699 |          -4.8648 |
[32m[20221208 14:46:10 @agent_ppo2.py:179][0m |          -0.0415 |           3.2229 |          -4.1885 |
[32m[20221208 14:46:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:46:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.52
[32m[20221208 14:46:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.58
[32m[20221208 14:46:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.86
[32m[20221208 14:46:11 @agent_ppo2.py:137][0m Total time:       9.61 min
[32m[20221208 14:46:11 @agent_ppo2.py:139][0m 772096 total steps have happened
[32m[20221208 14:46:11 @agent_ppo2.py:115][0m #------------------------ Iteration 377 --------------------------#
[32m[20221208 14:46:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:11 @agent_ppo2.py:179][0m |           0.0455 |          10.5658 |          -3.3231 |
[32m[20221208 14:46:11 @agent_ppo2.py:179][0m |           0.0798 |           9.4370 |          -1.3875 |
[32m[20221208 14:46:11 @agent_ppo2.py:179][0m |           0.0424 |           9.1004 |           0.0029 |
[32m[20221208 14:46:11 @agent_ppo2.py:179][0m |           0.0010 |           9.0704 |          -1.6063 |
[32m[20221208 14:46:11 @agent_ppo2.py:179][0m |          -0.0233 |           8.9312 |          -2.2595 |
[32m[20221208 14:46:11 @agent_ppo2.py:179][0m |          -0.0360 |           8.8736 |          -2.5052 |
[32m[20221208 14:46:12 @agent_ppo2.py:179][0m |          -0.0438 |           8.7478 |          -3.0863 |
[32m[20221208 14:46:12 @agent_ppo2.py:179][0m |          -0.0440 |           8.7079 |          -3.7602 |
[32m[20221208 14:46:12 @agent_ppo2.py:179][0m |          -0.0526 |           8.6460 |          -3.7047 |
[32m[20221208 14:46:12 @agent_ppo2.py:179][0m |          -0.0544 |           8.5809 |          -4.2420 |
[32m[20221208 14:46:12 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.36
[32m[20221208 14:46:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 199.70
[32m[20221208 14:46:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.51
[32m[20221208 14:46:12 @agent_ppo2.py:137][0m Total time:       9.63 min
[32m[20221208 14:46:12 @agent_ppo2.py:139][0m 774144 total steps have happened
[32m[20221208 14:46:12 @agent_ppo2.py:115][0m #------------------------ Iteration 378 --------------------------#
[32m[20221208 14:46:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |           0.0733 |           8.1175 |          -3.4061 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |           0.0329 |           6.7304 |          -2.9007 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |           0.0009 |           6.3101 |          -3.7176 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0257 |           5.9938 |          -4.3515 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0397 |           5.7992 |          -4.7451 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0455 |           5.6218 |          -5.1132 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0474 |           5.4644 |          -5.3933 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0553 |           5.3174 |          -5.9648 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0584 |           5.2152 |          -6.4790 |
[32m[20221208 14:46:13 @agent_ppo2.py:179][0m |          -0.0607 |           5.1042 |          -6.9754 |
[32m[20221208 14:46:13 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.39
[32m[20221208 14:46:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.42
[32m[20221208 14:46:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 207.46
[32m[20221208 14:46:14 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 207.46
[32m[20221208 14:46:14 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 207.46
[32m[20221208 14:46:14 @agent_ppo2.py:137][0m Total time:       9.66 min
[32m[20221208 14:46:14 @agent_ppo2.py:139][0m 776192 total steps have happened
[32m[20221208 14:46:14 @agent_ppo2.py:115][0m #------------------------ Iteration 379 --------------------------#
[32m[20221208 14:46:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:14 @agent_ppo2.py:179][0m |           0.0491 |           7.7840 |          -4.8968 |
[32m[20221208 14:46:14 @agent_ppo2.py:179][0m |           0.0084 |           6.9999 |          -4.1240 |
[32m[20221208 14:46:14 @agent_ppo2.py:179][0m |          -0.0175 |           6.6731 |          -4.4339 |
[32m[20221208 14:46:14 @agent_ppo2.py:179][0m |          -0.0278 |           6.4769 |          -5.0263 |
[32m[20221208 14:46:14 @agent_ppo2.py:179][0m |          -0.0369 |           6.3356 |          -6.0264 |
[32m[20221208 14:46:15 @agent_ppo2.py:179][0m |          -0.0425 |           6.2584 |          -6.6287 |
[32m[20221208 14:46:15 @agent_ppo2.py:179][0m |          -0.0431 |           6.0708 |          -6.5525 |
[32m[20221208 14:46:15 @agent_ppo2.py:179][0m |          -0.0441 |           5.9607 |          -7.2023 |
[32m[20221208 14:46:15 @agent_ppo2.py:179][0m |          -0.0500 |           5.8760 |          -7.8526 |
[32m[20221208 14:46:15 @agent_ppo2.py:179][0m |          -0.0526 |           5.7868 |          -7.5255 |
[32m[20221208 14:46:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 148.77
[32m[20221208 14:46:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 224.20
[32m[20221208 14:46:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 173.20
[32m[20221208 14:46:15 @agent_ppo2.py:137][0m Total time:       9.68 min
[32m[20221208 14:46:15 @agent_ppo2.py:139][0m 778240 total steps have happened
[32m[20221208 14:46:15 @agent_ppo2.py:115][0m #------------------------ Iteration 380 --------------------------#
[32m[20221208 14:46:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |           0.0666 |           8.9379 |          -8.2831 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |           0.0475 |           8.2762 |          -4.7083 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |           0.0224 |           8.0853 |          -5.1232 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0075 |           7.9025 |          -6.8987 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0238 |           7.8182 |          -8.0603 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0325 |           7.7213 |          -8.9535 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0402 |           7.6397 |          -9.4264 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0429 |           7.5818 |         -10.2723 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0494 |           7.5244 |         -10.6241 |
[32m[20221208 14:46:16 @agent_ppo2.py:179][0m |          -0.0526 |           7.4543 |         -10.5568 |
[32m[20221208 14:46:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.38
[32m[20221208 14:46:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 202.52
[32m[20221208 14:46:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 172.91
[32m[20221208 14:46:17 @agent_ppo2.py:137][0m Total time:       9.71 min
[32m[20221208 14:46:17 @agent_ppo2.py:139][0m 780288 total steps have happened
[32m[20221208 14:46:17 @agent_ppo2.py:115][0m #------------------------ Iteration 381 --------------------------#
[32m[20221208 14:46:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:17 @agent_ppo2.py:179][0m |           0.0352 |           4.9644 |          -8.5999 |
[32m[20221208 14:46:17 @agent_ppo2.py:179][0m |           0.0323 |           4.5939 |          -8.0524 |
[32m[20221208 14:46:17 @agent_ppo2.py:179][0m |           0.0019 |           4.4701 |         -10.0407 |
[32m[20221208 14:46:17 @agent_ppo2.py:179][0m |           0.0006 |           4.3324 |          -8.9114 |
[32m[20221208 14:46:17 @agent_ppo2.py:179][0m |          -0.0183 |           4.2725 |          -9.1303 |
[32m[20221208 14:46:18 @agent_ppo2.py:179][0m |          -0.0297 |           4.2021 |         -10.1620 |
[32m[20221208 14:46:18 @agent_ppo2.py:179][0m |          -0.0400 |           4.1443 |         -10.0318 |
[32m[20221208 14:46:18 @agent_ppo2.py:179][0m |          -0.0393 |           4.0785 |         -10.6744 |
[32m[20221208 14:46:18 @agent_ppo2.py:179][0m |          -0.0271 |           4.0230 |          -9.5016 |
[32m[20221208 14:46:18 @agent_ppo2.py:179][0m |          -0.0366 |           3.9737 |          -8.9054 |
[32m[20221208 14:46:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:46:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.35
[32m[20221208 14:46:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.90
[32m[20221208 14:46:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.23
[32m[20221208 14:46:18 @agent_ppo2.py:137][0m Total time:       9.73 min
[32m[20221208 14:46:18 @agent_ppo2.py:139][0m 782336 total steps have happened
[32m[20221208 14:46:18 @agent_ppo2.py:115][0m #------------------------ Iteration 382 --------------------------#
[32m[20221208 14:46:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |           0.0696 |          10.8897 |          -9.8078 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |           0.0657 |           9.8338 |          -7.7309 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |           0.0313 |           9.3088 |          -7.2080 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |           0.0262 |           9.0350 |          -7.1640 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |          -0.0179 |           8.8316 |          -9.5405 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |          -0.0333 |           8.6338 |         -10.7686 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |          -0.0450 |           8.4354 |         -10.7156 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |          -0.0530 |           8.2828 |         -11.2333 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |          -0.0556 |           8.1322 |         -11.4437 |
[32m[20221208 14:46:19 @agent_ppo2.py:179][0m |          -0.0587 |           8.0463 |         -11.0368 |
[32m[20221208 14:46:19 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 194.83
[32m[20221208 14:46:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.54
[32m[20221208 14:46:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.58
[32m[20221208 14:46:20 @agent_ppo2.py:137][0m Total time:       9.76 min
[32m[20221208 14:46:20 @agent_ppo2.py:139][0m 784384 total steps have happened
[32m[20221208 14:46:20 @agent_ppo2.py:115][0m #------------------------ Iteration 383 --------------------------#
[32m[20221208 14:46:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:20 @agent_ppo2.py:179][0m |           0.0547 |          12.5109 |          -9.7833 |
[32m[20221208 14:46:20 @agent_ppo2.py:179][0m |           0.0390 |          11.7682 |          -8.1382 |
[32m[20221208 14:46:20 @agent_ppo2.py:179][0m |           0.0040 |          11.4905 |          -9.6276 |
[32m[20221208 14:46:20 @agent_ppo2.py:179][0m |          -0.0212 |          11.3284 |          -9.7253 |
[32m[20221208 14:46:21 @agent_ppo2.py:179][0m |          -0.0345 |          11.2017 |         -10.5008 |
[32m[20221208 14:46:21 @agent_ppo2.py:179][0m |          -0.0422 |          11.1059 |         -10.5289 |
[32m[20221208 14:46:21 @agent_ppo2.py:179][0m |          -0.0495 |          11.0033 |         -11.0173 |
[32m[20221208 14:46:21 @agent_ppo2.py:179][0m |          -0.0515 |          10.9066 |         -11.3259 |
[32m[20221208 14:46:21 @agent_ppo2.py:179][0m |          -0.0479 |          10.8874 |         -10.9820 |
[32m[20221208 14:46:21 @agent_ppo2.py:179][0m |          -0.0554 |          10.7411 |         -12.2862 |
[32m[20221208 14:46:21 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.69
[32m[20221208 14:46:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 209.67
[32m[20221208 14:46:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.64
[32m[20221208 14:46:21 @agent_ppo2.py:137][0m Total time:       9.78 min
[32m[20221208 14:46:21 @agent_ppo2.py:139][0m 786432 total steps have happened
[32m[20221208 14:46:21 @agent_ppo2.py:115][0m #------------------------ Iteration 384 --------------------------#
[32m[20221208 14:46:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |           0.0340 |           4.6984 |         -11.3432 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |           0.0302 |           4.3394 |          -9.3844 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |           0.0062 |           4.1897 |         -11.0722 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0136 |           4.0843 |         -13.7460 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0081 |           4.0234 |         -13.0562 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0240 |           3.9286 |         -13.5881 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0328 |           3.8744 |         -14.3718 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0351 |           3.8375 |         -15.3811 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0426 |           3.8004 |         -15.3596 |
[32m[20221208 14:46:22 @agent_ppo2.py:179][0m |          -0.0429 |           3.7347 |         -15.9311 |
[32m[20221208 14:46:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:46:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.09
[32m[20221208 14:46:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.28
[32m[20221208 14:46:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.43
[32m[20221208 14:46:23 @agent_ppo2.py:137][0m Total time:       9.81 min
[32m[20221208 14:46:23 @agent_ppo2.py:139][0m 788480 total steps have happened
[32m[20221208 14:46:23 @agent_ppo2.py:115][0m #------------------------ Iteration 385 --------------------------#
[32m[20221208 14:46:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:23 @agent_ppo2.py:179][0m |           0.0814 |          10.2862 |          -7.4811 |
[32m[20221208 14:46:23 @agent_ppo2.py:179][0m |           0.0481 |           9.0518 |          -3.4404 |
[32m[20221208 14:46:23 @agent_ppo2.py:179][0m |          -0.0074 |           8.7298 |          -6.2325 |
[32m[20221208 14:46:23 @agent_ppo2.py:179][0m |          -0.0274 |           8.5108 |          -7.2342 |
[32m[20221208 14:46:24 @agent_ppo2.py:179][0m |          -0.0486 |           8.3216 |          -8.2281 |
[32m[20221208 14:46:24 @agent_ppo2.py:179][0m |          -0.0566 |           8.1094 |          -8.2936 |
[32m[20221208 14:46:24 @agent_ppo2.py:179][0m |          -0.0612 |           7.9702 |          -9.0687 |
[32m[20221208 14:46:24 @agent_ppo2.py:179][0m |          -0.0650 |           7.8974 |          -9.8615 |
[32m[20221208 14:46:24 @agent_ppo2.py:179][0m |          -0.0743 |           7.7515 |         -10.2721 |
[32m[20221208 14:46:24 @agent_ppo2.py:179][0m |          -0.0739 |           7.6023 |         -10.9741 |
[32m[20221208 14:46:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.25
[32m[20221208 14:46:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.40
[32m[20221208 14:46:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.11
[32m[20221208 14:46:24 @agent_ppo2.py:137][0m Total time:       9.83 min
[32m[20221208 14:46:24 @agent_ppo2.py:139][0m 790528 total steps have happened
[32m[20221208 14:46:24 @agent_ppo2.py:115][0m #------------------------ Iteration 386 --------------------------#
[32m[20221208 14:46:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |           0.0587 |           8.9760 |         -13.3421 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |           0.0354 |           7.5399 |         -10.8223 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |           0.0023 |           7.0063 |         -12.4343 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0214 |           6.5819 |         -13.3043 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0324 |           6.3794 |         -14.2957 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0393 |           6.1029 |         -14.5703 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0442 |           5.9257 |         -15.5566 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0472 |           5.7762 |         -15.4012 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0529 |           5.6549 |         -16.5125 |
[32m[20221208 14:46:25 @agent_ppo2.py:179][0m |          -0.0551 |           5.6159 |         -16.5023 |
[32m[20221208 14:46:25 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.24
[32m[20221208 14:46:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.01
[32m[20221208 14:46:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 187.94
[32m[20221208 14:46:26 @agent_ppo2.py:137][0m Total time:       9.86 min
[32m[20221208 14:46:26 @agent_ppo2.py:139][0m 792576 total steps have happened
[32m[20221208 14:46:26 @agent_ppo2.py:115][0m #------------------------ Iteration 387 --------------------------#
[32m[20221208 14:46:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:46:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:26 @agent_ppo2.py:179][0m |           0.0597 |           9.0278 |         -13.8385 |
[32m[20221208 14:46:26 @agent_ppo2.py:179][0m |           0.0197 |           8.0146 |         -13.1654 |
[32m[20221208 14:46:26 @agent_ppo2.py:179][0m |          -0.0077 |           7.6864 |         -12.9123 |
[32m[20221208 14:46:26 @agent_ppo2.py:179][0m |          -0.0263 |           7.4801 |         -14.0765 |
[32m[20221208 14:46:27 @agent_ppo2.py:179][0m |          -0.0325 |           7.3315 |         -15.4080 |
[32m[20221208 14:46:27 @agent_ppo2.py:179][0m |          -0.0372 |           7.2256 |         -15.6729 |
[32m[20221208 14:46:27 @agent_ppo2.py:179][0m |          -0.0331 |           7.1269 |         -15.9673 |
[32m[20221208 14:46:27 @agent_ppo2.py:179][0m |          -0.0435 |           7.0646 |         -16.1077 |
[32m[20221208 14:46:27 @agent_ppo2.py:179][0m |          -0.0513 |           6.9897 |         -17.4647 |
[32m[20221208 14:46:27 @agent_ppo2.py:179][0m |          -0.0548 |           6.9149 |         -18.3331 |
[32m[20221208 14:46:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.46
[32m[20221208 14:46:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.73
[32m[20221208 14:46:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.70
[32m[20221208 14:46:27 @agent_ppo2.py:137][0m Total time:       9.88 min
[32m[20221208 14:46:27 @agent_ppo2.py:139][0m 794624 total steps have happened
[32m[20221208 14:46:27 @agent_ppo2.py:115][0m #------------------------ Iteration 388 --------------------------#
[32m[20221208 14:46:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |           0.0574 |           9.8760 |         -17.2819 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |           0.0783 |           8.8074 |          -8.0661 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |           0.0215 |           8.3306 |         -11.3575 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0080 |           8.0625 |         -12.8954 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0229 |           7.8965 |         -13.7787 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0362 |           7.6752 |         -14.2726 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0424 |           7.5542 |         -14.7214 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0468 |           7.4593 |         -15.4809 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0509 |           7.3537 |         -16.0853 |
[32m[20221208 14:46:28 @agent_ppo2.py:179][0m |          -0.0563 |           7.3057 |         -16.5733 |
[32m[20221208 14:46:28 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 192.08
[32m[20221208 14:46:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 216.34
[32m[20221208 14:46:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.14
[32m[20221208 14:46:29 @agent_ppo2.py:137][0m Total time:       9.91 min
[32m[20221208 14:46:29 @agent_ppo2.py:139][0m 796672 total steps have happened
[32m[20221208 14:46:29 @agent_ppo2.py:115][0m #------------------------ Iteration 389 --------------------------#
[32m[20221208 14:46:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:29 @agent_ppo2.py:179][0m |           0.0549 |           6.8180 |         -14.0258 |
[32m[20221208 14:46:29 @agent_ppo2.py:179][0m |           0.0346 |           6.0162 |         -11.6192 |
[32m[20221208 14:46:29 @agent_ppo2.py:179][0m |           0.0083 |           5.7533 |         -14.0110 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0185 |           5.5781 |         -14.7104 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0275 |           5.4694 |         -15.5899 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0341 |           5.3263 |         -15.8299 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0400 |           5.2668 |         -17.4938 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0440 |           5.1629 |         -17.8574 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0492 |           5.0800 |         -18.1785 |
[32m[20221208 14:46:30 @agent_ppo2.py:179][0m |          -0.0486 |           5.0419 |         -18.0729 |
[32m[20221208 14:46:30 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.60
[32m[20221208 14:46:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 214.51
[32m[20221208 14:46:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.47
[32m[20221208 14:46:30 @agent_ppo2.py:137][0m Total time:       9.93 min
[32m[20221208 14:46:30 @agent_ppo2.py:139][0m 798720 total steps have happened
[32m[20221208 14:46:30 @agent_ppo2.py:115][0m #------------------------ Iteration 390 --------------------------#
[32m[20221208 14:46:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |           0.0914 |           5.4011 |         -16.8377 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |           0.0617 |           4.5419 |         -12.2510 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |           0.0181 |           4.3965 |         -13.2761 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |           0.0122 |           4.2945 |         -14.6024 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |          -0.0074 |           4.2481 |         -15.8541 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |          -0.0179 |           4.2339 |         -16.2328 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |          -0.0256 |           4.2233 |         -17.4763 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |          -0.0310 |           4.1318 |         -17.8472 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |          -0.0344 |           4.1004 |         -18.5104 |
[32m[20221208 14:46:31 @agent_ppo2.py:179][0m |          -0.0413 |           4.0668 |         -19.1602 |
[32m[20221208 14:46:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:46:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.85
[32m[20221208 14:46:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.48
[32m[20221208 14:46:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 209.75
[32m[20221208 14:46:32 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 209.75
[32m[20221208 14:46:32 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 209.75
[32m[20221208 14:46:32 @agent_ppo2.py:137][0m Total time:       9.96 min
[32m[20221208 14:46:32 @agent_ppo2.py:139][0m 800768 total steps have happened
[32m[20221208 14:46:32 @agent_ppo2.py:115][0m #------------------------ Iteration 391 --------------------------#
[32m[20221208 14:46:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:32 @agent_ppo2.py:179][0m |           0.0886 |          12.5901 |         -12.4200 |
[32m[20221208 14:46:32 @agent_ppo2.py:179][0m |           0.0550 |          11.2370 |          -8.4352 |
[32m[20221208 14:46:32 @agent_ppo2.py:179][0m |          -0.0040 |          10.5381 |         -11.7283 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0238 |          10.1402 |         -12.9996 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0370 |           9.7862 |         -13.8444 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0444 |           9.5680 |         -14.5674 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0479 |           9.3012 |         -15.3478 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0547 |           8.9699 |         -16.3234 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0558 |           8.7594 |         -16.4774 |
[32m[20221208 14:46:33 @agent_ppo2.py:179][0m |          -0.0568 |           8.5245 |         -16.5172 |
[32m[20221208 14:46:33 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:46:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.68
[32m[20221208 14:46:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.71
[32m[20221208 14:46:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.92
[32m[20221208 14:46:33 @agent_ppo2.py:137][0m Total time:       9.98 min
[32m[20221208 14:46:33 @agent_ppo2.py:139][0m 802816 total steps have happened
[32m[20221208 14:46:33 @agent_ppo2.py:115][0m #------------------------ Iteration 392 --------------------------#
[32m[20221208 14:46:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |           0.0523 |           6.7288 |         -17.5650 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |           0.0383 |           5.9479 |         -16.3662 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |           0.0050 |           5.7019 |         -15.3674 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0038 |           5.4979 |         -15.0608 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0190 |           5.3812 |         -16.2547 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0350 |           5.3237 |         -16.5700 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0365 |           5.2182 |         -14.9107 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0429 |           5.1656 |         -16.6921 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0434 |           5.0768 |         -17.2783 |
[32m[20221208 14:46:34 @agent_ppo2.py:179][0m |          -0.0480 |           5.0744 |         -18.4397 |
[32m[20221208 14:46:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:46:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.24
[32m[20221208 14:46:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.98
[32m[20221208 14:46:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.00
[32m[20221208 14:46:35 @agent_ppo2.py:137][0m Total time:      10.01 min
[32m[20221208 14:46:35 @agent_ppo2.py:139][0m 804864 total steps have happened
[32m[20221208 14:46:35 @agent_ppo2.py:115][0m #------------------------ Iteration 393 --------------------------#
[32m[20221208 14:46:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:35 @agent_ppo2.py:179][0m |           0.0628 |          11.8779 |         -10.5922 |
[32m[20221208 14:46:35 @agent_ppo2.py:179][0m |           0.0488 |          11.0503 |         -10.3344 |
[32m[20221208 14:46:35 @agent_ppo2.py:179][0m |           0.0091 |          10.8571 |         -11.0783 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0094 |          10.6235 |         -11.6427 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0338 |          10.4822 |         -11.9318 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0422 |          10.3540 |         -12.5706 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0503 |          10.3094 |         -12.7165 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0561 |          10.2841 |         -13.4330 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0609 |          10.1914 |         -13.8084 |
[32m[20221208 14:46:36 @agent_ppo2.py:179][0m |          -0.0635 |          10.1383 |         -14.3406 |
[32m[20221208 14:46:36 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.31
[32m[20221208 14:46:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.21
[32m[20221208 14:46:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.17
[32m[20221208 14:46:36 @agent_ppo2.py:137][0m Total time:      10.03 min
[32m[20221208 14:46:36 @agent_ppo2.py:139][0m 806912 total steps have happened
[32m[20221208 14:46:36 @agent_ppo2.py:115][0m #------------------------ Iteration 394 --------------------------#
[32m[20221208 14:46:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:46:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |           0.0707 |           5.8458 |          -6.7810 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |           0.0275 |           5.0076 |          -2.5804 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0102 |           4.7498 |          -3.3055 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0330 |           4.5519 |          -4.7933 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0451 |           4.4095 |          -5.7512 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0523 |           4.2347 |          -6.7735 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0602 |           4.1271 |          -7.0989 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0599 |           3.9720 |          -7.9550 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0641 |           3.9048 |          -7.9743 |
[32m[20221208 14:46:37 @agent_ppo2.py:179][0m |          -0.0684 |           3.7985 |          -8.3376 |
[32m[20221208 14:46:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:46:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.49
[32m[20221208 14:46:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.33
[32m[20221208 14:46:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.18
[32m[20221208 14:46:38 @agent_ppo2.py:137][0m Total time:      10.06 min
[32m[20221208 14:46:38 @agent_ppo2.py:139][0m 808960 total steps have happened
[32m[20221208 14:46:38 @agent_ppo2.py:115][0m #------------------------ Iteration 395 --------------------------#
[32m[20221208 14:46:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:38 @agent_ppo2.py:179][0m |           0.0465 |           9.6247 |         -19.5651 |
[32m[20221208 14:46:38 @agent_ppo2.py:179][0m |           0.0216 |           8.9724 |         -18.6704 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |           0.0073 |           8.6481 |         -17.9736 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0244 |           8.4052 |         -20.6246 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0318 |           8.2610 |         -20.6181 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0398 |           8.0748 |         -21.4059 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0431 |           7.9686 |         -21.3904 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0430 |           7.8702 |         -22.6117 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0507 |           7.7416 |         -22.7421 |
[32m[20221208 14:46:39 @agent_ppo2.py:179][0m |          -0.0568 |           7.6646 |         -23.1786 |
[32m[20221208 14:46:39 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.24
[32m[20221208 14:46:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.85
[32m[20221208 14:46:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.78
[32m[20221208 14:46:39 @agent_ppo2.py:137][0m Total time:      10.09 min
[32m[20221208 14:46:39 @agent_ppo2.py:139][0m 811008 total steps have happened
[32m[20221208 14:46:39 @agent_ppo2.py:115][0m #------------------------ Iteration 396 --------------------------#
[32m[20221208 14:46:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |           0.1892 |          12.2792 |         -13.3332 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |           0.0661 |          11.2446 |          -8.5030 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |           0.0053 |          11.0669 |         -11.7168 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0278 |          10.8909 |         -13.8119 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0439 |          10.8015 |         -15.1077 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0525 |          10.6628 |         -15.2124 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0569 |          10.5787 |         -16.2182 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0641 |          10.4338 |         -16.8178 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0682 |          10.3865 |         -17.4022 |
[32m[20221208 14:46:40 @agent_ppo2.py:179][0m |          -0.0655 |          10.2924 |         -17.3559 |
[32m[20221208 14:46:40 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.79
[32m[20221208 14:46:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.24
[32m[20221208 14:46:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.80
[32m[20221208 14:46:41 @agent_ppo2.py:137][0m Total time:      10.11 min
[32m[20221208 14:46:41 @agent_ppo2.py:139][0m 813056 total steps have happened
[32m[20221208 14:46:41 @agent_ppo2.py:115][0m #------------------------ Iteration 397 --------------------------#
[32m[20221208 14:46:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:41 @agent_ppo2.py:179][0m |           0.0411 |           5.7531 |         -13.8164 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |           0.0270 |           4.6265 |          -5.9139 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0063 |           4.3233 |          -8.2876 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0246 |           4.1227 |          -8.4734 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0302 |           3.9752 |          -8.2277 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0403 |           3.8423 |          -8.3879 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0411 |           3.7342 |          -8.9787 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0473 |           3.6543 |          -9.5483 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0499 |           3.5693 |          -9.6157 |
[32m[20221208 14:46:42 @agent_ppo2.py:179][0m |          -0.0536 |           3.4985 |         -10.5933 |
[32m[20221208 14:46:42 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.85
[32m[20221208 14:46:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.88
[32m[20221208 14:46:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.93
[32m[20221208 14:46:42 @agent_ppo2.py:137][0m Total time:      10.14 min
[32m[20221208 14:46:42 @agent_ppo2.py:139][0m 815104 total steps have happened
[32m[20221208 14:46:42 @agent_ppo2.py:115][0m #------------------------ Iteration 398 --------------------------#
[32m[20221208 14:46:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |           0.0497 |           8.6219 |         -21.2169 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |           0.0312 |           8.0001 |         -19.2352 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |           0.0046 |           7.7922 |         -20.7892 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |          -0.0262 |           7.5310 |         -22.6644 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |          -0.0358 |           7.3542 |         -22.9062 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |          -0.0451 |           7.3162 |         -24.3538 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |          -0.0487 |           7.1807 |         -24.0627 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |          -0.0533 |           7.0668 |         -24.5608 |
[32m[20221208 14:46:43 @agent_ppo2.py:179][0m |          -0.0562 |           7.0681 |         -25.4654 |
[32m[20221208 14:46:44 @agent_ppo2.py:179][0m |          -0.0568 |           6.9386 |         -25.4929 |
[32m[20221208 14:46:44 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.59
[32m[20221208 14:46:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.27
[32m[20221208 14:46:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 203.45
[32m[20221208 14:46:44 @agent_ppo2.py:137][0m Total time:      10.16 min
[32m[20221208 14:46:44 @agent_ppo2.py:139][0m 817152 total steps have happened
[32m[20221208 14:46:44 @agent_ppo2.py:115][0m #------------------------ Iteration 399 --------------------------#
[32m[20221208 14:46:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:44 @agent_ppo2.py:179][0m |           0.1020 |           5.8558 |         -15.5938 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |           0.0799 |           5.3508 |          -6.8518 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |           0.0269 |           5.1707 |          -5.4067 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0064 |           5.0130 |          -7.4733 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0220 |           4.8561 |          -8.7892 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0347 |           4.7620 |          -9.6183 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0413 |           4.7191 |         -10.0377 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0472 |           4.6685 |         -10.6237 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0502 |           4.6128 |         -11.5557 |
[32m[20221208 14:46:45 @agent_ppo2.py:179][0m |          -0.0507 |           4.5159 |         -11.9262 |
[32m[20221208 14:46:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:46:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.82
[32m[20221208 14:46:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.92
[32m[20221208 14:46:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.44
[32m[20221208 14:46:45 @agent_ppo2.py:137][0m Total time:      10.19 min
[32m[20221208 14:46:45 @agent_ppo2.py:139][0m 819200 total steps have happened
[32m[20221208 14:46:45 @agent_ppo2.py:115][0m #------------------------ Iteration 400 --------------------------#
[32m[20221208 14:46:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |           0.0550 |           7.9843 |         -21.8890 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |           0.0398 |           7.4523 |         -16.2803 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |           0.0002 |           7.2286 |         -15.1043 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |          -0.0204 |           7.0637 |         -17.0418 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |          -0.0344 |           6.8844 |         -16.5225 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |          -0.0420 |           6.7819 |         -17.6466 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |          -0.0488 |           6.6866 |         -18.2311 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |          -0.0533 |           6.6502 |         -19.2877 |
[32m[20221208 14:46:46 @agent_ppo2.py:179][0m |          -0.0584 |           6.4921 |         -20.1691 |
[32m[20221208 14:46:47 @agent_ppo2.py:179][0m |          -0.0579 |           6.4267 |         -20.9143 |
[32m[20221208 14:46:47 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.49
[32m[20221208 14:46:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.88
[32m[20221208 14:46:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.75
[32m[20221208 14:46:47 @agent_ppo2.py:137][0m Total time:      10.21 min
[32m[20221208 14:46:47 @agent_ppo2.py:139][0m 821248 total steps have happened
[32m[20221208 14:46:47 @agent_ppo2.py:115][0m #------------------------ Iteration 401 --------------------------#
[32m[20221208 14:46:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:47 @agent_ppo2.py:179][0m |           0.0698 |           7.6238 |         -28.3174 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |           0.1175 |           7.0300 |         -19.2139 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |           0.0462 |           6.8031 |         -21.8154 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |           0.0305 |           6.6523 |         -23.3001 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |           0.0118 |           6.5144 |         -24.0537 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |          -0.0159 |           6.3700 |         -26.5188 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |          -0.0266 |           6.2284 |         -28.9804 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |          -0.0346 |           6.0848 |         -28.6889 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |          -0.0370 |           5.9619 |         -29.4205 |
[32m[20221208 14:46:48 @agent_ppo2.py:179][0m |          -0.0439 |           5.8360 |         -30.8269 |
[32m[20221208 14:46:48 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:46:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.00
[32m[20221208 14:46:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.63
[32m[20221208 14:46:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.97
[32m[20221208 14:46:48 @agent_ppo2.py:137][0m Total time:      10.24 min
[32m[20221208 14:46:48 @agent_ppo2.py:139][0m 823296 total steps have happened
[32m[20221208 14:46:48 @agent_ppo2.py:115][0m #------------------------ Iteration 402 --------------------------#
[32m[20221208 14:46:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |           0.0317 |           4.9021 |         -31.0982 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |           0.0231 |           4.4775 |         -29.4103 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0035 |           4.2602 |         -25.9755 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0229 |           4.0851 |         -26.9502 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0337 |           3.9156 |         -31.8829 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0369 |           3.7846 |         -31.1026 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0429 |           3.6770 |         -33.5837 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0450 |           3.6421 |         -34.0821 |
[32m[20221208 14:46:49 @agent_ppo2.py:179][0m |          -0.0450 |           3.4899 |         -34.2572 |
[32m[20221208 14:46:50 @agent_ppo2.py:179][0m |          -0.0518 |           3.4066 |         -32.8458 |
[32m[20221208 14:46:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:46:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.91
[32m[20221208 14:46:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.28
[32m[20221208 14:46:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 199.78
[32m[20221208 14:46:50 @agent_ppo2.py:137][0m Total time:      10.26 min
[32m[20221208 14:46:50 @agent_ppo2.py:139][0m 825344 total steps have happened
[32m[20221208 14:46:50 @agent_ppo2.py:115][0m #------------------------ Iteration 403 --------------------------#
[32m[20221208 14:46:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:50 @agent_ppo2.py:179][0m |           0.0913 |          10.5520 |         -28.2371 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |           0.1381 |           9.7591 |          -8.9077 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |           0.0795 |           9.3716 |         -15.5098 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |           0.0291 |           9.0839 |         -18.5888 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |           0.0017 |           8.9543 |         -24.3696 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |          -0.0155 |           8.7547 |         -25.2231 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |          -0.0278 |           8.6327 |         -28.2118 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |          -0.0358 |           8.5140 |         -28.2335 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |          -0.0445 |           8.4758 |         -29.3236 |
[32m[20221208 14:46:51 @agent_ppo2.py:179][0m |          -0.0466 |           8.3663 |         -31.0371 |
[32m[20221208 14:46:51 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:46:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.98
[32m[20221208 14:46:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.73
[32m[20221208 14:46:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.16
[32m[20221208 14:46:51 @agent_ppo2.py:137][0m Total time:      10.29 min
[32m[20221208 14:46:51 @agent_ppo2.py:139][0m 827392 total steps have happened
[32m[20221208 14:46:51 @agent_ppo2.py:115][0m #------------------------ Iteration 404 --------------------------#
[32m[20221208 14:46:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |           0.0635 |          10.5642 |         -24.8484 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |           0.0503 |           9.6674 |         -22.2820 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |           0.0030 |           9.1406 |         -24.3826 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |          -0.0230 |           8.7106 |         -25.9474 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |          -0.0320 |           8.4616 |         -27.7709 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |          -0.0413 |           8.1699 |         -28.3623 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |          -0.0505 |           7.8925 |         -29.2934 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |          -0.0550 |           7.7085 |         -30.3158 |
[32m[20221208 14:46:52 @agent_ppo2.py:179][0m |          -0.0580 |           7.5345 |         -31.2152 |
[32m[20221208 14:46:53 @agent_ppo2.py:179][0m |          -0.0551 |           7.3938 |         -31.2072 |
[32m[20221208 14:46:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.55
[32m[20221208 14:46:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.28
[32m[20221208 14:46:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.30
[32m[20221208 14:46:53 @agent_ppo2.py:137][0m Total time:      10.31 min
[32m[20221208 14:46:53 @agent_ppo2.py:139][0m 829440 total steps have happened
[32m[20221208 14:46:53 @agent_ppo2.py:115][0m #------------------------ Iteration 405 --------------------------#
[32m[20221208 14:46:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |           0.0727 |           8.2493 |         -24.1818 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |           0.0683 |           7.5097 |         -17.5285 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |           0.0305 |           7.2433 |         -22.9591 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0061 |           6.9953 |         -26.2964 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0230 |           6.8228 |         -27.7241 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0315 |           6.7148 |         -28.5273 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0344 |           6.5971 |         -28.7390 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0401 |           6.5244 |         -30.0943 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0417 |           6.5389 |         -30.4026 |
[32m[20221208 14:46:54 @agent_ppo2.py:179][0m |          -0.0469 |           6.4155 |         -32.2784 |
[32m[20221208 14:46:54 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:46:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.30
[32m[20221208 14:46:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 211.25
[32m[20221208 14:46:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.66
[32m[20221208 14:46:54 @agent_ppo2.py:137][0m Total time:      10.34 min
[32m[20221208 14:46:54 @agent_ppo2.py:139][0m 831488 total steps have happened
[32m[20221208 14:46:54 @agent_ppo2.py:115][0m #------------------------ Iteration 406 --------------------------#
[32m[20221208 14:46:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |           0.0479 |          11.5751 |         -27.7369 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |           0.0303 |          11.0986 |         -22.6419 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |           0.0070 |          10.9910 |         -26.1153 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |          -0.0124 |          10.7619 |         -26.4973 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |          -0.0315 |          10.6278 |         -27.7297 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |          -0.0312 |          10.5608 |         -28.5948 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |          -0.0442 |          10.4399 |         -29.0558 |
[32m[20221208 14:46:55 @agent_ppo2.py:179][0m |          -0.0486 |          10.3039 |         -29.3821 |
[32m[20221208 14:46:56 @agent_ppo2.py:179][0m |          -0.0525 |          10.2613 |         -30.4834 |
[32m[20221208 14:46:56 @agent_ppo2.py:179][0m |          -0.0576 |          10.1735 |         -30.4210 |
[32m[20221208 14:46:56 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 195.35
[32m[20221208 14:46:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.38
[32m[20221208 14:46:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.55
[32m[20221208 14:46:56 @agent_ppo2.py:137][0m Total time:      10.36 min
[32m[20221208 14:46:56 @agent_ppo2.py:139][0m 833536 total steps have happened
[32m[20221208 14:46:56 @agent_ppo2.py:115][0m #------------------------ Iteration 407 --------------------------#
[32m[20221208 14:46:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |           0.0787 |          11.6503 |         -24.8505 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |           0.0688 |          11.2549 |         -16.5504 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |           0.0202 |          11.0501 |         -21.5206 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0057 |          10.9424 |         -23.4253 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0222 |          10.8678 |         -24.2196 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0327 |          10.7731 |         -25.3930 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0384 |          10.7064 |         -26.9082 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0414 |          10.6978 |         -27.1904 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0445 |          10.6137 |         -28.4142 |
[32m[20221208 14:46:57 @agent_ppo2.py:179][0m |          -0.0448 |          10.5977 |         -28.3416 |
[32m[20221208 14:46:57 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 187.69
[32m[20221208 14:46:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.52
[32m[20221208 14:46:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.01
[32m[20221208 14:46:58 @agent_ppo2.py:137][0m Total time:      10.39 min
[32m[20221208 14:46:58 @agent_ppo2.py:139][0m 835584 total steps have happened
[32m[20221208 14:46:58 @agent_ppo2.py:115][0m #------------------------ Iteration 408 --------------------------#
[32m[20221208 14:46:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:46:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |           0.0491 |           5.4095 |         -22.2455 |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |           0.0310 |           4.5402 |         -18.8834 |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |           0.0314 |           4.3305 |         -10.2692 |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |           0.0245 |           4.2209 |          -6.4736 |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |          -0.0048 |           4.1210 |          -7.3787 |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |          -0.0203 |           4.0611 |          -8.3345 |
[32m[20221208 14:46:58 @agent_ppo2.py:179][0m |          -0.0280 |           4.0028 |          -9.2659 |
[32m[20221208 14:46:59 @agent_ppo2.py:179][0m |          -0.0317 |           3.9541 |          -9.8708 |
[32m[20221208 14:46:59 @agent_ppo2.py:179][0m |          -0.0332 |           3.9212 |         -10.9879 |
[32m[20221208 14:46:59 @agent_ppo2.py:179][0m |          -0.0392 |           3.8305 |         -11.7764 |
[32m[20221208 14:46:59 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:46:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.20
[32m[20221208 14:46:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.18
[32m[20221208 14:46:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 196.50
[32m[20221208 14:46:59 @agent_ppo2.py:137][0m Total time:      10.41 min
[32m[20221208 14:46:59 @agent_ppo2.py:139][0m 837632 total steps have happened
[32m[20221208 14:46:59 @agent_ppo2.py:115][0m #------------------------ Iteration 409 --------------------------#
[32m[20221208 14:47:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |           0.0801 |          11.1608 |         -25.8315 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |           0.0805 |          10.3601 |         -16.8565 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |           0.0111 |           9.8704 |         -20.6203 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0222 |           9.6230 |         -23.4320 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0318 |           9.4440 |         -24.4667 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0436 |           9.3045 |         -25.4294 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0504 |           9.1538 |         -26.4727 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0532 |           8.9909 |         -27.3767 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0548 |           8.9140 |         -28.2721 |
[32m[20221208 14:47:00 @agent_ppo2.py:179][0m |          -0.0563 |           8.7629 |         -26.9966 |
[32m[20221208 14:47:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:47:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.10
[32m[20221208 14:47:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.49
[32m[20221208 14:47:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.91
[32m[20221208 14:47:01 @agent_ppo2.py:137][0m Total time:      10.44 min
[32m[20221208 14:47:01 @agent_ppo2.py:139][0m 839680 total steps have happened
[32m[20221208 14:47:01 @agent_ppo2.py:115][0m #------------------------ Iteration 410 --------------------------#
[32m[20221208 14:47:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:01 @agent_ppo2.py:179][0m |           0.1464 |           8.4022 |         -26.1426 |
[32m[20221208 14:47:01 @agent_ppo2.py:179][0m |           0.0889 |           7.7203 |         -22.2490 |
[32m[20221208 14:47:01 @agent_ppo2.py:179][0m |           0.0478 |           7.3874 |         -22.7386 |
[32m[20221208 14:47:01 @agent_ppo2.py:179][0m |           0.0142 |           7.1489 |         -24.4752 |
[32m[20221208 14:47:01 @agent_ppo2.py:179][0m |          -0.0031 |           7.0646 |         -26.4204 |
[32m[20221208 14:47:01 @agent_ppo2.py:179][0m |          -0.0158 |           6.9013 |         -28.0022 |
[32m[20221208 14:47:02 @agent_ppo2.py:179][0m |          -0.0231 |           6.6911 |         -28.6549 |
[32m[20221208 14:47:02 @agent_ppo2.py:179][0m |          -0.0294 |           6.6732 |         -30.3456 |
[32m[20221208 14:47:02 @agent_ppo2.py:179][0m |          -0.0386 |           6.5160 |         -31.4291 |
[32m[20221208 14:47:02 @agent_ppo2.py:179][0m |          -0.0447 |           6.4673 |         -32.4792 |
[32m[20221208 14:47:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.60
[32m[20221208 14:47:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.26
[32m[20221208 14:47:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.05
[32m[20221208 14:47:02 @agent_ppo2.py:137][0m Total time:      10.46 min
[32m[20221208 14:47:02 @agent_ppo2.py:139][0m 841728 total steps have happened
[32m[20221208 14:47:02 @agent_ppo2.py:115][0m #------------------------ Iteration 411 --------------------------#
[32m[20221208 14:47:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |           0.0788 |          12.5813 |         -26.2373 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |           0.0537 |          11.5278 |         -20.9293 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0061 |          11.1927 |         -25.6648 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0231 |          10.9341 |         -28.2043 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0366 |          10.7490 |         -29.0775 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0441 |          10.5500 |         -29.8077 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0509 |          10.3952 |         -31.1808 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0540 |          10.2794 |         -32.7363 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0585 |          10.1005 |         -32.5501 |
[32m[20221208 14:47:03 @agent_ppo2.py:179][0m |          -0.0628 |          10.0797 |         -33.4225 |
[32m[20221208 14:47:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.51
[32m[20221208 14:47:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 203.54
[32m[20221208 14:47:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.90
[32m[20221208 14:47:04 @agent_ppo2.py:137][0m Total time:      10.49 min
[32m[20221208 14:47:04 @agent_ppo2.py:139][0m 843776 total steps have happened
[32m[20221208 14:47:04 @agent_ppo2.py:115][0m #------------------------ Iteration 412 --------------------------#
[32m[20221208 14:47:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:47:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:04 @agent_ppo2.py:179][0m |           0.0444 |           5.4268 |         -32.6318 |
[32m[20221208 14:47:04 @agent_ppo2.py:179][0m |           0.0687 |           4.9294 |         -21.1566 |
[32m[20221208 14:47:04 @agent_ppo2.py:179][0m |           0.1268 |           4.8190 |         -18.5680 |
[32m[20221208 14:47:04 @agent_ppo2.py:179][0m |           0.0136 |           4.7456 |         -15.5490 |
[32m[20221208 14:47:04 @agent_ppo2.py:179][0m |          -0.0144 |           4.6431 |         -19.3930 |
[32m[20221208 14:47:04 @agent_ppo2.py:179][0m |          -0.0319 |           4.5716 |         -20.8667 |
[32m[20221208 14:47:05 @agent_ppo2.py:179][0m |          -0.0388 |           4.5133 |         -21.3803 |
[32m[20221208 14:47:05 @agent_ppo2.py:179][0m |          -0.0470 |           4.4853 |         -22.3915 |
[32m[20221208 14:47:05 @agent_ppo2.py:179][0m |          -0.0516 |           4.4302 |         -24.1637 |
[32m[20221208 14:47:05 @agent_ppo2.py:179][0m |          -0.0591 |           4.4058 |         -24.6045 |
[32m[20221208 14:47:05 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:47:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.90
[32m[20221208 14:47:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.03
[32m[20221208 14:47:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.23
[32m[20221208 14:47:05 @agent_ppo2.py:137][0m Total time:      10.51 min
[32m[20221208 14:47:05 @agent_ppo2.py:139][0m 845824 total steps have happened
[32m[20221208 14:47:05 @agent_ppo2.py:115][0m #------------------------ Iteration 413 --------------------------#
[32m[20221208 14:47:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |           0.0902 |           7.1281 |         -30.2183 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |           0.0371 |           6.6743 |         -19.7563 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0075 |           6.3327 |         -21.5973 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0086 |           6.1901 |         -23.1354 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0320 |           6.0033 |         -24.2423 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0476 |           5.8281 |         -26.1479 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0518 |           5.7887 |         -26.4156 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0597 |           5.6386 |         -28.2026 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0611 |           5.6051 |         -28.5059 |
[32m[20221208 14:47:06 @agent_ppo2.py:179][0m |          -0.0582 |           5.6050 |         -28.5705 |
[32m[20221208 14:47:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.41
[32m[20221208 14:47:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.13
[32m[20221208 14:47:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.98
[32m[20221208 14:47:07 @agent_ppo2.py:137][0m Total time:      10.54 min
[32m[20221208 14:47:07 @agent_ppo2.py:139][0m 847872 total steps have happened
[32m[20221208 14:47:07 @agent_ppo2.py:115][0m #------------------------ Iteration 414 --------------------------#
[32m[20221208 14:47:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:07 @agent_ppo2.py:179][0m |           0.0669 |           5.0593 |         -37.3671 |
[32m[20221208 14:47:07 @agent_ppo2.py:179][0m |           0.0980 |           4.5525 |         -24.0672 |
[32m[20221208 14:47:07 @agent_ppo2.py:179][0m |           0.0446 |           4.4241 |         -14.7785 |
[32m[20221208 14:47:07 @agent_ppo2.py:179][0m |           0.0060 |           4.2813 |         -18.5818 |
[32m[20221208 14:47:07 @agent_ppo2.py:179][0m |          -0.0092 |           4.2521 |         -21.2779 |
[32m[20221208 14:47:07 @agent_ppo2.py:179][0m |          -0.0237 |           4.1469 |         -23.4779 |
[32m[20221208 14:47:08 @agent_ppo2.py:179][0m |          -0.0281 |           4.1093 |         -25.1586 |
[32m[20221208 14:47:08 @agent_ppo2.py:179][0m |          -0.0355 |           4.0561 |         -25.1613 |
[32m[20221208 14:47:08 @agent_ppo2.py:179][0m |          -0.0370 |           4.0079 |         -25.1103 |
[32m[20221208 14:47:08 @agent_ppo2.py:179][0m |          -0.0437 |           3.9930 |         -25.3638 |
[32m[20221208 14:47:08 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:47:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.26
[32m[20221208 14:47:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.20
[32m[20221208 14:47:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 201.05
[32m[20221208 14:47:08 @agent_ppo2.py:137][0m Total time:      10.56 min
[32m[20221208 14:47:08 @agent_ppo2.py:139][0m 849920 total steps have happened
[32m[20221208 14:47:08 @agent_ppo2.py:115][0m #------------------------ Iteration 415 --------------------------#
[32m[20221208 14:47:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |           0.0474 |          11.3704 |         -39.6978 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |           0.0178 |          10.2983 |         -36.1791 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0207 |           9.8102 |         -39.0802 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0306 |           9.4502 |         -40.5279 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0447 |           9.1065 |         -41.8414 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0514 |           8.9419 |         -42.7012 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0555 |           8.7316 |         -43.6104 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0613 |           8.5506 |         -44.2073 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0633 |           8.4766 |         -43.2971 |
[32m[20221208 14:47:09 @agent_ppo2.py:179][0m |          -0.0642 |           8.3356 |         -45.0946 |
[32m[20221208 14:47:09 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:47:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.48
[32m[20221208 14:47:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.56
[32m[20221208 14:47:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.54
[32m[20221208 14:47:10 @agent_ppo2.py:137][0m Total time:      10.59 min
[32m[20221208 14:47:10 @agent_ppo2.py:139][0m 851968 total steps have happened
[32m[20221208 14:47:10 @agent_ppo2.py:115][0m #------------------------ Iteration 416 --------------------------#
[32m[20221208 14:47:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:10 @agent_ppo2.py:179][0m |           0.0869 |           6.7104 |         -30.7365 |
[32m[20221208 14:47:10 @agent_ppo2.py:179][0m |           0.0687 |           5.8020 |         -26.6842 |
[32m[20221208 14:47:10 @agent_ppo2.py:179][0m |           0.0473 |           5.5379 |         -24.8314 |
[32m[20221208 14:47:10 @agent_ppo2.py:179][0m |           0.0101 |           5.3966 |         -29.7856 |
[32m[20221208 14:47:10 @agent_ppo2.py:179][0m |          -0.0060 |           5.2775 |         -32.3793 |
[32m[20221208 14:47:10 @agent_ppo2.py:179][0m |          -0.0109 |           5.1708 |         -34.7494 |
[32m[20221208 14:47:11 @agent_ppo2.py:179][0m |          -0.0184 |           5.1301 |         -34.3361 |
[32m[20221208 14:47:11 @agent_ppo2.py:179][0m |          -0.0305 |           5.1060 |         -35.4359 |
[32m[20221208 14:47:11 @agent_ppo2.py:179][0m |          -0.0353 |           5.0066 |         -37.9190 |
[32m[20221208 14:47:11 @agent_ppo2.py:179][0m |          -0.0356 |           4.9576 |         -38.4383 |
[32m[20221208 14:47:11 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:47:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.27
[32m[20221208 14:47:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.45
[32m[20221208 14:47:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.92
[32m[20221208 14:47:11 @agent_ppo2.py:137][0m Total time:      10.61 min
[32m[20221208 14:47:11 @agent_ppo2.py:139][0m 854016 total steps have happened
[32m[20221208 14:47:11 @agent_ppo2.py:115][0m #------------------------ Iteration 417 --------------------------#
[32m[20221208 14:47:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |           0.0805 |          12.2071 |         -34.7017 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |           0.0437 |          11.0355 |         -29.7076 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0023 |          10.6212 |         -37.0695 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0262 |          10.2828 |         -40.1850 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0330 |          10.0207 |         -41.1417 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0425 |           9.8802 |         -41.5219 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0478 |           9.7136 |         -43.8389 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0521 |           9.6303 |         -44.4105 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0560 |           9.4694 |         -45.5416 |
[32m[20221208 14:47:12 @agent_ppo2.py:179][0m |          -0.0608 |           9.3951 |         -47.2672 |
[32m[20221208 14:47:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:47:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 188.74
[32m[20221208 14:47:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 206.95
[32m[20221208 14:47:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.46
[32m[20221208 14:47:13 @agent_ppo2.py:137][0m Total time:      10.64 min
[32m[20221208 14:47:13 @agent_ppo2.py:139][0m 856064 total steps have happened
[32m[20221208 14:47:13 @agent_ppo2.py:115][0m #------------------------ Iteration 418 --------------------------#
[32m[20221208 14:47:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:13 @agent_ppo2.py:179][0m |           0.0658 |          10.0805 |         -38.2616 |
[32m[20221208 14:47:13 @agent_ppo2.py:179][0m |           0.0486 |           9.0209 |         -31.2926 |
[32m[20221208 14:47:13 @agent_ppo2.py:179][0m |           0.0123 |           8.5230 |         -33.9941 |
[32m[20221208 14:47:13 @agent_ppo2.py:179][0m |          -0.0246 |           8.1482 |         -38.3525 |
[32m[20221208 14:47:13 @agent_ppo2.py:179][0m |          -0.0365 |           7.8647 |         -41.3839 |
[32m[20221208 14:47:14 @agent_ppo2.py:179][0m |          -0.0441 |           7.6098 |         -42.5071 |
[32m[20221208 14:47:14 @agent_ppo2.py:179][0m |          -0.0490 |           7.4273 |         -43.9613 |
[32m[20221208 14:47:14 @agent_ppo2.py:179][0m |          -0.0559 |           7.2177 |         -44.7326 |
[32m[20221208 14:47:14 @agent_ppo2.py:179][0m |          -0.0529 |           7.0795 |         -44.7406 |
[32m[20221208 14:47:14 @agent_ppo2.py:179][0m |          -0.0565 |           6.8809 |         -45.8857 |
[32m[20221208 14:47:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.72
[32m[20221208 14:47:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.03
[32m[20221208 14:47:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.87
[32m[20221208 14:47:14 @agent_ppo2.py:137][0m Total time:      10.67 min
[32m[20221208 14:47:14 @agent_ppo2.py:139][0m 858112 total steps have happened
[32m[20221208 14:47:14 @agent_ppo2.py:115][0m #------------------------ Iteration 419 --------------------------#
[32m[20221208 14:47:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |           0.0281 |           3.9676 |         -25.4520 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0148 |           3.5643 |         -14.9237 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0186 |           3.5326 |         -14.9079 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0161 |           3.5281 |         -14.2629 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0235 |           3.4968 |         -14.8172 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0263 |           3.4830 |         -15.3738 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0246 |           3.4770 |         -15.6119 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0209 |           3.4837 |         -15.2691 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0235 |           3.5003 |         -15.5765 |
[32m[20221208 14:47:15 @agent_ppo2.py:179][0m |          -0.0238 |           3.4615 |         -15.9555 |
[32m[20221208 14:47:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:47:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.87
[32m[20221208 14:47:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.19
[32m[20221208 14:47:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.85
[32m[20221208 14:47:16 @agent_ppo2.py:137][0m Total time:      10.69 min
[32m[20221208 14:47:16 @agent_ppo2.py:139][0m 860160 total steps have happened
[32m[20221208 14:47:16 @agent_ppo2.py:115][0m #------------------------ Iteration 420 --------------------------#
[32m[20221208 14:47:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:16 @agent_ppo2.py:179][0m |           0.1094 |           5.7686 |         -46.9267 |
[32m[20221208 14:47:16 @agent_ppo2.py:179][0m |           0.0825 |           4.7671 |         -34.0502 |
[32m[20221208 14:47:16 @agent_ppo2.py:179][0m |           0.0204 |           4.1840 |         -39.3685 |
[32m[20221208 14:47:16 @agent_ppo2.py:179][0m |          -0.0194 |           3.8469 |         -42.0951 |
[32m[20221208 14:47:16 @agent_ppo2.py:179][0m |          -0.0357 |           3.5759 |         -44.4193 |
[32m[20221208 14:47:16 @agent_ppo2.py:179][0m |          -0.0453 |           3.4143 |         -46.2795 |
[32m[20221208 14:47:17 @agent_ppo2.py:179][0m |          -0.0490 |           3.2718 |         -46.4279 |
[32m[20221208 14:47:17 @agent_ppo2.py:179][0m |          -0.0550 |           3.1671 |         -49.1907 |
[32m[20221208 14:47:17 @agent_ppo2.py:179][0m |          -0.0618 |           3.0483 |         -52.2937 |
[32m[20221208 14:47:17 @agent_ppo2.py:179][0m |          -0.0671 |           2.9705 |         -55.9071 |
[32m[20221208 14:47:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.64
[32m[20221208 14:47:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.16
[32m[20221208 14:47:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.12
[32m[20221208 14:47:17 @agent_ppo2.py:137][0m Total time:      10.71 min
[32m[20221208 14:47:17 @agent_ppo2.py:139][0m 862208 total steps have happened
[32m[20221208 14:47:17 @agent_ppo2.py:115][0m #------------------------ Iteration 421 --------------------------#
[32m[20221208 14:47:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |           0.0385 |           5.7689 |         -50.2970 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |           0.0058 |           3.8221 |         -50.4356 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0142 |           3.2405 |         -52.8841 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0285 |           2.9250 |         -52.7117 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0389 |           2.7111 |         -52.4469 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0483 |           2.5550 |         -53.9392 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0516 |           2.4198 |         -53.1382 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0561 |           2.3123 |         -54.7591 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0576 |           2.2408 |         -55.2488 |
[32m[20221208 14:47:18 @agent_ppo2.py:179][0m |          -0.0569 |           2.1481 |         -55.2693 |
[32m[20221208 14:47:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.77
[32m[20221208 14:47:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.62
[32m[20221208 14:47:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.07
[32m[20221208 14:47:19 @agent_ppo2.py:137][0m Total time:      10.74 min
[32m[20221208 14:47:19 @agent_ppo2.py:139][0m 864256 total steps have happened
[32m[20221208 14:47:19 @agent_ppo2.py:115][0m #------------------------ Iteration 422 --------------------------#
[32m[20221208 14:47:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:19 @agent_ppo2.py:179][0m |           0.0861 |           5.9129 |         -48.5173 |
[32m[20221208 14:47:19 @agent_ppo2.py:179][0m |           0.0650 |           5.0235 |         -44.5316 |
[32m[20221208 14:47:19 @agent_ppo2.py:179][0m |           0.0199 |           4.7466 |         -48.2623 |
[32m[20221208 14:47:19 @agent_ppo2.py:179][0m |           0.0201 |           4.5145 |         -44.9281 |
[32m[20221208 14:47:19 @agent_ppo2.py:179][0m |          -0.0040 |           4.3262 |         -46.2047 |
[32m[20221208 14:47:19 @agent_ppo2.py:179][0m |          -0.0205 |           4.1892 |         -50.3176 |
[32m[20221208 14:47:20 @agent_ppo2.py:179][0m |          -0.0358 |           4.0899 |         -52.3032 |
[32m[20221208 14:47:20 @agent_ppo2.py:179][0m |          -0.0355 |           3.9982 |         -54.7552 |
[32m[20221208 14:47:20 @agent_ppo2.py:179][0m |          -0.0427 |           3.9100 |         -55.1132 |
[32m[20221208 14:47:20 @agent_ppo2.py:179][0m |          -0.0512 |           3.8204 |         -58.1343 |
[32m[20221208 14:47:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.84
[32m[20221208 14:47:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 194.26
[32m[20221208 14:47:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.24
[32m[20221208 14:47:20 @agent_ppo2.py:137][0m Total time:      10.76 min
[32m[20221208 14:47:20 @agent_ppo2.py:139][0m 866304 total steps have happened
[32m[20221208 14:47:20 @agent_ppo2.py:115][0m #------------------------ Iteration 423 --------------------------#
[32m[20221208 14:47:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |           0.0794 |           5.5001 |         -48.0781 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |           0.0824 |           4.7929 |         -26.7325 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |           0.0025 |           4.4268 |         -31.1038 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0299 |           4.2177 |         -35.9057 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0437 |           4.0633 |         -37.1745 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0505 |           3.9568 |         -37.7846 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0543 |           3.8401 |         -39.5625 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0601 |           3.7818 |         -39.3253 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0649 |           3.6884 |         -40.1160 |
[32m[20221208 14:47:21 @agent_ppo2.py:179][0m |          -0.0684 |           3.6665 |         -40.8641 |
[32m[20221208 14:47:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.37
[32m[20221208 14:47:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.29
[32m[20221208 14:47:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.23
[32m[20221208 14:47:22 @agent_ppo2.py:137][0m Total time:      10.79 min
[32m[20221208 14:47:22 @agent_ppo2.py:139][0m 868352 total steps have happened
[32m[20221208 14:47:22 @agent_ppo2.py:115][0m #------------------------ Iteration 424 --------------------------#
[32m[20221208 14:47:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:22 @agent_ppo2.py:179][0m |           0.0696 |           4.1931 |         -39.0669 |
[32m[20221208 14:47:22 @agent_ppo2.py:179][0m |           0.0037 |           3.5809 |         -18.6361 |
[32m[20221208 14:47:22 @agent_ppo2.py:179][0m |          -0.0216 |           3.2458 |         -19.3409 |
[32m[20221208 14:47:22 @agent_ppo2.py:179][0m |          -0.0380 |           3.0185 |         -21.6115 |
[32m[20221208 14:47:22 @agent_ppo2.py:179][0m |          -0.0495 |           2.8305 |         -22.7732 |
[32m[20221208 14:47:22 @agent_ppo2.py:179][0m |          -0.0536 |           2.7368 |         -23.9328 |
[32m[20221208 14:47:23 @agent_ppo2.py:179][0m |          -0.0605 |           2.6394 |         -24.4294 |
[32m[20221208 14:47:23 @agent_ppo2.py:179][0m |          -0.0607 |           2.5468 |         -25.8951 |
[32m[20221208 14:47:23 @agent_ppo2.py:179][0m |          -0.0634 |           2.4698 |         -26.7136 |
[32m[20221208 14:47:23 @agent_ppo2.py:179][0m |          -0.0696 |           2.4200 |         -27.4549 |
[32m[20221208 14:47:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:47:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.97
[32m[20221208 14:47:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.43
[32m[20221208 14:47:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.28
[32m[20221208 14:47:23 @agent_ppo2.py:137][0m Total time:      10.81 min
[32m[20221208 14:47:23 @agent_ppo2.py:139][0m 870400 total steps have happened
[32m[20221208 14:47:23 @agent_ppo2.py:115][0m #------------------------ Iteration 425 --------------------------#
[32m[20221208 14:47:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |           0.0273 |           1.0272 |         -61.1275 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |           0.0626 |           0.8885 |         -48.8934 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0124 |           0.8508 |         -43.7421 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0062 |           0.8371 |         -44.1846 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0160 |           0.8162 |         -45.2766 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0181 |           0.8149 |         -46.0436 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0236 |           0.8062 |         -46.2092 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0329 |           0.8023 |         -47.2892 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0080 |           0.8062 |         -45.5907 |
[32m[20221208 14:47:24 @agent_ppo2.py:179][0m |          -0.0246 |           0.7937 |         -47.4998 |
[32m[20221208 14:47:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.54
[32m[20221208 14:47:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.88
[32m[20221208 14:47:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.10
[32m[20221208 14:47:25 @agent_ppo2.py:137][0m Total time:      10.84 min
[32m[20221208 14:47:25 @agent_ppo2.py:139][0m 872448 total steps have happened
[32m[20221208 14:47:25 @agent_ppo2.py:115][0m #------------------------ Iteration 426 --------------------------#
[32m[20221208 14:47:25 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:47:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:25 @agent_ppo2.py:179][0m |           0.0818 |           8.2209 |         -63.4340 |
[32m[20221208 14:47:25 @agent_ppo2.py:179][0m |           0.0321 |           6.9860 |         -54.6775 |
[32m[20221208 14:47:25 @agent_ppo2.py:179][0m |          -0.0064 |           6.4006 |         -58.4212 |
[32m[20221208 14:47:25 @agent_ppo2.py:179][0m |          -0.0307 |           5.9938 |         -61.3973 |
[32m[20221208 14:47:26 @agent_ppo2.py:179][0m |          -0.0496 |           5.7368 |         -64.1132 |
[32m[20221208 14:47:26 @agent_ppo2.py:179][0m |          -0.0558 |           5.5225 |         -63.9822 |
[32m[20221208 14:47:26 @agent_ppo2.py:179][0m |          -0.0555 |           5.3470 |         -64.3709 |
[32m[20221208 14:47:26 @agent_ppo2.py:179][0m |          -0.0598 |           5.1411 |         -65.9538 |
[32m[20221208 14:47:26 @agent_ppo2.py:179][0m |          -0.0682 |           5.0698 |         -68.0009 |
[32m[20221208 14:47:26 @agent_ppo2.py:179][0m |          -0.0749 |           4.9374 |         -69.5499 |
[32m[20221208 14:47:26 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:47:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.63
[32m[20221208 14:47:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.43
[32m[20221208 14:47:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.28
[32m[20221208 14:47:26 @agent_ppo2.py:137][0m Total time:      10.87 min
[32m[20221208 14:47:26 @agent_ppo2.py:139][0m 874496 total steps have happened
[32m[20221208 14:47:26 @agent_ppo2.py:115][0m #------------------------ Iteration 427 --------------------------#
[32m[20221208 14:47:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |           0.0523 |           5.7381 |         -39.9286 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |           0.0198 |           5.1089 |         -18.6404 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0110 |           4.8060 |         -21.5916 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0328 |           4.6176 |         -24.6897 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0396 |           4.4756 |         -25.4923 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0484 |           4.3304 |         -26.9468 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0552 |           4.2542 |         -28.1251 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0595 |           4.1932 |         -28.7296 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0556 |           4.0855 |         -28.5798 |
[32m[20221208 14:47:27 @agent_ppo2.py:179][0m |          -0.0409 |           4.0299 |         -27.2120 |
[32m[20221208 14:47:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:47:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.40
[32m[20221208 14:47:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.98
[32m[20221208 14:47:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.75
[32m[20221208 14:47:28 @agent_ppo2.py:137][0m Total time:      10.89 min
[32m[20221208 14:47:28 @agent_ppo2.py:139][0m 876544 total steps have happened
[32m[20221208 14:47:28 @agent_ppo2.py:115][0m #------------------------ Iteration 428 --------------------------#
[32m[20221208 14:47:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:28 @agent_ppo2.py:179][0m |           0.0866 |           8.2855 |         -68.0339 |
[32m[20221208 14:47:28 @agent_ppo2.py:179][0m |           0.0743 |           7.0020 |         -42.2607 |
[32m[20221208 14:47:28 @agent_ppo2.py:179][0m |           0.0026 |           6.3954 |         -53.3861 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0222 |           6.0428 |         -56.6576 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0483 |           5.8081 |         -60.4333 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0585 |           5.6160 |         -62.9316 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0622 |           5.4297 |         -63.7545 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0681 |           5.3083 |         -65.0065 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0716 |           5.2027 |         -66.4875 |
[32m[20221208 14:47:29 @agent_ppo2.py:179][0m |          -0.0781 |           5.0767 |         -68.2544 |
[32m[20221208 14:47:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.51
[32m[20221208 14:47:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.32
[32m[20221208 14:47:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 196.64
[32m[20221208 14:47:29 @agent_ppo2.py:137][0m Total time:      10.92 min
[32m[20221208 14:47:29 @agent_ppo2.py:139][0m 878592 total steps have happened
[32m[20221208 14:47:29 @agent_ppo2.py:115][0m #------------------------ Iteration 429 --------------------------#
[32m[20221208 14:47:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |           0.0659 |           6.3358 |         -55.5016 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |           0.0445 |           5.5761 |         -46.1048 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |           0.0045 |           5.1506 |         -56.3941 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |           0.0079 |           4.9127 |         -62.9638 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |          -0.0054 |           4.7567 |         -62.1615 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |          -0.0255 |           4.6973 |         -64.0535 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |          -0.0339 |           4.5815 |         -69.2790 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |          -0.0455 |           4.4838 |         -72.4239 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |          -0.0490 |           4.4504 |         -73.2691 |
[32m[20221208 14:47:30 @agent_ppo2.py:179][0m |          -0.0503 |           4.3452 |         -73.6579 |
[32m[20221208 14:47:30 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:47:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.76
[32m[20221208 14:47:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.31
[32m[20221208 14:47:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.23
[32m[20221208 14:47:31 @agent_ppo2.py:137][0m Total time:      10.94 min
[32m[20221208 14:47:31 @agent_ppo2.py:139][0m 880640 total steps have happened
[32m[20221208 14:47:31 @agent_ppo2.py:115][0m #------------------------ Iteration 430 --------------------------#
[32m[20221208 14:47:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:47:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:31 @agent_ppo2.py:179][0m |           0.0750 |           9.1655 |         -77.8575 |
[32m[20221208 14:47:31 @agent_ppo2.py:179][0m |           0.0561 |           7.4649 |         -60.8622 |
[32m[20221208 14:47:31 @agent_ppo2.py:179][0m |           0.0025 |           7.0505 |         -77.7331 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0257 |           6.8093 |         -85.3978 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0362 |           6.6980 |         -89.1005 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0442 |           6.5860 |         -88.5345 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0501 |           6.4734 |         -91.5765 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0548 |           6.4100 |         -92.9411 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0563 |           6.3314 |         -93.1441 |
[32m[20221208 14:47:32 @agent_ppo2.py:179][0m |          -0.0590 |           6.2647 |         -94.9904 |
[32m[20221208 14:47:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.55
[32m[20221208 14:47:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.46
[32m[20221208 14:47:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.41
[32m[20221208 14:47:32 @agent_ppo2.py:137][0m Total time:      10.97 min
[32m[20221208 14:47:32 @agent_ppo2.py:139][0m 882688 total steps have happened
[32m[20221208 14:47:32 @agent_ppo2.py:115][0m #------------------------ Iteration 431 --------------------------#
[32m[20221208 14:47:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |           0.0723 |           5.1947 |         -78.7702 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |           0.0388 |           4.6440 |         -73.0717 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0022 |           4.4319 |         -77.5415 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0299 |           4.2890 |         -81.1495 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0346 |           4.1969 |         -82.1500 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0445 |           4.0671 |         -84.7714 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0562 |           3.9705 |         -86.3594 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0537 |           3.8893 |         -87.2364 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0658 |           3.8096 |         -87.3680 |
[32m[20221208 14:47:33 @agent_ppo2.py:179][0m |          -0.0648 |           3.7708 |         -89.0885 |
[32m[20221208 14:47:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.75
[32m[20221208 14:47:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 193.72
[32m[20221208 14:47:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.06
[32m[20221208 14:47:34 @agent_ppo2.py:137][0m Total time:      10.99 min
[32m[20221208 14:47:34 @agent_ppo2.py:139][0m 884736 total steps have happened
[32m[20221208 14:47:34 @agent_ppo2.py:115][0m #------------------------ Iteration 432 --------------------------#
[32m[20221208 14:47:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:34 @agent_ppo2.py:179][0m |           0.2246 |           5.0378 |         -84.0259 |
[32m[20221208 14:47:34 @agent_ppo2.py:179][0m |           0.0895 |           4.2232 |         -68.6422 |
[32m[20221208 14:47:34 @agent_ppo2.py:179][0m |           0.0299 |           3.8117 |         -74.2114 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0043 |           3.5724 |         -82.5000 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0239 |           3.3610 |         -87.5115 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0298 |           3.1540 |         -85.8232 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0359 |           3.0696 |         -88.1077 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0402 |           2.9548 |         -91.8626 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0436 |           2.8116 |         -90.9206 |
[32m[20221208 14:47:35 @agent_ppo2.py:179][0m |          -0.0486 |           2.6786 |         -96.1498 |
[32m[20221208 14:47:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.37
[32m[20221208 14:47:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.32
[32m[20221208 14:47:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.26
[32m[20221208 14:47:35 @agent_ppo2.py:137][0m Total time:      11.02 min
[32m[20221208 14:47:35 @agent_ppo2.py:139][0m 886784 total steps have happened
[32m[20221208 14:47:35 @agent_ppo2.py:115][0m #------------------------ Iteration 433 --------------------------#
[32m[20221208 14:47:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |           0.0546 |           8.3922 |         -70.3922 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |           0.0441 |           7.3876 |         -63.7771 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0010 |           6.7945 |         -66.2228 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0266 |           6.5184 |         -68.0488 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0454 |           6.2336 |         -70.7881 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0494 |           6.0819 |         -68.9227 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0542 |           5.9721 |         -71.9412 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0624 |           5.8650 |         -74.1674 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0656 |           5.7186 |         -76.4945 |
[32m[20221208 14:47:36 @agent_ppo2.py:179][0m |          -0.0702 |           5.6255 |         -77.9639 |
[32m[20221208 14:47:36 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:47:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.66
[32m[20221208 14:47:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.14
[32m[20221208 14:47:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.18
[32m[20221208 14:47:37 @agent_ppo2.py:137][0m Total time:      11.04 min
[32m[20221208 14:47:37 @agent_ppo2.py:139][0m 888832 total steps have happened
[32m[20221208 14:47:37 @agent_ppo2.py:115][0m #------------------------ Iteration 434 --------------------------#
[32m[20221208 14:47:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:37 @agent_ppo2.py:179][0m |           0.0448 |           8.2015 |         -85.8694 |
[32m[20221208 14:47:37 @agent_ppo2.py:179][0m |           0.0397 |           7.4342 |         -79.1541 |
[32m[20221208 14:47:37 @agent_ppo2.py:179][0m |           0.0018 |           7.1572 |         -88.0322 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0269 |           7.0103 |         -91.2040 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0334 |           6.8346 |         -93.9856 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0454 |           6.6650 |         -94.5825 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0502 |           6.5594 |         -96.5197 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0524 |           6.5261 |         -95.4094 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0556 |           6.4151 |         -97.9058 |
[32m[20221208 14:47:38 @agent_ppo2.py:179][0m |          -0.0568 |           6.3257 |         -99.1654 |
[32m[20221208 14:47:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.42
[32m[20221208 14:47:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 202.68
[32m[20221208 14:47:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.29
[32m[20221208 14:47:38 @agent_ppo2.py:137][0m Total time:      11.07 min
[32m[20221208 14:47:38 @agent_ppo2.py:139][0m 890880 total steps have happened
[32m[20221208 14:47:38 @agent_ppo2.py:115][0m #------------------------ Iteration 435 --------------------------#
[32m[20221208 14:47:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |           0.0562 |           2.3486 |         -88.7799 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |           0.1073 |           1.7285 |         -41.4681 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |           0.0833 |           1.5273 |         -37.5312 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |           0.0014 |           1.4324 |         -87.7331 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |          -0.0182 |           1.3568 |         -90.9498 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |          -0.0178 |           1.2977 |         -91.8090 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |          -0.0179 |           1.2530 |         -91.0060 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |          -0.0178 |           1.2223 |         -88.9045 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |          -0.0344 |           1.1724 |         -93.0559 |
[32m[20221208 14:47:39 @agent_ppo2.py:179][0m |          -0.0385 |           1.1394 |         -94.7874 |
[32m[20221208 14:47:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.70
[32m[20221208 14:47:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.73
[32m[20221208 14:47:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.13
[32m[20221208 14:47:40 @agent_ppo2.py:137][0m Total time:      11.09 min
[32m[20221208 14:47:40 @agent_ppo2.py:139][0m 892928 total steps have happened
[32m[20221208 14:47:40 @agent_ppo2.py:115][0m #------------------------ Iteration 436 --------------------------#
[32m[20221208 14:47:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:40 @agent_ppo2.py:179][0m |           0.0979 |           4.8956 |         -34.3330 |
[32m[20221208 14:47:40 @agent_ppo2.py:179][0m |           0.0671 |           4.4338 |         -15.9908 |
[32m[20221208 14:47:40 @agent_ppo2.py:179][0m |           0.0170 |           4.2774 |         -21.3124 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0062 |           4.1982 |         -27.6226 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0194 |           4.1336 |         -29.9217 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0261 |           4.0852 |         -30.7793 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0282 |           4.0662 |         -32.5787 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0333 |           4.0739 |         -33.7585 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0394 |           3.9860 |         -35.1280 |
[32m[20221208 14:47:41 @agent_ppo2.py:179][0m |          -0.0402 |           3.9243 |         -35.7530 |
[32m[20221208 14:47:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.76
[32m[20221208 14:47:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.72
[32m[20221208 14:47:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.39
[32m[20221208 14:47:41 @agent_ppo2.py:137][0m Total time:      11.12 min
[32m[20221208 14:47:41 @agent_ppo2.py:139][0m 894976 total steps have happened
[32m[20221208 14:47:41 @agent_ppo2.py:115][0m #------------------------ Iteration 437 --------------------------#
[32m[20221208 14:47:42 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:47:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |           0.0746 |           4.5891 |         -93.4728 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |           0.0658 |           3.9508 |         -77.5285 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |           0.0164 |           3.6537 |         -91.8099 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0076 |           3.3871 |         -95.9817 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0214 |           3.2388 |        -100.0129 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0335 |           3.1083 |        -103.8510 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0300 |           2.9841 |        -104.5065 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0408 |           2.9274 |        -104.8279 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0488 |           2.8355 |        -109.4053 |
[32m[20221208 14:47:42 @agent_ppo2.py:179][0m |          -0.0512 |           2.7937 |        -111.6611 |
[32m[20221208 14:47:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.64
[32m[20221208 14:47:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.73
[32m[20221208 14:47:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.70
[32m[20221208 14:47:43 @agent_ppo2.py:137][0m Total time:      11.14 min
[32m[20221208 14:47:43 @agent_ppo2.py:139][0m 897024 total steps have happened
[32m[20221208 14:47:43 @agent_ppo2.py:115][0m #------------------------ Iteration 438 --------------------------#
[32m[20221208 14:47:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:43 @agent_ppo2.py:179][0m |           0.0661 |           7.2625 |         -91.0118 |
[32m[20221208 14:47:43 @agent_ppo2.py:179][0m |           0.0424 |           6.7194 |         -82.6042 |
[32m[20221208 14:47:43 @agent_ppo2.py:179][0m |          -0.0008 |           6.5069 |         -99.3043 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0131 |           6.3513 |        -103.5055 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0301 |           6.2907 |        -107.8983 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0374 |           6.1311 |        -112.0061 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0417 |           6.0502 |        -110.6715 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0443 |           5.9834 |        -114.0800 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0421 |           5.9249 |        -111.6568 |
[32m[20221208 14:47:44 @agent_ppo2.py:179][0m |          -0.0496 |           5.8757 |        -117.2743 |
[32m[20221208 14:47:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.25
[32m[20221208 14:47:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.82
[32m[20221208 14:47:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.80
[32m[20221208 14:47:44 @agent_ppo2.py:137][0m Total time:      11.17 min
[32m[20221208 14:47:44 @agent_ppo2.py:139][0m 899072 total steps have happened
[32m[20221208 14:47:44 @agent_ppo2.py:115][0m #------------------------ Iteration 439 --------------------------#
[32m[20221208 14:47:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |           0.0717 |           4.1360 |         -68.7252 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |           0.0305 |           3.5899 |         -52.9229 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0042 |           3.3817 |         -66.1307 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0200 |           3.2530 |         -69.7666 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0296 |           3.1515 |         -73.5483 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0267 |           3.0641 |         -70.9283 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0388 |           2.9932 |         -75.9010 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0460 |           2.9261 |         -79.1728 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0483 |           2.9150 |         -79.9998 |
[32m[20221208 14:47:45 @agent_ppo2.py:179][0m |          -0.0483 |           2.8082 |         -80.2283 |
[32m[20221208 14:47:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.02
[32m[20221208 14:47:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.78
[32m[20221208 14:47:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.04
[32m[20221208 14:47:46 @agent_ppo2.py:137][0m Total time:      11.19 min
[32m[20221208 14:47:46 @agent_ppo2.py:139][0m 901120 total steps have happened
[32m[20221208 14:47:46 @agent_ppo2.py:115][0m #------------------------ Iteration 440 --------------------------#
[32m[20221208 14:47:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:47:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:46 @agent_ppo2.py:179][0m |           0.0950 |           1.2090 |         -71.9761 |
[32m[20221208 14:47:46 @agent_ppo2.py:179][0m |           0.0635 |           1.1082 |         -34.2328 |
[32m[20221208 14:47:46 @agent_ppo2.py:179][0m |           0.0441 |           1.0880 |         -35.1494 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0446 |           1.0780 |         -35.6505 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0658 |           1.0822 |         -31.6319 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0485 |           1.0597 |         -34.2507 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0448 |           1.0769 |         -35.2578 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0421 |           1.0568 |         -34.4675 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0451 |           1.0788 |         -34.8383 |
[32m[20221208 14:47:47 @agent_ppo2.py:179][0m |           0.0453 |           1.0502 |         -35.3914 |
[32m[20221208 14:47:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:47:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.29
[32m[20221208 14:47:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.04
[32m[20221208 14:47:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.70
[32m[20221208 14:47:47 @agent_ppo2.py:137][0m Total time:      11.22 min
[32m[20221208 14:47:47 @agent_ppo2.py:139][0m 903168 total steps have happened
[32m[20221208 14:47:47 @agent_ppo2.py:115][0m #------------------------ Iteration 441 --------------------------#
[32m[20221208 14:47:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:47:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |           0.0270 |           0.9352 |         -43.0166 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0013 |           0.7618 |         -31.4928 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0089 |           0.7375 |         -32.2146 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0089 |           0.7298 |         -31.7393 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0093 |           0.7242 |         -33.0508 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0092 |           0.7220 |         -33.2381 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0129 |           0.7152 |         -32.9763 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0143 |           0.7132 |         -33.1417 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0104 |           0.7137 |         -33.4410 |
[32m[20221208 14:47:48 @agent_ppo2.py:179][0m |          -0.0106 |           0.7111 |         -33.1619 |
[32m[20221208 14:47:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:47:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.56
[32m[20221208 14:47:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.15
[32m[20221208 14:47:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.42
[32m[20221208 14:47:49 @agent_ppo2.py:137][0m Total time:      11.24 min
[32m[20221208 14:47:49 @agent_ppo2.py:139][0m 905216 total steps have happened
[32m[20221208 14:47:49 @agent_ppo2.py:115][0m #------------------------ Iteration 442 --------------------------#
[32m[20221208 14:47:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:49 @agent_ppo2.py:179][0m |           0.0357 |           1.2936 |         -97.6509 |
[32m[20221208 14:47:49 @agent_ppo2.py:179][0m |           0.0230 |           0.8003 |         -67.2106 |
[32m[20221208 14:47:49 @agent_ppo2.py:179][0m |           0.0121 |           0.7123 |         -68.8570 |
[32m[20221208 14:47:49 @agent_ppo2.py:179][0m |           0.0230 |           0.6861 |         -62.6947 |
[32m[20221208 14:47:50 @agent_ppo2.py:179][0m |           0.0126 |           0.6796 |         -69.1741 |
[32m[20221208 14:47:50 @agent_ppo2.py:179][0m |           0.0223 |           0.6731 |         -68.0148 |
[32m[20221208 14:47:50 @agent_ppo2.py:179][0m |           0.0114 |           0.6709 |         -67.8897 |
[32m[20221208 14:47:50 @agent_ppo2.py:179][0m |           0.0070 |           0.6686 |         -70.0431 |
[32m[20221208 14:47:50 @agent_ppo2.py:179][0m |           0.0060 |           0.6740 |         -71.5000 |
[32m[20221208 14:47:50 @agent_ppo2.py:179][0m |           0.0113 |           0.6690 |         -71.6109 |
[32m[20221208 14:47:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:47:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.41
[32m[20221208 14:47:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.72
[32m[20221208 14:47:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.87
[32m[20221208 14:47:50 @agent_ppo2.py:137][0m Total time:      11.27 min
[32m[20221208 14:47:50 @agent_ppo2.py:139][0m 907264 total steps have happened
[32m[20221208 14:47:50 @agent_ppo2.py:115][0m #------------------------ Iteration 443 --------------------------#
[32m[20221208 14:47:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |           0.0485 |           3.4421 |        -107.8216 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |           0.0389 |           2.9529 |         -72.4144 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |           0.0042 |           2.7853 |         -66.2700 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0156 |           2.6852 |         -71.0209 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0362 |           2.5935 |         -72.7761 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0399 |           2.5164 |         -76.1661 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0475 |           2.4673 |         -78.0689 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0507 |           2.3949 |         -79.1678 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0559 |           2.3581 |         -82.0030 |
[32m[20221208 14:47:51 @agent_ppo2.py:179][0m |          -0.0568 |           2.3361 |         -85.3827 |
[32m[20221208 14:47:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.11
[32m[20221208 14:47:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.34
[32m[20221208 14:47:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.41
[32m[20221208 14:47:52 @agent_ppo2.py:137][0m Total time:      11.29 min
[32m[20221208 14:47:52 @agent_ppo2.py:139][0m 909312 total steps have happened
[32m[20221208 14:47:52 @agent_ppo2.py:115][0m #------------------------ Iteration 444 --------------------------#
[32m[20221208 14:47:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:52 @agent_ppo2.py:179][0m |           0.0544 |           2.1890 |        -108.3835 |
[32m[20221208 14:47:52 @agent_ppo2.py:179][0m |           0.0908 |           1.7192 |         -73.1124 |
[32m[20221208 14:47:52 @agent_ppo2.py:179][0m |           0.0074 |           1.5197 |         -69.6965 |
[32m[20221208 14:47:52 @agent_ppo2.py:179][0m |          -0.0173 |           1.4078 |         -72.9662 |
[32m[20221208 14:47:53 @agent_ppo2.py:179][0m |          -0.0269 |           1.3384 |         -75.2228 |
[32m[20221208 14:47:53 @agent_ppo2.py:179][0m |          -0.0402 |           1.2706 |         -80.2205 |
[32m[20221208 14:47:53 @agent_ppo2.py:179][0m |          -0.0510 |           1.2281 |         -86.2299 |
[32m[20221208 14:47:53 @agent_ppo2.py:179][0m |          -0.0571 |           1.1876 |         -86.8253 |
[32m[20221208 14:47:53 @agent_ppo2.py:179][0m |          -0.0613 |           1.1549 |         -90.5501 |
[32m[20221208 14:47:53 @agent_ppo2.py:179][0m |          -0.0571 |           1.1332 |         -88.3729 |
[32m[20221208 14:47:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:47:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.19
[32m[20221208 14:47:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.31
[32m[20221208 14:47:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.08
[32m[20221208 14:47:53 @agent_ppo2.py:137][0m Total time:      11.32 min
[32m[20221208 14:47:53 @agent_ppo2.py:139][0m 911360 total steps have happened
[32m[20221208 14:47:53 @agent_ppo2.py:115][0m #------------------------ Iteration 445 --------------------------#
[32m[20221208 14:47:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |           0.0698 |           1.3863 |        -115.0363 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |           0.0360 |           1.2375 |         -79.1419 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0099 |           1.1624 |         -85.2473 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0245 |           1.1148 |         -89.6221 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0345 |           1.0668 |         -89.8509 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0395 |           1.0472 |         -93.9551 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0398 |           1.0194 |         -93.8962 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0452 |           0.9868 |         -98.7071 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0446 |           0.9706 |         -96.5757 |
[32m[20221208 14:47:54 @agent_ppo2.py:179][0m |          -0.0488 |           0.9565 |        -100.3591 |
[32m[20221208 14:47:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:47:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.04
[32m[20221208 14:47:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.94
[32m[20221208 14:47:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.38
[32m[20221208 14:47:55 @agent_ppo2.py:137][0m Total time:      11.34 min
[32m[20221208 14:47:55 @agent_ppo2.py:139][0m 913408 total steps have happened
[32m[20221208 14:47:55 @agent_ppo2.py:115][0m #------------------------ Iteration 446 --------------------------#
[32m[20221208 14:47:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:55 @agent_ppo2.py:179][0m |           0.0816 |           4.3019 |        -116.6928 |
[32m[20221208 14:47:55 @agent_ppo2.py:179][0m |           0.0430 |           3.8874 |         -65.2201 |
[32m[20221208 14:47:55 @agent_ppo2.py:179][0m |           0.0270 |           3.7418 |         -43.5714 |
[32m[20221208 14:47:55 @agent_ppo2.py:179][0m |           0.0014 |           3.6054 |         -67.4166 |
[32m[20221208 14:47:56 @agent_ppo2.py:179][0m |           0.0143 |           3.5291 |         -69.5877 |
[32m[20221208 14:47:56 @agent_ppo2.py:179][0m |           0.0135 |           3.5773 |         -74.6400 |
[32m[20221208 14:47:56 @agent_ppo2.py:179][0m |          -0.0007 |           3.3996 |         -38.5048 |
[32m[20221208 14:47:56 @agent_ppo2.py:179][0m |          -0.0136 |           3.3567 |         -39.5940 |
[32m[20221208 14:47:56 @agent_ppo2.py:179][0m |          -0.0218 |           3.3189 |         -41.4362 |
[32m[20221208 14:47:56 @agent_ppo2.py:179][0m |          -0.0195 |           3.3035 |         -42.3884 |
[32m[20221208 14:47:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:47:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.76
[32m[20221208 14:47:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.58
[32m[20221208 14:47:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.52
[32m[20221208 14:47:56 @agent_ppo2.py:137][0m Total time:      11.37 min
[32m[20221208 14:47:56 @agent_ppo2.py:139][0m 915456 total steps have happened
[32m[20221208 14:47:56 @agent_ppo2.py:115][0m #------------------------ Iteration 447 --------------------------#
[32m[20221208 14:47:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:47:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0249 |           0.6242 |         -65.4082 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0215 |           0.5711 |         -39.3367 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0162 |           0.5567 |         -39.8971 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0024 |           0.5443 |         -41.2075 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0046 |           0.5392 |         -40.4427 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0019 |           0.5363 |         -41.5530 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0036 |           0.5315 |         -42.3781 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0006 |           0.5301 |         -42.9551 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0042 |           0.5285 |         -42.1902 |
[32m[20221208 14:47:57 @agent_ppo2.py:179][0m |           0.0020 |           0.5278 |         -44.5893 |
[32m[20221208 14:47:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:47:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.15
[32m[20221208 14:47:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.71
[32m[20221208 14:47:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.46
[32m[20221208 14:47:58 @agent_ppo2.py:137][0m Total time:      11.39 min
[32m[20221208 14:47:58 @agent_ppo2.py:139][0m 917504 total steps have happened
[32m[20221208 14:47:58 @agent_ppo2.py:115][0m #------------------------ Iteration 448 --------------------------#
[32m[20221208 14:47:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:47:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:47:58 @agent_ppo2.py:179][0m |           0.0884 |           0.6681 |         -73.4981 |
[32m[20221208 14:47:58 @agent_ppo2.py:179][0m |           0.0846 |           0.5935 |         -39.9791 |
[32m[20221208 14:47:58 @agent_ppo2.py:179][0m |           0.0739 |           0.5776 |         -40.8110 |
[32m[20221208 14:47:58 @agent_ppo2.py:179][0m |           0.0889 |           0.5718 |         -34.7774 |
[32m[20221208 14:47:58 @agent_ppo2.py:179][0m |           0.0666 |           0.5641 |         -44.1083 |
[32m[20221208 14:47:59 @agent_ppo2.py:179][0m |           0.0723 |           0.5569 |         -42.9411 |
[32m[20221208 14:47:59 @agent_ppo2.py:179][0m |           0.0645 |           0.5580 |         -44.2730 |
[32m[20221208 14:47:59 @agent_ppo2.py:179][0m |           0.0661 |           0.5562 |         -42.9864 |
[32m[20221208 14:47:59 @agent_ppo2.py:179][0m |           0.0869 |           0.5512 |         -36.2692 |
[32m[20221208 14:47:59 @agent_ppo2.py:179][0m |           0.0697 |           0.5489 |         -40.3109 |
[32m[20221208 14:47:59 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:47:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.70
[32m[20221208 14:47:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.88
[32m[20221208 14:47:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.28
[32m[20221208 14:47:59 @agent_ppo2.py:137][0m Total time:      11.42 min
[32m[20221208 14:47:59 @agent_ppo2.py:139][0m 919552 total steps have happened
[32m[20221208 14:47:59 @agent_ppo2.py:115][0m #------------------------ Iteration 449 --------------------------#
[32m[20221208 14:48:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |           0.0986 |           6.3923 |        -120.9067 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |           0.0662 |           4.9130 |         -82.3358 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |           0.0163 |           4.5149 |        -112.1673 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0150 |           4.2665 |        -129.0790 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0271 |           4.1853 |        -134.2480 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0318 |           3.9599 |        -137.4944 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0440 |           3.8979 |        -142.4554 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0413 |           3.7593 |        -142.9135 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0474 |           3.7124 |        -143.3915 |
[32m[20221208 14:48:00 @agent_ppo2.py:179][0m |          -0.0521 |           3.6479 |        -143.3902 |
[32m[20221208 14:48:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:48:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.04
[32m[20221208 14:48:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.43
[32m[20221208 14:48:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.78
[32m[20221208 14:48:01 @agent_ppo2.py:137][0m Total time:      11.44 min
[32m[20221208 14:48:01 @agent_ppo2.py:139][0m 921600 total steps have happened
[32m[20221208 14:48:01 @agent_ppo2.py:115][0m #------------------------ Iteration 450 --------------------------#
[32m[20221208 14:48:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:01 @agent_ppo2.py:179][0m |           0.1233 |           8.1798 |         -95.5644 |
[32m[20221208 14:48:01 @agent_ppo2.py:179][0m |           0.1152 |           6.5143 |         -49.0960 |
[32m[20221208 14:48:01 @agent_ppo2.py:179][0m |           0.0581 |           5.9429 |         -53.8731 |
[32m[20221208 14:48:01 @agent_ppo2.py:179][0m |           0.0247 |           5.5768 |         -53.1136 |
[32m[20221208 14:48:01 @agent_ppo2.py:179][0m |          -0.0006 |           5.3541 |         -64.5090 |
[32m[20221208 14:48:02 @agent_ppo2.py:179][0m |          -0.0145 |           5.1597 |         -67.7596 |
[32m[20221208 14:48:02 @agent_ppo2.py:179][0m |          -0.0302 |           4.9406 |         -75.6351 |
[32m[20221208 14:48:02 @agent_ppo2.py:179][0m |          -0.0351 |           4.8735 |         -77.5888 |
[32m[20221208 14:48:02 @agent_ppo2.py:179][0m |          -0.0437 |           4.6298 |         -78.2437 |
[32m[20221208 14:48:02 @agent_ppo2.py:179][0m |          -0.0481 |           4.5346 |         -83.7382 |
[32m[20221208 14:48:02 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:48:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.09
[32m[20221208 14:48:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.99
[32m[20221208 14:48:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.45
[32m[20221208 14:48:02 @agent_ppo2.py:137][0m Total time:      11.47 min
[32m[20221208 14:48:02 @agent_ppo2.py:139][0m 923648 total steps have happened
[32m[20221208 14:48:02 @agent_ppo2.py:115][0m #------------------------ Iteration 451 --------------------------#
[32m[20221208 14:48:03 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:48:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |           0.0765 |           5.6293 |        -127.2329 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |           0.0348 |           4.2052 |        -120.0067 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |           0.0264 |           3.7141 |        -123.7468 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0024 |           3.4237 |        -129.3807 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0247 |           3.2023 |        -134.5751 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0361 |           3.0459 |        -143.3366 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0391 |           2.9055 |        -143.1121 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0483 |           2.8065 |        -148.5747 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0525 |           2.7172 |        -150.1752 |
[32m[20221208 14:48:03 @agent_ppo2.py:179][0m |          -0.0502 |           2.6481 |        -149.4978 |
[32m[20221208 14:48:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.20
[32m[20221208 14:48:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.48
[32m[20221208 14:48:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.84
[32m[20221208 14:48:04 @agent_ppo2.py:137][0m Total time:      11.49 min
[32m[20221208 14:48:04 @agent_ppo2.py:139][0m 925696 total steps have happened
[32m[20221208 14:48:04 @agent_ppo2.py:115][0m #------------------------ Iteration 452 --------------------------#
[32m[20221208 14:48:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:04 @agent_ppo2.py:179][0m |           0.0680 |           6.8516 |        -131.2950 |
[32m[20221208 14:48:04 @agent_ppo2.py:179][0m |           0.0239 |           6.0659 |        -124.1184 |
[32m[20221208 14:48:04 @agent_ppo2.py:179][0m |          -0.0044 |           5.8669 |        -128.0684 |
[32m[20221208 14:48:04 @agent_ppo2.py:179][0m |          -0.0240 |           5.6966 |        -132.7471 |
[32m[20221208 14:48:05 @agent_ppo2.py:179][0m |          -0.0256 |           5.5283 |        -132.7587 |
[32m[20221208 14:48:05 @agent_ppo2.py:179][0m |          -0.0396 |           5.4444 |        -139.9077 |
[32m[20221208 14:48:05 @agent_ppo2.py:179][0m |          -0.0481 |           5.3582 |        -141.4170 |
[32m[20221208 14:48:05 @agent_ppo2.py:179][0m |          -0.0518 |           5.2966 |        -140.5439 |
[32m[20221208 14:48:05 @agent_ppo2.py:179][0m |          -0.0547 |           5.2559 |        -143.2519 |
[32m[20221208 14:48:05 @agent_ppo2.py:179][0m |          -0.0546 |           5.2223 |        -145.1734 |
[32m[20221208 14:48:05 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:48:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.16
[32m[20221208 14:48:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.68
[32m[20221208 14:48:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.04
[32m[20221208 14:48:05 @agent_ppo2.py:137][0m Total time:      11.52 min
[32m[20221208 14:48:05 @agent_ppo2.py:139][0m 927744 total steps have happened
[32m[20221208 14:48:05 @agent_ppo2.py:115][0m #------------------------ Iteration 453 --------------------------#
[32m[20221208 14:48:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |           0.0665 |           4.3227 |        -126.7966 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |           0.0599 |           3.9501 |        -116.1110 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |           0.0092 |           3.7620 |        -130.0425 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0129 |           3.6411 |        -135.9669 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0253 |           3.5697 |        -136.7903 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0345 |           3.5002 |        -142.6782 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0330 |           3.4321 |        -146.9835 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0293 |           3.3936 |        -143.5079 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0403 |           3.3217 |        -147.3756 |
[32m[20221208 14:48:06 @agent_ppo2.py:179][0m |          -0.0465 |           3.2881 |        -147.8247 |
[32m[20221208 14:48:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.79
[32m[20221208 14:48:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.37
[32m[20221208 14:48:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.22
[32m[20221208 14:48:07 @agent_ppo2.py:137][0m Total time:      11.54 min
[32m[20221208 14:48:07 @agent_ppo2.py:139][0m 929792 total steps have happened
[32m[20221208 14:48:07 @agent_ppo2.py:115][0m #------------------------ Iteration 454 --------------------------#
[32m[20221208 14:48:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:07 @agent_ppo2.py:179][0m |           0.0527 |           4.2616 |        -122.1770 |
[32m[20221208 14:48:07 @agent_ppo2.py:179][0m |           0.0186 |           3.6277 |        -116.4218 |
[32m[20221208 14:48:07 @agent_ppo2.py:179][0m |           0.0064 |           3.4567 |        -123.3220 |
[32m[20221208 14:48:07 @agent_ppo2.py:179][0m |          -0.0085 |           3.3613 |        -125.9568 |
[32m[20221208 14:48:08 @agent_ppo2.py:179][0m |          -0.0117 |           3.2666 |        -112.5398 |
[32m[20221208 14:48:08 @agent_ppo2.py:179][0m |          -0.0230 |           3.2087 |        -124.5814 |
[32m[20221208 14:48:08 @agent_ppo2.py:179][0m |          -0.0395 |           3.1470 |        -134.6417 |
[32m[20221208 14:48:08 @agent_ppo2.py:179][0m |          -0.0408 |           3.1080 |        -135.2966 |
[32m[20221208 14:48:08 @agent_ppo2.py:179][0m |          -0.0418 |           3.0627 |        -132.0668 |
[32m[20221208 14:48:08 @agent_ppo2.py:179][0m |          -0.0482 |           3.0019 |        -139.4470 |
[32m[20221208 14:48:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.77
[32m[20221208 14:48:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.31
[32m[20221208 14:48:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.44
[32m[20221208 14:48:08 @agent_ppo2.py:137][0m Total time:      11.57 min
[32m[20221208 14:48:08 @agent_ppo2.py:139][0m 931840 total steps have happened
[32m[20221208 14:48:08 @agent_ppo2.py:115][0m #------------------------ Iteration 455 --------------------------#
[32m[20221208 14:48:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |           0.0767 |           6.7139 |        -123.0843 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |           0.0245 |           6.0673 |        -106.0087 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |           0.0204 |           5.9834 |        -103.0013 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0055 |           5.7445 |        -114.7435 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0191 |           5.6443 |        -116.6732 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0305 |           5.5558 |        -121.6574 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0342 |           5.4605 |        -119.9846 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0438 |           5.3960 |        -133.1189 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0406 |           5.3368 |        -130.0755 |
[32m[20221208 14:48:09 @agent_ppo2.py:179][0m |          -0.0495 |           5.2415 |        -133.6049 |
[32m[20221208 14:48:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.40
[32m[20221208 14:48:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.46
[32m[20221208 14:48:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.14
[32m[20221208 14:48:10 @agent_ppo2.py:137][0m Total time:      11.59 min
[32m[20221208 14:48:10 @agent_ppo2.py:139][0m 933888 total steps have happened
[32m[20221208 14:48:10 @agent_ppo2.py:115][0m #------------------------ Iteration 456 --------------------------#
[32m[20221208 14:48:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:10 @agent_ppo2.py:179][0m |           0.0333 |           1.1181 |        -139.1398 |
[32m[20221208 14:48:10 @agent_ppo2.py:179][0m |           0.0649 |           0.8483 |         -95.3804 |
[32m[20221208 14:48:10 @agent_ppo2.py:179][0m |           0.0682 |           0.7800 |        -112.4248 |
[32m[20221208 14:48:10 @agent_ppo2.py:179][0m |           0.0361 |           0.7606 |        -108.2108 |
[32m[20221208 14:48:11 @agent_ppo2.py:179][0m |           0.0429 |           0.7373 |        -106.4703 |
[32m[20221208 14:48:11 @agent_ppo2.py:179][0m |          -0.0029 |           0.7319 |        -137.0348 |
[32m[20221208 14:48:11 @agent_ppo2.py:179][0m |           0.0117 |           0.7149 |        -132.9666 |
[32m[20221208 14:48:11 @agent_ppo2.py:179][0m |           0.0143 |           0.7014 |        -119.9870 |
[32m[20221208 14:48:11 @agent_ppo2.py:179][0m |           0.0205 |           0.6940 |        -127.1064 |
[32m[20221208 14:48:11 @agent_ppo2.py:179][0m |          -0.0044 |           0.6872 |        -142.3730 |
[32m[20221208 14:48:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.27
[32m[20221208 14:48:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 27.97
[32m[20221208 14:48:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.98
[32m[20221208 14:48:11 @agent_ppo2.py:137][0m Total time:      11.62 min
[32m[20221208 14:48:11 @agent_ppo2.py:139][0m 935936 total steps have happened
[32m[20221208 14:48:11 @agent_ppo2.py:115][0m #------------------------ Iteration 457 --------------------------#
[32m[20221208 14:48:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |           0.0714 |           3.4006 |        -117.2632 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |           0.0805 |           2.4869 |         -95.1134 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |           0.0359 |           2.1856 |         -76.1639 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |           0.0086 |           2.0015 |         -86.1262 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |          -0.0149 |           1.8865 |         -99.6656 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |          -0.0368 |           1.7693 |        -113.8654 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |          -0.0332 |           1.7028 |        -121.0387 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |          -0.0392 |           1.6204 |        -124.6952 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |          -0.0497 |           1.5567 |        -127.6186 |
[32m[20221208 14:48:12 @agent_ppo2.py:179][0m |          -0.0512 |           1.5113 |        -132.6171 |
[32m[20221208 14:48:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.50
[32m[20221208 14:48:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.13
[32m[20221208 14:48:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.28
[32m[20221208 14:48:13 @agent_ppo2.py:137][0m Total time:      11.64 min
[32m[20221208 14:48:13 @agent_ppo2.py:139][0m 937984 total steps have happened
[32m[20221208 14:48:13 @agent_ppo2.py:115][0m #------------------------ Iteration 458 --------------------------#
[32m[20221208 14:48:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:13 @agent_ppo2.py:179][0m |           0.0745 |           4.1627 |        -117.5076 |
[32m[20221208 14:48:13 @agent_ppo2.py:179][0m |           0.0425 |           3.2374 |        -104.9420 |
[32m[20221208 14:48:13 @agent_ppo2.py:179][0m |           0.0031 |           2.9109 |        -127.2362 |
[32m[20221208 14:48:13 @agent_ppo2.py:179][0m |          -0.0141 |           2.7476 |        -131.6786 |
[32m[20221208 14:48:14 @agent_ppo2.py:179][0m |          -0.0244 |           2.6309 |        -132.3123 |
[32m[20221208 14:48:14 @agent_ppo2.py:179][0m |          -0.0311 |           2.5444 |        -136.6062 |
[32m[20221208 14:48:14 @agent_ppo2.py:179][0m |          -0.0342 |           2.4794 |        -135.1522 |
[32m[20221208 14:48:14 @agent_ppo2.py:179][0m |          -0.0423 |           2.4207 |        -141.1625 |
[32m[20221208 14:48:14 @agent_ppo2.py:179][0m |          -0.0398 |           2.3570 |        -141.4224 |
[32m[20221208 14:48:14 @agent_ppo2.py:179][0m |          -0.0427 |           2.2988 |        -140.5854 |
[32m[20221208 14:48:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.83
[32m[20221208 14:48:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.59
[32m[20221208 14:48:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.07
[32m[20221208 14:48:14 @agent_ppo2.py:137][0m Total time:      11.67 min
[32m[20221208 14:48:14 @agent_ppo2.py:139][0m 940032 total steps have happened
[32m[20221208 14:48:14 @agent_ppo2.py:115][0m #------------------------ Iteration 459 --------------------------#
[32m[20221208 14:48:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:48:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |           0.0682 |           4.7301 |        -132.3868 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |           0.0544 |           3.9552 |        -117.7277 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |           0.0040 |           3.6704 |        -133.1520 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0238 |           3.6180 |        -138.9859 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0378 |           3.5030 |        -141.7588 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0428 |           3.4224 |        -144.6783 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0508 |           3.3151 |        -146.1299 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0561 |           3.2835 |        -148.2787 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0605 |           3.2271 |        -146.8476 |
[32m[20221208 14:48:15 @agent_ppo2.py:179][0m |          -0.0595 |           3.1586 |        -151.6810 |
[32m[20221208 14:48:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.42
[32m[20221208 14:48:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.30
[32m[20221208 14:48:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.70
[32m[20221208 14:48:16 @agent_ppo2.py:137][0m Total time:      11.69 min
[32m[20221208 14:48:16 @agent_ppo2.py:139][0m 942080 total steps have happened
[32m[20221208 14:48:16 @agent_ppo2.py:115][0m #------------------------ Iteration 460 --------------------------#
[32m[20221208 14:48:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:16 @agent_ppo2.py:179][0m |           0.0598 |           4.5868 |        -128.0946 |
[32m[20221208 14:48:16 @agent_ppo2.py:179][0m |           0.0512 |           4.0271 |         -83.0996 |
[32m[20221208 14:48:16 @agent_ppo2.py:179][0m |           0.0084 |           3.7169 |         -91.5286 |
[32m[20221208 14:48:16 @agent_ppo2.py:179][0m |          -0.0149 |           3.5800 |         -96.7203 |
[32m[20221208 14:48:17 @agent_ppo2.py:179][0m |          -0.0289 |           3.4794 |        -100.4877 |
[32m[20221208 14:48:17 @agent_ppo2.py:179][0m |          -0.0389 |           3.3526 |        -101.2561 |
[32m[20221208 14:48:17 @agent_ppo2.py:179][0m |          -0.0424 |           3.3187 |        -104.4937 |
[32m[20221208 14:48:17 @agent_ppo2.py:179][0m |          -0.0474 |           3.2719 |        -105.5492 |
[32m[20221208 14:48:17 @agent_ppo2.py:179][0m |          -0.0542 |           3.1650 |        -106.1947 |
[32m[20221208 14:48:17 @agent_ppo2.py:179][0m |          -0.0536 |           3.1435 |        -108.5731 |
[32m[20221208 14:48:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.12
[32m[20221208 14:48:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.87
[32m[20221208 14:48:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.19
[32m[20221208 14:48:17 @agent_ppo2.py:137][0m Total time:      11.72 min
[32m[20221208 14:48:17 @agent_ppo2.py:139][0m 944128 total steps have happened
[32m[20221208 14:48:17 @agent_ppo2.py:115][0m #------------------------ Iteration 461 --------------------------#
[32m[20221208 14:48:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |           0.0722 |           4.1230 |        -130.3754 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |           0.2507 |           3.3122 |        -107.9527 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |           0.0148 |           2.9921 |        -100.9466 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0261 |           2.8024 |        -103.6485 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0349 |           2.6708 |        -109.3173 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0400 |           2.5072 |        -105.7878 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0472 |           2.4310 |        -112.4373 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0573 |           2.3263 |        -115.4297 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0621 |           2.2490 |        -113.6551 |
[32m[20221208 14:48:18 @agent_ppo2.py:179][0m |          -0.0634 |           2.1573 |        -115.5395 |
[32m[20221208 14:48:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.56
[32m[20221208 14:48:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.71
[32m[20221208 14:48:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.48
[32m[20221208 14:48:19 @agent_ppo2.py:137][0m Total time:      11.74 min
[32m[20221208 14:48:19 @agent_ppo2.py:139][0m 946176 total steps have happened
[32m[20221208 14:48:19 @agent_ppo2.py:115][0m #------------------------ Iteration 462 --------------------------#
[32m[20221208 14:48:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:19 @agent_ppo2.py:179][0m |           0.0624 |           4.2604 |        -134.4968 |
[32m[20221208 14:48:19 @agent_ppo2.py:179][0m |           0.0348 |           3.3950 |        -129.8547 |
[32m[20221208 14:48:19 @agent_ppo2.py:179][0m |           0.0104 |           3.1309 |        -124.1291 |
[32m[20221208 14:48:19 @agent_ppo2.py:179][0m |          -0.0122 |           2.9626 |        -133.0766 |
[32m[20221208 14:48:20 @agent_ppo2.py:179][0m |          -0.0189 |           2.8941 |        -127.8976 |
[32m[20221208 14:48:20 @agent_ppo2.py:179][0m |          -0.0320 |           2.7829 |        -131.5221 |
[32m[20221208 14:48:20 @agent_ppo2.py:179][0m |          -0.0379 |           2.7037 |        -140.6349 |
[32m[20221208 14:48:20 @agent_ppo2.py:179][0m |          -0.0363 |           2.6627 |        -106.8321 |
[32m[20221208 14:48:20 @agent_ppo2.py:179][0m |          -0.0464 |           2.5928 |         -97.8008 |
[32m[20221208 14:48:20 @agent_ppo2.py:179][0m |          -0.0464 |           2.5402 |         -86.2807 |
[32m[20221208 14:48:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.48
[32m[20221208 14:48:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.93
[32m[20221208 14:48:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.44
[32m[20221208 14:48:20 @agent_ppo2.py:137][0m Total time:      11.77 min
[32m[20221208 14:48:20 @agent_ppo2.py:139][0m 948224 total steps have happened
[32m[20221208 14:48:20 @agent_ppo2.py:115][0m #------------------------ Iteration 463 --------------------------#
[32m[20221208 14:48:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |           0.0810 |           6.6107 |        -130.1511 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |           0.0446 |           5.6155 |        -116.7834 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |           0.0043 |           5.2647 |        -116.6765 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0319 |           5.0232 |        -130.2771 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0402 |           4.8937 |        -131.7027 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0522 |           4.6945 |        -137.5205 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0523 |           4.5775 |        -141.1201 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0614 |           4.4836 |        -141.8683 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0557 |           4.4134 |        -140.1542 |
[32m[20221208 14:48:21 @agent_ppo2.py:179][0m |          -0.0660 |           4.3106 |        -146.3462 |
[32m[20221208 14:48:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.82
[32m[20221208 14:48:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.91
[32m[20221208 14:48:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.20
[32m[20221208 14:48:22 @agent_ppo2.py:137][0m Total time:      11.79 min
[32m[20221208 14:48:22 @agent_ppo2.py:139][0m 950272 total steps have happened
[32m[20221208 14:48:22 @agent_ppo2.py:115][0m #------------------------ Iteration 464 --------------------------#
[32m[20221208 14:48:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:22 @agent_ppo2.py:179][0m |           0.0617 |           8.2125 |        -148.5048 |
[32m[20221208 14:48:22 @agent_ppo2.py:179][0m |           0.0171 |           7.0676 |        -141.8255 |
[32m[20221208 14:48:22 @agent_ppo2.py:179][0m |          -0.0076 |           6.7527 |        -142.9823 |
[32m[20221208 14:48:22 @agent_ppo2.py:179][0m |          -0.0192 |           6.6205 |        -141.4959 |
[32m[20221208 14:48:23 @agent_ppo2.py:179][0m |          -0.0472 |           6.4966 |        -149.0384 |
[32m[20221208 14:48:23 @agent_ppo2.py:179][0m |          -0.0584 |           6.3867 |        -151.2561 |
[32m[20221208 14:48:23 @agent_ppo2.py:179][0m |          -0.0643 |           6.2650 |        -152.5844 |
[32m[20221208 14:48:23 @agent_ppo2.py:179][0m |          -0.0725 |           6.2413 |        -156.6989 |
[32m[20221208 14:48:23 @agent_ppo2.py:179][0m |          -0.0731 |           6.1844 |        -158.4148 |
[32m[20221208 14:48:23 @agent_ppo2.py:179][0m |          -0.0797 |           6.1169 |        -160.9865 |
[32m[20221208 14:48:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.21
[32m[20221208 14:48:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.00
[32m[20221208 14:48:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.53
[32m[20221208 14:48:23 @agent_ppo2.py:137][0m Total time:      11.82 min
[32m[20221208 14:48:23 @agent_ppo2.py:139][0m 952320 total steps have happened
[32m[20221208 14:48:23 @agent_ppo2.py:115][0m #------------------------ Iteration 465 --------------------------#
[32m[20221208 14:48:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |           0.0574 |           4.4665 |        -141.5768 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |           0.0438 |           4.1352 |        -123.3169 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0080 |           3.9419 |        -132.9377 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0283 |           3.8078 |        -140.5629 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0426 |           3.7307 |        -143.2345 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0533 |           3.6328 |        -143.9451 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0602 |           3.5774 |        -146.3495 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0598 |           3.5161 |        -148.0032 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0602 |           3.4735 |        -146.3711 |
[32m[20221208 14:48:24 @agent_ppo2.py:179][0m |          -0.0660 |           3.4188 |        -146.6119 |
[32m[20221208 14:48:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.34
[32m[20221208 14:48:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.92
[32m[20221208 14:48:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.11
[32m[20221208 14:48:25 @agent_ppo2.py:137][0m Total time:      11.84 min
[32m[20221208 14:48:25 @agent_ppo2.py:139][0m 954368 total steps have happened
[32m[20221208 14:48:25 @agent_ppo2.py:115][0m #------------------------ Iteration 466 --------------------------#
[32m[20221208 14:48:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:25 @agent_ppo2.py:179][0m |           0.0762 |           4.3694 |        -131.3986 |
[32m[20221208 14:48:25 @agent_ppo2.py:179][0m |           0.1019 |           3.9085 |         -70.9867 |
[32m[20221208 14:48:25 @agent_ppo2.py:179][0m |           0.0116 |           3.7042 |         -81.0498 |
[32m[20221208 14:48:25 @agent_ppo2.py:179][0m |          -0.0200 |           3.5744 |         -85.2402 |
[32m[20221208 14:48:25 @agent_ppo2.py:179][0m |          -0.0328 |           3.4410 |         -89.8217 |
[32m[20221208 14:48:26 @agent_ppo2.py:179][0m |          -0.0492 |           3.3564 |         -94.1775 |
[32m[20221208 14:48:26 @agent_ppo2.py:179][0m |          -0.0574 |           3.2647 |         -96.6579 |
[32m[20221208 14:48:26 @agent_ppo2.py:179][0m |          -0.0601 |           3.1903 |         -98.0814 |
[32m[20221208 14:48:26 @agent_ppo2.py:179][0m |          -0.0646 |           3.1359 |         -99.3858 |
[32m[20221208 14:48:26 @agent_ppo2.py:179][0m |          -0.0702 |           3.0706 |        -102.0329 |
[32m[20221208 14:48:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.47
[32m[20221208 14:48:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.52
[32m[20221208 14:48:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.94
[32m[20221208 14:48:26 @agent_ppo2.py:137][0m Total time:      11.87 min
[32m[20221208 14:48:26 @agent_ppo2.py:139][0m 956416 total steps have happened
[32m[20221208 14:48:26 @agent_ppo2.py:115][0m #------------------------ Iteration 467 --------------------------#
[32m[20221208 14:48:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |           0.0599 |           6.5185 |        -145.2557 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |           0.0892 |           5.8924 |        -101.7929 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |           0.0324 |           5.5673 |        -116.6879 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0033 |           5.3528 |        -131.9550 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0300 |           5.2561 |        -144.3737 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0420 |           5.0854 |        -154.6888 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0493 |           4.9855 |        -158.3618 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0560 |           4.9001 |        -165.7446 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0591 |           4.8168 |        -166.4823 |
[32m[20221208 14:48:27 @agent_ppo2.py:179][0m |          -0.0638 |           4.7372 |        -170.5555 |
[32m[20221208 14:48:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:48:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 169.91
[32m[20221208 14:48:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.93
[32m[20221208 14:48:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.59
[32m[20221208 14:48:28 @agent_ppo2.py:137][0m Total time:      11.89 min
[32m[20221208 14:48:28 @agent_ppo2.py:139][0m 958464 total steps have happened
[32m[20221208 14:48:28 @agent_ppo2.py:115][0m #------------------------ Iteration 468 --------------------------#
[32m[20221208 14:48:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:28 @agent_ppo2.py:179][0m |           0.0602 |           3.1497 |        -159.3912 |
[32m[20221208 14:48:28 @agent_ppo2.py:179][0m |           0.0365 |           2.8274 |        -144.6898 |
[32m[20221208 14:48:28 @agent_ppo2.py:179][0m |          -0.0016 |           2.6200 |        -158.8636 |
[32m[20221208 14:48:28 @agent_ppo2.py:179][0m |          -0.0187 |           2.5111 |        -164.3968 |
[32m[20221208 14:48:29 @agent_ppo2.py:179][0m |          -0.0203 |           2.4061 |        -166.2893 |
[32m[20221208 14:48:29 @agent_ppo2.py:179][0m |          -0.0194 |           2.3274 |        -162.4438 |
[32m[20221208 14:48:29 @agent_ppo2.py:179][0m |          -0.0377 |           2.2675 |        -174.9655 |
[32m[20221208 14:48:29 @agent_ppo2.py:179][0m |          -0.0409 |           2.2153 |        -174.7173 |
[32m[20221208 14:48:29 @agent_ppo2.py:179][0m |          -0.0430 |           2.1769 |        -174.0094 |
[32m[20221208 14:48:29 @agent_ppo2.py:179][0m |          -0.0469 |           2.1075 |        -177.7182 |
[32m[20221208 14:48:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.63
[32m[20221208 14:48:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.56
[32m[20221208 14:48:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.81
[32m[20221208 14:48:29 @agent_ppo2.py:137][0m Total time:      11.92 min
[32m[20221208 14:48:29 @agent_ppo2.py:139][0m 960512 total steps have happened
[32m[20221208 14:48:29 @agent_ppo2.py:115][0m #------------------------ Iteration 469 --------------------------#
[32m[20221208 14:48:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |           0.0725 |           5.1634 |        -163.1386 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |           0.0889 |           4.5483 |        -138.3579 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |           0.0296 |           4.1786 |        -148.0542 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |           0.0073 |           3.9269 |        -154.2065 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |          -0.0200 |           3.7336 |        -166.9335 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |          -0.0321 |           3.5801 |        -171.4167 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |          -0.0391 |           3.4123 |        -173.9395 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |          -0.0439 |           3.2863 |        -174.5273 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |          -0.0498 |           3.1782 |        -183.3844 |
[32m[20221208 14:48:30 @agent_ppo2.py:179][0m |          -0.0539 |           3.0842 |        -186.0227 |
[32m[20221208 14:48:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:48:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.60
[32m[20221208 14:48:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.58
[32m[20221208 14:48:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.24
[32m[20221208 14:48:31 @agent_ppo2.py:137][0m Total time:      11.94 min
[32m[20221208 14:48:31 @agent_ppo2.py:139][0m 962560 total steps have happened
[32m[20221208 14:48:31 @agent_ppo2.py:115][0m #------------------------ Iteration 470 --------------------------#
[32m[20221208 14:48:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:31 @agent_ppo2.py:179][0m |           0.1071 |           0.7156 |        -124.6741 |
[32m[20221208 14:48:31 @agent_ppo2.py:179][0m |           0.0130 |           0.6132 |         -53.6545 |
[32m[20221208 14:48:31 @agent_ppo2.py:179][0m |           0.0005 |           0.5846 |         -53.2133 |
[32m[20221208 14:48:31 @agent_ppo2.py:179][0m |           0.0002 |           0.5714 |         -52.4752 |
[32m[20221208 14:48:32 @agent_ppo2.py:179][0m |          -0.0036 |           0.5627 |         -53.0483 |
[32m[20221208 14:48:32 @agent_ppo2.py:179][0m |          -0.0042 |           0.5538 |         -53.7719 |
[32m[20221208 14:48:32 @agent_ppo2.py:179][0m |          -0.0058 |           0.5482 |         -54.4909 |
[32m[20221208 14:48:32 @agent_ppo2.py:179][0m |          -0.0018 |           0.5431 |         -55.2998 |
[32m[20221208 14:48:32 @agent_ppo2.py:179][0m |          -0.0059 |           0.5437 |         -55.9405 |
[32m[20221208 14:48:32 @agent_ppo2.py:179][0m |          -0.0083 |           0.5367 |         -56.4933 |
[32m[20221208 14:48:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.73
[32m[20221208 14:48:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.16
[32m[20221208 14:48:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.44
[32m[20221208 14:48:32 @agent_ppo2.py:137][0m Total time:      11.97 min
[32m[20221208 14:48:32 @agent_ppo2.py:139][0m 964608 total steps have happened
[32m[20221208 14:48:32 @agent_ppo2.py:115][0m #------------------------ Iteration 471 --------------------------#
[32m[20221208 14:48:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |           0.0752 |           3.1996 |        -169.7823 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |           0.0569 |           2.9422 |         -91.6482 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |           0.0147 |           2.7826 |        -106.2086 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |           0.0043 |           2.7509 |        -105.1423 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |          -0.0185 |           2.6693 |        -118.8975 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |          -0.0228 |           2.6126 |        -119.7138 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |          -0.0296 |           2.6156 |        -124.5680 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |          -0.0345 |           2.5933 |        -129.2633 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |          -0.0389 |           2.5165 |        -130.4246 |
[32m[20221208 14:48:33 @agent_ppo2.py:179][0m |          -0.0415 |           2.4958 |        -132.9931 |
[32m[20221208 14:48:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.11
[32m[20221208 14:48:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.41
[32m[20221208 14:48:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.95
[32m[20221208 14:48:34 @agent_ppo2.py:137][0m Total time:      11.99 min
[32m[20221208 14:48:34 @agent_ppo2.py:139][0m 966656 total steps have happened
[32m[20221208 14:48:34 @agent_ppo2.py:115][0m #------------------------ Iteration 472 --------------------------#
[32m[20221208 14:48:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:34 @agent_ppo2.py:179][0m |           0.0489 |           3.0026 |        -163.6966 |
[32m[20221208 14:48:34 @agent_ppo2.py:179][0m |           0.0142 |           2.5678 |        -124.8917 |
[32m[20221208 14:48:34 @agent_ppo2.py:179][0m |           0.0103 |           2.3392 |         -62.9582 |
[32m[20221208 14:48:34 @agent_ppo2.py:179][0m |          -0.0139 |           2.1767 |         -56.7349 |
[32m[20221208 14:48:34 @agent_ppo2.py:179][0m |          -0.0267 |           2.0224 |         -55.1580 |
[32m[20221208 14:48:35 @agent_ppo2.py:179][0m |          -0.0371 |           1.9071 |         -59.7797 |
[32m[20221208 14:48:35 @agent_ppo2.py:179][0m |          -0.0431 |           1.8006 |         -64.0890 |
[32m[20221208 14:48:35 @agent_ppo2.py:179][0m |          -0.0433 |           1.7120 |         -64.8012 |
[32m[20221208 14:48:35 @agent_ppo2.py:179][0m |          -0.0496 |           1.6224 |         -68.0390 |
[32m[20221208 14:48:35 @agent_ppo2.py:179][0m |          -0.0509 |           1.5945 |         -69.4862 |
[32m[20221208 14:48:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.47
[32m[20221208 14:48:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.76
[32m[20221208 14:48:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.73
[32m[20221208 14:48:35 @agent_ppo2.py:137][0m Total time:      12.02 min
[32m[20221208 14:48:35 @agent_ppo2.py:139][0m 968704 total steps have happened
[32m[20221208 14:48:35 @agent_ppo2.py:115][0m #------------------------ Iteration 473 --------------------------#
[32m[20221208 14:48:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |           0.0804 |           6.7564 |        -194.3045 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |           0.0677 |           5.6896 |        -158.9154 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |           0.0220 |           5.4602 |        -176.8506 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0046 |           5.3051 |        -197.7882 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0218 |           5.2199 |        -206.6390 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0328 |           5.1346 |        -211.6649 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0383 |           5.0942 |        -219.7573 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0467 |           4.9799 |        -223.1687 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0502 |           4.9435 |        -226.0852 |
[32m[20221208 14:48:36 @agent_ppo2.py:179][0m |          -0.0557 |           4.9148 |        -229.1451 |
[32m[20221208 14:48:36 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:48:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.22
[32m[20221208 14:48:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.64
[32m[20221208 14:48:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.59
[32m[20221208 14:48:37 @agent_ppo2.py:137][0m Total time:      12.04 min
[32m[20221208 14:48:37 @agent_ppo2.py:139][0m 970752 total steps have happened
[32m[20221208 14:48:37 @agent_ppo2.py:115][0m #------------------------ Iteration 474 --------------------------#
[32m[20221208 14:48:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:48:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:37 @agent_ppo2.py:179][0m |           0.0840 |           5.7019 |        -157.2409 |
[32m[20221208 14:48:37 @agent_ppo2.py:179][0m |           0.0576 |           4.9122 |        -118.2708 |
[32m[20221208 14:48:37 @agent_ppo2.py:179][0m |           0.0191 |           4.6271 |        -118.9547 |
[32m[20221208 14:48:37 @agent_ppo2.py:179][0m |          -0.0210 |           4.4941 |        -137.4883 |
[32m[20221208 14:48:38 @agent_ppo2.py:179][0m |          -0.0395 |           4.3903 |        -144.9531 |
[32m[20221208 14:48:38 @agent_ppo2.py:179][0m |          -0.0513 |           4.2694 |        -153.1352 |
[32m[20221208 14:48:38 @agent_ppo2.py:179][0m |          -0.0589 |           4.2231 |        -160.5772 |
[32m[20221208 14:48:38 @agent_ppo2.py:179][0m |          -0.0593 |           4.0619 |        -161.1003 |
[32m[20221208 14:48:38 @agent_ppo2.py:179][0m |          -0.0665 |           4.0091 |        -168.6504 |
[32m[20221208 14:48:38 @agent_ppo2.py:179][0m |          -0.0715 |           3.9527 |        -170.0921 |
[32m[20221208 14:48:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.75
[32m[20221208 14:48:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.02
[32m[20221208 14:48:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.73
[32m[20221208 14:48:38 @agent_ppo2.py:137][0m Total time:      12.07 min
[32m[20221208 14:48:38 @agent_ppo2.py:139][0m 972800 total steps have happened
[32m[20221208 14:48:38 @agent_ppo2.py:115][0m #------------------------ Iteration 475 --------------------------#
[32m[20221208 14:48:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |           0.0398 |           1.7937 |        -216.1462 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0054 |           1.1070 |        -216.1675 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0180 |           0.9789 |        -224.2868 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0205 |           0.9021 |        -223.6891 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0245 |           0.8474 |        -227.1882 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0222 |           0.8181 |        -226.6938 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0269 |           0.7859 |        -230.9027 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0211 |           0.7515 |        -225.6134 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0237 |           0.7364 |        -219.2872 |
[32m[20221208 14:48:39 @agent_ppo2.py:179][0m |          -0.0236 |           0.7212 |        -229.4078 |
[32m[20221208 14:48:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 29.74
[32m[20221208 14:48:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.91
[32m[20221208 14:48:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.05
[32m[20221208 14:48:40 @agent_ppo2.py:137][0m Total time:      12.09 min
[32m[20221208 14:48:40 @agent_ppo2.py:139][0m 974848 total steps have happened
[32m[20221208 14:48:40 @agent_ppo2.py:115][0m #------------------------ Iteration 476 --------------------------#
[32m[20221208 14:48:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:40 @agent_ppo2.py:179][0m |           0.0417 |           6.9583 |        -205.4064 |
[32m[20221208 14:48:40 @agent_ppo2.py:179][0m |           0.0931 |           5.9733 |        -168.3104 |
[32m[20221208 14:48:40 @agent_ppo2.py:179][0m |           0.0579 |           5.7851 |        -140.8786 |
[32m[20221208 14:48:40 @agent_ppo2.py:179][0m |           0.0030 |           5.6584 |        -141.6723 |
[32m[20221208 14:48:40 @agent_ppo2.py:179][0m |          -0.0352 |           5.5669 |        -140.4711 |
[32m[20221208 14:48:41 @agent_ppo2.py:179][0m |          -0.0517 |           5.4956 |        -147.1323 |
[32m[20221208 14:48:41 @agent_ppo2.py:179][0m |          -0.0643 |           5.4292 |        -154.7420 |
[32m[20221208 14:48:41 @agent_ppo2.py:179][0m |          -0.0664 |           5.3857 |        -156.4230 |
[32m[20221208 14:48:41 @agent_ppo2.py:179][0m |          -0.0793 |           5.2859 |        -168.8230 |
[32m[20221208 14:48:41 @agent_ppo2.py:179][0m |          -0.0814 |           5.2718 |        -169.0632 |
[32m[20221208 14:48:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:48:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.41
[32m[20221208 14:48:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.33
[32m[20221208 14:48:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.27
[32m[20221208 14:48:41 @agent_ppo2.py:137][0m Total time:      12.12 min
[32m[20221208 14:48:41 @agent_ppo2.py:139][0m 976896 total steps have happened
[32m[20221208 14:48:41 @agent_ppo2.py:115][0m #------------------------ Iteration 477 --------------------------#
[32m[20221208 14:48:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.1148 |           1.7649 |        -174.4563 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0639 |           1.2760 |         -91.1459 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0599 |           1.1581 |         -85.3212 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0459 |           1.1214 |         -92.4652 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0136 |           1.0775 |        -135.3860 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0018 |           1.0627 |        -151.1929 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |          -0.0050 |           1.0497 |        -154.4911 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0961 |           1.0648 |        -117.7168 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |           0.0457 |           1.0468 |         -92.4230 |
[32m[20221208 14:48:42 @agent_ppo2.py:179][0m |          -0.0019 |           1.0366 |        -143.8807 |
[32m[20221208 14:48:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:48:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.67
[32m[20221208 14:48:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 29.54
[32m[20221208 14:48:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.20
[32m[20221208 14:48:43 @agent_ppo2.py:137][0m Total time:      12.14 min
[32m[20221208 14:48:43 @agent_ppo2.py:139][0m 978944 total steps have happened
[32m[20221208 14:48:43 @agent_ppo2.py:115][0m #------------------------ Iteration 478 --------------------------#
[32m[20221208 14:48:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:43 @agent_ppo2.py:179][0m |           0.0936 |           5.9878 |        -233.3267 |
[32m[20221208 14:48:43 @agent_ppo2.py:179][0m |           0.0535 |           4.9592 |        -195.3983 |
[32m[20221208 14:48:43 @agent_ppo2.py:179][0m |          -0.0144 |           4.7533 |        -217.5593 |
[32m[20221208 14:48:43 @agent_ppo2.py:179][0m |          -0.0425 |           4.6357 |        -234.6765 |
[32m[20221208 14:48:43 @agent_ppo2.py:179][0m |          -0.0524 |           4.5293 |        -238.4865 |
[32m[20221208 14:48:44 @agent_ppo2.py:179][0m |          -0.0625 |           4.4582 |        -241.5862 |
[32m[20221208 14:48:44 @agent_ppo2.py:179][0m |          -0.0692 |           4.3752 |        -245.2159 |
[32m[20221208 14:48:44 @agent_ppo2.py:179][0m |          -0.0769 |           4.3126 |        -249.3750 |
[32m[20221208 14:48:44 @agent_ppo2.py:179][0m |          -0.0846 |           4.2839 |        -253.7036 |
[32m[20221208 14:48:44 @agent_ppo2.py:179][0m |          -0.0861 |           4.1792 |        -255.3800 |
[32m[20221208 14:48:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.88
[32m[20221208 14:48:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.88
[32m[20221208 14:48:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.14
[32m[20221208 14:48:44 @agent_ppo2.py:137][0m Total time:      12.17 min
[32m[20221208 14:48:44 @agent_ppo2.py:139][0m 980992 total steps have happened
[32m[20221208 14:48:44 @agent_ppo2.py:115][0m #------------------------ Iteration 479 --------------------------#
[32m[20221208 14:48:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.0778 |           0.9990 |        -237.8925 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1203 |           0.8434 |        -169.2472 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1484 |           0.8264 |         -59.2669 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1174 |           0.7981 |        -117.4716 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.0735 |           0.8045 |        -185.4487 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.0636 |           0.7881 |        -210.5041 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1595 |           0.7834 |         -81.8894 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1884 |           0.7819 |           0.5805 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1660 |           0.7781 |           0.4269 |
[32m[20221208 14:48:45 @agent_ppo2.py:179][0m |           0.1684 |           0.7810 |         -16.9874 |
[32m[20221208 14:48:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.94
[32m[20221208 14:48:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.16
[32m[20221208 14:48:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.30
[32m[20221208 14:48:46 @agent_ppo2.py:137][0m Total time:      12.19 min
[32m[20221208 14:48:46 @agent_ppo2.py:139][0m 983040 total steps have happened
[32m[20221208 14:48:46 @agent_ppo2.py:115][0m #------------------------ Iteration 480 --------------------------#
[32m[20221208 14:48:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:48:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:46 @agent_ppo2.py:179][0m |           0.0664 |           1.7803 |        -210.5601 |
[32m[20221208 14:48:46 @agent_ppo2.py:179][0m |           0.1204 |           1.4843 |        -103.3189 |
[32m[20221208 14:48:46 @agent_ppo2.py:179][0m |           0.0702 |           1.3560 |         -72.1546 |
[32m[20221208 14:48:46 @agent_ppo2.py:179][0m |           0.0291 |           1.2866 |        -111.8823 |
[32m[20221208 14:48:46 @agent_ppo2.py:179][0m |           0.0050 |           1.2310 |        -139.8407 |
[32m[20221208 14:48:47 @agent_ppo2.py:179][0m |          -0.0046 |           1.1767 |        -151.0177 |
[32m[20221208 14:48:47 @agent_ppo2.py:179][0m |          -0.0161 |           1.1386 |        -155.4423 |
[32m[20221208 14:48:47 @agent_ppo2.py:179][0m |          -0.0192 |           1.1166 |        -161.6294 |
[32m[20221208 14:48:47 @agent_ppo2.py:179][0m |          -0.0212 |           1.0841 |        -164.1372 |
[32m[20221208 14:48:47 @agent_ppo2.py:179][0m |          -0.0064 |           1.0640 |        -144.1908 |
[32m[20221208 14:48:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.98
[32m[20221208 14:48:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.70
[32m[20221208 14:48:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.62
[32m[20221208 14:48:47 @agent_ppo2.py:137][0m Total time:      12.21 min
[32m[20221208 14:48:47 @agent_ppo2.py:139][0m 985088 total steps have happened
[32m[20221208 14:48:47 @agent_ppo2.py:115][0m #------------------------ Iteration 481 --------------------------#
[32m[20221208 14:48:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |           0.0563 |           2.6322 |        -168.9413 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |           0.0119 |           2.2478 |        -150.2348 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0029 |           2.1646 |        -148.4171 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0084 |           2.1267 |        -150.8650 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0106 |           2.0901 |        -151.7295 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0255 |           2.0853 |        -157.5889 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0247 |           2.0908 |        -156.5992 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0315 |           2.0706 |        -162.9798 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0371 |           2.0567 |        -166.0419 |
[32m[20221208 14:48:48 @agent_ppo2.py:179][0m |          -0.0349 |           2.0261 |        -164.6683 |
[32m[20221208 14:48:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.66
[32m[20221208 14:48:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.24
[32m[20221208 14:48:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.00
[32m[20221208 14:48:49 @agent_ppo2.py:137][0m Total time:      12.24 min
[32m[20221208 14:48:49 @agent_ppo2.py:139][0m 987136 total steps have happened
[32m[20221208 14:48:49 @agent_ppo2.py:115][0m #------------------------ Iteration 482 --------------------------#
[32m[20221208 14:48:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:48:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:49 @agent_ppo2.py:179][0m |           0.0636 |           1.6126 |        -242.4573 |
[32m[20221208 14:48:49 @agent_ppo2.py:179][0m |           0.0227 |           1.2139 |        -193.4968 |
[32m[20221208 14:48:49 @agent_ppo2.py:179][0m |           0.0067 |           1.0967 |        -229.1153 |
[32m[20221208 14:48:49 @agent_ppo2.py:179][0m |           0.0039 |           1.0413 |        -202.6173 |
[32m[20221208 14:48:49 @agent_ppo2.py:179][0m |          -0.0185 |           0.9958 |        -153.1007 |
[32m[20221208 14:48:49 @agent_ppo2.py:179][0m |          -0.0311 |           0.9685 |        -157.1004 |
[32m[20221208 14:48:50 @agent_ppo2.py:179][0m |          -0.0383 |           0.9435 |        -158.9859 |
[32m[20221208 14:48:50 @agent_ppo2.py:179][0m |          -0.0477 |           0.9274 |        -160.4541 |
[32m[20221208 14:48:50 @agent_ppo2.py:179][0m |          -0.0503 |           0.9016 |        -164.9438 |
[32m[20221208 14:48:50 @agent_ppo2.py:179][0m |          -0.0523 |           0.8936 |        -164.1877 |
[32m[20221208 14:48:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.83
[32m[20221208 14:48:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.46
[32m[20221208 14:48:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.08
[32m[20221208 14:48:50 @agent_ppo2.py:137][0m Total time:      12.26 min
[32m[20221208 14:48:50 @agent_ppo2.py:139][0m 989184 total steps have happened
[32m[20221208 14:48:50 @agent_ppo2.py:115][0m #------------------------ Iteration 483 --------------------------#
[32m[20221208 14:48:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |           0.1223 |           4.5926 |        -225.2673 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |           0.1115 |           4.0372 |        -172.8909 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |           0.0301 |           3.8587 |        -185.7817 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0164 |           3.6576 |        -207.7413 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0415 |           3.5018 |        -217.9021 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0513 |           3.4255 |        -220.8509 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0611 |           3.3514 |        -227.1455 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0692 |           3.2940 |        -230.1805 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0739 |           3.1883 |        -240.0636 |
[32m[20221208 14:48:51 @agent_ppo2.py:179][0m |          -0.0799 |           3.1323 |        -241.3732 |
[32m[20221208 14:48:51 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.22
[32m[20221208 14:48:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.29
[32m[20221208 14:48:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.81
[32m[20221208 14:48:52 @agent_ppo2.py:137][0m Total time:      12.29 min
[32m[20221208 14:48:52 @agent_ppo2.py:139][0m 991232 total steps have happened
[32m[20221208 14:48:52 @agent_ppo2.py:115][0m #------------------------ Iteration 484 --------------------------#
[32m[20221208 14:48:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:52 @agent_ppo2.py:179][0m |           0.0649 |           1.7191 |        -207.8524 |
[32m[20221208 14:48:52 @agent_ppo2.py:179][0m |           0.0381 |           1.4696 |        -181.0645 |
[32m[20221208 14:48:52 @agent_ppo2.py:179][0m |           0.0137 |           1.3542 |        -160.7054 |
[32m[20221208 14:48:52 @agent_ppo2.py:179][0m |          -0.0164 |           1.2803 |        -165.5494 |
[32m[20221208 14:48:52 @agent_ppo2.py:179][0m |          -0.0302 |           1.2284 |        -171.2457 |
[32m[20221208 14:48:52 @agent_ppo2.py:179][0m |          -0.0408 |           1.2026 |        -172.5871 |
[32m[20221208 14:48:53 @agent_ppo2.py:179][0m |          -0.0458 |           1.1740 |        -178.4474 |
[32m[20221208 14:48:53 @agent_ppo2.py:179][0m |          -0.0481 |           1.1491 |        -174.1342 |
[32m[20221208 14:48:53 @agent_ppo2.py:179][0m |          -0.0468 |           1.1193 |        -178.6075 |
[32m[20221208 14:48:53 @agent_ppo2.py:179][0m |          -0.0501 |           1.1031 |        -179.0217 |
[32m[20221208 14:48:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.87
[32m[20221208 14:48:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.58
[32m[20221208 14:48:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.29
[32m[20221208 14:48:53 @agent_ppo2.py:137][0m Total time:      12.31 min
[32m[20221208 14:48:53 @agent_ppo2.py:139][0m 993280 total steps have happened
[32m[20221208 14:48:53 @agent_ppo2.py:115][0m #------------------------ Iteration 485 --------------------------#
[32m[20221208 14:48:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |           0.1376 |           2.8615 |        -245.5313 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |           0.0995 |           2.4681 |         -84.6512 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |           0.0346 |           2.3079 |         -73.7231 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |           0.0076 |           2.1519 |        -100.5733 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |          -0.0115 |           2.0360 |        -115.5848 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |          -0.0228 |           1.9613 |        -136.0625 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |          -0.0297 |           1.9007 |        -119.7231 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |          -0.0358 |           1.8311 |        -135.9845 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |          -0.0373 |           1.7730 |        -162.9276 |
[32m[20221208 14:48:54 @agent_ppo2.py:179][0m |          -0.0397 |           1.7342 |        -169.3609 |
[32m[20221208 14:48:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.92
[32m[20221208 14:48:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.58
[32m[20221208 14:48:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.80
[32m[20221208 14:48:55 @agent_ppo2.py:137][0m Total time:      12.34 min
[32m[20221208 14:48:55 @agent_ppo2.py:139][0m 995328 total steps have happened
[32m[20221208 14:48:55 @agent_ppo2.py:115][0m #------------------------ Iteration 486 --------------------------#
[32m[20221208 14:48:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:48:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |           0.0734 |           3.5622 |        -237.2478 |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |           0.0533 |           3.0712 |        -215.7144 |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |           0.0064 |           2.9587 |        -226.0237 |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |          -0.0245 |           2.7949 |        -230.0456 |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |          -0.0383 |           2.6901 |        -230.9808 |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |          -0.0524 |           2.6313 |        -234.2468 |
[32m[20221208 14:48:55 @agent_ppo2.py:179][0m |          -0.0585 |           2.5744 |        -230.5517 |
[32m[20221208 14:48:56 @agent_ppo2.py:179][0m |          -0.0635 |           2.5501 |        -229.8192 |
[32m[20221208 14:48:56 @agent_ppo2.py:179][0m |          -0.0590 |           2.5031 |        -219.3555 |
[32m[20221208 14:48:56 @agent_ppo2.py:179][0m |          -0.0707 |           2.4836 |        -226.2524 |
[32m[20221208 14:48:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:48:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.66
[32m[20221208 14:48:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.05
[32m[20221208 14:48:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.34
[32m[20221208 14:48:56 @agent_ppo2.py:137][0m Total time:      12.36 min
[32m[20221208 14:48:56 @agent_ppo2.py:139][0m 997376 total steps have happened
[32m[20221208 14:48:56 @agent_ppo2.py:115][0m #------------------------ Iteration 487 --------------------------#
[32m[20221208 14:48:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:48:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |           0.0543 |           1.5016 |        -234.3420 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |           0.0234 |           1.1701 |        -226.5457 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |           0.0036 |           1.0342 |        -234.9217 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |           0.0012 |           0.9612 |        -228.2591 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |          -0.0135 |           0.9079 |        -238.3817 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |          -0.0117 |           0.8772 |        -234.3057 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |           0.0298 |           0.8493 |        -191.1243 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |          -0.0148 |           0.8299 |        -222.1269 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |          -0.0197 |           0.8110 |        -238.8667 |
[32m[20221208 14:48:57 @agent_ppo2.py:179][0m |          -0.0272 |           0.8035 |        -237.5143 |
[32m[20221208 14:48:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:48:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 33.33
[32m[20221208 14:48:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.85
[32m[20221208 14:48:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.77
[32m[20221208 14:48:58 @agent_ppo2.py:137][0m Total time:      12.39 min
[32m[20221208 14:48:58 @agent_ppo2.py:139][0m 999424 total steps have happened
[32m[20221208 14:48:58 @agent_ppo2.py:115][0m #------------------------ Iteration 488 --------------------------#
[32m[20221208 14:48:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:48:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.1064 |           1.2187 |        -178.2082 |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.0982 |           1.0325 |        -116.5372 |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.0452 |           0.9588 |        -200.5164 |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.0378 |           0.8850 |        -182.0632 |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.0225 |           0.8411 |        -192.6620 |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.0035 |           0.7898 |        -233.4976 |
[32m[20221208 14:48:58 @agent_ppo2.py:179][0m |           0.0621 |           0.7628 |        -192.1193 |
[32m[20221208 14:48:59 @agent_ppo2.py:179][0m |           0.0266 |           0.7364 |        -228.8484 |
[32m[20221208 14:48:59 @agent_ppo2.py:179][0m |          -0.0169 |           0.7286 |        -251.6484 |
[32m[20221208 14:48:59 @agent_ppo2.py:179][0m |          -0.0111 |           0.6927 |        -249.7345 |
[32m[20221208 14:48:59 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:48:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 29.36
[32m[20221208 14:48:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 39.87
[32m[20221208 14:48:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.22
[32m[20221208 14:48:59 @agent_ppo2.py:137][0m Total time:      12.41 min
[32m[20221208 14:48:59 @agent_ppo2.py:139][0m 1001472 total steps have happened
[32m[20221208 14:48:59 @agent_ppo2.py:115][0m #------------------------ Iteration 489 --------------------------#
[32m[20221208 14:48:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |           0.0597 |           1.0693 |        -151.7084 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |           0.0289 |           0.8952 |         -88.1430 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0007 |           0.8752 |        -136.7674 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0073 |           0.8905 |        -153.2613 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0227 |           0.8485 |        -162.7503 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0180 |           0.8335 |        -161.4201 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0227 |           0.8400 |        -167.6286 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0167 |           0.8403 |        -169.2673 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |          -0.0060 |           0.8371 |        -154.7064 |
[32m[20221208 14:49:00 @agent_ppo2.py:179][0m |           0.0141 |           0.8269 |        -130.4127 |
[32m[20221208 14:49:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 20.16
[32m[20221208 14:49:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.75
[32m[20221208 14:49:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.53
[32m[20221208 14:49:01 @agent_ppo2.py:137][0m Total time:      12.44 min
[32m[20221208 14:49:01 @agent_ppo2.py:139][0m 1003520 total steps have happened
[32m[20221208 14:49:01 @agent_ppo2.py:115][0m #------------------------ Iteration 490 --------------------------#
[32m[20221208 14:49:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |           0.0613 |           3.0624 |        -176.6508 |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |           0.0339 |           2.3027 |        -144.4647 |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |          -0.0072 |           2.1345 |        -160.3535 |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |          -0.0315 |           2.0766 |        -170.0582 |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |          -0.0437 |           2.0274 |        -170.6868 |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |          -0.0574 |           1.9862 |        -182.2004 |
[32m[20221208 14:49:01 @agent_ppo2.py:179][0m |          -0.0626 |           1.9454 |        -182.4322 |
[32m[20221208 14:49:02 @agent_ppo2.py:179][0m |          -0.0696 |           1.8967 |        -188.7319 |
[32m[20221208 14:49:02 @agent_ppo2.py:179][0m |          -0.0715 |           1.8854 |        -192.0851 |
[32m[20221208 14:49:02 @agent_ppo2.py:179][0m |          -0.0729 |           1.8393 |        -190.7792 |
[32m[20221208 14:49:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.14
[32m[20221208 14:49:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.46
[32m[20221208 14:49:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.63
[32m[20221208 14:49:02 @agent_ppo2.py:137][0m Total time:      12.46 min
[32m[20221208 14:49:02 @agent_ppo2.py:139][0m 1005568 total steps have happened
[32m[20221208 14:49:02 @agent_ppo2.py:115][0m #------------------------ Iteration 491 --------------------------#
[32m[20221208 14:49:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |           0.0427 |           1.1633 |        -289.1202 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |           0.0344 |           0.9394 |        -266.5575 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |           0.0095 |           0.8309 |        -259.6145 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0159 |           0.7605 |        -271.9110 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0301 |           0.7114 |        -285.7182 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0316 |           0.6793 |        -283.6329 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0364 |           0.6489 |        -276.9756 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0410 |           0.6239 |        -288.7698 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0406 |           0.6050 |        -289.1590 |
[32m[20221208 14:49:03 @agent_ppo2.py:179][0m |          -0.0428 |           0.5940 |        -295.2601 |
[32m[20221208 14:49:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.49
[32m[20221208 14:49:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.90
[32m[20221208 14:49:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.57
[32m[20221208 14:49:04 @agent_ppo2.py:137][0m Total time:      12.49 min
[32m[20221208 14:49:04 @agent_ppo2.py:139][0m 1007616 total steps have happened
[32m[20221208 14:49:04 @agent_ppo2.py:115][0m #------------------------ Iteration 492 --------------------------#
[32m[20221208 14:49:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |           0.1433 |           4.1830 |        -226.9760 |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |           0.0508 |           3.0824 |        -196.0510 |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |           0.0085 |           2.8821 |        -206.0943 |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |          -0.0119 |           2.8085 |        -211.8289 |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |          -0.0307 |           2.6766 |        -224.8263 |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |          -0.0424 |           2.6080 |        -235.3215 |
[32m[20221208 14:49:04 @agent_ppo2.py:179][0m |          -0.0501 |           2.5636 |        -236.3420 |
[32m[20221208 14:49:05 @agent_ppo2.py:179][0m |          -0.0556 |           2.4979 |        -240.1898 |
[32m[20221208 14:49:05 @agent_ppo2.py:179][0m |          -0.0611 |           2.4757 |        -239.1148 |
[32m[20221208 14:49:05 @agent_ppo2.py:179][0m |          -0.0619 |           2.3901 |        -245.9658 |
[32m[20221208 14:49:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.35
[32m[20221208 14:49:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.90
[32m[20221208 14:49:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.27
[32m[20221208 14:49:05 @agent_ppo2.py:137][0m Total time:      12.51 min
[32m[20221208 14:49:05 @agent_ppo2.py:139][0m 1009664 total steps have happened
[32m[20221208 14:49:05 @agent_ppo2.py:115][0m #------------------------ Iteration 493 --------------------------#
[32m[20221208 14:49:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0602 |           2.5165 |        -138.7032 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0317 |           2.2441 |        -105.7957 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0156 |           2.2153 |        -109.5932 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0121 |           2.2070 |        -112.3882 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0099 |           2.1568 |        -110.6626 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0068 |           2.1734 |        -112.4159 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0027 |           2.1424 |        -119.2770 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0066 |           2.1483 |        -113.8934 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0016 |           2.1707 |        -115.2563 |
[32m[20221208 14:49:06 @agent_ppo2.py:179][0m |           0.0022 |           2.1445 |        -113.0696 |
[32m[20221208 14:49:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:49:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.20
[32m[20221208 14:49:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.44
[32m[20221208 14:49:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.45
[32m[20221208 14:49:07 @agent_ppo2.py:137][0m Total time:      12.54 min
[32m[20221208 14:49:07 @agent_ppo2.py:139][0m 1011712 total steps have happened
[32m[20221208 14:49:07 @agent_ppo2.py:115][0m #------------------------ Iteration 494 --------------------------#
[32m[20221208 14:49:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |           0.0617 |           2.7431 |        -240.0764 |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |           0.0282 |           2.3144 |        -205.8981 |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |          -0.0096 |           2.1593 |        -213.2263 |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |          -0.0341 |           2.0752 |        -226.3263 |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |          -0.0458 |           2.0095 |        -223.1121 |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |          -0.0567 |           1.9485 |        -233.8545 |
[32m[20221208 14:49:07 @agent_ppo2.py:179][0m |          -0.0669 |           1.9243 |        -236.6147 |
[32m[20221208 14:49:08 @agent_ppo2.py:179][0m |          -0.0745 |           1.8705 |        -242.4906 |
[32m[20221208 14:49:08 @agent_ppo2.py:179][0m |          -0.0721 |           1.8872 |        -242.6859 |
[32m[20221208 14:49:08 @agent_ppo2.py:179][0m |          -0.0734 |           1.8438 |        -242.6192 |
[32m[20221208 14:49:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.60
[32m[20221208 14:49:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.88
[32m[20221208 14:49:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.42
[32m[20221208 14:49:08 @agent_ppo2.py:137][0m Total time:      12.56 min
[32m[20221208 14:49:08 @agent_ppo2.py:139][0m 1013760 total steps have happened
[32m[20221208 14:49:08 @agent_ppo2.py:115][0m #------------------------ Iteration 495 --------------------------#
[32m[20221208 14:49:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |           0.0769 |           2.7432 |        -270.9745 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |           0.0861 |           2.3901 |        -212.8376 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |           0.0755 |           2.2458 |        -186.1268 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |           0.0049 |           2.1199 |        -180.1431 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |          -0.0162 |           2.0673 |        -191.0221 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |          -0.0292 |           2.0122 |        -190.0188 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |          -0.0454 |           1.9335 |        -197.5803 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |          -0.0552 |           1.9173 |        -205.2190 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |          -0.0635 |           1.8731 |        -206.9265 |
[32m[20221208 14:49:09 @agent_ppo2.py:179][0m |          -0.0662 |           1.8351 |        -210.8470 |
[32m[20221208 14:49:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:49:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.44
[32m[20221208 14:49:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.25
[32m[20221208 14:49:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.32
[32m[20221208 14:49:10 @agent_ppo2.py:137][0m Total time:      12.59 min
[32m[20221208 14:49:10 @agent_ppo2.py:139][0m 1015808 total steps have happened
[32m[20221208 14:49:10 @agent_ppo2.py:115][0m #------------------------ Iteration 496 --------------------------#
[32m[20221208 14:49:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |           0.0716 |           0.6328 |        -219.6336 |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |           0.0814 |           0.5615 |        -136.9249 |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |           0.0045 |           0.5405 |        -183.9442 |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |           0.0578 |           0.5319 |        -142.1409 |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |           0.0090 |           0.5254 |        -179.5882 |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |          -0.0183 |           0.5341 |        -195.8734 |
[32m[20221208 14:49:10 @agent_ppo2.py:179][0m |          -0.0193 |           0.5222 |        -193.6778 |
[32m[20221208 14:49:11 @agent_ppo2.py:179][0m |          -0.0164 |           0.5256 |        -199.2385 |
[32m[20221208 14:49:11 @agent_ppo2.py:179][0m |          -0.0108 |           0.5242 |        -197.1591 |
[32m[20221208 14:49:11 @agent_ppo2.py:179][0m |          -0.0234 |           0.5225 |        -200.9824 |
[32m[20221208 14:49:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.79
[32m[20221208 14:49:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.95
[32m[20221208 14:49:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.60
[32m[20221208 14:49:11 @agent_ppo2.py:137][0m Total time:      12.61 min
[32m[20221208 14:49:11 @agent_ppo2.py:139][0m 1017856 total steps have happened
[32m[20221208 14:49:11 @agent_ppo2.py:115][0m #------------------------ Iteration 497 --------------------------#
[32m[20221208 14:49:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |           0.0308 |           1.6148 |        -232.6620 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0162 |           1.2049 |        -211.3726 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0311 |           1.0528 |        -222.9426 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0262 |           0.9452 |        -217.3179 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0381 |           0.8923 |        -228.9999 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0372 |           0.8564 |        -230.2659 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0403 |           0.8452 |        -232.1726 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0443 |           0.8253 |        -234.6276 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0485 |           0.8132 |        -237.1427 |
[32m[20221208 14:49:12 @agent_ppo2.py:179][0m |          -0.0458 |           0.8125 |        -237.3414 |
[32m[20221208 14:49:12 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.64
[32m[20221208 14:49:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.85
[32m[20221208 14:49:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.24
[32m[20221208 14:49:13 @agent_ppo2.py:137][0m Total time:      12.64 min
[32m[20221208 14:49:13 @agent_ppo2.py:139][0m 1019904 total steps have happened
[32m[20221208 14:49:13 @agent_ppo2.py:115][0m #------------------------ Iteration 498 --------------------------#
[32m[20221208 14:49:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |           0.0885 |           1.5087 |        -293.1114 |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |           0.0588 |           1.1459 |        -238.4585 |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |           0.0054 |           0.9698 |        -248.1283 |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |          -0.0207 |           0.8955 |        -261.0740 |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |          -0.0306 |           0.8294 |        -265.9408 |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |          -0.0336 |           0.7942 |        -263.0493 |
[32m[20221208 14:49:13 @agent_ppo2.py:179][0m |          -0.0375 |           0.7588 |        -269.0557 |
[32m[20221208 14:49:14 @agent_ppo2.py:179][0m |          -0.0430 |           0.7395 |        -274.0535 |
[32m[20221208 14:49:14 @agent_ppo2.py:179][0m |          -0.0491 |           0.7105 |        -280.9767 |
[32m[20221208 14:49:14 @agent_ppo2.py:179][0m |          -0.0512 |           0.6867 |        -279.8204 |
[32m[20221208 14:49:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:49:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.60
[32m[20221208 14:49:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 39.75
[32m[20221208 14:49:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.71
[32m[20221208 14:49:14 @agent_ppo2.py:137][0m Total time:      12.66 min
[32m[20221208 14:49:14 @agent_ppo2.py:139][0m 1021952 total steps have happened
[32m[20221208 14:49:14 @agent_ppo2.py:115][0m #------------------------ Iteration 499 --------------------------#
[32m[20221208 14:49:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0247 |           0.5155 |        -297.0256 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0173 |           0.4115 |        -287.8275 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0149 |           0.3968 |        -278.1983 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0187 |           0.3871 |        -283.2159 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0026 |           0.3844 |        -288.5427 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |          -0.0017 |           0.3825 |        -297.4830 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0004 |           0.3802 |        -295.1398 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0388 |           0.3816 |        -247.3724 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0183 |           0.3770 |        -267.9823 |
[32m[20221208 14:49:15 @agent_ppo2.py:179][0m |           0.0040 |           0.3772 |        -281.0477 |
[32m[20221208 14:49:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.82
[32m[20221208 14:49:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.19
[32m[20221208 14:49:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.68
[32m[20221208 14:49:16 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 209.75
[32m[20221208 14:49:16 @agent_ppo2.py:137][0m Total time:      12.69 min
[32m[20221208 14:49:16 @agent_ppo2.py:139][0m 1024000 total steps have happened
[32m[20221208 14:49:16 @agent_ppo2.py:115][0m #------------------------ Iteration 500 --------------------------#
[32m[20221208 14:49:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |           0.0464 |           1.5045 |        -165.4399 |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |          -0.0089 |           1.3414 |        -159.4966 |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |          -0.0326 |           1.2622 |        -167.4044 |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |          -0.0498 |           1.2322 |        -174.8106 |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |          -0.0586 |           1.2221 |        -181.9317 |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |          -0.0638 |           1.1997 |        -186.6264 |
[32m[20221208 14:49:16 @agent_ppo2.py:179][0m |          -0.0687 |           1.1799 |        -189.1943 |
[32m[20221208 14:49:17 @agent_ppo2.py:179][0m |          -0.0704 |           1.1658 |        -189.7188 |
[32m[20221208 14:49:17 @agent_ppo2.py:179][0m |          -0.0738 |           1.1594 |        -191.7580 |
[32m[20221208 14:49:17 @agent_ppo2.py:179][0m |          -0.0720 |           1.1565 |        -196.0956 |
[32m[20221208 14:49:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.95
[32m[20221208 14:49:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.23
[32m[20221208 14:49:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.26
[32m[20221208 14:49:17 @agent_ppo2.py:137][0m Total time:      12.71 min
[32m[20221208 14:49:17 @agent_ppo2.py:139][0m 1026048 total steps have happened
[32m[20221208 14:49:17 @agent_ppo2.py:115][0m #------------------------ Iteration 501 --------------------------#
[32m[20221208 14:49:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |           0.1194 |           2.0266 |        -299.4705 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |           0.0885 |           1.6612 |        -255.7131 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |           0.0079 |           1.5759 |        -272.3658 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0227 |           1.5167 |        -304.2502 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0352 |           1.4859 |        -305.8934 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0476 |           1.4494 |        -309.4437 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0555 |           1.4282 |        -319.3479 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0580 |           1.4330 |        -325.4529 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0641 |           1.3869 |        -328.2815 |
[32m[20221208 14:49:18 @agent_ppo2.py:179][0m |          -0.0717 |           1.3686 |        -331.9418 |
[32m[20221208 14:49:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.46
[32m[20221208 14:49:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.70
[32m[20221208 14:49:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.35
[32m[20221208 14:49:19 @agent_ppo2.py:137][0m Total time:      12.74 min
[32m[20221208 14:49:19 @agent_ppo2.py:139][0m 1028096 total steps have happened
[32m[20221208 14:49:19 @agent_ppo2.py:115][0m #------------------------ Iteration 502 --------------------------#
[32m[20221208 14:49:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |           0.0756 |           0.5844 |        -308.7853 |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |           0.1004 |           0.4890 |        -253.0116 |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |           0.0096 |           0.4666 |        -289.0145 |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |          -0.0041 |           0.4493 |        -299.0988 |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |          -0.0077 |           0.4370 |        -292.1414 |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |          -0.0101 |           0.4293 |        -299.2810 |
[32m[20221208 14:49:19 @agent_ppo2.py:179][0m |          -0.0147 |           0.4219 |        -301.4005 |
[32m[20221208 14:49:20 @agent_ppo2.py:179][0m |          -0.0126 |           0.4220 |        -291.3407 |
[32m[20221208 14:49:20 @agent_ppo2.py:179][0m |          -0.0089 |           0.4180 |        -297.5027 |
[32m[20221208 14:49:20 @agent_ppo2.py:179][0m |          -0.0174 |           0.4140 |        -296.4174 |
[32m[20221208 14:49:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:49:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 26.30
[32m[20221208 14:49:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 33.14
[32m[20221208 14:49:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.73
[32m[20221208 14:49:20 @agent_ppo2.py:137][0m Total time:      12.76 min
[32m[20221208 14:49:20 @agent_ppo2.py:139][0m 1030144 total steps have happened
[32m[20221208 14:49:20 @agent_ppo2.py:115][0m #------------------------ Iteration 503 --------------------------#
[32m[20221208 14:49:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |           0.1258 |           0.9880 |        -270.5865 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |           0.1153 |           0.7905 |        -123.9595 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |           0.0705 |           0.7247 |        -128.3139 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |           0.0190 |           0.6830 |        -187.1868 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |          -0.0126 |           0.6505 |        -221.9922 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |          -0.0271 |           0.6353 |        -266.7071 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |          -0.0396 |           0.6114 |        -285.7733 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |          -0.0391 |           0.5985 |        -297.6263 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |          -0.0447 |           0.5861 |        -295.9245 |
[32m[20221208 14:49:21 @agent_ppo2.py:179][0m |          -0.0514 |           0.5690 |        -301.4958 |
[32m[20221208 14:49:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.17
[32m[20221208 14:49:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.05
[32m[20221208 14:49:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.26
[32m[20221208 14:49:22 @agent_ppo2.py:137][0m Total time:      12.79 min
[32m[20221208 14:49:22 @agent_ppo2.py:139][0m 1032192 total steps have happened
[32m[20221208 14:49:22 @agent_ppo2.py:115][0m #------------------------ Iteration 504 --------------------------#
[32m[20221208 14:49:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |           0.0613 |           1.4233 |        -281.8640 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |           0.0450 |           1.1956 |        -268.6244 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |          -0.0048 |           1.1160 |        -294.4427 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |          -0.0267 |           1.0529 |        -298.9876 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |          -0.0330 |           1.0068 |        -300.1203 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |          -0.0375 |           0.9751 |        -307.1699 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |          -0.0522 |           0.9370 |        -309.7318 |
[32m[20221208 14:49:22 @agent_ppo2.py:179][0m |          -0.0526 |           0.8987 |        -308.4103 |
[32m[20221208 14:49:23 @agent_ppo2.py:179][0m |          -0.0527 |           0.8761 |        -309.0143 |
[32m[20221208 14:49:23 @agent_ppo2.py:179][0m |          -0.0589 |           0.8582 |        -321.7316 |
[32m[20221208 14:49:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.20
[32m[20221208 14:49:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.03
[32m[20221208 14:49:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.64
[32m[20221208 14:49:23 @agent_ppo2.py:137][0m Total time:      12.81 min
[32m[20221208 14:49:23 @agent_ppo2.py:139][0m 1034240 total steps have happened
[32m[20221208 14:49:23 @agent_ppo2.py:115][0m #------------------------ Iteration 505 --------------------------#
[32m[20221208 14:49:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |           0.1398 |           2.8165 |        -229.6751 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |           0.0812 |           2.2351 |        -177.3904 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |           0.0051 |           2.0057 |        -216.5622 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0196 |           1.9618 |        -235.9416 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0382 |           1.8702 |        -247.9981 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0473 |           1.8041 |        -251.3363 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0564 |           1.7675 |        -253.6759 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0644 |           1.7278 |        -262.7133 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0724 |           1.7204 |        -265.5610 |
[32m[20221208 14:49:24 @agent_ppo2.py:179][0m |          -0.0706 |           1.6633 |        -269.0770 |
[32m[20221208 14:49:24 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:49:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.00
[32m[20221208 14:49:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.18
[32m[20221208 14:49:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.33
[32m[20221208 14:49:25 @agent_ppo2.py:137][0m Total time:      12.84 min
[32m[20221208 14:49:25 @agent_ppo2.py:139][0m 1036288 total steps have happened
[32m[20221208 14:49:25 @agent_ppo2.py:115][0m #------------------------ Iteration 506 --------------------------#
[32m[20221208 14:49:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:25 @agent_ppo2.py:179][0m |           0.0714 |           1.2107 |        -305.1192 |
[32m[20221208 14:49:25 @agent_ppo2.py:179][0m |           0.0191 |           0.9039 |        -280.5594 |
[32m[20221208 14:49:25 @agent_ppo2.py:179][0m |          -0.0007 |           0.8042 |        -289.6804 |
[32m[20221208 14:49:25 @agent_ppo2.py:179][0m |          -0.0350 |           0.7510 |        -311.1574 |
[32m[20221208 14:49:25 @agent_ppo2.py:179][0m |          -0.0448 |           0.7112 |        -317.8575 |
[32m[20221208 14:49:25 @agent_ppo2.py:179][0m |          -0.0532 |           0.6851 |        -318.7521 |
[32m[20221208 14:49:26 @agent_ppo2.py:179][0m |          -0.0613 |           0.6657 |        -328.8533 |
[32m[20221208 14:49:26 @agent_ppo2.py:179][0m |          -0.0638 |           0.6507 |        -334.4851 |
[32m[20221208 14:49:26 @agent_ppo2.py:179][0m |          -0.0599 |           0.6408 |        -328.5999 |
[32m[20221208 14:49:26 @agent_ppo2.py:179][0m |          -0.0639 |           0.6300 |        -329.3236 |
[32m[20221208 14:49:26 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:49:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.78
[32m[20221208 14:49:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.60
[32m[20221208 14:49:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.60
[32m[20221208 14:49:26 @agent_ppo2.py:137][0m Total time:      12.86 min
[32m[20221208 14:49:26 @agent_ppo2.py:139][0m 1038336 total steps have happened
[32m[20221208 14:49:26 @agent_ppo2.py:115][0m #------------------------ Iteration 507 --------------------------#
[32m[20221208 14:49:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |           0.1065 |           3.5461 |        -263.6815 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |           0.0241 |           2.8549 |        -198.3738 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0274 |           2.5854 |        -217.3550 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0542 |           2.4129 |        -227.9530 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0618 |           2.3026 |        -233.7339 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0681 |           2.2161 |        -235.7599 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0790 |           2.1602 |        -248.7499 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0806 |           2.0778 |        -251.1518 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0891 |           2.0520 |        -261.8203 |
[32m[20221208 14:49:27 @agent_ppo2.py:179][0m |          -0.0901 |           2.0002 |        -266.3902 |
[32m[20221208 14:49:27 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:49:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.20
[32m[20221208 14:49:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.06
[32m[20221208 14:49:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.11
[32m[20221208 14:49:28 @agent_ppo2.py:137][0m Total time:      12.89 min
[32m[20221208 14:49:28 @agent_ppo2.py:139][0m 1040384 total steps have happened
[32m[20221208 14:49:28 @agent_ppo2.py:115][0m #------------------------ Iteration 508 --------------------------#
[32m[20221208 14:49:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:28 @agent_ppo2.py:179][0m |           0.0937 |           2.6797 |        -302.4049 |
[32m[20221208 14:49:28 @agent_ppo2.py:179][0m |           0.0727 |           2.3287 |        -278.4351 |
[32m[20221208 14:49:28 @agent_ppo2.py:179][0m |           0.0004 |           2.1561 |        -291.4060 |
[32m[20221208 14:49:28 @agent_ppo2.py:179][0m |          -0.0305 |           2.0373 |        -311.0164 |
[32m[20221208 14:49:29 @agent_ppo2.py:179][0m |          -0.0409 |           1.9463 |        -317.2453 |
[32m[20221208 14:49:29 @agent_ppo2.py:179][0m |          -0.0544 |           1.8774 |        -322.7822 |
[32m[20221208 14:49:29 @agent_ppo2.py:179][0m |          -0.0640 |           1.8076 |        -326.8859 |
[32m[20221208 14:49:29 @agent_ppo2.py:179][0m |          -0.0664 |           1.7413 |        -329.7132 |
[32m[20221208 14:49:29 @agent_ppo2.py:179][0m |          -0.0745 |           1.7046 |        -330.1858 |
[32m[20221208 14:49:29 @agent_ppo2.py:179][0m |          -0.0769 |           1.6410 |        -334.8177 |
[32m[20221208 14:49:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:49:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.42
[32m[20221208 14:49:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.58
[32m[20221208 14:49:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.77
[32m[20221208 14:49:29 @agent_ppo2.py:137][0m Total time:      12.92 min
[32m[20221208 14:49:29 @agent_ppo2.py:139][0m 1042432 total steps have happened
[32m[20221208 14:49:29 @agent_ppo2.py:115][0m #------------------------ Iteration 509 --------------------------#
[32m[20221208 14:49:30 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:49:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |           0.1034 |           1.2415 |        -219.5159 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |           0.0757 |           1.1614 |        -145.1033 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |           0.0372 |           1.1654 |        -113.9630 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |           0.0136 |           1.1576 |        -127.3333 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |          -0.0127 |           1.1599 |        -142.9238 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |          -0.0163 |           1.1390 |        -145.3833 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |          -0.0130 |           1.1466 |        -144.1110 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |          -0.0122 |           1.1505 |        -144.1224 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |          -0.0164 |           1.1356 |        -149.4730 |
[32m[20221208 14:49:30 @agent_ppo2.py:179][0m |          -0.0171 |           1.1277 |        -150.0283 |
[32m[20221208 14:49:30 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:49:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.02
[32m[20221208 14:49:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.69
[32m[20221208 14:49:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.90
[32m[20221208 14:49:31 @agent_ppo2.py:137][0m Total time:      12.94 min
[32m[20221208 14:49:31 @agent_ppo2.py:139][0m 1044480 total steps have happened
[32m[20221208 14:49:31 @agent_ppo2.py:115][0m #------------------------ Iteration 510 --------------------------#
[32m[20221208 14:49:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:31 @agent_ppo2.py:179][0m |           0.0949 |           1.6235 |        -276.8372 |
[32m[20221208 14:49:31 @agent_ppo2.py:179][0m |           0.0747 |           1.3889 |        -137.5819 |
[32m[20221208 14:49:31 @agent_ppo2.py:179][0m |          -0.0070 |           1.2852 |        -134.3088 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0406 |           1.2286 |        -140.8505 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0553 |           1.1692 |        -152.5143 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0656 |           1.1446 |        -151.8997 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0775 |           1.1294 |        -158.1536 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0805 |           1.0960 |        -162.8154 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0831 |           1.0761 |        -165.5181 |
[32m[20221208 14:49:32 @agent_ppo2.py:179][0m |          -0.0873 |           1.0587 |        -171.9078 |
[32m[20221208 14:49:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.75
[32m[20221208 14:49:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.97
[32m[20221208 14:49:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 26.31
[32m[20221208 14:49:32 @agent_ppo2.py:137][0m Total time:      12.97 min
[32m[20221208 14:49:32 @agent_ppo2.py:139][0m 1046528 total steps have happened
[32m[20221208 14:49:32 @agent_ppo2.py:115][0m #------------------------ Iteration 511 --------------------------#
[32m[20221208 14:49:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |           0.0700 |           2.5893 |        -340.0308 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |           0.0582 |           2.0314 |        -304.5511 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0002 |           1.8763 |        -317.9740 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0219 |           1.7839 |        -332.3813 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0391 |           1.7302 |        -342.3265 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0489 |           1.6973 |        -346.4022 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0513 |           1.6476 |        -353.0887 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0539 |           1.5966 |        -356.1572 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0575 |           1.5994 |        -353.4692 |
[32m[20221208 14:49:33 @agent_ppo2.py:179][0m |          -0.0552 |           1.5260 |        -358.9627 |
[32m[20221208 14:49:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.20
[32m[20221208 14:49:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.96
[32m[20221208 14:49:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.69
[32m[20221208 14:49:34 @agent_ppo2.py:137][0m Total time:      12.99 min
[32m[20221208 14:49:34 @agent_ppo2.py:139][0m 1048576 total steps have happened
[32m[20221208 14:49:34 @agent_ppo2.py:115][0m #------------------------ Iteration 512 --------------------------#
[32m[20221208 14:49:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:34 @agent_ppo2.py:179][0m |           0.0575 |           2.7277 |        -334.2841 |
[32m[20221208 14:49:34 @agent_ppo2.py:179][0m |           0.0667 |           2.0235 |        -294.2703 |
[32m[20221208 14:49:34 @agent_ppo2.py:179][0m |           0.0693 |           1.8869 |        -238.8489 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |           0.0085 |           1.7968 |        -188.3114 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |          -0.0263 |           1.7536 |        -200.7116 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |          -0.0501 |           1.7020 |        -213.7789 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |          -0.0647 |           1.6694 |        -220.1515 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |          -0.0678 |           1.6534 |        -224.1902 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |          -0.0749 |           1.6262 |        -233.7082 |
[32m[20221208 14:49:35 @agent_ppo2.py:179][0m |          -0.0770 |           1.5917 |        -235.0838 |
[32m[20221208 14:49:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.33
[32m[20221208 14:49:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.80
[32m[20221208 14:49:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.73
[32m[20221208 14:49:35 @agent_ppo2.py:137][0m Total time:      13.02 min
[32m[20221208 14:49:35 @agent_ppo2.py:139][0m 1050624 total steps have happened
[32m[20221208 14:49:35 @agent_ppo2.py:115][0m #------------------------ Iteration 513 --------------------------#
[32m[20221208 14:49:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |           0.0741 |           2.6123 |        -255.5739 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |           0.0297 |           2.1322 |        -154.4181 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0148 |           1.9663 |        -160.6337 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0395 |           1.8743 |        -175.7029 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0477 |           1.8207 |        -181.7382 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0594 |           1.7823 |        -192.6389 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0640 |           1.7564 |        -199.1490 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0683 |           1.7099 |        -203.1867 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0710 |           1.7034 |        -206.7286 |
[32m[20221208 14:49:36 @agent_ppo2.py:179][0m |          -0.0749 |           1.6833 |        -212.7559 |
[32m[20221208 14:49:36 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.36
[32m[20221208 14:49:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.96
[32m[20221208 14:49:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.38
[32m[20221208 14:49:37 @agent_ppo2.py:137][0m Total time:      13.04 min
[32m[20221208 14:49:37 @agent_ppo2.py:139][0m 1052672 total steps have happened
[32m[20221208 14:49:37 @agent_ppo2.py:115][0m #------------------------ Iteration 514 --------------------------#
[32m[20221208 14:49:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:37 @agent_ppo2.py:179][0m |           0.1697 |           3.1469 |        -292.5190 |
[32m[20221208 14:49:37 @agent_ppo2.py:179][0m |           0.1025 |           2.4475 |        -209.7291 |
[32m[20221208 14:49:37 @agent_ppo2.py:179][0m |           0.0375 |           2.2089 |        -261.5426 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0006 |           2.0829 |        -301.7114 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0220 |           1.9866 |        -314.3283 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0348 |           1.9050 |        -326.0918 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0447 |           1.8649 |        -333.7030 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0532 |           1.7962 |        -338.2980 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0586 |           1.7488 |        -344.6058 |
[32m[20221208 14:49:38 @agent_ppo2.py:179][0m |          -0.0599 |           1.7118 |        -346.1326 |
[32m[20221208 14:49:38 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.21
[32m[20221208 14:49:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.43
[32m[20221208 14:49:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.40
[32m[20221208 14:49:38 @agent_ppo2.py:137][0m Total time:      13.07 min
[32m[20221208 14:49:38 @agent_ppo2.py:139][0m 1054720 total steps have happened
[32m[20221208 14:49:38 @agent_ppo2.py:115][0m #------------------------ Iteration 515 --------------------------#
[32m[20221208 14:49:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |           0.4531 |           2.0800 |        -314.5896 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |           0.0404 |           1.3531 |        -236.4623 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0032 |           1.1964 |        -246.0348 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0274 |           1.1309 |        -262.9012 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0386 |           1.0757 |        -268.9760 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0452 |           1.0321 |        -273.1285 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0480 |           1.0191 |        -279.2888 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0562 |           0.9929 |        -283.6839 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0567 |           0.9820 |        -291.2871 |
[32m[20221208 14:49:39 @agent_ppo2.py:179][0m |          -0.0590 |           0.9682 |        -288.6486 |
[32m[20221208 14:49:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.90
[32m[20221208 14:49:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.91
[32m[20221208 14:49:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.86
[32m[20221208 14:49:40 @agent_ppo2.py:137][0m Total time:      13.09 min
[32m[20221208 14:49:40 @agent_ppo2.py:139][0m 1056768 total steps have happened
[32m[20221208 14:49:40 @agent_ppo2.py:115][0m #------------------------ Iteration 516 --------------------------#
[32m[20221208 14:49:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:40 @agent_ppo2.py:179][0m |           0.0891 |           1.6465 |        -317.7873 |
[32m[20221208 14:49:40 @agent_ppo2.py:179][0m |           0.0726 |           1.3574 |        -206.9508 |
[32m[20221208 14:49:40 @agent_ppo2.py:179][0m |           0.1316 |           1.2427 |        -215.9787 |
[32m[20221208 14:49:40 @agent_ppo2.py:179][0m |          -0.0026 |           1.1549 |        -222.7889 |
[32m[20221208 14:49:41 @agent_ppo2.py:179][0m |          -0.0300 |           1.0927 |        -238.6080 |
[32m[20221208 14:49:41 @agent_ppo2.py:179][0m |          -0.0413 |           1.0546 |        -246.4623 |
[32m[20221208 14:49:41 @agent_ppo2.py:179][0m |          -0.0464 |           1.0287 |        -252.3897 |
[32m[20221208 14:49:41 @agent_ppo2.py:179][0m |          -0.0538 |           0.9874 |        -261.8712 |
[32m[20221208 14:49:41 @agent_ppo2.py:179][0m |          -0.0569 |           0.9603 |        -261.5191 |
[32m[20221208 14:49:41 @agent_ppo2.py:179][0m |          -0.0619 |           0.9304 |        -263.8368 |
[32m[20221208 14:49:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.27
[32m[20221208 14:49:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.08
[32m[20221208 14:49:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.42
[32m[20221208 14:49:41 @agent_ppo2.py:137][0m Total time:      13.12 min
[32m[20221208 14:49:41 @agent_ppo2.py:139][0m 1058816 total steps have happened
[32m[20221208 14:49:41 @agent_ppo2.py:115][0m #------------------------ Iteration 517 --------------------------#
[32m[20221208 14:49:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |           0.0690 |           1.7718 |        -395.8386 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |           0.0486 |           1.4930 |        -355.2067 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0023 |           1.4036 |        -367.4314 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0216 |           1.3361 |        -380.2169 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0357 |           1.2989 |        -384.2108 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0412 |           1.2423 |        -392.4582 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0483 |           1.2210 |        -396.2050 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0497 |           1.1775 |        -402.4554 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0551 |           1.1527 |        -410.4372 |
[32m[20221208 14:49:42 @agent_ppo2.py:179][0m |          -0.0578 |           1.1290 |        -412.9937 |
[32m[20221208 14:49:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.09
[32m[20221208 14:49:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.79
[32m[20221208 14:49:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.20
[32m[20221208 14:49:43 @agent_ppo2.py:137][0m Total time:      13.14 min
[32m[20221208 14:49:43 @agent_ppo2.py:139][0m 1060864 total steps have happened
[32m[20221208 14:49:43 @agent_ppo2.py:115][0m #------------------------ Iteration 518 --------------------------#
[32m[20221208 14:49:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:43 @agent_ppo2.py:179][0m |           0.1234 |           2.7473 |        -305.2953 |
[32m[20221208 14:49:43 @agent_ppo2.py:179][0m |           0.0903 |           2.4194 |        -168.7611 |
[32m[20221208 14:49:43 @agent_ppo2.py:179][0m |           0.0374 |           2.3287 |        -249.3758 |
[32m[20221208 14:49:43 @agent_ppo2.py:179][0m |           0.0018 |           2.2173 |        -302.0185 |
[32m[20221208 14:49:44 @agent_ppo2.py:179][0m |          -0.0180 |           2.1681 |        -319.9631 |
[32m[20221208 14:49:44 @agent_ppo2.py:179][0m |          -0.0351 |           2.1321 |        -330.2741 |
[32m[20221208 14:49:44 @agent_ppo2.py:179][0m |          -0.0390 |           2.0900 |        -334.2218 |
[32m[20221208 14:49:44 @agent_ppo2.py:179][0m |          -0.0477 |           2.0560 |        -346.5121 |
[32m[20221208 14:49:44 @agent_ppo2.py:179][0m |          -0.0578 |           2.0656 |        -360.5314 |
[32m[20221208 14:49:44 @agent_ppo2.py:179][0m |          -0.0540 |           2.0196 |        -352.6735 |
[32m[20221208 14:49:44 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:49:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.72
[32m[20221208 14:49:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.45
[32m[20221208 14:49:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.44
[32m[20221208 14:49:44 @agent_ppo2.py:137][0m Total time:      13.17 min
[32m[20221208 14:49:44 @agent_ppo2.py:139][0m 1062912 total steps have happened
[32m[20221208 14:49:44 @agent_ppo2.py:115][0m #------------------------ Iteration 519 --------------------------#
[32m[20221208 14:49:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |           0.0727 |           2.3227 |        -391.8497 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |           0.0300 |           2.0365 |        -379.6955 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |           0.0128 |           1.9080 |        -369.2890 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0061 |           1.8483 |        -395.3317 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0271 |           1.7597 |        -413.4864 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0413 |           1.6827 |        -428.2923 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0473 |           1.6475 |        -426.2166 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0520 |           1.6203 |        -428.9377 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0561 |           1.5808 |        -428.8894 |
[32m[20221208 14:49:45 @agent_ppo2.py:179][0m |          -0.0604 |           1.5618 |        -434.3658 |
[32m[20221208 14:49:45 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.58
[32m[20221208 14:49:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.26
[32m[20221208 14:49:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 20.64
[32m[20221208 14:49:46 @agent_ppo2.py:137][0m Total time:      13.19 min
[32m[20221208 14:49:46 @agent_ppo2.py:139][0m 1064960 total steps have happened
[32m[20221208 14:49:46 @agent_ppo2.py:115][0m #------------------------ Iteration 520 --------------------------#
[32m[20221208 14:49:46 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:49:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:46 @agent_ppo2.py:179][0m |           0.1163 |           4.1627 |        -344.0394 |
[32m[20221208 14:49:46 @agent_ppo2.py:179][0m |           0.0607 |           3.5241 |        -298.4782 |
[32m[20221208 14:49:46 @agent_ppo2.py:179][0m |           0.0203 |           3.2820 |        -337.8175 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0160 |           3.1205 |        -337.1033 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0400 |           2.9970 |        -342.8920 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0481 |           2.9096 |        -359.0364 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0549 |           2.8410 |        -356.1551 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0589 |           2.7556 |        -357.9986 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0684 |           2.7664 |        -360.8830 |
[32m[20221208 14:49:47 @agent_ppo2.py:179][0m |          -0.0689 |           2.6375 |        -368.0162 |
[32m[20221208 14:49:47 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:49:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.01
[32m[20221208 14:49:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.64
[32m[20221208 14:49:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.79
[32m[20221208 14:49:47 @agent_ppo2.py:137][0m Total time:      13.22 min
[32m[20221208 14:49:47 @agent_ppo2.py:139][0m 1067008 total steps have happened
[32m[20221208 14:49:47 @agent_ppo2.py:115][0m #------------------------ Iteration 521 --------------------------#
[32m[20221208 14:49:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |           0.0787 |           3.2351 |        -357.4680 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |           0.0811 |           2.6347 |        -312.7679 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |           0.0173 |           2.4216 |        -324.8877 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0221 |           2.2729 |        -343.6176 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0423 |           2.1729 |        -370.8459 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0511 |           2.0660 |        -370.7625 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0495 |           1.9986 |        -365.2731 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0582 |           1.9324 |        -375.7153 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0622 |           1.8674 |        -372.1835 |
[32m[20221208 14:49:48 @agent_ppo2.py:179][0m |          -0.0638 |           1.8378 |        -374.2779 |
[32m[20221208 14:49:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.56
[32m[20221208 14:49:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.70
[32m[20221208 14:49:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.63
[32m[20221208 14:49:49 @agent_ppo2.py:137][0m Total time:      13.24 min
[32m[20221208 14:49:49 @agent_ppo2.py:139][0m 1069056 total steps have happened
[32m[20221208 14:49:49 @agent_ppo2.py:115][0m #------------------------ Iteration 522 --------------------------#
[32m[20221208 14:49:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:49 @agent_ppo2.py:179][0m |           0.1371 |           4.7207 |        -361.4200 |
[32m[20221208 14:49:49 @agent_ppo2.py:179][0m |           0.1053 |           4.1159 |        -216.9644 |
[32m[20221208 14:49:49 @agent_ppo2.py:179][0m |           0.0127 |           3.8861 |        -280.2556 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0233 |           3.7990 |        -315.9389 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0434 |           3.7006 |        -328.7766 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0537 |           3.6285 |        -339.0941 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0606 |           3.6134 |        -343.2472 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0681 |           3.5584 |        -357.9382 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0719 |           3.5474 |        -360.1516 |
[32m[20221208 14:49:50 @agent_ppo2.py:179][0m |          -0.0771 |           3.4710 |        -361.5052 |
[32m[20221208 14:49:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.35
[32m[20221208 14:49:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.70
[32m[20221208 14:49:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.28
[32m[20221208 14:49:50 @agent_ppo2.py:137][0m Total time:      13.27 min
[32m[20221208 14:49:50 @agent_ppo2.py:139][0m 1071104 total steps have happened
[32m[20221208 14:49:50 @agent_ppo2.py:115][0m #------------------------ Iteration 523 --------------------------#
[32m[20221208 14:49:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |           0.1163 |           3.7353 |        -369.7605 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |           0.2187 |           3.2550 |        -264.1364 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |           0.0725 |           3.0488 |        -240.9036 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |           0.0163 |           2.9111 |        -288.4871 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |          -0.0155 |           2.8198 |        -313.0545 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |          -0.0296 |           2.7316 |        -323.3536 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |          -0.0428 |           2.6882 |        -331.5405 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |          -0.0533 |           2.6265 |        -336.0037 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |          -0.0548 |           2.5962 |        -348.5547 |
[32m[20221208 14:49:51 @agent_ppo2.py:179][0m |          -0.0645 |           2.5622 |        -349.8790 |
[32m[20221208 14:49:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:49:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.35
[32m[20221208 14:49:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.92
[32m[20221208 14:49:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.56
[32m[20221208 14:49:52 @agent_ppo2.py:137][0m Total time:      13.29 min
[32m[20221208 14:49:52 @agent_ppo2.py:139][0m 1073152 total steps have happened
[32m[20221208 14:49:52 @agent_ppo2.py:115][0m #------------------------ Iteration 524 --------------------------#
[32m[20221208 14:49:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:49:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:52 @agent_ppo2.py:179][0m |           0.0768 |           2.3628 |        -368.0819 |
[32m[20221208 14:49:52 @agent_ppo2.py:179][0m |           2.0533 |           2.0201 |        -223.7701 |
[32m[20221208 14:49:52 @agent_ppo2.py:179][0m |           0.0301 |           1.9309 |        -226.3066 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0007 |           1.8731 |        -232.8789 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0203 |           1.8368 |        -251.9773 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0298 |           1.8041 |        -267.8614 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0464 |           1.7979 |        -288.1321 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0540 |           1.7624 |        -297.0119 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0609 |           1.7545 |        -307.5917 |
[32m[20221208 14:49:53 @agent_ppo2.py:179][0m |          -0.0644 |           1.7353 |        -304.7679 |
[32m[20221208 14:49:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:49:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.60
[32m[20221208 14:49:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.36
[32m[20221208 14:49:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.95
[32m[20221208 14:49:53 @agent_ppo2.py:137][0m Total time:      13.32 min
[32m[20221208 14:49:53 @agent_ppo2.py:139][0m 1075200 total steps have happened
[32m[20221208 14:49:53 @agent_ppo2.py:115][0m #------------------------ Iteration 525 --------------------------#
[32m[20221208 14:49:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |           0.1707 |           2.6723 |        -284.4964 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |           0.0353 |           2.3641 |        -258.3962 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |           0.0203 |           2.2177 |        -274.0078 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0009 |           2.0718 |        -284.0576 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0106 |           1.9921 |        -290.8477 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0160 |           1.9059 |        -295.1309 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0212 |           1.8490 |        -296.5240 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0262 |           1.8065 |        -301.4170 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0276 |           1.7631 |        -304.2594 |
[32m[20221208 14:49:54 @agent_ppo2.py:179][0m |          -0.0266 |           1.7432 |        -304.5660 |
[32m[20221208 14:49:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:49:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.48
[32m[20221208 14:49:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.96
[32m[20221208 14:49:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.27
[32m[20221208 14:49:55 @agent_ppo2.py:137][0m Total time:      13.34 min
[32m[20221208 14:49:55 @agent_ppo2.py:139][0m 1077248 total steps have happened
[32m[20221208 14:49:55 @agent_ppo2.py:115][0m #------------------------ Iteration 526 --------------------------#
[32m[20221208 14:49:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:55 @agent_ppo2.py:179][0m |           0.0823 |           2.0604 |        -392.6138 |
[32m[20221208 14:49:55 @agent_ppo2.py:179][0m |           0.0155 |           1.5484 |        -372.6987 |
[32m[20221208 14:49:55 @agent_ppo2.py:179][0m |          -0.0101 |           1.4097 |        -346.5910 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0242 |           1.3242 |        -385.6120 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0458 |           1.2542 |        -396.9979 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0535 |           1.2226 |        -385.4361 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0632 |           1.1920 |        -390.5301 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0713 |           1.1614 |        -403.0250 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0683 |           1.1279 |        -407.1632 |
[32m[20221208 14:49:56 @agent_ppo2.py:179][0m |          -0.0694 |           1.1063 |        -399.0396 |
[32m[20221208 14:49:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.56
[32m[20221208 14:49:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.50
[32m[20221208 14:49:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.26
[32m[20221208 14:49:56 @agent_ppo2.py:137][0m Total time:      13.37 min
[32m[20221208 14:49:56 @agent_ppo2.py:139][0m 1079296 total steps have happened
[32m[20221208 14:49:56 @agent_ppo2.py:115][0m #------------------------ Iteration 527 --------------------------#
[32m[20221208 14:49:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |           0.1338 |           3.9615 |        -385.4965 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |           0.0526 |           3.6237 |        -352.2570 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |           0.0007 |           3.4437 |        -384.3786 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0187 |           3.3804 |        -402.0192 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0372 |           3.2814 |        -405.5866 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0477 |           3.2506 |        -422.7508 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0490 |           3.2001 |        -425.7584 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0554 |           3.1511 |        -427.9643 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0649 |           3.1331 |        -434.5578 |
[32m[20221208 14:49:57 @agent_ppo2.py:179][0m |          -0.0673 |           3.1107 |        -443.0254 |
[32m[20221208 14:49:57 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:49:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.24
[32m[20221208 14:49:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.10
[32m[20221208 14:49:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.50
[32m[20221208 14:49:58 @agent_ppo2.py:137][0m Total time:      13.39 min
[32m[20221208 14:49:58 @agent_ppo2.py:139][0m 1081344 total steps have happened
[32m[20221208 14:49:58 @agent_ppo2.py:115][0m #------------------------ Iteration 528 --------------------------#
[32m[20221208 14:49:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:49:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:49:58 @agent_ppo2.py:179][0m |           0.0884 |           5.2729 |        -365.3457 |
[32m[20221208 14:49:58 @agent_ppo2.py:179][0m |           0.0931 |           4.9538 |        -251.6043 |
[32m[20221208 14:49:58 @agent_ppo2.py:179][0m |           0.0404 |           4.8469 |        -296.2390 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |           0.0041 |           4.7391 |        -335.6938 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |          -0.0183 |           4.6918 |        -353.7213 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |          -0.0311 |           4.5637 |        -367.9368 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |          -0.0359 |           4.5437 |        -382.2450 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |          -0.0441 |           4.4391 |        -388.2636 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |          -0.0549 |           4.3721 |        -402.2661 |
[32m[20221208 14:49:59 @agent_ppo2.py:179][0m |          -0.0584 |           4.3835 |        -405.6018 |
[32m[20221208 14:49:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:49:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.01
[32m[20221208 14:49:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.06
[32m[20221208 14:49:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.63
[32m[20221208 14:49:59 @agent_ppo2.py:137][0m Total time:      13.42 min
[32m[20221208 14:49:59 @agent_ppo2.py:139][0m 1083392 total steps have happened
[32m[20221208 14:49:59 @agent_ppo2.py:115][0m #------------------------ Iteration 529 --------------------------#
[32m[20221208 14:50:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |           0.0511 |           1.2658 |        -343.4236 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0185 |           1.0486 |        -262.0066 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0344 |           1.0123 |        -271.3489 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0330 |           0.9930 |        -275.9451 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0367 |           0.9870 |        -276.2458 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0428 |           0.9933 |        -289.5107 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0382 |           0.9890 |        -285.0016 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0250 |           0.9595 |        -284.0209 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0383 |           0.9880 |        -286.7787 |
[32m[20221208 14:50:00 @agent_ppo2.py:179][0m |          -0.0411 |           0.9841 |        -292.0100 |
[32m[20221208 14:50:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:50:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 19.41
[32m[20221208 14:50:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.02
[32m[20221208 14:50:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.31
[32m[20221208 14:50:01 @agent_ppo2.py:137][0m Total time:      13.44 min
[32m[20221208 14:50:01 @agent_ppo2.py:139][0m 1085440 total steps have happened
[32m[20221208 14:50:01 @agent_ppo2.py:115][0m #------------------------ Iteration 530 --------------------------#
[32m[20221208 14:50:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:01 @agent_ppo2.py:179][0m |           0.0905 |           3.4243 |        -355.7014 |
[32m[20221208 14:50:01 @agent_ppo2.py:179][0m |           0.0610 |           3.0946 |        -352.6294 |
[32m[20221208 14:50:01 @agent_ppo2.py:179][0m |           0.0224 |           2.9985 |        -372.9844 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0077 |           2.9240 |        -405.0277 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0194 |           2.8464 |        -411.8394 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0326 |           2.7939 |        -425.0148 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0433 |           2.7642 |        -443.9191 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0487 |           2.7222 |        -453.2031 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0550 |           2.7108 |        -454.2879 |
[32m[20221208 14:50:02 @agent_ppo2.py:179][0m |          -0.0593 |           2.6498 |        -467.0123 |
[32m[20221208 14:50:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.13
[32m[20221208 14:50:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.13
[32m[20221208 14:50:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.29
[32m[20221208 14:50:02 @agent_ppo2.py:137][0m Total time:      13.47 min
[32m[20221208 14:50:02 @agent_ppo2.py:139][0m 1087488 total steps have happened
[32m[20221208 14:50:02 @agent_ppo2.py:115][0m #------------------------ Iteration 531 --------------------------#
[32m[20221208 14:50:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |           0.0594 |           3.9160 |        -412.5969 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |           0.0907 |           3.6892 |        -323.1059 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |           0.0449 |           3.5256 |        -347.1037 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0033 |           3.4765 |        -375.2646 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0294 |           3.3605 |        -400.8846 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0378 |           3.3154 |        -403.5132 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0482 |           3.3048 |        -415.2200 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0520 |           3.2376 |        -419.1776 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0520 |           3.2463 |        -418.6554 |
[32m[20221208 14:50:03 @agent_ppo2.py:179][0m |          -0.0601 |           3.1682 |        -428.8888 |
[32m[20221208 14:50:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.20
[32m[20221208 14:50:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.44
[32m[20221208 14:50:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.21
[32m[20221208 14:50:04 @agent_ppo2.py:137][0m Total time:      13.49 min
[32m[20221208 14:50:04 @agent_ppo2.py:139][0m 1089536 total steps have happened
[32m[20221208 14:50:04 @agent_ppo2.py:115][0m #------------------------ Iteration 532 --------------------------#
[32m[20221208 14:50:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:04 @agent_ppo2.py:179][0m |           0.0841 |           2.1893 |        -382.0566 |
[32m[20221208 14:50:04 @agent_ppo2.py:179][0m |           0.0367 |           1.7916 |        -362.2198 |
[32m[20221208 14:50:04 @agent_ppo2.py:179][0m |           0.0039 |           1.6704 |        -384.7307 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0197 |           1.5755 |        -411.5669 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0346 |           1.5265 |        -419.0491 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0340 |           1.4499 |        -417.0300 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0462 |           1.4306 |        -433.5090 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0484 |           1.3831 |        -431.4231 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0514 |           1.3531 |        -436.1961 |
[32m[20221208 14:50:05 @agent_ppo2.py:179][0m |          -0.0508 |           1.3242 |        -439.3462 |
[32m[20221208 14:50:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.41
[32m[20221208 14:50:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.97
[32m[20221208 14:50:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.18
[32m[20221208 14:50:05 @agent_ppo2.py:137][0m Total time:      13.52 min
[32m[20221208 14:50:05 @agent_ppo2.py:139][0m 1091584 total steps have happened
[32m[20221208 14:50:05 @agent_ppo2.py:115][0m #------------------------ Iteration 533 --------------------------#
[32m[20221208 14:50:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |           1.0797 |           1.1198 |        -270.5686 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |           0.1324 |           0.8983 |        -171.8626 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |           0.0279 |           0.8549 |        -229.3193 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |           0.0037 |           0.8119 |        -244.5385 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |          -0.0220 |           0.7928 |        -254.4726 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |          -0.0414 |           0.7676 |        -276.5008 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |          -0.0369 |           0.7464 |        -270.1594 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |          -0.0483 |           0.7400 |        -275.6045 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |          -0.0541 |           0.7319 |        -273.2621 |
[32m[20221208 14:50:06 @agent_ppo2.py:179][0m |          -0.0602 |           0.7173 |        -282.7003 |
[32m[20221208 14:50:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.43
[32m[20221208 14:50:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.93
[32m[20221208 14:50:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.16
[32m[20221208 14:50:07 @agent_ppo2.py:137][0m Total time:      13.54 min
[32m[20221208 14:50:07 @agent_ppo2.py:139][0m 1093632 total steps have happened
[32m[20221208 14:50:07 @agent_ppo2.py:115][0m #------------------------ Iteration 534 --------------------------#
[32m[20221208 14:50:07 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:50:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:07 @agent_ppo2.py:179][0m |           0.0908 |           2.6719 |        -348.8233 |
[32m[20221208 14:50:07 @agent_ppo2.py:179][0m |           0.0866 |           2.1880 |        -229.1796 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |           0.0047 |           2.0265 |        -261.9867 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0183 |           1.9464 |        -275.9397 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0396 |           1.9103 |        -289.2082 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0540 |           1.8469 |        -302.0612 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0556 |           1.7904 |        -304.5862 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0641 |           1.7545 |        -308.9423 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0713 |           1.7370 |        -309.9129 |
[32m[20221208 14:50:08 @agent_ppo2.py:179][0m |          -0.0748 |           1.6976 |        -320.4899 |
[32m[20221208 14:50:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.56
[32m[20221208 14:50:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.90
[32m[20221208 14:50:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.19
[32m[20221208 14:50:08 @agent_ppo2.py:137][0m Total time:      13.57 min
[32m[20221208 14:50:08 @agent_ppo2.py:139][0m 1095680 total steps have happened
[32m[20221208 14:50:08 @agent_ppo2.py:115][0m #------------------------ Iteration 535 --------------------------#
[32m[20221208 14:50:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |           0.0945 |           2.0399 |        -409.3246 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |           0.0429 |           1.3847 |        -362.3913 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |           0.0295 |           1.2178 |        -306.6305 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0087 |           1.1244 |        -293.3620 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0282 |           1.0644 |        -304.6084 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0409 |           1.0239 |        -309.3121 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0514 |           0.9853 |        -313.8880 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0547 |           0.9586 |        -317.0203 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0605 |           0.9335 |        -329.1792 |
[32m[20221208 14:50:09 @agent_ppo2.py:179][0m |          -0.0668 |           0.9174 |        -333.3609 |
[32m[20221208 14:50:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.85
[32m[20221208 14:50:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.99
[32m[20221208 14:50:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.49
[32m[20221208 14:50:10 @agent_ppo2.py:137][0m Total time:      13.59 min
[32m[20221208 14:50:10 @agent_ppo2.py:139][0m 1097728 total steps have happened
[32m[20221208 14:50:10 @agent_ppo2.py:115][0m #------------------------ Iteration 536 --------------------------#
[32m[20221208 14:50:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:10 @agent_ppo2.py:179][0m |           0.0436 |           4.7945 |        -476.5620 |
[32m[20221208 14:50:10 @agent_ppo2.py:179][0m |           0.0412 |           4.0354 |        -389.1718 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0005 |           3.8392 |        -360.3032 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0311 |           3.6023 |        -412.7936 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0407 |           3.5082 |        -432.0386 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0463 |           3.4065 |        -425.8371 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0539 |           3.2724 |        -403.4358 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0606 |           3.1854 |        -422.5828 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0652 |           3.1088 |        -418.7626 |
[32m[20221208 14:50:11 @agent_ppo2.py:179][0m |          -0.0685 |           3.0549 |        -426.8406 |
[32m[20221208 14:50:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.87
[32m[20221208 14:50:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.26
[32m[20221208 14:50:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.36
[32m[20221208 14:50:11 @agent_ppo2.py:137][0m Total time:      13.62 min
[32m[20221208 14:50:11 @agent_ppo2.py:139][0m 1099776 total steps have happened
[32m[20221208 14:50:11 @agent_ppo2.py:115][0m #------------------------ Iteration 537 --------------------------#
[32m[20221208 14:50:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |           0.0569 |           2.1722 |        -408.5154 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |           0.0367 |           1.6683 |        -357.0689 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |           0.0298 |           1.4488 |        -300.9919 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |           0.0240 |           1.3023 |        -251.3722 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |           0.0038 |           1.2303 |        -231.7303 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |          -0.0088 |           1.1383 |        -248.3311 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |          -0.0153 |           1.0883 |        -272.1655 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |          -0.0261 |           1.0543 |        -268.4384 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |          -0.0302 |           1.0014 |        -279.9016 |
[32m[20221208 14:50:12 @agent_ppo2.py:179][0m |          -0.0275 |           0.9709 |        -297.8885 |
[32m[20221208 14:50:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.21
[32m[20221208 14:50:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.46
[32m[20221208 14:50:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.30
[32m[20221208 14:50:13 @agent_ppo2.py:137][0m Total time:      13.64 min
[32m[20221208 14:50:13 @agent_ppo2.py:139][0m 1101824 total steps have happened
[32m[20221208 14:50:13 @agent_ppo2.py:115][0m #------------------------ Iteration 538 --------------------------#
[32m[20221208 14:50:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:13 @agent_ppo2.py:179][0m |           0.2654 |           1.3892 |        -225.8376 |
[32m[20221208 14:50:13 @agent_ppo2.py:179][0m |           0.0923 |           1.0545 |        -193.1081 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |           0.0232 |           1.0199 |        -258.4206 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |           0.0132 |           0.9997 |        -261.2071 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |          -0.0141 |           0.9697 |        -272.8023 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |          -0.0119 |           0.9688 |        -274.9212 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |          -0.0104 |           0.9528 |        -276.5564 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |          -0.0329 |           0.9475 |        -283.6727 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |          -0.0144 |           0.9403 |        -276.2139 |
[32m[20221208 14:50:14 @agent_ppo2.py:179][0m |          -0.0191 |           0.9427 |        -282.3268 |
[32m[20221208 14:50:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:50:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.70
[32m[20221208 14:50:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 28.75
[32m[20221208 14:50:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.47
[32m[20221208 14:50:14 @agent_ppo2.py:137][0m Total time:      13.67 min
[32m[20221208 14:50:14 @agent_ppo2.py:139][0m 1103872 total steps have happened
[32m[20221208 14:50:14 @agent_ppo2.py:115][0m #------------------------ Iteration 539 --------------------------#
[32m[20221208 14:50:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |           0.0792 |           2.0660 |        -376.6280 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |           0.0463 |           1.5158 |        -340.7568 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0073 |           1.3946 |        -354.7444 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0407 |           1.3471 |        -366.3519 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0485 |           1.2799 |        -370.5169 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0580 |           1.2646 |        -356.5800 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0652 |           1.2127 |        -373.1338 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0721 |           1.2042 |        -374.9490 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0717 |           1.1957 |        -375.2111 |
[32m[20221208 14:50:15 @agent_ppo2.py:179][0m |          -0.0814 |           1.1404 |        -383.1079 |
[32m[20221208 14:50:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.30
[32m[20221208 14:50:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.00
[32m[20221208 14:50:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.41
[32m[20221208 14:50:16 @agent_ppo2.py:137][0m Total time:      13.69 min
[32m[20221208 14:50:16 @agent_ppo2.py:139][0m 1105920 total steps have happened
[32m[20221208 14:50:16 @agent_ppo2.py:115][0m #------------------------ Iteration 540 --------------------------#
[32m[20221208 14:50:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:16 @agent_ppo2.py:179][0m |           0.0725 |           2.0227 |        -349.0728 |
[32m[20221208 14:50:16 @agent_ppo2.py:179][0m |           0.0494 |           1.6732 |        -310.2097 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |           0.0079 |           1.5500 |        -331.0218 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0137 |           1.4437 |        -349.7698 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0315 |           1.3768 |        -370.1573 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0402 |           1.3238 |        -375.9941 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0473 |           1.2778 |        -375.5250 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0526 |           1.2428 |        -377.9374 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0565 |           1.2058 |        -380.4952 |
[32m[20221208 14:50:17 @agent_ppo2.py:179][0m |          -0.0580 |           1.1696 |        -385.0296 |
[32m[20221208 14:50:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.50
[32m[20221208 14:50:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.29
[32m[20221208 14:50:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.29
[32m[20221208 14:50:17 @agent_ppo2.py:137][0m Total time:      13.72 min
[32m[20221208 14:50:17 @agent_ppo2.py:139][0m 1107968 total steps have happened
[32m[20221208 14:50:17 @agent_ppo2.py:115][0m #------------------------ Iteration 541 --------------------------#
[32m[20221208 14:50:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |           0.0678 |           2.8033 |        -369.3098 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |           0.0429 |           2.4038 |        -312.0695 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0006 |           2.2332 |        -357.9957 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0353 |           2.1082 |        -388.3513 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0516 |           2.0459 |        -390.7076 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0613 |           1.9564 |        -405.0980 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0686 |           1.9140 |        -402.7595 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0716 |           1.8515 |        -410.6167 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0786 |           1.8192 |        -417.5477 |
[32m[20221208 14:50:18 @agent_ppo2.py:179][0m |          -0.0803 |           1.7743 |        -416.5965 |
[32m[20221208 14:50:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.39
[32m[20221208 14:50:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.22
[32m[20221208 14:50:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.38
[32m[20221208 14:50:19 @agent_ppo2.py:137][0m Total time:      13.74 min
[32m[20221208 14:50:19 @agent_ppo2.py:139][0m 1110016 total steps have happened
[32m[20221208 14:50:19 @agent_ppo2.py:115][0m #------------------------ Iteration 542 --------------------------#
[32m[20221208 14:50:19 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:50:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:19 @agent_ppo2.py:179][0m |           0.0904 |           2.4230 |        -350.7979 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |           0.0620 |           2.0576 |        -267.0121 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |           0.0118 |           1.8845 |        -291.3082 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0187 |           1.7635 |        -327.6061 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0398 |           1.6800 |        -349.3181 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0528 |           1.6212 |        -351.7948 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0592 |           1.5556 |        -360.6970 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0637 |           1.4983 |        -366.6553 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0677 |           1.4422 |        -371.4050 |
[32m[20221208 14:50:20 @agent_ppo2.py:179][0m |          -0.0731 |           1.3941 |        -381.7269 |
[32m[20221208 14:50:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.40
[32m[20221208 14:50:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.02
[32m[20221208 14:50:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.40
[32m[20221208 14:50:20 @agent_ppo2.py:137][0m Total time:      13.77 min
[32m[20221208 14:50:20 @agent_ppo2.py:139][0m 1112064 total steps have happened
[32m[20221208 14:50:20 @agent_ppo2.py:115][0m #------------------------ Iteration 543 --------------------------#
[32m[20221208 14:50:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |           0.0880 |           3.1146 |        -389.8183 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |           0.0780 |           2.6374 |        -349.0078 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |           0.0348 |           2.3786 |        -345.5705 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |          -0.0025 |           2.2116 |        -393.2007 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |          -0.0216 |           2.0557 |        -408.1075 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |          -0.0361 |           1.9727 |        -409.1951 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |          -0.0485 |           1.8820 |        -416.9425 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |          -0.0474 |           1.7913 |        -421.0287 |
[32m[20221208 14:50:21 @agent_ppo2.py:179][0m |          -0.0544 |           1.7322 |        -424.4174 |
[32m[20221208 14:50:22 @agent_ppo2.py:179][0m |          -0.0598 |           1.6641 |        -428.5298 |
[32m[20221208 14:50:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:50:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.76
[32m[20221208 14:50:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.43
[32m[20221208 14:50:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.11
[32m[20221208 14:50:22 @agent_ppo2.py:137][0m Total time:      13.79 min
[32m[20221208 14:50:22 @agent_ppo2.py:139][0m 1114112 total steps have happened
[32m[20221208 14:50:22 @agent_ppo2.py:115][0m #------------------------ Iteration 544 --------------------------#
[32m[20221208 14:50:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:22 @agent_ppo2.py:179][0m |           0.0553 |           2.2268 |        -388.9304 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |           0.0216 |           1.6183 |        -362.0613 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0130 |           1.4115 |        -353.2375 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0362 |           1.3089 |        -394.2815 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0511 |           1.2102 |        -407.8352 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0640 |           1.1636 |        -421.9566 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0663 |           1.1120 |        -420.4349 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0705 |           1.0687 |        -421.4633 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0697 |           1.0454 |        -417.7010 |
[32m[20221208 14:50:23 @agent_ppo2.py:179][0m |          -0.0727 |           1.0047 |        -415.3643 |
[32m[20221208 14:50:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.39
[32m[20221208 14:50:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.72
[32m[20221208 14:50:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.98
[32m[20221208 14:50:23 @agent_ppo2.py:137][0m Total time:      13.82 min
[32m[20221208 14:50:23 @agent_ppo2.py:139][0m 1116160 total steps have happened
[32m[20221208 14:50:23 @agent_ppo2.py:115][0m #------------------------ Iteration 545 --------------------------#
[32m[20221208 14:50:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |           0.0708 |           2.6811 |        -371.0388 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |           0.0412 |           1.9010 |        -349.9189 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0115 |           1.6871 |        -361.0677 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0269 |           1.5387 |        -369.3668 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0333 |           1.4320 |        -356.2921 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0280 |           1.3644 |        -292.1366 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0404 |           1.2907 |        -294.0184 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0560 |           1.2535 |        -348.0059 |
[32m[20221208 14:50:24 @agent_ppo2.py:179][0m |          -0.0628 |           1.2116 |        -375.6544 |
[32m[20221208 14:50:25 @agent_ppo2.py:179][0m |          -0.0653 |           1.1762 |        -369.5712 |
[32m[20221208 14:50:25 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:50:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.46
[32m[20221208 14:50:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.07
[32m[20221208 14:50:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.81
[32m[20221208 14:50:25 @agent_ppo2.py:137][0m Total time:      13.84 min
[32m[20221208 14:50:25 @agent_ppo2.py:139][0m 1118208 total steps have happened
[32m[20221208 14:50:25 @agent_ppo2.py:115][0m #------------------------ Iteration 546 --------------------------#
[32m[20221208 14:50:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:25 @agent_ppo2.py:179][0m |           0.1006 |           2.4384 |        -312.2794 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |           0.1028 |           2.0141 |        -163.2944 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |           0.0476 |           1.8225 |        -158.7433 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |           0.0299 |           1.7386 |        -181.8619 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |           0.0321 |           1.6662 |        -149.3912 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |           0.0105 |           1.6035 |        -181.1062 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |           0.0006 |           1.5809 |        -205.9585 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |          -0.0115 |           1.5454 |        -210.0686 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |          -0.0114 |           1.5264 |        -221.2105 |
[32m[20221208 14:50:26 @agent_ppo2.py:179][0m |          -0.0078 |           1.4935 |        -227.7710 |
[32m[20221208 14:50:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:50:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.93
[32m[20221208 14:50:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.45
[32m[20221208 14:50:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.71
[32m[20221208 14:50:26 @agent_ppo2.py:137][0m Total time:      13.87 min
[32m[20221208 14:50:26 @agent_ppo2.py:139][0m 1120256 total steps have happened
[32m[20221208 14:50:26 @agent_ppo2.py:115][0m #------------------------ Iteration 547 --------------------------#
[32m[20221208 14:50:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |           0.1081 |           3.4187 |        -347.0874 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |           0.1318 |           2.8934 |        -230.5942 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |           0.0836 |           2.6972 |        -249.6846 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |           0.0345 |           2.5912 |        -291.9462 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |          -0.0047 |           2.4900 |        -313.5654 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |          -0.0170 |           2.4252 |        -332.9482 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |          -0.0366 |           2.3641 |        -355.8900 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |          -0.0364 |           2.3096 |        -354.1030 |
[32m[20221208 14:50:27 @agent_ppo2.py:179][0m |          -0.0488 |           2.2622 |        -363.2852 |
[32m[20221208 14:50:28 @agent_ppo2.py:179][0m |          -0.0567 |           2.2344 |        -377.0213 |
[32m[20221208 14:50:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.21
[32m[20221208 14:50:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.78
[32m[20221208 14:50:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.81
[32m[20221208 14:50:28 @agent_ppo2.py:137][0m Total time:      13.89 min
[32m[20221208 14:50:28 @agent_ppo2.py:139][0m 1122304 total steps have happened
[32m[20221208 14:50:28 @agent_ppo2.py:115][0m #------------------------ Iteration 548 --------------------------#
[32m[20221208 14:50:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:28 @agent_ppo2.py:179][0m |           0.0886 |           2.9362 |        -362.7896 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |           0.0690 |           2.5718 |        -310.1120 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |           0.0759 |           2.4367 |        -277.8779 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |           0.0342 |           2.3462 |        -252.0613 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |          -0.0066 |           2.2728 |        -278.9761 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |          -0.0252 |           2.2521 |        -294.6302 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |          -0.0390 |           2.1982 |        -304.6092 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |          -0.0460 |           2.1805 |        -303.7938 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |          -0.0498 |           2.1657 |        -317.6264 |
[32m[20221208 14:50:29 @agent_ppo2.py:179][0m |          -0.0552 |           2.1216 |        -325.6588 |
[32m[20221208 14:50:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.64
[32m[20221208 14:50:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.00
[32m[20221208 14:50:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.82
[32m[20221208 14:50:29 @agent_ppo2.py:137][0m Total time:      13.92 min
[32m[20221208 14:50:29 @agent_ppo2.py:139][0m 1124352 total steps have happened
[32m[20221208 14:50:29 @agent_ppo2.py:115][0m #------------------------ Iteration 549 --------------------------#
[32m[20221208 14:50:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |           0.0622 |           2.4977 |        -359.1285 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |           0.0603 |           2.2408 |        -280.1196 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |           0.0102 |           2.1315 |        -275.7092 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |          -0.0063 |           2.0576 |        -262.7318 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |          -0.0278 |           2.0275 |        -291.7569 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |          -0.0429 |           1.9684 |        -300.9282 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |          -0.0515 |           1.9299 |        -312.5738 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |          -0.0201 |           1.8681 |        -293.6745 |
[32m[20221208 14:50:30 @agent_ppo2.py:179][0m |          -0.0018 |           1.8373 |        -272.8574 |
[32m[20221208 14:50:31 @agent_ppo2.py:179][0m |           0.0587 |           1.8010 |        -185.6409 |
[32m[20221208 14:50:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.50
[32m[20221208 14:50:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.27
[32m[20221208 14:50:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.91
[32m[20221208 14:50:31 @agent_ppo2.py:137][0m Total time:      13.94 min
[32m[20221208 14:50:31 @agent_ppo2.py:139][0m 1126400 total steps have happened
[32m[20221208 14:50:31 @agent_ppo2.py:115][0m #------------------------ Iteration 550 --------------------------#
[32m[20221208 14:50:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:31 @agent_ppo2.py:179][0m |           0.0679 |           3.3609 |        -360.6005 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |           0.0285 |           3.1396 |        -341.8384 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0082 |           3.0295 |        -367.5487 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0258 |           2.9744 |        -376.0952 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0381 |           2.9323 |        -388.7133 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0486 |           2.8946 |        -393.6791 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0568 |           2.8283 |        -395.2248 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0582 |           2.8129 |        -405.5382 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0639 |           2.7994 |        -406.9624 |
[32m[20221208 14:50:32 @agent_ppo2.py:179][0m |          -0.0622 |           2.7698 |        -404.5009 |
[32m[20221208 14:50:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:50:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.91
[32m[20221208 14:50:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.68
[32m[20221208 14:50:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.42
[32m[20221208 14:50:32 @agent_ppo2.py:137][0m Total time:      13.97 min
[32m[20221208 14:50:32 @agent_ppo2.py:139][0m 1128448 total steps have happened
[32m[20221208 14:50:32 @agent_ppo2.py:115][0m #------------------------ Iteration 551 --------------------------#
[32m[20221208 14:50:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |           0.1367 |           4.3731 |        -348.8582 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |           0.1165 |           4.0390 |        -301.9781 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |           0.0418 |           3.8736 |        -302.5120 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |          -0.0029 |           3.8077 |        -344.5415 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |          -0.0258 |           3.7191 |        -367.1875 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |          -0.0341 |           3.6665 |        -371.3865 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |          -0.0489 |           3.6124 |        -386.6040 |
[32m[20221208 14:50:33 @agent_ppo2.py:179][0m |          -0.0559 |           3.5791 |        -389.4648 |
[32m[20221208 14:50:34 @agent_ppo2.py:179][0m |          -0.0610 |           3.5285 |        -395.3263 |
[32m[20221208 14:50:34 @agent_ppo2.py:179][0m |          -0.0655 |           3.4821 |        -395.6847 |
[32m[20221208 14:50:34 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:50:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.04
[32m[20221208 14:50:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.71
[32m[20221208 14:50:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.43
[32m[20221208 14:50:34 @agent_ppo2.py:137][0m Total time:      14.00 min
[32m[20221208 14:50:34 @agent_ppo2.py:139][0m 1130496 total steps have happened
[32m[20221208 14:50:34 @agent_ppo2.py:115][0m #------------------------ Iteration 552 --------------------------#
[32m[20221208 14:50:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |           0.0399 |           4.1832 |        -394.6785 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |           0.0502 |           3.8418 |        -346.5864 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |           0.0228 |           3.7028 |        -349.8217 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0198 |           3.6065 |        -372.1544 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0250 |           3.5339 |        -370.1789 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0394 |           3.5028 |        -382.3090 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0510 |           3.4546 |        -392.0631 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0558 |           3.3892 |        -402.9161 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0640 |           3.3602 |        -408.2347 |
[32m[20221208 14:50:35 @agent_ppo2.py:179][0m |          -0.0676 |           3.3078 |        -413.8858 |
[32m[20221208 14:50:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.62
[32m[20221208 14:50:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.61
[32m[20221208 14:50:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.63
[32m[20221208 14:50:36 @agent_ppo2.py:137][0m Total time:      14.02 min
[32m[20221208 14:50:36 @agent_ppo2.py:139][0m 1132544 total steps have happened
[32m[20221208 14:50:36 @agent_ppo2.py:115][0m #------------------------ Iteration 553 --------------------------#
[32m[20221208 14:50:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |           0.0620 |           3.0544 |        -360.3844 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |           0.0119 |           2.7552 |        -342.8121 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |          -0.0011 |           2.6014 |        -356.9870 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |          -0.0368 |           2.4882 |        -379.4416 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |          -0.0479 |           2.4187 |        -387.5247 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |          -0.0550 |           2.3677 |        -390.4277 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |          -0.0555 |           2.3231 |        -386.5755 |
[32m[20221208 14:50:36 @agent_ppo2.py:179][0m |          -0.0629 |           2.3091 |        -393.6232 |
[32m[20221208 14:50:37 @agent_ppo2.py:179][0m |          -0.0620 |           2.3034 |        -400.0454 |
[32m[20221208 14:50:37 @agent_ppo2.py:179][0m |          -0.0650 |           2.2655 |        -397.2264 |
[32m[20221208 14:50:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.66
[32m[20221208 14:50:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.09
[32m[20221208 14:50:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.23
[32m[20221208 14:50:37 @agent_ppo2.py:137][0m Total time:      14.05 min
[32m[20221208 14:50:37 @agent_ppo2.py:139][0m 1134592 total steps have happened
[32m[20221208 14:50:37 @agent_ppo2.py:115][0m #------------------------ Iteration 554 --------------------------#
[32m[20221208 14:50:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |           0.0794 |           3.1169 |        -399.7398 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |           0.0752 |           2.6272 |        -311.1199 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |           0.0185 |           2.3948 |        -251.7909 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0283 |           2.2597 |        -279.4303 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0468 |           2.1901 |        -300.2695 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0571 |           2.1342 |        -303.9464 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0645 |           2.0656 |        -312.7057 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0732 |           2.0295 |        -322.3576 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0742 |           1.9874 |        -328.4989 |
[32m[20221208 14:50:38 @agent_ppo2.py:179][0m |          -0.0724 |           1.9552 |        -322.6313 |
[32m[20221208 14:50:38 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.37
[32m[20221208 14:50:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.80
[32m[20221208 14:50:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.49
[32m[20221208 14:50:39 @agent_ppo2.py:137][0m Total time:      14.07 min
[32m[20221208 14:50:39 @agent_ppo2.py:139][0m 1136640 total steps have happened
[32m[20221208 14:50:39 @agent_ppo2.py:115][0m #------------------------ Iteration 555 --------------------------#
[32m[20221208 14:50:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |           0.0987 |           2.7544 |        -392.9302 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |           0.0912 |           2.3404 |        -284.7518 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |           0.0006 |           2.1541 |        -254.4073 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |          -0.0321 |           2.0573 |        -279.3698 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |          -0.0534 |           2.0101 |        -287.6118 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |          -0.0652 |           1.9555 |        -301.1137 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |          -0.0735 |           1.9324 |        -305.6587 |
[32m[20221208 14:50:39 @agent_ppo2.py:179][0m |          -0.0773 |           1.8833 |        -316.5132 |
[32m[20221208 14:50:40 @agent_ppo2.py:179][0m |          -0.0796 |           1.8656 |        -317.3755 |
[32m[20221208 14:50:40 @agent_ppo2.py:179][0m |          -0.0846 |           1.8362 |        -320.8795 |
[32m[20221208 14:50:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.69
[32m[20221208 14:50:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.03
[32m[20221208 14:50:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.91
[32m[20221208 14:50:40 @agent_ppo2.py:137][0m Total time:      14.10 min
[32m[20221208 14:50:40 @agent_ppo2.py:139][0m 1138688 total steps have happened
[32m[20221208 14:50:40 @agent_ppo2.py:115][0m #------------------------ Iteration 556 --------------------------#
[32m[20221208 14:50:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |           0.0980 |           2.7617 |        -370.9207 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |           0.0309 |           2.4665 |        -238.8743 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0032 |           2.3709 |        -248.5186 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0276 |           2.2747 |        -253.6146 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0444 |           2.2294 |        -280.4106 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0545 |           2.1854 |        -290.1368 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0555 |           2.1291 |        -295.6755 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0588 |           2.1160 |        -296.6508 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0661 |           2.0909 |        -310.0979 |
[32m[20221208 14:50:41 @agent_ppo2.py:179][0m |          -0.0700 |           2.0536 |        -314.5630 |
[32m[20221208 14:50:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.76
[32m[20221208 14:50:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.39
[32m[20221208 14:50:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.35
[32m[20221208 14:50:42 @agent_ppo2.py:137][0m Total time:      14.12 min
[32m[20221208 14:50:42 @agent_ppo2.py:139][0m 1140736 total steps have happened
[32m[20221208 14:50:42 @agent_ppo2.py:115][0m #------------------------ Iteration 557 --------------------------#
[32m[20221208 14:50:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |           0.0956 |           2.8967 |        -471.5596 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |           0.0725 |           2.4573 |        -338.1384 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |           0.0026 |           2.2837 |        -314.7930 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |          -0.0258 |           2.1750 |        -333.5339 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |          -0.0360 |           2.1039 |        -339.5434 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |          -0.0525 |           2.0572 |        -355.6230 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |          -0.0574 |           2.0051 |        -369.2446 |
[32m[20221208 14:50:42 @agent_ppo2.py:179][0m |          -0.0662 |           1.9746 |        -377.2781 |
[32m[20221208 14:50:43 @agent_ppo2.py:179][0m |          -0.0691 |           1.9669 |        -381.3577 |
[32m[20221208 14:50:43 @agent_ppo2.py:179][0m |          -0.0740 |           1.9104 |        -389.1865 |
[32m[20221208 14:50:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.21
[32m[20221208 14:50:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.92
[32m[20221208 14:50:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.93
[32m[20221208 14:50:43 @agent_ppo2.py:137][0m Total time:      14.15 min
[32m[20221208 14:50:43 @agent_ppo2.py:139][0m 1142784 total steps have happened
[32m[20221208 14:50:43 @agent_ppo2.py:115][0m #------------------------ Iteration 558 --------------------------#
[32m[20221208 14:50:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |           0.1106 |           3.6188 |        -472.4036 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |           0.0716 |           3.0260 |        -382.6169 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |           0.0080 |           2.8747 |        -433.4127 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0140 |           2.7728 |        -466.6636 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0231 |           2.7097 |        -476.7454 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0381 |           2.6674 |        -488.9719 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0430 |           2.6001 |        -495.8775 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0535 |           2.5697 |        -510.5137 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0582 |           2.5459 |        -517.2569 |
[32m[20221208 14:50:44 @agent_ppo2.py:179][0m |          -0.0650 |           2.5385 |        -527.1849 |
[32m[20221208 14:50:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:50:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.75
[32m[20221208 14:50:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.12
[32m[20221208 14:50:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.82
[32m[20221208 14:50:45 @agent_ppo2.py:137][0m Total time:      14.17 min
[32m[20221208 14:50:45 @agent_ppo2.py:139][0m 1144832 total steps have happened
[32m[20221208 14:50:45 @agent_ppo2.py:115][0m #------------------------ Iteration 559 --------------------------#
[32m[20221208 14:50:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |           0.0932 |           4.1566 |        -474.9679 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |           0.0699 |           3.6207 |        -347.9862 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |           0.0040 |           3.4486 |        -429.9699 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |          -0.0250 |           3.3308 |        -462.1806 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |          -0.0423 |           3.2621 |        -467.0205 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |          -0.0461 |           3.1983 |        -478.9638 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |          -0.0558 |           3.1683 |        -481.6137 |
[32m[20221208 14:50:45 @agent_ppo2.py:179][0m |          -0.0635 |           3.1264 |        -500.2237 |
[32m[20221208 14:50:46 @agent_ppo2.py:179][0m |          -0.0678 |           3.0854 |        -506.1170 |
[32m[20221208 14:50:46 @agent_ppo2.py:179][0m |          -0.0716 |           3.0686 |        -510.7724 |
[32m[20221208 14:50:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.58
[32m[20221208 14:50:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.29
[32m[20221208 14:50:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.61
[32m[20221208 14:50:46 @agent_ppo2.py:137][0m Total time:      14.20 min
[32m[20221208 14:50:46 @agent_ppo2.py:139][0m 1146880 total steps have happened
[32m[20221208 14:50:46 @agent_ppo2.py:115][0m #------------------------ Iteration 560 --------------------------#
[32m[20221208 14:50:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:50:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |           0.0874 |           4.6545 |        -500.5590 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |           0.0607 |           4.2797 |        -412.5787 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |           0.0099 |           4.1533 |        -452.9450 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0183 |           4.0523 |        -479.3178 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0335 |           3.9642 |        -492.4067 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0483 |           3.8872 |        -507.3976 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0587 |           3.8383 |        -519.6268 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0642 |           3.8044 |        -528.2118 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0697 |           3.7591 |        -536.4017 |
[32m[20221208 14:50:47 @agent_ppo2.py:179][0m |          -0.0711 |           3.7019 |        -534.7049 |
[32m[20221208 14:50:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.39
[32m[20221208 14:50:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.78
[32m[20221208 14:50:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.68
[32m[20221208 14:50:48 @agent_ppo2.py:137][0m Total time:      14.22 min
[32m[20221208 14:50:48 @agent_ppo2.py:139][0m 1148928 total steps have happened
[32m[20221208 14:50:48 @agent_ppo2.py:115][0m #------------------------ Iteration 561 --------------------------#
[32m[20221208 14:50:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |           0.0514 |           3.3613 |        -477.0235 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |           0.0601 |           2.6920 |        -404.1551 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |           0.0096 |           2.3913 |        -424.5959 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |          -0.0220 |           2.2484 |        -454.2195 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |          -0.0385 |           2.1013 |        -469.2008 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |          -0.0482 |           2.0198 |        -471.5984 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |          -0.0561 |           1.9533 |        -476.5540 |
[32m[20221208 14:50:48 @agent_ppo2.py:179][0m |          -0.0648 |           1.8909 |        -481.1508 |
[32m[20221208 14:50:49 @agent_ppo2.py:179][0m |          -0.0707 |           1.8366 |        -490.4932 |
[32m[20221208 14:50:49 @agent_ppo2.py:179][0m |          -0.0690 |           1.8130 |        -495.6789 |
[32m[20221208 14:50:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.19
[32m[20221208 14:50:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.82
[32m[20221208 14:50:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.16
[32m[20221208 14:50:49 @agent_ppo2.py:137][0m Total time:      14.25 min
[32m[20221208 14:50:49 @agent_ppo2.py:139][0m 1150976 total steps have happened
[32m[20221208 14:50:49 @agent_ppo2.py:115][0m #------------------------ Iteration 562 --------------------------#
[32m[20221208 14:50:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |           0.0687 |           3.0689 |        -484.4389 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |           0.0397 |           2.8088 |        -423.8957 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0106 |           2.7093 |        -460.2425 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0198 |           2.5994 |        -457.3247 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0351 |           2.5599 |        -472.2394 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0488 |           2.5030 |        -481.4586 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0570 |           2.4685 |        -502.7356 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0577 |           2.4338 |        -497.2468 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0658 |           2.4203 |        -503.3987 |
[32m[20221208 14:50:50 @agent_ppo2.py:179][0m |          -0.0701 |           2.3917 |        -514.1701 |
[32m[20221208 14:50:50 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.67
[32m[20221208 14:50:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.32
[32m[20221208 14:50:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.43
[32m[20221208 14:50:51 @agent_ppo2.py:137][0m Total time:      14.27 min
[32m[20221208 14:50:51 @agent_ppo2.py:139][0m 1153024 total steps have happened
[32m[20221208 14:50:51 @agent_ppo2.py:115][0m #------------------------ Iteration 563 --------------------------#
[32m[20221208 14:50:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |           0.0694 |           2.2592 |        -358.2468 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |           0.0388 |           1.5915 |        -273.2144 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |          -0.0176 |           1.4105 |        -296.5152 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |          -0.0453 |           1.2988 |        -325.1008 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |          -0.0586 |           1.2041 |        -339.0810 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |          -0.0732 |           1.1483 |        -344.5476 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |          -0.0824 |           1.1156 |        -351.3887 |
[32m[20221208 14:50:51 @agent_ppo2.py:179][0m |          -0.0836 |           1.0813 |        -359.0225 |
[32m[20221208 14:50:52 @agent_ppo2.py:179][0m |          -0.0861 |           1.0632 |        -365.7588 |
[32m[20221208 14:50:52 @agent_ppo2.py:179][0m |          -0.0902 |           1.0381 |        -371.4481 |
[32m[20221208 14:50:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.07
[32m[20221208 14:50:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.80
[32m[20221208 14:50:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.72
[32m[20221208 14:50:52 @agent_ppo2.py:137][0m Total time:      14.30 min
[32m[20221208 14:50:52 @agent_ppo2.py:139][0m 1155072 total steps have happened
[32m[20221208 14:50:52 @agent_ppo2.py:115][0m #------------------------ Iteration 564 --------------------------#
[32m[20221208 14:50:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |           0.0660 |           0.6274 |        -508.9601 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |           0.0074 |           0.4564 |        -491.4909 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |           0.0074 |           0.4119 |        -474.2568 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |          -0.0140 |           0.3957 |        -483.5893 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |          -0.0048 |           0.3807 |        -472.6102 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |           0.0021 |           0.3742 |        -485.9750 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |          -0.0024 |           0.3671 |        -463.1488 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |          -0.0146 |           0.3647 |        -486.6766 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |          -0.0158 |           0.3580 |        -492.5455 |
[32m[20221208 14:50:53 @agent_ppo2.py:179][0m |          -0.0196 |           0.3577 |        -485.9813 |
[32m[20221208 14:50:53 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:50:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.93
[32m[20221208 14:50:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.56
[32m[20221208 14:50:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.77
[32m[20221208 14:50:53 @agent_ppo2.py:137][0m Total time:      14.32 min
[32m[20221208 14:50:53 @agent_ppo2.py:139][0m 1157120 total steps have happened
[32m[20221208 14:50:53 @agent_ppo2.py:115][0m #------------------------ Iteration 565 --------------------------#
[32m[20221208 14:50:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |           0.1077 |           3.8409 |        -452.4434 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |           0.0905 |           3.0806 |        -383.7767 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |           0.0120 |           2.7905 |        -415.2314 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |          -0.0215 |           2.6199 |        -452.5830 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |          -0.0352 |           2.5150 |        -468.0385 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |          -0.0496 |           2.4532 |        -476.4531 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |          -0.0570 |           2.3238 |        -478.7430 |
[32m[20221208 14:50:54 @agent_ppo2.py:179][0m |          -0.0540 |           2.2382 |        -486.6113 |
[32m[20221208 14:50:55 @agent_ppo2.py:179][0m |          -0.0196 |           2.1710 |        -463.8599 |
[32m[20221208 14:50:55 @agent_ppo2.py:179][0m |          -0.0602 |           2.1344 |        -483.3880 |
[32m[20221208 14:50:55 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:50:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.16
[32m[20221208 14:50:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.32
[32m[20221208 14:50:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.86
[32m[20221208 14:50:55 @agent_ppo2.py:137][0m Total time:      14.35 min
[32m[20221208 14:50:55 @agent_ppo2.py:139][0m 1159168 total steps have happened
[32m[20221208 14:50:55 @agent_ppo2.py:115][0m #------------------------ Iteration 566 --------------------------#
[32m[20221208 14:50:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |           0.1925 |           1.3276 |        -436.3482 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |           0.1158 |           1.0803 |        -259.5860 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |           0.0764 |           0.9804 |        -265.9134 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |           0.0356 |           0.9375 |        -222.2660 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |           0.0069 |           0.8943 |        -212.5998 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |          -0.0109 |           0.8669 |        -226.6606 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |          -0.0221 |           0.8455 |        -228.3629 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |          -0.0258 |           0.8261 |        -239.8823 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |          -0.0340 |           0.8085 |        -232.1253 |
[32m[20221208 14:50:56 @agent_ppo2.py:179][0m |          -0.0410 |           0.7895 |        -247.8947 |
[32m[20221208 14:50:56 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:50:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.95
[32m[20221208 14:50:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.66
[32m[20221208 14:50:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.56
[32m[20221208 14:50:57 @agent_ppo2.py:137][0m Total time:      14.37 min
[32m[20221208 14:50:57 @agent_ppo2.py:139][0m 1161216 total steps have happened
[32m[20221208 14:50:57 @agent_ppo2.py:115][0m #------------------------ Iteration 567 --------------------------#
[32m[20221208 14:50:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |           0.0814 |           1.3031 |        -441.0455 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |           0.0485 |           1.0643 |        -416.3662 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |           0.0107 |           0.9806 |        -443.8564 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |          -0.0121 |           0.9219 |        -456.6118 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |          -0.0262 |           0.8808 |        -470.3105 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |          -0.0325 |           0.8570 |        -481.0258 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |          -0.0428 |           0.8274 |        -488.2532 |
[32m[20221208 14:50:57 @agent_ppo2.py:179][0m |          -0.0459 |           0.7974 |        -493.9631 |
[32m[20221208 14:50:58 @agent_ppo2.py:179][0m |          -0.0407 |           0.7806 |        -488.3706 |
[32m[20221208 14:50:58 @agent_ppo2.py:179][0m |          -0.0502 |           0.7794 |        -499.9082 |
[32m[20221208 14:50:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:50:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.78
[32m[20221208 14:50:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.74
[32m[20221208 14:50:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.51
[32m[20221208 14:50:58 @agent_ppo2.py:137][0m Total time:      14.40 min
[32m[20221208 14:50:58 @agent_ppo2.py:139][0m 1163264 total steps have happened
[32m[20221208 14:50:58 @agent_ppo2.py:115][0m #------------------------ Iteration 568 --------------------------#
[32m[20221208 14:50:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:50:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |           0.0640 |           4.1765 |        -460.1658 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |           0.0347 |           3.5611 |        -414.3206 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0160 |           3.3432 |        -437.6275 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0361 |           3.1716 |        -459.3591 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0494 |           3.0479 |        -461.1451 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0549 |           2.9769 |        -465.0527 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0581 |           2.9151 |        -462.4780 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0674 |           2.8073 |        -464.3013 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0708 |           2.7685 |        -475.7744 |
[32m[20221208 14:50:59 @agent_ppo2.py:179][0m |          -0.0787 |           2.7080 |        -481.5453 |
[32m[20221208 14:50:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.75
[32m[20221208 14:51:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.58
[32m[20221208 14:51:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.80
[32m[20221208 14:51:00 @agent_ppo2.py:137][0m Total time:      14.42 min
[32m[20221208 14:51:00 @agent_ppo2.py:139][0m 1165312 total steps have happened
[32m[20221208 14:51:00 @agent_ppo2.py:115][0m #------------------------ Iteration 569 --------------------------#
[32m[20221208 14:51:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |           0.1102 |           4.2218 |        -440.7475 |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |           0.0925 |           3.9714 |        -356.4929 |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |           0.0207 |           3.8067 |        -429.6562 |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |          -0.0129 |           3.6803 |        -462.3552 |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |          -0.0342 |           3.6292 |        -492.2419 |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |          -0.0397 |           3.5501 |        -494.7625 |
[32m[20221208 14:51:00 @agent_ppo2.py:179][0m |          -0.0485 |           3.4746 |        -501.0962 |
[32m[20221208 14:51:01 @agent_ppo2.py:179][0m |          -0.0549 |           3.4529 |        -515.7946 |
[32m[20221208 14:51:01 @agent_ppo2.py:179][0m |          -0.0615 |           3.4032 |        -513.0361 |
[32m[20221208 14:51:01 @agent_ppo2.py:179][0m |          -0.0673 |           3.3784 |        -528.3115 |
[32m[20221208 14:51:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.57
[32m[20221208 14:51:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.70
[32m[20221208 14:51:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.10
[32m[20221208 14:51:01 @agent_ppo2.py:137][0m Total time:      14.45 min
[32m[20221208 14:51:01 @agent_ppo2.py:139][0m 1167360 total steps have happened
[32m[20221208 14:51:01 @agent_ppo2.py:115][0m #------------------------ Iteration 570 --------------------------#
[32m[20221208 14:51:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |           0.0601 |           2.4234 |        -473.6331 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |           0.0321 |           2.1855 |        -460.2972 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0052 |           2.0667 |        -469.4940 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0151 |           2.0298 |        -474.3320 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0367 |           1.9825 |        -474.1165 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0416 |           1.9244 |        -492.4751 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0469 |           1.8894 |        -492.1259 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0531 |           1.8631 |        -501.6053 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0544 |           1.8335 |        -504.3675 |
[32m[20221208 14:51:02 @agent_ppo2.py:179][0m |          -0.0599 |           1.8141 |        -507.4146 |
[32m[20221208 14:51:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.99
[32m[20221208 14:51:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.55
[32m[20221208 14:51:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.79
[32m[20221208 14:51:03 @agent_ppo2.py:137][0m Total time:      14.47 min
[32m[20221208 14:51:03 @agent_ppo2.py:139][0m 1169408 total steps have happened
[32m[20221208 14:51:03 @agent_ppo2.py:115][0m #------------------------ Iteration 571 --------------------------#
[32m[20221208 14:51:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.0986 |           1.4024 |        -440.5267 |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.1313 |           1.2341 |        -288.2666 |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.0704 |           1.1833 |        -329.9508 |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.0590 |           1.1464 |        -347.1550 |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.0471 |           1.1375 |        -354.9348 |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.0448 |           1.0970 |        -353.4308 |
[32m[20221208 14:51:03 @agent_ppo2.py:179][0m |           0.0406 |           1.0859 |        -356.5262 |
[32m[20221208 14:51:04 @agent_ppo2.py:179][0m |           0.0423 |           1.0859 |        -357.6414 |
[32m[20221208 14:51:04 @agent_ppo2.py:179][0m |           0.0395 |           1.0682 |        -358.3021 |
[32m[20221208 14:51:04 @agent_ppo2.py:179][0m |           0.0388 |           1.0541 |        -350.4536 |
[32m[20221208 14:51:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:51:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.82
[32m[20221208 14:51:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.81
[32m[20221208 14:51:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.73
[32m[20221208 14:51:04 @agent_ppo2.py:137][0m Total time:      14.50 min
[32m[20221208 14:51:04 @agent_ppo2.py:139][0m 1171456 total steps have happened
[32m[20221208 14:51:04 @agent_ppo2.py:115][0m #------------------------ Iteration 572 --------------------------#
[32m[20221208 14:51:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |           0.0675 |           4.6556 |        -429.1931 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |           0.0681 |           3.9234 |        -321.4641 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |           0.0218 |           3.7043 |        -361.2492 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0026 |           3.5813 |        -384.1436 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0352 |           3.4875 |        -417.9392 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0469 |           3.4300 |        -428.1376 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0542 |           3.3736 |        -445.8348 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0601 |           3.3183 |        -451.5620 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0665 |           3.3044 |        -464.1141 |
[32m[20221208 14:51:05 @agent_ppo2.py:179][0m |          -0.0668 |           3.2243 |        -463.7131 |
[32m[20221208 14:51:05 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.92
[32m[20221208 14:51:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.19
[32m[20221208 14:51:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.43
[32m[20221208 14:51:06 @agent_ppo2.py:137][0m Total time:      14.52 min
[32m[20221208 14:51:06 @agent_ppo2.py:139][0m 1173504 total steps have happened
[32m[20221208 14:51:06 @agent_ppo2.py:115][0m #------------------------ Iteration 573 --------------------------#
[32m[20221208 14:51:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |           0.1217 |           3.5842 |        -442.4187 |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |           0.0881 |           3.2448 |        -292.4868 |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |           0.0409 |           3.1569 |        -336.2735 |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |           0.0067 |           3.0756 |        -394.5822 |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |          -0.0142 |           3.0359 |        -437.3989 |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |          -0.0237 |           2.9924 |        -450.9414 |
[32m[20221208 14:51:06 @agent_ppo2.py:179][0m |          -0.0322 |           2.9609 |        -460.9279 |
[32m[20221208 14:51:07 @agent_ppo2.py:179][0m |          -0.0401 |           2.9425 |        -465.0655 |
[32m[20221208 14:51:07 @agent_ppo2.py:179][0m |          -0.0430 |           2.9011 |        -474.5873 |
[32m[20221208 14:51:07 @agent_ppo2.py:179][0m |          -0.0488 |           2.8592 |        -485.2051 |
[32m[20221208 14:51:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.06
[32m[20221208 14:51:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.25
[32m[20221208 14:51:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.37
[32m[20221208 14:51:07 @agent_ppo2.py:137][0m Total time:      14.55 min
[32m[20221208 14:51:07 @agent_ppo2.py:139][0m 1175552 total steps have happened
[32m[20221208 14:51:07 @agent_ppo2.py:115][0m #------------------------ Iteration 574 --------------------------#
[32m[20221208 14:51:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |           0.0783 |           4.7644 |        -425.4932 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |           0.0504 |           4.1607 |        -368.0137 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |           0.0011 |           3.9768 |        -418.3156 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0254 |           3.8003 |        -450.1108 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0394 |           3.7158 |        -464.3485 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0518 |           3.5956 |        -480.4434 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0563 |           3.5363 |        -491.5284 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0588 |           3.4762 |        -498.5846 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0624 |           3.3956 |        -500.3853 |
[32m[20221208 14:51:08 @agent_ppo2.py:179][0m |          -0.0656 |           3.3836 |        -506.2864 |
[32m[20221208 14:51:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.17
[32m[20221208 14:51:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.97
[32m[20221208 14:51:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.06
[32m[20221208 14:51:09 @agent_ppo2.py:137][0m Total time:      14.57 min
[32m[20221208 14:51:09 @agent_ppo2.py:139][0m 1177600 total steps have happened
[32m[20221208 14:51:09 @agent_ppo2.py:115][0m #------------------------ Iteration 575 --------------------------#
[32m[20221208 14:51:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |           0.0784 |           2.7947 |        -418.7069 |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |           2.0405 |           2.5341 |        -248.8534 |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |           0.0180 |           2.4417 |        -223.4075 |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |          -0.0215 |           2.3643 |        -260.2322 |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |          -0.0342 |           2.3160 |        -270.7106 |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |          -0.0541 |           2.2458 |        -287.5373 |
[32m[20221208 14:51:09 @agent_ppo2.py:179][0m |          -0.0643 |           2.1895 |        -306.6425 |
[32m[20221208 14:51:10 @agent_ppo2.py:179][0m |          -0.0694 |           2.1302 |        -308.1786 |
[32m[20221208 14:51:10 @agent_ppo2.py:179][0m |          -0.0708 |           2.1084 |        -317.2140 |
[32m[20221208 14:51:10 @agent_ppo2.py:179][0m |          -0.0768 |           2.0737 |        -314.5501 |
[32m[20221208 14:51:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.99
[32m[20221208 14:51:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.19
[32m[20221208 14:51:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.73
[32m[20221208 14:51:10 @agent_ppo2.py:137][0m Total time:      14.60 min
[32m[20221208 14:51:10 @agent_ppo2.py:139][0m 1179648 total steps have happened
[32m[20221208 14:51:10 @agent_ppo2.py:115][0m #------------------------ Iteration 576 --------------------------#
[32m[20221208 14:51:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |           0.0554 |           3.7953 |        -458.9493 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |           0.0803 |           3.4628 |        -394.3260 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |           0.0056 |           3.3361 |        -445.3069 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0219 |           3.2111 |        -471.8472 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0318 |           3.1382 |        -474.4924 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0397 |           3.0822 |        -486.5129 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0432 |           3.0152 |        -504.3797 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0468 |           2.9521 |        -504.8345 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0524 |           2.9164 |        -511.5472 |
[32m[20221208 14:51:11 @agent_ppo2.py:179][0m |          -0.0526 |           2.8936 |        -509.3405 |
[32m[20221208 14:51:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:51:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.62
[32m[20221208 14:51:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.40
[32m[20221208 14:51:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.93
[32m[20221208 14:51:12 @agent_ppo2.py:137][0m Total time:      14.62 min
[32m[20221208 14:51:12 @agent_ppo2.py:139][0m 1181696 total steps have happened
[32m[20221208 14:51:12 @agent_ppo2.py:115][0m #------------------------ Iteration 577 --------------------------#
[32m[20221208 14:51:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:12 @agent_ppo2.py:179][0m |           0.0868 |           1.2407 |        -382.4661 |
[32m[20221208 14:51:12 @agent_ppo2.py:179][0m |           0.0774 |           0.9584 |        -123.7782 |
[32m[20221208 14:51:12 @agent_ppo2.py:179][0m |           0.0035 |           0.8657 |        -127.3068 |
[32m[20221208 14:51:12 @agent_ppo2.py:179][0m |          -0.0296 |           0.8221 |        -156.7064 |
[32m[20221208 14:51:12 @agent_ppo2.py:179][0m |          -0.0485 |           0.7900 |        -169.7520 |
[32m[20221208 14:51:12 @agent_ppo2.py:179][0m |          -0.0583 |           0.7737 |        -180.4224 |
[32m[20221208 14:51:13 @agent_ppo2.py:179][0m |          -0.0608 |           0.7608 |        -189.6947 |
[32m[20221208 14:51:13 @agent_ppo2.py:179][0m |          -0.0661 |           0.7367 |        -194.9834 |
[32m[20221208 14:51:13 @agent_ppo2.py:179][0m |          -0.0633 |           0.7224 |        -190.9019 |
[32m[20221208 14:51:13 @agent_ppo2.py:179][0m |          -0.0746 |           0.7152 |        -208.8592 |
[32m[20221208 14:51:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:51:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.63
[32m[20221208 14:51:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.28
[32m[20221208 14:51:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.88
[32m[20221208 14:51:13 @agent_ppo2.py:137][0m Total time:      14.65 min
[32m[20221208 14:51:13 @agent_ppo2.py:139][0m 1183744 total steps have happened
[32m[20221208 14:51:13 @agent_ppo2.py:115][0m #------------------------ Iteration 578 --------------------------#
[32m[20221208 14:51:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |           0.0670 |           2.7368 |        -458.7282 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |           0.0573 |           2.5254 |        -444.6577 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |           0.0250 |           2.3918 |        -366.3354 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |           0.0042 |           2.2832 |        -373.0429 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |           0.0048 |           2.2302 |        -378.1562 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |          -0.0241 |           2.1751 |        -429.8857 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |          -0.0431 |           2.1405 |        -475.5840 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |          -0.0490 |           2.0953 |        -491.1087 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |          -0.0495 |           2.0655 |        -510.5213 |
[32m[20221208 14:51:14 @agent_ppo2.py:179][0m |          -0.0535 |           2.0173 |        -498.3236 |
[32m[20221208 14:51:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.30
[32m[20221208 14:51:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.54
[32m[20221208 14:51:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.52
[32m[20221208 14:51:15 @agent_ppo2.py:137][0m Total time:      14.67 min
[32m[20221208 14:51:15 @agent_ppo2.py:139][0m 1185792 total steps have happened
[32m[20221208 14:51:15 @agent_ppo2.py:115][0m #------------------------ Iteration 579 --------------------------#
[32m[20221208 14:51:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:15 @agent_ppo2.py:179][0m |           0.0525 |           3.8286 |        -446.3692 |
[32m[20221208 14:51:15 @agent_ppo2.py:179][0m |           0.0512 |           3.5494 |        -363.5590 |
[32m[20221208 14:51:15 @agent_ppo2.py:179][0m |           0.0301 |           3.3983 |        -297.8246 |
[32m[20221208 14:51:15 @agent_ppo2.py:179][0m |          -0.0218 |           3.3020 |        -308.5569 |
[32m[20221208 14:51:15 @agent_ppo2.py:179][0m |          -0.0391 |           3.2402 |        -324.1684 |
[32m[20221208 14:51:15 @agent_ppo2.py:179][0m |          -0.0483 |           3.2101 |        -328.0048 |
[32m[20221208 14:51:16 @agent_ppo2.py:179][0m |          -0.0573 |           3.1334 |        -343.8572 |
[32m[20221208 14:51:16 @agent_ppo2.py:179][0m |          -0.0586 |           3.1106 |        -346.9732 |
[32m[20221208 14:51:16 @agent_ppo2.py:179][0m |          -0.0640 |           3.0576 |        -356.1520 |
[32m[20221208 14:51:16 @agent_ppo2.py:179][0m |          -0.0657 |           3.0467 |        -359.4569 |
[32m[20221208 14:51:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.62
[32m[20221208 14:51:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.66
[32m[20221208 14:51:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.69
[32m[20221208 14:51:16 @agent_ppo2.py:137][0m Total time:      14.70 min
[32m[20221208 14:51:16 @agent_ppo2.py:139][0m 1187840 total steps have happened
[32m[20221208 14:51:16 @agent_ppo2.py:115][0m #------------------------ Iteration 580 --------------------------#
[32m[20221208 14:51:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |           0.1006 |           3.9983 |        -442.6627 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |           0.1083 |           3.6312 |        -345.6605 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |           0.0312 |           3.5117 |        -376.4882 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |           0.0137 |           3.4195 |        -396.1739 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |          -0.0161 |           3.3975 |        -453.3002 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |          -0.0232 |           3.2964 |        -466.2258 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |          -0.0361 |           3.2590 |        -487.2038 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |          -0.0466 |           3.2067 |        -506.4885 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |          -0.0515 |           3.1550 |        -510.6838 |
[32m[20221208 14:51:17 @agent_ppo2.py:179][0m |          -0.0482 |           3.1319 |        -519.1146 |
[32m[20221208 14:51:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.29
[32m[20221208 14:51:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.37
[32m[20221208 14:51:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.03
[32m[20221208 14:51:18 @agent_ppo2.py:137][0m Total time:      14.72 min
[32m[20221208 14:51:18 @agent_ppo2.py:139][0m 1189888 total steps have happened
[32m[20221208 14:51:18 @agent_ppo2.py:115][0m #------------------------ Iteration 581 --------------------------#
[32m[20221208 14:51:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:18 @agent_ppo2.py:179][0m |           0.0701 |           2.6391 |        -470.0710 |
[32m[20221208 14:51:18 @agent_ppo2.py:179][0m |           0.0585 |           2.3238 |        -291.8934 |
[32m[20221208 14:51:18 @agent_ppo2.py:179][0m |           0.0180 |           2.2140 |        -255.4668 |
[32m[20221208 14:51:18 @agent_ppo2.py:179][0m |          -0.0052 |           2.1066 |        -287.0495 |
[32m[20221208 14:51:18 @agent_ppo2.py:179][0m |          -0.0072 |           2.0238 |        -251.8464 |
[32m[20221208 14:51:18 @agent_ppo2.py:179][0m |          -0.0235 |           1.9375 |        -280.4798 |
[32m[20221208 14:51:19 @agent_ppo2.py:179][0m |          -0.0269 |           1.8993 |        -321.4538 |
[32m[20221208 14:51:19 @agent_ppo2.py:179][0m |          -0.0327 |           1.7992 |        -311.9712 |
[32m[20221208 14:51:19 @agent_ppo2.py:179][0m |          -0.0382 |           1.7574 |        -353.3928 |
[32m[20221208 14:51:19 @agent_ppo2.py:179][0m |          -0.0417 |           1.7086 |        -430.7774 |
[32m[20221208 14:51:19 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.30
[32m[20221208 14:51:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.13
[32m[20221208 14:51:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.57
[32m[20221208 14:51:19 @agent_ppo2.py:137][0m Total time:      14.75 min
[32m[20221208 14:51:19 @agent_ppo2.py:139][0m 1191936 total steps have happened
[32m[20221208 14:51:19 @agent_ppo2.py:115][0m #------------------------ Iteration 582 --------------------------#
[32m[20221208 14:51:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |           0.0608 |           3.3497 |        -478.2839 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |           0.0128 |           3.1274 |        -451.5039 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |           0.0015 |           2.9868 |        -462.8809 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0004 |           2.9141 |        -445.3039 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0246 |           2.8741 |        -460.5326 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0372 |           2.7888 |        -468.8411 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0471 |           2.7623 |        -479.0725 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0468 |           2.7236 |        -479.6880 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0553 |           2.7130 |        -483.2837 |
[32m[20221208 14:51:20 @agent_ppo2.py:179][0m |          -0.0553 |           2.6614 |        -486.5485 |
[32m[20221208 14:51:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.11
[32m[20221208 14:51:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.96
[32m[20221208 14:51:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.31
[32m[20221208 14:51:21 @agent_ppo2.py:137][0m Total time:      14.77 min
[32m[20221208 14:51:21 @agent_ppo2.py:139][0m 1193984 total steps have happened
[32m[20221208 14:51:21 @agent_ppo2.py:115][0m #------------------------ Iteration 583 --------------------------#
[32m[20221208 14:51:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:21 @agent_ppo2.py:179][0m |           0.2364 |           4.0765 |        -434.2248 |
[32m[20221208 14:51:21 @agent_ppo2.py:179][0m |           0.0848 |           3.6744 |        -370.7932 |
[32m[20221208 14:51:21 @agent_ppo2.py:179][0m |           0.0414 |           3.5554 |        -360.9804 |
[32m[20221208 14:51:21 @agent_ppo2.py:179][0m |          -0.0005 |           3.4368 |        -417.0779 |
[32m[20221208 14:51:21 @agent_ppo2.py:179][0m |          -0.0096 |           3.3608 |        -431.6564 |
[32m[20221208 14:51:21 @agent_ppo2.py:179][0m |          -0.0262 |           3.3019 |        -449.5051 |
[32m[20221208 14:51:22 @agent_ppo2.py:179][0m |          -0.0390 |           3.2362 |        -476.1305 |
[32m[20221208 14:51:22 @agent_ppo2.py:179][0m |          -0.0439 |           3.1879 |        -486.8156 |
[32m[20221208 14:51:22 @agent_ppo2.py:179][0m |          -0.0335 |           3.1438 |        -489.0291 |
[32m[20221208 14:51:22 @agent_ppo2.py:179][0m |          -0.0388 |           3.0647 |        -488.7464 |
[32m[20221208 14:51:22 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.48
[32m[20221208 14:51:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.08
[32m[20221208 14:51:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.79
[32m[20221208 14:51:22 @agent_ppo2.py:137][0m Total time:      14.80 min
[32m[20221208 14:51:22 @agent_ppo2.py:139][0m 1196032 total steps have happened
[32m[20221208 14:51:22 @agent_ppo2.py:115][0m #------------------------ Iteration 584 --------------------------#
[32m[20221208 14:51:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |           0.1405 |           4.2019 |        -364.5434 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |           0.1403 |           3.5478 |        -294.8124 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |           0.0563 |           3.3429 |        -258.2326 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |           0.0142 |           3.1926 |        -334.8273 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |          -0.0019 |           3.1254 |        -373.9928 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |           0.0047 |           3.0016 |        -345.1699 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |          -0.0150 |           2.9543 |        -364.2672 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |          -0.0244 |           2.8860 |        -391.4540 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |          -0.0345 |           2.7980 |        -421.1505 |
[32m[20221208 14:51:23 @agent_ppo2.py:179][0m |          -0.0349 |           2.7575 |        -388.8738 |
[32m[20221208 14:51:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:51:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.07
[32m[20221208 14:51:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.65
[32m[20221208 14:51:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.07
[32m[20221208 14:51:24 @agent_ppo2.py:137][0m Total time:      14.82 min
[32m[20221208 14:51:24 @agent_ppo2.py:139][0m 1198080 total steps have happened
[32m[20221208 14:51:24 @agent_ppo2.py:115][0m #------------------------ Iteration 585 --------------------------#
[32m[20221208 14:51:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:24 @agent_ppo2.py:179][0m |           0.0473 |           5.7399 |        -512.0895 |
[32m[20221208 14:51:24 @agent_ppo2.py:179][0m |           0.0378 |           5.2555 |        -452.9653 |
[32m[20221208 14:51:24 @agent_ppo2.py:179][0m |           0.0031 |           5.0302 |        -491.3168 |
[32m[20221208 14:51:24 @agent_ppo2.py:179][0m |          -0.0268 |           4.9119 |        -506.3218 |
[32m[20221208 14:51:24 @agent_ppo2.py:179][0m |          -0.0388 |           4.7697 |        -526.4174 |
[32m[20221208 14:51:25 @agent_ppo2.py:179][0m |          -0.0474 |           4.6839 |        -537.1437 |
[32m[20221208 14:51:25 @agent_ppo2.py:179][0m |          -0.0576 |           4.6312 |        -543.2303 |
[32m[20221208 14:51:25 @agent_ppo2.py:179][0m |          -0.0593 |           4.5661 |        -546.1751 |
[32m[20221208 14:51:25 @agent_ppo2.py:179][0m |          -0.0652 |           4.5404 |        -558.3313 |
[32m[20221208 14:51:25 @agent_ppo2.py:179][0m |          -0.0679 |           4.4416 |        -567.2884 |
[32m[20221208 14:51:25 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.05
[32m[20221208 14:51:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.31
[32m[20221208 14:51:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.61
[32m[20221208 14:51:25 @agent_ppo2.py:137][0m Total time:      14.85 min
[32m[20221208 14:51:25 @agent_ppo2.py:139][0m 1200128 total steps have happened
[32m[20221208 14:51:25 @agent_ppo2.py:115][0m #------------------------ Iteration 586 --------------------------#
[32m[20221208 14:51:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |           0.0719 |           2.3574 |        -441.7972 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |           0.0961 |           1.8021 |        -178.2320 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |           0.0224 |           1.5301 |        -198.1444 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |           0.0100 |           1.3350 |        -205.1159 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |           0.0074 |           1.1941 |        -206.7392 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |          -0.0150 |           1.1072 |        -221.7819 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |          -0.0208 |           1.0190 |        -234.3584 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |          -0.0244 |           0.9713 |        -241.4429 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |          -0.0249 |           0.9056 |        -239.5663 |
[32m[20221208 14:51:26 @agent_ppo2.py:179][0m |          -0.0307 |           0.8714 |        -252.9533 |
[32m[20221208 14:51:26 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.62
[32m[20221208 14:51:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.07
[32m[20221208 14:51:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.66
[32m[20221208 14:51:27 @agent_ppo2.py:137][0m Total time:      14.87 min
[32m[20221208 14:51:27 @agent_ppo2.py:139][0m 1202176 total steps have happened
[32m[20221208 14:51:27 @agent_ppo2.py:115][0m #------------------------ Iteration 587 --------------------------#
[32m[20221208 14:51:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:27 @agent_ppo2.py:179][0m |           0.0958 |           6.5637 |        -451.7523 |
[32m[20221208 14:51:27 @agent_ppo2.py:179][0m |           0.0891 |           5.8394 |        -352.6214 |
[32m[20221208 14:51:27 @agent_ppo2.py:179][0m |           0.0599 |           5.6251 |        -335.8836 |
[32m[20221208 14:51:27 @agent_ppo2.py:179][0m |           0.0020 |           5.5737 |        -425.7435 |
[32m[20221208 14:51:28 @agent_ppo2.py:179][0m |          -0.0159 |           5.4687 |        -468.9433 |
[32m[20221208 14:51:28 @agent_ppo2.py:179][0m |          -0.0261 |           5.4290 |        -477.5219 |
[32m[20221208 14:51:28 @agent_ppo2.py:179][0m |          -0.0205 |           5.3771 |        -472.8779 |
[32m[20221208 14:51:28 @agent_ppo2.py:179][0m |          -0.0316 |           5.3488 |        -487.6876 |
[32m[20221208 14:51:28 @agent_ppo2.py:179][0m |          -0.0452 |           5.2819 |        -511.3160 |
[32m[20221208 14:51:28 @agent_ppo2.py:179][0m |          -0.0536 |           5.2431 |        -522.5992 |
[32m[20221208 14:51:28 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.94
[32m[20221208 14:51:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.52
[32m[20221208 14:51:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.18
[32m[20221208 14:51:28 @agent_ppo2.py:137][0m Total time:      14.90 min
[32m[20221208 14:51:28 @agent_ppo2.py:139][0m 1204224 total steps have happened
[32m[20221208 14:51:28 @agent_ppo2.py:115][0m #------------------------ Iteration 588 --------------------------#
[32m[20221208 14:51:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |           0.0734 |           4.5216 |        -486.6276 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |           0.0573 |           4.0869 |        -385.3319 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |           0.0239 |           3.9342 |        -424.8559 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0091 |           3.7668 |        -483.1796 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0251 |           3.6973 |        -515.7690 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0376 |           3.6422 |        -531.8853 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0368 |           3.6020 |        -522.1953 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0318 |           3.5420 |        -534.2335 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0457 |           3.5267 |        -546.6759 |
[32m[20221208 14:51:29 @agent_ppo2.py:179][0m |          -0.0527 |           3.4822 |        -549.1929 |
[32m[20221208 14:51:29 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:51:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.08
[32m[20221208 14:51:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.60
[32m[20221208 14:51:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.38
[32m[20221208 14:51:30 @agent_ppo2.py:137][0m Total time:      14.93 min
[32m[20221208 14:51:30 @agent_ppo2.py:139][0m 1206272 total steps have happened
[32m[20221208 14:51:30 @agent_ppo2.py:115][0m #------------------------ Iteration 589 --------------------------#
[32m[20221208 14:51:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:30 @agent_ppo2.py:179][0m |           0.1992 |           4.3223 |        -439.9322 |
[32m[20221208 14:51:30 @agent_ppo2.py:179][0m |           0.0987 |           4.0472 |        -340.0832 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |           0.0481 |           3.9192 |        -410.3222 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |           0.0199 |           3.7648 |        -465.1782 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |          -0.0038 |           3.6791 |        -487.6871 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |          -0.0224 |           3.5844 |        -502.0661 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |          -0.0290 |           3.5586 |        -526.0712 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |          -0.0365 |           3.4812 |        -526.0523 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |          -0.0433 |           3.4472 |        -545.2897 |
[32m[20221208 14:51:31 @agent_ppo2.py:179][0m |          -0.0458 |           3.4021 |        -543.9772 |
[32m[20221208 14:51:31 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.81
[32m[20221208 14:51:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.58
[32m[20221208 14:51:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.98
[32m[20221208 14:51:31 @agent_ppo2.py:137][0m Total time:      14.95 min
[32m[20221208 14:51:31 @agent_ppo2.py:139][0m 1208320 total steps have happened
[32m[20221208 14:51:31 @agent_ppo2.py:115][0m #------------------------ Iteration 590 --------------------------#
[32m[20221208 14:51:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |           0.0578 |           2.6627 |        -461.2099 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |           0.0416 |           2.2993 |        -395.2908 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0008 |           2.2020 |        -441.2464 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0280 |           2.1201 |        -478.2918 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0329 |           2.0740 |        -477.9740 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0429 |           2.0430 |        -493.9069 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0483 |           2.0115 |        -496.4004 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0512 |           1.9800 |        -508.9664 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0487 |           1.9568 |        -499.8575 |
[32m[20221208 14:51:32 @agent_ppo2.py:179][0m |          -0.0501 |           1.9173 |        -504.2190 |
[32m[20221208 14:51:32 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.70
[32m[20221208 14:51:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.81
[32m[20221208 14:51:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.18
[32m[20221208 14:51:33 @agent_ppo2.py:137][0m Total time:      14.98 min
[32m[20221208 14:51:33 @agent_ppo2.py:139][0m 1210368 total steps have happened
[32m[20221208 14:51:33 @agent_ppo2.py:115][0m #------------------------ Iteration 591 --------------------------#
[32m[20221208 14:51:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:33 @agent_ppo2.py:179][0m |           0.0501 |           0.7506 |        -473.4528 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |           0.0124 |           0.5781 |        -455.2064 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |           0.0038 |           0.5514 |        -459.5522 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |           0.0394 |           0.5339 |        -338.3572 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |           0.0020 |           0.5163 |        -448.0490 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |           0.0194 |           0.5096 |        -420.8688 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |          -0.0075 |           0.5003 |        -474.2681 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |          -0.0070 |           0.4968 |        -449.8602 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |          -0.0134 |           0.4857 |        -465.6210 |
[32m[20221208 14:51:34 @agent_ppo2.py:179][0m |          -0.0110 |           0.4799 |        -470.2250 |
[32m[20221208 14:51:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:51:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.12
[32m[20221208 14:51:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.60
[32m[20221208 14:51:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.04
[32m[20221208 14:51:34 @agent_ppo2.py:137][0m Total time:      15.00 min
[32m[20221208 14:51:34 @agent_ppo2.py:139][0m 1212416 total steps have happened
[32m[20221208 14:51:34 @agent_ppo2.py:115][0m #------------------------ Iteration 592 --------------------------#
[32m[20221208 14:51:35 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:51:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |           0.0824 |           4.6160 |        -460.0521 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |           0.0445 |           4.0020 |        -394.3255 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |           0.0005 |           3.7421 |        -433.0807 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |          -0.0168 |           3.6623 |        -450.7813 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |          -0.0197 |           3.5278 |        -462.9176 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |          -0.0370 |           3.4478 |        -471.5155 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |          -0.0476 |           3.3431 |        -486.3999 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |          -0.0525 |           3.2879 |        -495.2040 |
[32m[20221208 14:51:35 @agent_ppo2.py:179][0m |          -0.0540 |           3.1748 |        -499.0815 |
[32m[20221208 14:51:36 @agent_ppo2.py:179][0m |          -0.0528 |           3.1194 |        -505.2022 |
[32m[20221208 14:51:36 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.96
[32m[20221208 14:51:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.74
[32m[20221208 14:51:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.17
[32m[20221208 14:51:36 @agent_ppo2.py:137][0m Total time:      15.03 min
[32m[20221208 14:51:36 @agent_ppo2.py:139][0m 1214464 total steps have happened
[32m[20221208 14:51:36 @agent_ppo2.py:115][0m #------------------------ Iteration 593 --------------------------#
[32m[20221208 14:51:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:36 @agent_ppo2.py:179][0m |           0.0802 |           4.9629 |        -432.7042 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |           0.0385 |           4.5532 |        -417.4341 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |           0.0156 |           4.3600 |        -452.3164 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0172 |           4.2367 |        -474.2114 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0319 |           4.1980 |        -490.7360 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0406 |           4.0868 |        -504.4038 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0423 |           4.0010 |        -503.1719 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0509 |           3.9818 |        -521.0214 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0463 |           3.9691 |        -511.5994 |
[32m[20221208 14:51:37 @agent_ppo2.py:179][0m |          -0.0490 |           3.9068 |        -519.9885 |
[32m[20221208 14:51:37 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.17
[32m[20221208 14:51:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.06
[32m[20221208 14:51:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.56
[32m[20221208 14:51:37 @agent_ppo2.py:137][0m Total time:      15.05 min
[32m[20221208 14:51:37 @agent_ppo2.py:139][0m 1216512 total steps have happened
[32m[20221208 14:51:37 @agent_ppo2.py:115][0m #------------------------ Iteration 594 --------------------------#
[32m[20221208 14:51:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |           0.0980 |           4.3516 |        -452.6089 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |           0.0506 |           3.9830 |        -373.0456 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |           0.0136 |           3.8838 |        -410.4552 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |          -0.0039 |           3.8311 |        -434.9209 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |          -0.0129 |           3.7354 |        -432.3377 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |          -0.0133 |           3.6672 |        -412.2101 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |          -0.0340 |           3.6616 |        -449.9163 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |          -0.0422 |           3.6013 |        -460.7831 |
[32m[20221208 14:51:38 @agent_ppo2.py:179][0m |          -0.0473 |           3.5944 |        -469.7001 |
[32m[20221208 14:51:39 @agent_ppo2.py:179][0m |          -0.0534 |           3.5271 |        -486.2143 |
[32m[20221208 14:51:39 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.93
[32m[20221208 14:51:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.31
[32m[20221208 14:51:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.32
[32m[20221208 14:51:39 @agent_ppo2.py:137][0m Total time:      15.08 min
[32m[20221208 14:51:39 @agent_ppo2.py:139][0m 1218560 total steps have happened
[32m[20221208 14:51:39 @agent_ppo2.py:115][0m #------------------------ Iteration 595 --------------------------#
[32m[20221208 14:51:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |           0.0573 |           7.2664 |        -419.1288 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |           0.0293 |           6.6977 |        -380.2065 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |           0.0232 |           6.4914 |        -416.4492 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0059 |           6.2459 |        -444.5535 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0141 |           6.0712 |        -463.5129 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0310 |           5.9488 |        -491.9919 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0426 |           5.7417 |        -505.4472 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0501 |           5.6125 |        -516.8281 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0533 |           5.4878 |        -528.7835 |
[32m[20221208 14:51:40 @agent_ppo2.py:179][0m |          -0.0573 |           5.3992 |        -535.8427 |
[32m[20221208 14:51:40 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.41
[32m[20221208 14:51:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.89
[32m[20221208 14:51:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.84
[32m[20221208 14:51:40 @agent_ppo2.py:137][0m Total time:      15.10 min
[32m[20221208 14:51:40 @agent_ppo2.py:139][0m 1220608 total steps have happened
[32m[20221208 14:51:40 @agent_ppo2.py:115][0m #------------------------ Iteration 596 --------------------------#
[32m[20221208 14:51:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |           0.0516 |           5.4746 |        -488.9967 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |           0.0597 |           4.8785 |        -369.1381 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |           0.0545 |           4.7735 |        -362.3000 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |           0.0055 |           4.6003 |        -433.9240 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |          -0.0179 |           4.4972 |        -464.8135 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |          -0.0280 |           4.4634 |        -473.9934 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |          -0.0401 |           4.3668 |        -489.0958 |
[32m[20221208 14:51:41 @agent_ppo2.py:179][0m |          -0.0452 |           4.3309 |        -495.5142 |
[32m[20221208 14:51:42 @agent_ppo2.py:179][0m |          -0.0500 |           4.3196 |        -503.9761 |
[32m[20221208 14:51:42 @agent_ppo2.py:179][0m |          -0.0503 |           4.2997 |        -513.5203 |
[32m[20221208 14:51:42 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.96
[32m[20221208 14:51:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.15
[32m[20221208 14:51:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.07
[32m[20221208 14:51:42 @agent_ppo2.py:137][0m Total time:      15.13 min
[32m[20221208 14:51:42 @agent_ppo2.py:139][0m 1222656 total steps have happened
[32m[20221208 14:51:42 @agent_ppo2.py:115][0m #------------------------ Iteration 597 --------------------------#
[32m[20221208 14:51:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |           0.0701 |           5.8820 |        -425.6790 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |           0.0651 |           5.3911 |        -354.3016 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |           0.0203 |           5.1533 |        -376.0567 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0127 |           4.9725 |        -422.1896 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0209 |           4.8538 |        -429.5993 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0354 |           4.7620 |        -448.1299 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0438 |           4.6838 |        -455.6021 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0503 |           4.6416 |        -467.9681 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0532 |           4.5466 |        -470.4450 |
[32m[20221208 14:51:43 @agent_ppo2.py:179][0m |          -0.0544 |           4.5199 |        -488.8253 |
[32m[20221208 14:51:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:51:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.33
[32m[20221208 14:51:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.57
[32m[20221208 14:51:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.81
[32m[20221208 14:51:43 @agent_ppo2.py:137][0m Total time:      15.15 min
[32m[20221208 14:51:43 @agent_ppo2.py:139][0m 1224704 total steps have happened
[32m[20221208 14:51:43 @agent_ppo2.py:115][0m #------------------------ Iteration 598 --------------------------#
[32m[20221208 14:51:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |           0.0805 |           4.4568 |        -426.8623 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |           0.0587 |           3.9097 |        -393.3552 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |           0.0261 |           3.6564 |        -422.7039 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |          -0.0103 |           3.4827 |        -432.9308 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |          -0.0326 |           3.3265 |        -461.2174 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |          -0.0372 |           3.2535 |        -469.6361 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |          -0.0445 |           3.1623 |        -482.9798 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |          -0.0397 |           3.1003 |        -474.2179 |
[32m[20221208 14:51:44 @agent_ppo2.py:179][0m |          -0.0503 |           3.0285 |        -490.7335 |
[32m[20221208 14:51:45 @agent_ppo2.py:179][0m |          -0.0561 |           2.9903 |        -496.5109 |
[32m[20221208 14:51:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:51:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.82
[32m[20221208 14:51:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.34
[32m[20221208 14:51:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.65
[32m[20221208 14:51:45 @agent_ppo2.py:137][0m Total time:      15.18 min
[32m[20221208 14:51:45 @agent_ppo2.py:139][0m 1226752 total steps have happened
[32m[20221208 14:51:45 @agent_ppo2.py:115][0m #------------------------ Iteration 599 --------------------------#
[32m[20221208 14:51:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |           0.0288 |           2.6643 |        -509.3323 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |           0.0095 |           2.3867 |        -497.8688 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0067 |           2.2762 |        -498.3735 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0198 |           2.2077 |        -514.4305 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0281 |           2.1566 |        -528.0618 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0316 |           2.0651 |        -522.0162 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0325 |           2.0153 |        -527.9475 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0353 |           1.9669 |        -533.5494 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0370 |           1.9714 |        -537.3203 |
[32m[20221208 14:51:46 @agent_ppo2.py:179][0m |          -0.0405 |           1.9156 |        -547.8848 |
[32m[20221208 14:51:46 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.14
[32m[20221208 14:51:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.68
[32m[20221208 14:51:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.43
[32m[20221208 14:51:46 @agent_ppo2.py:137][0m Total time:      15.20 min
[32m[20221208 14:51:46 @agent_ppo2.py:139][0m 1228800 total steps have happened
[32m[20221208 14:51:46 @agent_ppo2.py:115][0m #------------------------ Iteration 600 --------------------------#
[32m[20221208 14:51:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |           0.0853 |           5.1578 |        -468.7200 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |           0.0373 |           4.4239 |        -456.2094 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |           0.0111 |           4.1704 |        -477.4686 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |          -0.0108 |           3.9678 |        -497.3911 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |          -0.0257 |           3.7945 |        -506.2582 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |          -0.0388 |           3.6675 |        -516.3342 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |          -0.0445 |           3.5421 |        -520.3753 |
[32m[20221208 14:51:47 @agent_ppo2.py:179][0m |          -0.0500 |           3.4497 |        -531.0421 |
[32m[20221208 14:51:48 @agent_ppo2.py:179][0m |          -0.0524 |           3.3465 |        -543.4867 |
[32m[20221208 14:51:48 @agent_ppo2.py:179][0m |          -0.0561 |           3.2727 |        -550.1283 |
[32m[20221208 14:51:48 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.50
[32m[20221208 14:51:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.37
[32m[20221208 14:51:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.95
[32m[20221208 14:51:48 @agent_ppo2.py:137][0m Total time:      15.23 min
[32m[20221208 14:51:48 @agent_ppo2.py:139][0m 1230848 total steps have happened
[32m[20221208 14:51:48 @agent_ppo2.py:115][0m #------------------------ Iteration 601 --------------------------#
[32m[20221208 14:51:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |           0.1186 |           5.1808 |        -451.5360 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |           0.1255 |           4.4403 |        -323.4249 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |           0.0574 |           4.1450 |        -353.2492 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |           0.0141 |           3.9892 |        -406.1040 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |          -0.0116 |           3.7948 |        -439.6012 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |          -0.0253 |           3.6438 |        -458.0701 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |          -0.0348 |           3.5339 |        -468.8541 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |          -0.0424 |           3.4152 |        -478.7859 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |          -0.0521 |           3.3411 |        -492.7465 |
[32m[20221208 14:51:49 @agent_ppo2.py:179][0m |          -0.0529 |           3.2674 |        -493.9388 |
[32m[20221208 14:51:49 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.11
[32m[20221208 14:51:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.77
[32m[20221208 14:51:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.67
[32m[20221208 14:51:50 @agent_ppo2.py:137][0m Total time:      15.25 min
[32m[20221208 14:51:50 @agent_ppo2.py:139][0m 1232896 total steps have happened
[32m[20221208 14:51:50 @agent_ppo2.py:115][0m #------------------------ Iteration 602 --------------------------#
[32m[20221208 14:51:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |           0.0839 |           8.0065 |        -459.7714 |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |           0.0653 |           7.1688 |        -352.2414 |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |           0.0060 |           6.8102 |        -442.4648 |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |          -0.0278 |           6.6555 |        -481.5630 |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |          -0.0377 |           6.4196 |        -487.8742 |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |          -0.0488 |           6.3629 |        -503.9895 |
[32m[20221208 14:51:50 @agent_ppo2.py:179][0m |          -0.0565 |           6.2782 |        -518.4738 |
[32m[20221208 14:51:51 @agent_ppo2.py:179][0m |          -0.0547 |           6.1335 |        -509.7098 |
[32m[20221208 14:51:51 @agent_ppo2.py:179][0m |          -0.0615 |           5.9645 |        -520.8400 |
[32m[20221208 14:51:51 @agent_ppo2.py:179][0m |          -0.0671 |           5.9725 |        -527.3615 |
[32m[20221208 14:51:51 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.07
[32m[20221208 14:51:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.13
[32m[20221208 14:51:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.39
[32m[20221208 14:51:51 @agent_ppo2.py:137][0m Total time:      15.28 min
[32m[20221208 14:51:51 @agent_ppo2.py:139][0m 1234944 total steps have happened
[32m[20221208 14:51:51 @agent_ppo2.py:115][0m #------------------------ Iteration 603 --------------------------#
[32m[20221208 14:51:51 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:51:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |           0.0917 |           5.1656 |        -467.7535 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |           0.1150 |           4.7474 |        -378.9634 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |           0.0436 |           4.5580 |        -383.2331 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0030 |           4.4531 |        -436.6859 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0210 |           4.3868 |        -469.6000 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0323 |           4.3023 |        -479.5567 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0402 |           4.2277 |        -496.1292 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0469 |           4.1717 |        -505.8919 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0537 |           4.1571 |        -515.1074 |
[32m[20221208 14:51:52 @agent_ppo2.py:179][0m |          -0.0542 |           4.1152 |        -519.6692 |
[32m[20221208 14:51:52 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.28
[32m[20221208 14:51:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.80
[32m[20221208 14:51:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.37
[32m[20221208 14:51:53 @agent_ppo2.py:137][0m Total time:      15.31 min
[32m[20221208 14:51:53 @agent_ppo2.py:139][0m 1236992 total steps have happened
[32m[20221208 14:51:53 @agent_ppo2.py:115][0m #------------------------ Iteration 604 --------------------------#
[32m[20221208 14:51:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |           0.2287 |           4.4862 |        -459.6494 |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |           0.0978 |           4.2066 |        -351.0931 |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |           0.0421 |           4.0505 |        -391.3964 |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |           0.0019 |           3.9530 |        -449.7145 |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |          -0.0166 |           3.8762 |        -472.8328 |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |          -0.0291 |           3.7386 |        -483.6261 |
[32m[20221208 14:51:53 @agent_ppo2.py:179][0m |          -0.0358 |           3.6928 |        -500.2945 |
[32m[20221208 14:51:54 @agent_ppo2.py:179][0m |          -0.0440 |           3.5978 |        -509.7618 |
[32m[20221208 14:51:54 @agent_ppo2.py:179][0m |          -0.0459 |           3.5437 |        -515.3209 |
[32m[20221208 14:51:54 @agent_ppo2.py:179][0m |          -0.0500 |           3.4970 |        -524.5342 |
[32m[20221208 14:51:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:51:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.04
[32m[20221208 14:51:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.58
[32m[20221208 14:51:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.57
[32m[20221208 14:51:54 @agent_ppo2.py:137][0m Total time:      15.33 min
[32m[20221208 14:51:54 @agent_ppo2.py:139][0m 1239040 total steps have happened
[32m[20221208 14:51:54 @agent_ppo2.py:115][0m #------------------------ Iteration 605 --------------------------#
[32m[20221208 14:51:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |           0.0701 |           0.9825 |        -474.2089 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |           0.0378 |           0.7728 |        -402.1717 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |           0.0152 |           0.7037 |        -420.9762 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |           0.0061 |           0.6608 |        -415.4336 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |          -0.0079 |           0.6256 |        -430.9604 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |          -0.0213 |           0.5982 |        -451.4259 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |          -0.0284 |           0.5799 |        -464.3683 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |          -0.0317 |           0.5643 |        -464.6686 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |          -0.0325 |           0.5520 |        -473.0529 |
[32m[20221208 14:51:55 @agent_ppo2.py:179][0m |          -0.0297 |           0.5361 |        -471.7376 |
[32m[20221208 14:51:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:51:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.07
[32m[20221208 14:51:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.02
[32m[20221208 14:51:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.26
[32m[20221208 14:51:56 @agent_ppo2.py:137][0m Total time:      15.35 min
[32m[20221208 14:51:56 @agent_ppo2.py:139][0m 1241088 total steps have happened
[32m[20221208 14:51:56 @agent_ppo2.py:115][0m #------------------------ Iteration 606 --------------------------#
[32m[20221208 14:51:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |           0.0797 |           6.3609 |        -458.6259 |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |           0.0905 |           5.8633 |        -345.1528 |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |           0.0477 |           5.7689 |        -363.8566 |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |           0.0139 |           5.6256 |        -400.9065 |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |          -0.0150 |           5.4751 |        -426.7766 |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |          -0.0280 |           5.4287 |        -442.1646 |
[32m[20221208 14:51:56 @agent_ppo2.py:179][0m |          -0.0404 |           5.3541 |        -453.7843 |
[32m[20221208 14:51:57 @agent_ppo2.py:179][0m |          -0.0440 |           5.2838 |        -457.6769 |
[32m[20221208 14:51:57 @agent_ppo2.py:179][0m |          -0.0474 |           5.2410 |        -473.1239 |
[32m[20221208 14:51:57 @agent_ppo2.py:179][0m |          -0.0572 |           5.1860 |        -481.3191 |
[32m[20221208 14:51:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:51:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.42
[32m[20221208 14:51:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.64
[32m[20221208 14:51:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.00
[32m[20221208 14:51:57 @agent_ppo2.py:137][0m Total time:      15.38 min
[32m[20221208 14:51:57 @agent_ppo2.py:139][0m 1243136 total steps have happened
[32m[20221208 14:51:57 @agent_ppo2.py:115][0m #------------------------ Iteration 607 --------------------------#
[32m[20221208 14:51:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:51:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |           0.0625 |           5.7805 |        -475.5318 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |           0.0368 |           5.1271 |        -434.0968 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0120 |           4.8983 |        -465.4930 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0354 |           4.7587 |        -479.7294 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0454 |           4.6953 |        -500.6160 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0528 |           4.5248 |        -502.6962 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0556 |           4.4634 |        -507.3319 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0603 |           4.3913 |        -504.4714 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0700 |           4.3570 |        -521.1077 |
[32m[20221208 14:51:58 @agent_ppo2.py:179][0m |          -0.0726 |           4.2783 |        -529.9968 |
[32m[20221208 14:51:58 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:51:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.30
[32m[20221208 14:51:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.36
[32m[20221208 14:51:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.51
[32m[20221208 14:51:59 @agent_ppo2.py:137][0m Total time:      15.41 min
[32m[20221208 14:51:59 @agent_ppo2.py:139][0m 1245184 total steps have happened
[32m[20221208 14:51:59 @agent_ppo2.py:115][0m #------------------------ Iteration 608 --------------------------#
[32m[20221208 14:51:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:51:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:51:59 @agent_ppo2.py:179][0m |           0.1469 |           5.7867 |        -411.8938 |
[32m[20221208 14:51:59 @agent_ppo2.py:179][0m |           0.1478 |           4.9230 |        -295.9817 |
[32m[20221208 14:51:59 @agent_ppo2.py:179][0m |           0.0440 |           4.6152 |        -356.5198 |
[32m[20221208 14:51:59 @agent_ppo2.py:179][0m |           0.0105 |           4.3747 |        -424.7231 |
[32m[20221208 14:51:59 @agent_ppo2.py:179][0m |          -0.0099 |           4.2693 |        -438.0703 |
[32m[20221208 14:51:59 @agent_ppo2.py:179][0m |          -0.0223 |           4.2138 |        -444.0753 |
[32m[20221208 14:52:00 @agent_ppo2.py:179][0m |          -0.0340 |           4.0697 |        -452.6393 |
[32m[20221208 14:52:00 @agent_ppo2.py:179][0m |          -0.0391 |           4.0578 |        -470.0249 |
[32m[20221208 14:52:00 @agent_ppo2.py:179][0m |          -0.0478 |           3.9494 |        -449.2787 |
[32m[20221208 14:52:00 @agent_ppo2.py:179][0m |          -0.0535 |           3.8852 |        -493.5784 |
[32m[20221208 14:52:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.54
[32m[20221208 14:52:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.02
[32m[20221208 14:52:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.80
[32m[20221208 14:52:00 @agent_ppo2.py:137][0m Total time:      15.43 min
[32m[20221208 14:52:00 @agent_ppo2.py:139][0m 1247232 total steps have happened
[32m[20221208 14:52:00 @agent_ppo2.py:115][0m #------------------------ Iteration 609 --------------------------#
[32m[20221208 14:52:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |           0.0563 |           3.9680 |        -392.1299 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |           0.0438 |           3.4038 |        -241.0266 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0093 |           3.3105 |        -272.7002 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0352 |           3.2316 |        -310.2761 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0449 |           3.1659 |        -332.3239 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0585 |           3.1280 |        -343.0000 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0643 |           3.0884 |        -360.9184 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0590 |           3.0819 |        -362.2031 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0672 |           3.0206 |        -366.0764 |
[32m[20221208 14:52:01 @agent_ppo2.py:179][0m |          -0.0745 |           2.9948 |        -378.1249 |
[32m[20221208 14:52:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.73
[32m[20221208 14:52:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.92
[32m[20221208 14:52:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.94
[32m[20221208 14:52:02 @agent_ppo2.py:137][0m Total time:      15.46 min
[32m[20221208 14:52:02 @agent_ppo2.py:139][0m 1249280 total steps have happened
[32m[20221208 14:52:02 @agent_ppo2.py:115][0m #------------------------ Iteration 610 --------------------------#
[32m[20221208 14:52:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:02 @agent_ppo2.py:179][0m |           0.0916 |           4.5462 |        -485.7197 |
[32m[20221208 14:52:02 @agent_ppo2.py:179][0m |           0.1407 |           4.2859 |        -313.8266 |
[32m[20221208 14:52:02 @agent_ppo2.py:179][0m |           0.0552 |           4.1191 |        -253.5919 |
[32m[20221208 14:52:02 @agent_ppo2.py:179][0m |           0.0190 |           4.0547 |        -307.0989 |
[32m[20221208 14:52:02 @agent_ppo2.py:179][0m |           0.0023 |           3.9648 |        -358.6248 |
[32m[20221208 14:52:03 @agent_ppo2.py:179][0m |          -0.0178 |           3.8928 |        -452.4096 |
[32m[20221208 14:52:03 @agent_ppo2.py:179][0m |          -0.0296 |           3.8666 |        -505.0856 |
[32m[20221208 14:52:03 @agent_ppo2.py:179][0m |          -0.0359 |           3.8299 |        -508.5485 |
[32m[20221208 14:52:03 @agent_ppo2.py:179][0m |          -0.0382 |           3.8041 |        -504.2233 |
[32m[20221208 14:52:03 @agent_ppo2.py:179][0m |          -0.0404 |           3.7904 |        -510.9519 |
[32m[20221208 14:52:03 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.72
[32m[20221208 14:52:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.35
[32m[20221208 14:52:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.54
[32m[20221208 14:52:03 @agent_ppo2.py:137][0m Total time:      15.48 min
[32m[20221208 14:52:03 @agent_ppo2.py:139][0m 1251328 total steps have happened
[32m[20221208 14:52:03 @agent_ppo2.py:115][0m #------------------------ Iteration 611 --------------------------#
[32m[20221208 14:52:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |           0.0671 |           2.6931 |        -328.7828 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |           0.0314 |           2.3722 |        -151.2436 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |           0.0004 |           2.2524 |        -150.4902 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0214 |           2.1753 |        -163.1046 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0377 |           2.1313 |        -181.8072 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0331 |           2.0801 |        -180.4599 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0453 |           2.0464 |        -191.3521 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0492 |           2.0128 |        -196.2452 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0531 |           1.9740 |        -203.9480 |
[32m[20221208 14:52:04 @agent_ppo2.py:179][0m |          -0.0610 |           1.9654 |        -213.0100 |
[32m[20221208 14:52:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:52:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.92
[32m[20221208 14:52:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.63
[32m[20221208 14:52:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.43
[32m[20221208 14:52:05 @agent_ppo2.py:137][0m Total time:      15.51 min
[32m[20221208 14:52:05 @agent_ppo2.py:139][0m 1253376 total steps have happened
[32m[20221208 14:52:05 @agent_ppo2.py:115][0m #------------------------ Iteration 612 --------------------------#
[32m[20221208 14:52:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:05 @agent_ppo2.py:179][0m |           0.0454 |           4.8437 |        -522.8464 |
[32m[20221208 14:52:05 @agent_ppo2.py:179][0m |           0.0415 |           4.5480 |        -430.8326 |
[32m[20221208 14:52:05 @agent_ppo2.py:179][0m |           0.0165 |           4.3446 |        -432.7359 |
[32m[20221208 14:52:05 @agent_ppo2.py:179][0m |          -0.0152 |           4.2144 |        -439.5393 |
[32m[20221208 14:52:05 @agent_ppo2.py:179][0m |          -0.0314 |           4.1504 |        -466.2419 |
[32m[20221208 14:52:06 @agent_ppo2.py:179][0m |          -0.0418 |           4.0865 |        -478.1768 |
[32m[20221208 14:52:06 @agent_ppo2.py:179][0m |          -0.0439 |           4.0078 |        -487.0351 |
[32m[20221208 14:52:06 @agent_ppo2.py:179][0m |          -0.0421 |           4.0088 |        -483.8187 |
[32m[20221208 14:52:06 @agent_ppo2.py:179][0m |          -0.0435 |           3.9154 |        -482.6037 |
[32m[20221208 14:52:06 @agent_ppo2.py:179][0m |          -0.0499 |           3.9051 |        -484.6529 |
[32m[20221208 14:52:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.62
[32m[20221208 14:52:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.25
[32m[20221208 14:52:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.56
[32m[20221208 14:52:06 @agent_ppo2.py:137][0m Total time:      15.53 min
[32m[20221208 14:52:06 @agent_ppo2.py:139][0m 1255424 total steps have happened
[32m[20221208 14:52:06 @agent_ppo2.py:115][0m #------------------------ Iteration 613 --------------------------#
[32m[20221208 14:52:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |           0.1078 |           2.9218 |        -445.1675 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |           0.0884 |           2.5710 |        -334.4569 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |           0.0482 |           2.4686 |        -282.4968 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0047 |           2.4165 |        -250.9352 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0222 |           2.3982 |        -263.4733 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0362 |           2.3481 |        -279.5497 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0530 |           2.3157 |        -295.8245 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0580 |           2.2810 |        -298.6735 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0665 |           2.2693 |        -313.6752 |
[32m[20221208 14:52:07 @agent_ppo2.py:179][0m |          -0.0704 |           2.2736 |        -321.1714 |
[32m[20221208 14:52:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.74
[32m[20221208 14:52:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.21
[32m[20221208 14:52:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.04
[32m[20221208 14:52:08 @agent_ppo2.py:137][0m Total time:      15.56 min
[32m[20221208 14:52:08 @agent_ppo2.py:139][0m 1257472 total steps have happened
[32m[20221208 14:52:08 @agent_ppo2.py:115][0m #------------------------ Iteration 614 --------------------------#
[32m[20221208 14:52:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:08 @agent_ppo2.py:179][0m |           0.0730 |           2.4963 |        -389.9691 |
[32m[20221208 14:52:08 @agent_ppo2.py:179][0m |           0.0468 |           2.1634 |        -217.0231 |
[32m[20221208 14:52:08 @agent_ppo2.py:179][0m |           0.0043 |           1.9819 |        -219.9666 |
[32m[20221208 14:52:08 @agent_ppo2.py:179][0m |          -0.0106 |           1.8824 |        -246.4585 |
[32m[20221208 14:52:09 @agent_ppo2.py:179][0m |          -0.0176 |           1.8103 |        -251.1680 |
[32m[20221208 14:52:09 @agent_ppo2.py:179][0m |          -0.0292 |           1.7400 |        -270.2440 |
[32m[20221208 14:52:09 @agent_ppo2.py:179][0m |          -0.0308 |           1.6756 |        -267.7492 |
[32m[20221208 14:52:09 @agent_ppo2.py:179][0m |          -0.0384 |           1.6043 |        -269.2995 |
[32m[20221208 14:52:09 @agent_ppo2.py:179][0m |          -0.0323 |           1.5476 |        -263.1028 |
[32m[20221208 14:52:09 @agent_ppo2.py:179][0m |          -0.0417 |           1.5388 |        -278.0739 |
[32m[20221208 14:52:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.39
[32m[20221208 14:52:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.35
[32m[20221208 14:52:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.62
[32m[20221208 14:52:09 @agent_ppo2.py:137][0m Total time:      15.58 min
[32m[20221208 14:52:09 @agent_ppo2.py:139][0m 1259520 total steps have happened
[32m[20221208 14:52:09 @agent_ppo2.py:115][0m #------------------------ Iteration 615 --------------------------#
[32m[20221208 14:52:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |           0.0963 |           6.5467 |        -477.1638 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |           0.1063 |           5.9826 |        -253.6647 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |           0.0552 |           5.7774 |        -313.3475 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |           0.0275 |           5.5893 |        -380.4942 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |           0.0099 |           5.4992 |        -409.0106 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |          -0.0077 |           5.3587 |        -440.3325 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |          -0.0242 |           5.2826 |        -468.8117 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |          -0.0265 |           5.2135 |        -477.9859 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |          -0.0323 |           5.1615 |        -490.3924 |
[32m[20221208 14:52:10 @agent_ppo2.py:179][0m |          -0.0173 |           5.1086 |        -484.3843 |
[32m[20221208 14:52:10 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:52:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.61
[32m[20221208 14:52:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.84
[32m[20221208 14:52:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.00
[32m[20221208 14:52:11 @agent_ppo2.py:137][0m Total time:      15.61 min
[32m[20221208 14:52:11 @agent_ppo2.py:139][0m 1261568 total steps have happened
[32m[20221208 14:52:11 @agent_ppo2.py:115][0m #------------------------ Iteration 616 --------------------------#
[32m[20221208 14:52:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:11 @agent_ppo2.py:179][0m |           0.0581 |           2.4082 |        -458.3320 |
[32m[20221208 14:52:11 @agent_ppo2.py:179][0m |           0.0541 |           2.1435 |        -308.6500 |
[32m[20221208 14:52:11 @agent_ppo2.py:179][0m |           0.0124 |           2.0156 |        -270.8013 |
[32m[20221208 14:52:11 @agent_ppo2.py:179][0m |          -0.0113 |           1.9170 |        -345.0240 |
[32m[20221208 14:52:12 @agent_ppo2.py:179][0m |          -0.0267 |           1.8568 |        -383.3337 |
[32m[20221208 14:52:12 @agent_ppo2.py:179][0m |          -0.0388 |           1.7942 |        -399.0197 |
[32m[20221208 14:52:12 @agent_ppo2.py:179][0m |          -0.0434 |           1.7518 |        -406.6270 |
[32m[20221208 14:52:12 @agent_ppo2.py:179][0m |          -0.0450 |           1.7053 |        -406.9600 |
[32m[20221208 14:52:12 @agent_ppo2.py:179][0m |          -0.0527 |           1.6929 |        -419.4899 |
[32m[20221208 14:52:12 @agent_ppo2.py:179][0m |          -0.0528 |           1.6671 |        -418.8503 |
[32m[20221208 14:52:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.09
[32m[20221208 14:52:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.93
[32m[20221208 14:52:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.18
[32m[20221208 14:52:12 @agent_ppo2.py:137][0m Total time:      15.63 min
[32m[20221208 14:52:12 @agent_ppo2.py:139][0m 1263616 total steps have happened
[32m[20221208 14:52:12 @agent_ppo2.py:115][0m #------------------------ Iteration 617 --------------------------#
[32m[20221208 14:52:13 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:52:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |           0.1003 |           4.0077 |        -466.6248 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |           0.0795 |           3.4318 |        -334.7727 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |           0.0278 |           3.2495 |        -378.2979 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |           0.0099 |           3.1086 |        -391.5101 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |          -0.0079 |           2.9477 |        -419.2142 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |          -0.0252 |           2.8568 |        -442.1232 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |          -0.0338 |           2.8022 |        -456.4264 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |          -0.0370 |           2.7170 |        -456.3690 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |          -0.0415 |           2.6764 |        -466.2893 |
[32m[20221208 14:52:13 @agent_ppo2.py:179][0m |          -0.0427 |           2.6059 |        -467.1536 |
[32m[20221208 14:52:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.31
[32m[20221208 14:52:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.62
[32m[20221208 14:52:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.00
[32m[20221208 14:52:14 @agent_ppo2.py:137][0m Total time:      15.66 min
[32m[20221208 14:52:14 @agent_ppo2.py:139][0m 1265664 total steps have happened
[32m[20221208 14:52:14 @agent_ppo2.py:115][0m #------------------------ Iteration 618 --------------------------#
[32m[20221208 14:52:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:14 @agent_ppo2.py:179][0m |           0.0843 |           7.0463 |        -551.4391 |
[32m[20221208 14:52:14 @agent_ppo2.py:179][0m |           0.0504 |           6.2556 |        -461.5314 |
[32m[20221208 14:52:14 @agent_ppo2.py:179][0m |          -0.0061 |           5.9911 |        -522.1137 |
[32m[20221208 14:52:14 @agent_ppo2.py:179][0m |          -0.0336 |           5.8180 |        -564.0307 |
[32m[20221208 14:52:15 @agent_ppo2.py:179][0m |          -0.0455 |           5.6646 |        -571.2424 |
[32m[20221208 14:52:15 @agent_ppo2.py:179][0m |          -0.0590 |           5.5261 |        -591.8384 |
[32m[20221208 14:52:15 @agent_ppo2.py:179][0m |          -0.0662 |           5.4301 |        -613.2766 |
[32m[20221208 14:52:15 @agent_ppo2.py:179][0m |          -0.0672 |           5.3308 |        -620.4034 |
[32m[20221208 14:52:15 @agent_ppo2.py:179][0m |          -0.0741 |           5.2861 |        -619.4500 |
[32m[20221208 14:52:15 @agent_ppo2.py:179][0m |          -0.0778 |           5.1866 |        -642.0487 |
[32m[20221208 14:52:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.86
[32m[20221208 14:52:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.90
[32m[20221208 14:52:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.91
[32m[20221208 14:52:15 @agent_ppo2.py:137][0m Total time:      15.68 min
[32m[20221208 14:52:15 @agent_ppo2.py:139][0m 1267712 total steps have happened
[32m[20221208 14:52:15 @agent_ppo2.py:115][0m #------------------------ Iteration 619 --------------------------#
[32m[20221208 14:52:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |           0.0997 |           4.8988 |        -396.4484 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |           0.0323 |           4.3286 |        -330.5828 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |           0.0050 |           4.1515 |        -346.9990 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0104 |           4.0061 |        -353.6666 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0236 |           3.9076 |        -369.6605 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0351 |           3.8070 |        -378.8818 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0402 |           3.7614 |        -373.0420 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0575 |           3.6936 |        -403.6356 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0579 |           3.6488 |        -404.3251 |
[32m[20221208 14:52:16 @agent_ppo2.py:179][0m |          -0.0663 |           3.5852 |        -415.4618 |
[32m[20221208 14:52:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.32
[32m[20221208 14:52:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.87
[32m[20221208 14:52:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.99
[32m[20221208 14:52:17 @agent_ppo2.py:137][0m Total time:      15.71 min
[32m[20221208 14:52:17 @agent_ppo2.py:139][0m 1269760 total steps have happened
[32m[20221208 14:52:17 @agent_ppo2.py:115][0m #------------------------ Iteration 620 --------------------------#
[32m[20221208 14:52:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:17 @agent_ppo2.py:179][0m |           0.1028 |           6.2261 |        -518.4081 |
[32m[20221208 14:52:17 @agent_ppo2.py:179][0m |           0.1068 |           5.7735 |        -377.8519 |
[32m[20221208 14:52:17 @agent_ppo2.py:179][0m |           0.0548 |           5.6252 |        -403.9337 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |           0.0131 |           5.5231 |        -491.8471 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |          -0.0120 |           5.4360 |        -531.5110 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |          -0.0270 |           5.3473 |        -547.9404 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |          -0.0392 |           5.3222 |        -573.0423 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |          -0.0434 |           5.3107 |        -587.2502 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |          -0.0496 |           5.1989 |        -599.1290 |
[32m[20221208 14:52:18 @agent_ppo2.py:179][0m |          -0.0501 |           5.1889 |        -598.9972 |
[32m[20221208 14:52:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.25
[32m[20221208 14:52:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.23
[32m[20221208 14:52:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.99
[32m[20221208 14:52:18 @agent_ppo2.py:137][0m Total time:      15.73 min
[32m[20221208 14:52:18 @agent_ppo2.py:139][0m 1271808 total steps have happened
[32m[20221208 14:52:18 @agent_ppo2.py:115][0m #------------------------ Iteration 621 --------------------------#
[32m[20221208 14:52:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |           0.0712 |           4.9362 |        -553.7892 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |           0.1348 |           4.3290 |        -336.9903 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |           0.0351 |           4.0434 |        -305.6772 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0097 |           3.8288 |        -375.4113 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0258 |           3.7059 |        -388.0441 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0419 |           3.5496 |        -414.4788 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0480 |           3.4470 |        -419.5037 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0573 |           3.3595 |        -437.9894 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0640 |           3.2967 |        -452.8648 |
[32m[20221208 14:52:19 @agent_ppo2.py:179][0m |          -0.0650 |           3.1962 |        -455.8598 |
[32m[20221208 14:52:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.25
[32m[20221208 14:52:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.97
[32m[20221208 14:52:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.89
[32m[20221208 14:52:20 @agent_ppo2.py:137][0m Total time:      15.76 min
[32m[20221208 14:52:20 @agent_ppo2.py:139][0m 1273856 total steps have happened
[32m[20221208 14:52:20 @agent_ppo2.py:115][0m #------------------------ Iteration 622 --------------------------#
[32m[20221208 14:52:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:20 @agent_ppo2.py:179][0m |           0.0754 |           4.5089 |        -533.2019 |
[32m[20221208 14:52:20 @agent_ppo2.py:179][0m |           0.0400 |           3.9745 |        -399.0978 |
[32m[20221208 14:52:20 @agent_ppo2.py:179][0m |           0.0050 |           3.7143 |        -409.6463 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0141 |           3.5419 |        -416.6202 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0308 |           3.4026 |        -443.6499 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0393 |           3.2853 |        -475.4081 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0479 |           3.2106 |        -497.3929 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0558 |           3.1193 |        -467.5387 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0613 |           3.0409 |        -482.0635 |
[32m[20221208 14:52:21 @agent_ppo2.py:179][0m |          -0.0656 |           2.9920 |        -504.1997 |
[32m[20221208 14:52:21 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.24
[32m[20221208 14:52:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.61
[32m[20221208 14:52:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.59
[32m[20221208 14:52:21 @agent_ppo2.py:137][0m Total time:      15.78 min
[32m[20221208 14:52:21 @agent_ppo2.py:139][0m 1275904 total steps have happened
[32m[20221208 14:52:21 @agent_ppo2.py:115][0m #------------------------ Iteration 623 --------------------------#
[32m[20221208 14:52:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |           0.1297 |           2.7396 |        -315.8684 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |           0.0628 |           2.4519 |        -132.0091 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |           0.0478 |           2.3517 |        -142.7388 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |           0.0187 |           2.3133 |        -178.6184 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |          -0.0022 |           2.2568 |        -195.6030 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |          -0.0149 |           2.1751 |        -205.1876 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |          -0.0252 |           2.1343 |        -223.5013 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |          -0.0301 |           2.1366 |        -236.9636 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |          -0.0362 |           2.0844 |        -237.4071 |
[32m[20221208 14:52:22 @agent_ppo2.py:179][0m |          -0.0404 |           2.0444 |        -246.4562 |
[32m[20221208 14:52:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.81
[32m[20221208 14:52:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.76
[32m[20221208 14:52:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.51
[32m[20221208 14:52:23 @agent_ppo2.py:137][0m Total time:      15.81 min
[32m[20221208 14:52:23 @agent_ppo2.py:139][0m 1277952 total steps have happened
[32m[20221208 14:52:23 @agent_ppo2.py:115][0m #------------------------ Iteration 624 --------------------------#
[32m[20221208 14:52:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:23 @agent_ppo2.py:179][0m |           0.0596 |           6.8297 |        -607.2035 |
[32m[20221208 14:52:23 @agent_ppo2.py:179][0m |           0.0711 |           6.3198 |        -514.0848 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |           0.0476 |           6.0150 |        -475.2967 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0078 |           5.9117 |        -569.6488 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0236 |           5.7795 |        -579.6759 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0374 |           5.6613 |        -585.3256 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0455 |           5.5603 |        -612.1578 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0469 |           5.5101 |        -610.4557 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0553 |           5.4328 |        -624.3036 |
[32m[20221208 14:52:24 @agent_ppo2.py:179][0m |          -0.0619 |           5.3913 |        -635.2831 |
[32m[20221208 14:52:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:52:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.10
[32m[20221208 14:52:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.11
[32m[20221208 14:52:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.38
[32m[20221208 14:52:24 @agent_ppo2.py:137][0m Total time:      15.83 min
[32m[20221208 14:52:24 @agent_ppo2.py:139][0m 1280000 total steps have happened
[32m[20221208 14:52:24 @agent_ppo2.py:115][0m #------------------------ Iteration 625 --------------------------#
[32m[20221208 14:52:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:52:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |           0.1805 |           4.2180 |        -512.7700 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |           0.1103 |           3.7516 |        -212.4662 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |           0.0618 |           3.5135 |        -272.1965 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |           0.0124 |           3.4055 |        -363.7932 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |          -0.0153 |           3.3045 |        -404.6090 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |          -0.0307 |           3.2671 |        -429.0278 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |          -0.0393 |           3.1697 |        -435.9601 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |          -0.0492 |           3.1216 |        -451.2355 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |          -0.0527 |           3.0657 |        -463.6914 |
[32m[20221208 14:52:25 @agent_ppo2.py:179][0m |          -0.0550 |           3.0507 |        -475.5733 |
[32m[20221208 14:52:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.56
[32m[20221208 14:52:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.89
[32m[20221208 14:52:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.32
[32m[20221208 14:52:26 @agent_ppo2.py:137][0m Total time:      15.86 min
[32m[20221208 14:52:26 @agent_ppo2.py:139][0m 1282048 total steps have happened
[32m[20221208 14:52:26 @agent_ppo2.py:115][0m #------------------------ Iteration 626 --------------------------#
[32m[20221208 14:52:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:26 @agent_ppo2.py:179][0m |           0.0679 |           4.3539 |        -589.1764 |
[32m[20221208 14:52:26 @agent_ppo2.py:179][0m |           0.0612 |           3.9199 |        -527.4812 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |           0.0228 |           3.7427 |        -532.3439 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0039 |           3.5922 |        -573.9145 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0189 |           3.4753 |        -612.2717 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0274 |           3.3756 |        -601.4369 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0427 |           3.3213 |        -625.3213 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0461 |           3.2445 |        -615.7630 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0512 |           3.1624 |        -616.0770 |
[32m[20221208 14:52:27 @agent_ppo2.py:179][0m |          -0.0545 |           3.1580 |        -612.8548 |
[32m[20221208 14:52:27 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.81
[32m[20221208 14:52:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.46
[32m[20221208 14:52:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.50
[32m[20221208 14:52:27 @agent_ppo2.py:137][0m Total time:      15.89 min
[32m[20221208 14:52:27 @agent_ppo2.py:139][0m 1284096 total steps have happened
[32m[20221208 14:52:27 @agent_ppo2.py:115][0m #------------------------ Iteration 627 --------------------------#
[32m[20221208 14:52:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |           0.0722 |           2.1819 |        -466.8090 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |           0.0757 |           2.0312 |        -182.0606 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |           0.0382 |           1.9002 |        -222.5020 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |           0.0247 |           1.8193 |        -232.0161 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |           0.0062 |           1.7719 |        -263.7582 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |           0.0007 |           1.6972 |        -270.0344 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |          -0.0040 |           1.6558 |        -259.0193 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |          -0.0150 |           1.6268 |        -280.0514 |
[32m[20221208 14:52:28 @agent_ppo2.py:179][0m |          -0.0178 |           1.5712 |        -301.6493 |
[32m[20221208 14:52:29 @agent_ppo2.py:179][0m |          -0.0207 |           1.5456 |        -303.6606 |
[32m[20221208 14:52:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:52:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.04
[32m[20221208 14:52:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.74
[32m[20221208 14:52:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.00
[32m[20221208 14:52:29 @agent_ppo2.py:137][0m Total time:      15.91 min
[32m[20221208 14:52:29 @agent_ppo2.py:139][0m 1286144 total steps have happened
[32m[20221208 14:52:29 @agent_ppo2.py:115][0m #------------------------ Iteration 628 --------------------------#
[32m[20221208 14:52:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:29 @agent_ppo2.py:179][0m |           0.1351 |           0.6461 |        -293.9723 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1674 |           0.6015 |          -4.5401 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1630 |           0.5933 |          -4.7739 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1622 |           0.5900 |          -5.2157 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1614 |           0.5878 |          -5.3721 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1610 |           0.5822 |          -6.7027 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1597 |           0.5810 |          -8.9847 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1572 |           0.5794 |         -13.3815 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1330 |           0.5796 |         -92.8987 |
[32m[20221208 14:52:30 @agent_ppo2.py:179][0m |           0.1342 |           0.5753 |        -131.1680 |
[32m[20221208 14:52:30 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:52:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 20.99
[32m[20221208 14:52:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.42
[32m[20221208 14:52:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.47
[32m[20221208 14:52:30 @agent_ppo2.py:137][0m Total time:      15.94 min
[32m[20221208 14:52:30 @agent_ppo2.py:139][0m 1288192 total steps have happened
[32m[20221208 14:52:30 @agent_ppo2.py:115][0m #------------------------ Iteration 629 --------------------------#
[32m[20221208 14:52:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |           0.1766 |           2.4969 |        -366.0342 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |           0.0803 |           2.0119 |         -92.8410 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |           0.0152 |           1.8121 |        -154.7681 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0120 |           1.7034 |        -194.1578 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0292 |           1.6419 |        -215.0904 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0408 |           1.5732 |        -233.4356 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0476 |           1.5334 |        -239.8717 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0547 |           1.4845 |        -255.9699 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0538 |           1.4690 |        -257.4733 |
[32m[20221208 14:52:31 @agent_ppo2.py:179][0m |          -0.0604 |           1.4179 |        -269.5229 |
[32m[20221208 14:52:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.29
[32m[20221208 14:52:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.43
[32m[20221208 14:52:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.88
[32m[20221208 14:52:32 @agent_ppo2.py:137][0m Total time:      15.96 min
[32m[20221208 14:52:32 @agent_ppo2.py:139][0m 1290240 total steps have happened
[32m[20221208 14:52:32 @agent_ppo2.py:115][0m #------------------------ Iteration 630 --------------------------#
[32m[20221208 14:52:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:32 @agent_ppo2.py:179][0m |           0.0492 |           1.8483 |        -577.9748 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |           0.0225 |           1.5839 |        -495.3159 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |           0.0019 |           1.5258 |        -518.9603 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |           0.0026 |           1.4965 |        -516.4845 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |          -0.0117 |           1.4550 |        -548.5536 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |           0.0116 |           1.4613 |        -452.6677 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |          -0.0209 |           1.4689 |        -537.1614 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |          -0.0439 |           1.4337 |        -604.9832 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |          -0.0379 |           1.3995 |        -597.9567 |
[32m[20221208 14:52:33 @agent_ppo2.py:179][0m |          -0.0277 |           1.3906 |        -561.9686 |
[32m[20221208 14:52:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:52:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.37
[32m[20221208 14:52:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.70
[32m[20221208 14:52:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.65
[32m[20221208 14:52:33 @agent_ppo2.py:137][0m Total time:      15.99 min
[32m[20221208 14:52:33 @agent_ppo2.py:139][0m 1292288 total steps have happened
[32m[20221208 14:52:33 @agent_ppo2.py:115][0m #------------------------ Iteration 631 --------------------------#
[32m[20221208 14:52:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |           0.0566 |           3.2180 |        -548.1074 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |           0.0227 |           2.8858 |        -370.4050 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |           0.0000 |           2.7860 |        -374.3117 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0167 |           2.7392 |        -412.1491 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0280 |           2.6865 |        -422.3258 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0333 |           2.6448 |        -434.5642 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0338 |           2.6312 |        -442.6904 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0385 |           2.5953 |        -457.3959 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0431 |           2.5658 |        -458.9621 |
[32m[20221208 14:52:34 @agent_ppo2.py:179][0m |          -0.0402 |           2.5230 |        -462.9218 |
[32m[20221208 14:52:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.76
[32m[20221208 14:52:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.17
[32m[20221208 14:52:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.57
[32m[20221208 14:52:35 @agent_ppo2.py:137][0m Total time:      16.01 min
[32m[20221208 14:52:35 @agent_ppo2.py:139][0m 1294336 total steps have happened
[32m[20221208 14:52:35 @agent_ppo2.py:115][0m #------------------------ Iteration 632 --------------------------#
[32m[20221208 14:52:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:35 @agent_ppo2.py:179][0m |           0.1077 |           0.6185 |        -641.0107 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.0815 |           0.4909 |        -516.3086 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.0745 |           0.4711 |        -578.8818 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.0755 |           0.4581 |        -451.2247 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.0931 |           0.4546 |        -446.7474 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.1240 |           0.4565 |        -123.7783 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.1350 |           0.4439 |         -34.3358 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.1023 |           0.4494 |        -155.9525 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.0901 |           0.4406 |        -364.7819 |
[32m[20221208 14:52:36 @agent_ppo2.py:179][0m |           0.0797 |           0.4369 |        -271.4654 |
[32m[20221208 14:52:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:52:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.53
[32m[20221208 14:52:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 23.88
[32m[20221208 14:52:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.33
[32m[20221208 14:52:36 @agent_ppo2.py:137][0m Total time:      16.04 min
[32m[20221208 14:52:36 @agent_ppo2.py:139][0m 1296384 total steps have happened
[32m[20221208 14:52:36 @agent_ppo2.py:115][0m #------------------------ Iteration 633 --------------------------#
[32m[20221208 14:52:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |           0.0539 |           1.9848 |        -452.5770 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |           0.0138 |           1.6555 |        -357.9891 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0274 |           1.6102 |        -376.5173 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0426 |           1.5764 |        -391.6844 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0481 |           1.5613 |        -401.3563 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0527 |           1.5465 |        -409.6624 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0571 |           1.5415 |        -408.6790 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0573 |           1.5436 |        -412.1088 |
[32m[20221208 14:52:37 @agent_ppo2.py:179][0m |          -0.0613 |           1.5324 |        -419.6033 |
[32m[20221208 14:52:38 @agent_ppo2.py:179][0m |          -0.0604 |           1.5156 |        -420.1831 |
[32m[20221208 14:52:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.69
[32m[20221208 14:52:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.39
[32m[20221208 14:52:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.98
[32m[20221208 14:52:38 @agent_ppo2.py:137][0m Total time:      16.06 min
[32m[20221208 14:52:38 @agent_ppo2.py:139][0m 1298432 total steps have happened
[32m[20221208 14:52:38 @agent_ppo2.py:115][0m #------------------------ Iteration 634 --------------------------#
[32m[20221208 14:52:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:38 @agent_ppo2.py:179][0m |           0.0436 |           1.2317 |        -560.3136 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |           0.0029 |           1.0061 |        -505.5680 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0223 |           0.9477 |        -535.9743 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0315 |           0.9259 |        -539.9884 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0436 |           0.9041 |        -553.1637 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0448 |           0.8753 |        -560.2663 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0459 |           0.8636 |        -557.3754 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0545 |           0.8573 |        -575.9219 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0569 |           0.8598 |        -583.7454 |
[32m[20221208 14:52:39 @agent_ppo2.py:179][0m |          -0.0530 |           0.8536 |        -573.0984 |
[32m[20221208 14:52:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:52:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 31.23
[32m[20221208 14:52:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.08
[32m[20221208 14:52:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.19
[32m[20221208 14:52:39 @agent_ppo2.py:137][0m Total time:      16.09 min
[32m[20221208 14:52:39 @agent_ppo2.py:139][0m 1300480 total steps have happened
[32m[20221208 14:52:39 @agent_ppo2.py:115][0m #------------------------ Iteration 635 --------------------------#
[32m[20221208 14:52:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |           0.3311 |           3.0083 |        -529.5499 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |           0.0918 |           2.5199 |        -430.3983 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |           0.0183 |           2.3596 |        -490.8451 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |          -0.0002 |           2.2291 |        -518.7849 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |          -0.0257 |           2.2027 |        -556.9525 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |          -0.0424 |           2.1355 |        -570.4043 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |          -0.0507 |           2.0806 |        -583.1432 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |          -0.0588 |           2.0472 |        -607.5502 |
[32m[20221208 14:52:40 @agent_ppo2.py:179][0m |          -0.0635 |           2.0390 |        -617.8143 |
[32m[20221208 14:52:41 @agent_ppo2.py:179][0m |          -0.0645 |           1.9685 |        -611.9021 |
[32m[20221208 14:52:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.86
[32m[20221208 14:52:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.55
[32m[20221208 14:52:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.13
[32m[20221208 14:52:41 @agent_ppo2.py:137][0m Total time:      16.11 min
[32m[20221208 14:52:41 @agent_ppo2.py:139][0m 1302528 total steps have happened
[32m[20221208 14:52:41 @agent_ppo2.py:115][0m #------------------------ Iteration 636 --------------------------#
[32m[20221208 14:52:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:41 @agent_ppo2.py:179][0m |           0.0510 |           2.2113 |        -683.5854 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |           0.0148 |           1.5904 |        -541.4503 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0186 |           1.4376 |        -551.4614 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0296 |           1.3618 |        -575.1339 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0422 |           1.3099 |        -587.9579 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0462 |           1.2812 |        -603.2804 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0493 |           1.2432 |        -629.1292 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0524 |           1.2296 |        -618.6285 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0581 |           1.2100 |        -633.3585 |
[32m[20221208 14:52:42 @agent_ppo2.py:179][0m |          -0.0593 |           1.1870 |        -638.3553 |
[32m[20221208 14:52:42 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.24
[32m[20221208 14:52:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.00
[32m[20221208 14:52:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.51
[32m[20221208 14:52:42 @agent_ppo2.py:137][0m Total time:      16.14 min
[32m[20221208 14:52:42 @agent_ppo2.py:139][0m 1304576 total steps have happened
[32m[20221208 14:52:42 @agent_ppo2.py:115][0m #------------------------ Iteration 637 --------------------------#
[32m[20221208 14:52:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.1278 |           1.8888 |        -411.2436 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.1132 |           1.6283 |        -178.3649 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0720 |           1.5604 |        -194.1210 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0622 |           1.5012 |        -225.0680 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0560 |           1.4894 |        -225.5743 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0469 |           1.4459 |        -243.0040 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0395 |           1.4150 |        -247.7784 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0358 |           1.4028 |        -258.9359 |
[32m[20221208 14:52:43 @agent_ppo2.py:179][0m |           0.0344 |           1.3782 |        -261.5648 |
[32m[20221208 14:52:44 @agent_ppo2.py:179][0m |           0.0301 |           1.3631 |        -260.9818 |
[32m[20221208 14:52:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.40
[32m[20221208 14:52:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.61
[32m[20221208 14:52:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.33
[32m[20221208 14:52:44 @agent_ppo2.py:137][0m Total time:      16.16 min
[32m[20221208 14:52:44 @agent_ppo2.py:139][0m 1306624 total steps have happened
[32m[20221208 14:52:44 @agent_ppo2.py:115][0m #------------------------ Iteration 638 --------------------------#
[32m[20221208 14:52:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |           0.1021 |           2.4787 |        -756.1418 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |           0.0609 |           1.8802 |        -720.2405 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |           0.0165 |           1.6563 |        -747.9345 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |           0.0036 |           1.5492 |        -755.8401 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |           0.0158 |           1.4523 |        -646.6302 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |          -0.0213 |           1.3979 |        -785.4747 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |          -0.0358 |           1.3458 |        -837.4034 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |          -0.0355 |           1.2898 |        -847.1208 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |           0.0018 |           1.2542 |        -675.7267 |
[32m[20221208 14:52:45 @agent_ppo2.py:179][0m |          -0.0204 |           1.2284 |        -549.1668 |
[32m[20221208 14:52:45 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:52:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.52
[32m[20221208 14:52:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.11
[32m[20221208 14:52:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.61
[32m[20221208 14:52:45 @agent_ppo2.py:137][0m Total time:      16.19 min
[32m[20221208 14:52:45 @agent_ppo2.py:139][0m 1308672 total steps have happened
[32m[20221208 14:52:45 @agent_ppo2.py:115][0m #------------------------ Iteration 639 --------------------------#
[32m[20221208 14:52:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |           0.0924 |           2.9844 |        -699.9388 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |           0.0367 |           2.2458 |        -708.6837 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0004 |           1.9681 |        -748.1391 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0174 |           1.8144 |        -756.1936 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0357 |           1.7241 |        -773.9684 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0426 |           1.6165 |        -808.7827 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0508 |           1.5589 |        -799.4169 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0534 |           1.4956 |        -813.0576 |
[32m[20221208 14:52:46 @agent_ppo2.py:179][0m |          -0.0596 |           1.4556 |        -834.3490 |
[32m[20221208 14:52:47 @agent_ppo2.py:179][0m |          -0.0588 |           1.4194 |        -828.6286 |
[32m[20221208 14:52:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:52:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.20
[32m[20221208 14:52:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.75
[32m[20221208 14:52:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.61
[32m[20221208 14:52:47 @agent_ppo2.py:137][0m Total time:      16.21 min
[32m[20221208 14:52:47 @agent_ppo2.py:139][0m 1310720 total steps have happened
[32m[20221208 14:52:47 @agent_ppo2.py:115][0m #------------------------ Iteration 640 --------------------------#
[32m[20221208 14:52:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0712 |           1.4595 |        -614.6252 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.1017 |           1.0530 |        -399.5252 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0597 |           0.9656 |        -283.6492 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0297 |           0.9018 |        -276.3868 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0103 |           0.8774 |        -289.0405 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0039 |           0.8491 |        -295.1193 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |          -0.0004 |           0.8362 |        -289.3470 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |          -0.0041 |           0.8162 |        -296.1466 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0016 |           0.8124 |        -290.4239 |
[32m[20221208 14:52:48 @agent_ppo2.py:179][0m |           0.0012 |           0.8052 |        -292.3618 |
[32m[20221208 14:52:48 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 14:52:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.67
[32m[20221208 14:52:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.08
[32m[20221208 14:52:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.06
[32m[20221208 14:52:48 @agent_ppo2.py:137][0m Total time:      16.24 min
[32m[20221208 14:52:48 @agent_ppo2.py:139][0m 1312768 total steps have happened
[32m[20221208 14:52:48 @agent_ppo2.py:115][0m #------------------------ Iteration 641 --------------------------#
[32m[20221208 14:52:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |           0.1256 |           2.6174 |        -766.9599 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |           0.0248 |           1.8371 |        -723.1833 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0112 |           1.5011 |        -733.3079 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0285 |           1.3402 |        -777.4170 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0404 |           1.2432 |        -791.3436 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0475 |           1.1666 |        -792.2333 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0555 |           1.1195 |        -807.7809 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0582 |           1.0686 |        -821.9451 |
[32m[20221208 14:52:49 @agent_ppo2.py:179][0m |          -0.0601 |           1.0513 |        -809.1912 |
[32m[20221208 14:52:50 @agent_ppo2.py:179][0m |          -0.0636 |           1.0032 |        -826.3911 |
[32m[20221208 14:52:50 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.48
[32m[20221208 14:52:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.07
[32m[20221208 14:52:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.29
[32m[20221208 14:52:50 @agent_ppo2.py:137][0m Total time:      16.26 min
[32m[20221208 14:52:50 @agent_ppo2.py:139][0m 1314816 total steps have happened
[32m[20221208 14:52:50 @agent_ppo2.py:115][0m #------------------------ Iteration 642 --------------------------#
[32m[20221208 14:52:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:50 @agent_ppo2.py:179][0m |           0.0785 |           3.6427 |        -805.4878 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |           0.0470 |           3.0743 |        -655.8022 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |           0.0212 |           2.8914 |        -699.9252 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0218 |           2.7748 |        -739.7156 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0388 |           2.6314 |        -773.7986 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0548 |           2.5796 |        -785.1028 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0632 |           2.4790 |        -808.4527 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0635 |           2.4457 |        -801.4020 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0688 |           2.3919 |        -795.6666 |
[32m[20221208 14:52:51 @agent_ppo2.py:179][0m |          -0.0711 |           2.2889 |        -820.3299 |
[32m[20221208 14:52:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.96
[32m[20221208 14:52:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.38
[32m[20221208 14:52:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.45
[32m[20221208 14:52:51 @agent_ppo2.py:137][0m Total time:      16.29 min
[32m[20221208 14:52:51 @agent_ppo2.py:139][0m 1316864 total steps have happened
[32m[20221208 14:52:51 @agent_ppo2.py:115][0m #------------------------ Iteration 643 --------------------------#
[32m[20221208 14:52:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |           0.1645 |           1.9909 |        -640.8630 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |           0.1507 |           1.6868 |        -480.5554 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |           0.0409 |           1.5931 |        -546.3243 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |           0.0092 |           1.5260 |        -591.9091 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |          -0.0213 |           1.4603 |        -641.2325 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |          -0.0350 |           1.4299 |        -673.9794 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |          -0.0448 |           1.3873 |        -710.2435 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |          -0.0455 |           1.3582 |        -717.2072 |
[32m[20221208 14:52:52 @agent_ppo2.py:179][0m |          -0.0544 |           1.3352 |        -737.6181 |
[32m[20221208 14:52:53 @agent_ppo2.py:179][0m |          -0.0551 |           1.3228 |        -753.8897 |
[32m[20221208 14:52:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:52:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.37
[32m[20221208 14:52:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.18
[32m[20221208 14:52:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.53
[32m[20221208 14:52:53 @agent_ppo2.py:137][0m Total time:      16.31 min
[32m[20221208 14:52:53 @agent_ppo2.py:139][0m 1318912 total steps have happened
[32m[20221208 14:52:53 @agent_ppo2.py:115][0m #------------------------ Iteration 644 --------------------------#
[32m[20221208 14:52:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |           0.1912 |           1.8891 |        -753.1958 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |           0.0906 |           1.4457 |        -587.4620 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |           0.0359 |           1.2245 |        -311.9112 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0125 |           1.0543 |        -257.0616 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0188 |           0.9727 |        -267.4211 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0307 |           0.8843 |        -276.5302 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0362 |           0.8484 |        -292.6378 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0362 |           0.7900 |        -282.5112 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0437 |           0.7514 |        -302.5704 |
[32m[20221208 14:52:54 @agent_ppo2.py:179][0m |          -0.0424 |           0.7232 |        -302.6261 |
[32m[20221208 14:52:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.04
[32m[20221208 14:52:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.62
[32m[20221208 14:52:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.14
[32m[20221208 14:52:54 @agent_ppo2.py:137][0m Total time:      16.34 min
[32m[20221208 14:52:54 @agent_ppo2.py:139][0m 1320960 total steps have happened
[32m[20221208 14:52:54 @agent_ppo2.py:115][0m #------------------------ Iteration 645 --------------------------#
[32m[20221208 14:52:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:52:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |           0.1225 |           3.3204 |        -643.4493 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |           0.2008 |           2.8749 |        -257.7268 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |           0.0958 |           2.5473 |        -327.2604 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |           0.0536 |           2.4801 |        -352.3222 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |           0.0142 |           2.3240 |        -423.3192 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |          -0.0085 |           2.2663 |        -482.2488 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |          -0.0140 |           2.1909 |        -481.6393 |
[32m[20221208 14:52:55 @agent_ppo2.py:179][0m |          -0.0281 |           2.1679 |        -540.3615 |
[32m[20221208 14:52:56 @agent_ppo2.py:179][0m |          -0.0452 |           2.1346 |        -564.6276 |
[32m[20221208 14:52:56 @agent_ppo2.py:179][0m |          -0.0352 |           2.0778 |        -586.2290 |
[32m[20221208 14:52:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:52:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.89
[32m[20221208 14:52:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.54
[32m[20221208 14:52:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.80
[32m[20221208 14:52:56 @agent_ppo2.py:137][0m Total time:      16.36 min
[32m[20221208 14:52:56 @agent_ppo2.py:139][0m 1323008 total steps have happened
[32m[20221208 14:52:56 @agent_ppo2.py:115][0m #------------------------ Iteration 646 --------------------------#
[32m[20221208 14:52:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |           0.0650 |           2.8655 |        -774.2033 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |           0.0372 |           2.2944 |        -731.6348 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |           0.0102 |           2.1346 |        -733.6592 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0132 |           1.9996 |        -734.9308 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0157 |           1.8958 |        -735.3356 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0273 |           1.8768 |        -746.6854 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0426 |           1.8125 |        -768.7239 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0519 |           1.7737 |        -785.0644 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0509 |           1.7378 |        -778.4029 |
[32m[20221208 14:52:57 @agent_ppo2.py:179][0m |          -0.0560 |           1.7044 |        -775.5659 |
[32m[20221208 14:52:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:52:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.44
[32m[20221208 14:52:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.69
[32m[20221208 14:52:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.19
[32m[20221208 14:52:57 @agent_ppo2.py:137][0m Total time:      16.39 min
[32m[20221208 14:52:57 @agent_ppo2.py:139][0m 1325056 total steps have happened
[32m[20221208 14:52:57 @agent_ppo2.py:115][0m #------------------------ Iteration 647 --------------------------#
[32m[20221208 14:52:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |           0.0796 |           1.9178 |        -580.5969 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |           0.0237 |           1.6636 |        -439.8781 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |          -0.0051 |           1.5759 |        -510.9981 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |          -0.0280 |           1.5269 |        -550.3213 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |          -0.0362 |           1.4662 |        -560.3028 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |          -0.0423 |           1.4559 |        -565.7824 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |          -0.0380 |           1.4507 |        -571.4940 |
[32m[20221208 14:52:58 @agent_ppo2.py:179][0m |          -0.0479 |           1.3964 |        -581.7156 |
[32m[20221208 14:52:59 @agent_ppo2.py:179][0m |          -0.0489 |           1.3942 |        -581.1277 |
[32m[20221208 14:52:59 @agent_ppo2.py:179][0m |          -0.0503 |           1.3703 |        -587.7195 |
[32m[20221208 14:52:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:52:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.84
[32m[20221208 14:52:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.73
[32m[20221208 14:52:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.58
[32m[20221208 14:52:59 @agent_ppo2.py:137][0m Total time:      16.41 min
[32m[20221208 14:52:59 @agent_ppo2.py:139][0m 1327104 total steps have happened
[32m[20221208 14:52:59 @agent_ppo2.py:115][0m #------------------------ Iteration 648 --------------------------#
[32m[20221208 14:52:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:52:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |           0.0871 |           1.2328 |        -705.9499 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |           0.0983 |           0.9430 |        -498.9835 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |           0.0065 |           0.8672 |        -376.3657 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0201 |           0.8293 |        -368.4500 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0386 |           0.8052 |        -396.0111 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0490 |           0.7730 |        -396.9910 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0481 |           0.7556 |        -398.8962 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0593 |           0.7360 |        -410.9401 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0626 |           0.7242 |        -416.1160 |
[32m[20221208 14:53:00 @agent_ppo2.py:179][0m |          -0.0673 |           0.7118 |        -434.2133 |
[32m[20221208 14:53:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:53:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.76
[32m[20221208 14:53:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.20
[32m[20221208 14:53:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.75
[32m[20221208 14:53:00 @agent_ppo2.py:137][0m Total time:      16.44 min
[32m[20221208 14:53:00 @agent_ppo2.py:139][0m 1329152 total steps have happened
[32m[20221208 14:53:00 @agent_ppo2.py:115][0m #------------------------ Iteration 649 --------------------------#
[32m[20221208 14:53:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |           0.2093 |           2.4947 |        -677.1660 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |           0.0566 |           1.9777 |        -572.6780 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |           0.0106 |           1.7712 |        -600.4801 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |          -0.0157 |           1.6550 |        -640.0254 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |          -0.0199 |           1.5552 |        -626.9582 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |          -0.0293 |           1.4792 |        -642.2565 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |          -0.0384 |           1.4077 |        -653.6992 |
[32m[20221208 14:53:01 @agent_ppo2.py:179][0m |          -0.0517 |           1.3595 |        -681.8607 |
[32m[20221208 14:53:02 @agent_ppo2.py:179][0m |          -0.0526 |           1.3153 |        -693.0574 |
[32m[20221208 14:53:02 @agent_ppo2.py:179][0m |          -0.0569 |           1.2617 |        -698.2382 |
[32m[20221208 14:53:02 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:53:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.39
[32m[20221208 14:53:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.17
[32m[20221208 14:53:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.74
[32m[20221208 14:53:02 @agent_ppo2.py:137][0m Total time:      16.46 min
[32m[20221208 14:53:02 @agent_ppo2.py:139][0m 1331200 total steps have happened
[32m[20221208 14:53:02 @agent_ppo2.py:115][0m #------------------------ Iteration 650 --------------------------#
[32m[20221208 14:53:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |           0.1042 |           1.5377 |        -610.5704 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |           0.0501 |           1.1807 |        -341.7654 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |           0.0071 |           1.0630 |        -373.5269 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0158 |           0.9897 |        -394.2024 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0269 |           0.9426 |        -412.4175 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0288 |           0.9114 |        -412.5584 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0373 |           0.8895 |        -431.6671 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0441 |           0.8713 |        -431.0626 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0495 |           0.8519 |        -436.4370 |
[32m[20221208 14:53:03 @agent_ppo2.py:179][0m |          -0.0534 |           0.8373 |        -462.8943 |
[32m[20221208 14:53:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.70
[32m[20221208 14:53:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.83
[32m[20221208 14:53:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.50
[32m[20221208 14:53:04 @agent_ppo2.py:137][0m Total time:      16.49 min
[32m[20221208 14:53:04 @agent_ppo2.py:139][0m 1333248 total steps have happened
[32m[20221208 14:53:04 @agent_ppo2.py:115][0m #------------------------ Iteration 651 --------------------------#
[32m[20221208 14:53:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0649 |           2.3078 |        -662.7824 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0829 |           1.8540 |        -426.1190 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0660 |           1.7102 |        -425.2426 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0492 |           1.5310 |        -313.6696 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0306 |           1.4571 |        -321.2414 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0125 |           1.3782 |        -347.3815 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0023 |           1.3317 |        -370.5255 |
[32m[20221208 14:53:04 @agent_ppo2.py:179][0m |           0.0001 |           1.2703 |        -410.7609 |
[32m[20221208 14:53:05 @agent_ppo2.py:179][0m |          -0.0001 |           1.2189 |        -411.2922 |
[32m[20221208 14:53:05 @agent_ppo2.py:179][0m |          -0.0053 |           1.1840 |        -509.5029 |
[32m[20221208 14:53:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:53:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.85
[32m[20221208 14:53:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.60
[32m[20221208 14:53:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.51
[32m[20221208 14:53:05 @agent_ppo2.py:137][0m Total time:      16.51 min
[32m[20221208 14:53:05 @agent_ppo2.py:139][0m 1335296 total steps have happened
[32m[20221208 14:53:05 @agent_ppo2.py:115][0m #------------------------ Iteration 652 --------------------------#
[32m[20221208 14:53:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |           0.1975 |           2.0195 |        -531.2133 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |           0.0410 |           1.6143 |        -275.3600 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0042 |           1.4492 |        -332.0889 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0340 |           1.3527 |        -347.7742 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0467 |           1.2945 |        -368.9437 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0586 |           1.2351 |        -379.5704 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0655 |           1.2193 |        -381.8781 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0700 |           1.1752 |        -387.2440 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0696 |           1.1522 |        -392.9697 |
[32m[20221208 14:53:06 @agent_ppo2.py:179][0m |          -0.0777 |           1.1262 |        -403.5686 |
[32m[20221208 14:53:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:53:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.75
[32m[20221208 14:53:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.40
[32m[20221208 14:53:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.53
[32m[20221208 14:53:07 @agent_ppo2.py:137][0m Total time:      16.54 min
[32m[20221208 14:53:07 @agent_ppo2.py:139][0m 1337344 total steps have happened
[32m[20221208 14:53:07 @agent_ppo2.py:115][0m #------------------------ Iteration 653 --------------------------#
[32m[20221208 14:53:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |           0.1110 |           1.0143 |        -749.5058 |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |           0.1879 |           0.8374 |        -335.0611 |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |           0.0885 |           0.7601 |        -299.5628 |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |           0.0332 |           0.7089 |        -390.9056 |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |           0.0086 |           0.6694 |        -471.0709 |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |          -0.0128 |           0.6452 |        -529.7810 |
[32m[20221208 14:53:07 @agent_ppo2.py:179][0m |          -0.0218 |           0.6207 |        -537.1076 |
[32m[20221208 14:53:08 @agent_ppo2.py:179][0m |          -0.0305 |           0.6074 |        -566.3242 |
[32m[20221208 14:53:08 @agent_ppo2.py:179][0m |          -0.0327 |           0.5943 |        -583.7927 |
[32m[20221208 14:53:08 @agent_ppo2.py:179][0m |          -0.0344 |           0.5732 |        -596.9088 |
[32m[20221208 14:53:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.23
[32m[20221208 14:53:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.94
[32m[20221208 14:53:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.12
[32m[20221208 14:53:08 @agent_ppo2.py:137][0m Total time:      16.56 min
[32m[20221208 14:53:08 @agent_ppo2.py:139][0m 1339392 total steps have happened
[32m[20221208 14:53:08 @agent_ppo2.py:115][0m #------------------------ Iteration 654 --------------------------#
[32m[20221208 14:53:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |           0.1057 |           3.4768 |        -734.8727 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |           0.0701 |           2.8819 |        -620.7917 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |           0.0119 |           2.6724 |        -639.0869 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0189 |           2.5649 |        -690.9831 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0327 |           2.4685 |        -714.3057 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0465 |           2.4131 |        -724.6438 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0548 |           2.3783 |        -750.8838 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0599 |           2.3272 |        -742.9704 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0675 |           2.2632 |        -756.6395 |
[32m[20221208 14:53:09 @agent_ppo2.py:179][0m |          -0.0679 |           2.2120 |        -766.9977 |
[32m[20221208 14:53:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.01
[32m[20221208 14:53:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.21
[32m[20221208 14:53:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.51
[32m[20221208 14:53:10 @agent_ppo2.py:137][0m Total time:      16.59 min
[32m[20221208 14:53:10 @agent_ppo2.py:139][0m 1341440 total steps have happened
[32m[20221208 14:53:10 @agent_ppo2.py:115][0m #------------------------ Iteration 655 --------------------------#
[32m[20221208 14:53:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |           0.0736 |           3.2323 |        -723.7439 |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |           0.0480 |           2.6843 |        -623.2778 |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |           0.0027 |           2.5230 |        -636.6128 |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |          -0.0177 |           2.4293 |        -651.8402 |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |          -0.0359 |           2.3299 |        -689.1393 |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |          -0.0461 |           2.2581 |        -706.2595 |
[32m[20221208 14:53:10 @agent_ppo2.py:179][0m |          -0.0538 |           2.2117 |        -721.4936 |
[32m[20221208 14:53:11 @agent_ppo2.py:179][0m |          -0.0593 |           2.1764 |        -744.4562 |
[32m[20221208 14:53:11 @agent_ppo2.py:179][0m |          -0.0654 |           2.1246 |        -740.4262 |
[32m[20221208 14:53:11 @agent_ppo2.py:179][0m |          -0.0668 |           2.0721 |        -754.7883 |
[32m[20221208 14:53:11 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.25
[32m[20221208 14:53:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.24
[32m[20221208 14:53:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.05
[32m[20221208 14:53:11 @agent_ppo2.py:137][0m Total time:      16.61 min
[32m[20221208 14:53:11 @agent_ppo2.py:139][0m 1343488 total steps have happened
[32m[20221208 14:53:11 @agent_ppo2.py:115][0m #------------------------ Iteration 656 --------------------------#
[32m[20221208 14:53:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |           0.0656 |           2.4216 |        -818.0281 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |           0.0249 |           1.9118 |        -803.2577 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0033 |           1.6915 |        -820.1009 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0225 |           1.5566 |        -815.8389 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0410 |           1.4880 |        -841.3620 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0456 |           1.4049 |        -849.5139 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0579 |           1.3657 |        -856.6506 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0627 |           1.3207 |        -844.6494 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0656 |           1.2906 |        -858.2772 |
[32m[20221208 14:53:12 @agent_ppo2.py:179][0m |          -0.0736 |           1.2618 |        -850.0140 |
[32m[20221208 14:53:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.46
[32m[20221208 14:53:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.89
[32m[20221208 14:53:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.38
[32m[20221208 14:53:13 @agent_ppo2.py:137][0m Total time:      16.64 min
[32m[20221208 14:53:13 @agent_ppo2.py:139][0m 1345536 total steps have happened
[32m[20221208 14:53:13 @agent_ppo2.py:115][0m #------------------------ Iteration 657 --------------------------#
[32m[20221208 14:53:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:13 @agent_ppo2.py:179][0m |           0.1207 |           1.5658 |        -710.9407 |
[32m[20221208 14:53:13 @agent_ppo2.py:179][0m |           0.3350 |           1.2662 |        -296.0949 |
[32m[20221208 14:53:13 @agent_ppo2.py:179][0m |           0.0727 |           1.1347 |        -304.4778 |
[32m[20221208 14:53:13 @agent_ppo2.py:179][0m |           0.0266 |           1.0399 |        -422.8865 |
[32m[20221208 14:53:13 @agent_ppo2.py:179][0m |          -0.0030 |           0.9792 |        -553.6239 |
[32m[20221208 14:53:13 @agent_ppo2.py:179][0m |          -0.0254 |           0.9435 |        -650.2722 |
[32m[20221208 14:53:14 @agent_ppo2.py:179][0m |          -0.0354 |           0.9062 |        -698.8950 |
[32m[20221208 14:53:14 @agent_ppo2.py:179][0m |          -0.0535 |           0.8823 |        -749.0576 |
[32m[20221208 14:53:14 @agent_ppo2.py:179][0m |          -0.0552 |           0.8543 |        -758.6897 |
[32m[20221208 14:53:14 @agent_ppo2.py:179][0m |          -0.0567 |           0.8387 |        -776.7909 |
[32m[20221208 14:53:14 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:53:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.02
[32m[20221208 14:53:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.61
[32m[20221208 14:53:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.49
[32m[20221208 14:53:14 @agent_ppo2.py:137][0m Total time:      16.66 min
[32m[20221208 14:53:14 @agent_ppo2.py:139][0m 1347584 total steps have happened
[32m[20221208 14:53:14 @agent_ppo2.py:115][0m #------------------------ Iteration 658 --------------------------#
[32m[20221208 14:53:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |           0.0829 |           3.2594 |        -796.7069 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |           0.0882 |           2.7173 |        -648.0486 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |           0.0111 |           2.5185 |        -757.7829 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0304 |           2.4628 |        -783.0109 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0406 |           2.4000 |        -799.1285 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0485 |           2.3277 |        -823.1691 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0533 |           2.2729 |        -825.8211 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0570 |           2.2394 |        -834.0738 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0618 |           2.2106 |        -848.6000 |
[32m[20221208 14:53:15 @agent_ppo2.py:179][0m |          -0.0682 |           2.1613 |        -859.3500 |
[32m[20221208 14:53:15 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:53:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.85
[32m[20221208 14:53:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.68
[32m[20221208 14:53:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.07
[32m[20221208 14:53:16 @agent_ppo2.py:137][0m Total time:      16.69 min
[32m[20221208 14:53:16 @agent_ppo2.py:139][0m 1349632 total steps have happened
[32m[20221208 14:53:16 @agent_ppo2.py:115][0m #------------------------ Iteration 659 --------------------------#
[32m[20221208 14:53:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:16 @agent_ppo2.py:179][0m |           0.0926 |           1.2895 |        -730.8461 |
[32m[20221208 14:53:16 @agent_ppo2.py:179][0m |           0.1682 |           1.0260 |        -372.8244 |
[32m[20221208 14:53:16 @agent_ppo2.py:179][0m |           0.0576 |           0.9474 |        -429.2680 |
[32m[20221208 14:53:16 @agent_ppo2.py:179][0m |           0.0084 |           0.8866 |        -574.7974 |
[32m[20221208 14:53:16 @agent_ppo2.py:179][0m |          -0.0156 |           0.8492 |        -677.3671 |
[32m[20221208 14:53:17 @agent_ppo2.py:179][0m |          -0.0310 |           0.8125 |        -728.9665 |
[32m[20221208 14:53:17 @agent_ppo2.py:179][0m |          -0.0355 |           0.7864 |        -742.8091 |
[32m[20221208 14:53:17 @agent_ppo2.py:179][0m |          -0.0461 |           0.7697 |        -762.0571 |
[32m[20221208 14:53:17 @agent_ppo2.py:179][0m |          -0.0538 |           0.7533 |        -788.4298 |
[32m[20221208 14:53:17 @agent_ppo2.py:179][0m |          -0.0481 |           0.7327 |        -803.5905 |
[32m[20221208 14:53:17 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.60
[32m[20221208 14:53:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.44
[32m[20221208 14:53:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.19
[32m[20221208 14:53:17 @agent_ppo2.py:137][0m Total time:      16.72 min
[32m[20221208 14:53:17 @agent_ppo2.py:139][0m 1351680 total steps have happened
[32m[20221208 14:53:17 @agent_ppo2.py:115][0m #------------------------ Iteration 660 --------------------------#
[32m[20221208 14:53:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |           0.0892 |           2.5140 |        -728.2179 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |           0.0370 |           2.0529 |        -715.2117 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |           0.0024 |           1.9007 |        -769.5422 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0141 |           1.7816 |        -796.8337 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0287 |           1.7150 |        -804.9126 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0384 |           1.6363 |        -813.3139 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0431 |           1.5903 |        -839.4183 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0469 |           1.5680 |        -853.5392 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0438 |           1.5453 |        -821.5166 |
[32m[20221208 14:53:18 @agent_ppo2.py:179][0m |          -0.0515 |           1.4849 |        -843.1237 |
[32m[20221208 14:53:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.05
[32m[20221208 14:53:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.23
[32m[20221208 14:53:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.84
[32m[20221208 14:53:19 @agent_ppo2.py:137][0m Total time:      16.74 min
[32m[20221208 14:53:19 @agent_ppo2.py:139][0m 1353728 total steps have happened
[32m[20221208 14:53:19 @agent_ppo2.py:115][0m #------------------------ Iteration 661 --------------------------#
[32m[20221208 14:53:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:19 @agent_ppo2.py:179][0m |           0.0627 |           2.7302 |        -816.3258 |
[32m[20221208 14:53:19 @agent_ppo2.py:179][0m |           0.0068 |           2.3602 |        -773.0564 |
[32m[20221208 14:53:19 @agent_ppo2.py:179][0m |          -0.0179 |           2.2381 |        -782.3658 |
[32m[20221208 14:53:19 @agent_ppo2.py:179][0m |          -0.0364 |           2.1493 |        -806.6367 |
[32m[20221208 14:53:19 @agent_ppo2.py:179][0m |          -0.0481 |           2.0451 |        -803.4301 |
[32m[20221208 14:53:20 @agent_ppo2.py:179][0m |          -0.0572 |           2.0138 |        -816.5341 |
[32m[20221208 14:53:20 @agent_ppo2.py:179][0m |          -0.0648 |           1.9762 |        -835.6564 |
[32m[20221208 14:53:20 @agent_ppo2.py:179][0m |          -0.0678 |           1.9201 |        -841.7658 |
[32m[20221208 14:53:20 @agent_ppo2.py:179][0m |          -0.0730 |           1.9047 |        -852.9128 |
[32m[20221208 14:53:20 @agent_ppo2.py:179][0m |          -0.0737 |           1.8718 |        -863.2934 |
[32m[20221208 14:53:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.67
[32m[20221208 14:53:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.29
[32m[20221208 14:53:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.92
[32m[20221208 14:53:20 @agent_ppo2.py:137][0m Total time:      16.77 min
[32m[20221208 14:53:20 @agent_ppo2.py:139][0m 1355776 total steps have happened
[32m[20221208 14:53:20 @agent_ppo2.py:115][0m #------------------------ Iteration 662 --------------------------#
[32m[20221208 14:53:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |           0.0715 |           3.5423 |        -784.9033 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |           0.0749 |           3.1122 |        -625.4755 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |           0.0522 |           2.9014 |        -607.6072 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |           0.0058 |           2.8022 |        -675.9532 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |          -0.0243 |           2.6870 |        -725.6400 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |          -0.0432 |           2.6420 |        -747.8131 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |          -0.0507 |           2.6009 |        -763.8122 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |          -0.0532 |           2.5410 |        -781.0724 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |          -0.0618 |           2.5178 |        -791.1922 |
[32m[20221208 14:53:21 @agent_ppo2.py:179][0m |          -0.0585 |           2.4967 |        -790.6803 |
[32m[20221208 14:53:21 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.95
[32m[20221208 14:53:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.39
[32m[20221208 14:53:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.49
[32m[20221208 14:53:22 @agent_ppo2.py:137][0m Total time:      16.79 min
[32m[20221208 14:53:22 @agent_ppo2.py:139][0m 1357824 total steps have happened
[32m[20221208 14:53:22 @agent_ppo2.py:115][0m #------------------------ Iteration 663 --------------------------#
[32m[20221208 14:53:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:22 @agent_ppo2.py:179][0m |           0.1351 |           1.3061 |        -714.0358 |
[32m[20221208 14:53:22 @agent_ppo2.py:179][0m |           0.1061 |           1.0536 |        -534.8804 |
[32m[20221208 14:53:22 @agent_ppo2.py:179][0m |           0.0399 |           0.9398 |        -573.6359 |
[32m[20221208 14:53:22 @agent_ppo2.py:179][0m |           0.0081 |           0.8762 |        -674.2767 |
[32m[20221208 14:53:23 @agent_ppo2.py:179][0m |          -0.0204 |           0.8331 |        -731.3206 |
[32m[20221208 14:53:23 @agent_ppo2.py:179][0m |          -0.0244 |           0.7897 |        -744.9324 |
[32m[20221208 14:53:23 @agent_ppo2.py:179][0m |          -0.0389 |           0.7698 |        -767.4836 |
[32m[20221208 14:53:23 @agent_ppo2.py:179][0m |          -0.0381 |           0.7471 |        -754.7024 |
[32m[20221208 14:53:23 @agent_ppo2.py:179][0m |          -0.0450 |           0.7215 |        -773.9768 |
[32m[20221208 14:53:23 @agent_ppo2.py:179][0m |          -0.0515 |           0.7063 |        -784.1348 |
[32m[20221208 14:53:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.65
[32m[20221208 14:53:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.80
[32m[20221208 14:53:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.10
[32m[20221208 14:53:23 @agent_ppo2.py:137][0m Total time:      16.82 min
[32m[20221208 14:53:23 @agent_ppo2.py:139][0m 1359872 total steps have happened
[32m[20221208 14:53:23 @agent_ppo2.py:115][0m #------------------------ Iteration 664 --------------------------#
[32m[20221208 14:53:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |           0.0973 |           3.7722 |        -746.9561 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |           0.0769 |           3.1889 |        -578.0312 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |           0.0385 |           3.0330 |        -597.9845 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0119 |           2.8726 |        -685.3108 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0238 |           2.7884 |        -717.7836 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0423 |           2.7596 |        -748.4845 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0485 |           2.6402 |        -749.2738 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0529 |           2.6109 |        -764.2219 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0597 |           2.5539 |        -772.6565 |
[32m[20221208 14:53:24 @agent_ppo2.py:179][0m |          -0.0631 |           2.5143 |        -765.7411 |
[32m[20221208 14:53:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.10
[32m[20221208 14:53:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.68
[32m[20221208 14:53:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.02
[32m[20221208 14:53:25 @agent_ppo2.py:137][0m Total time:      16.84 min
[32m[20221208 14:53:25 @agent_ppo2.py:139][0m 1361920 total steps have happened
[32m[20221208 14:53:25 @agent_ppo2.py:115][0m #------------------------ Iteration 665 --------------------------#
[32m[20221208 14:53:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:25 @agent_ppo2.py:179][0m |           0.0467 |           3.3217 |        -731.0229 |
[32m[20221208 14:53:25 @agent_ppo2.py:179][0m |           0.0644 |           2.5403 |        -623.7775 |
[32m[20221208 14:53:25 @agent_ppo2.py:179][0m |          -0.0067 |           2.2668 |        -682.0115 |
[32m[20221208 14:53:25 @agent_ppo2.py:179][0m |          -0.0309 |           2.0646 |        -732.3925 |
[32m[20221208 14:53:26 @agent_ppo2.py:179][0m |          -0.0446 |           1.9036 |        -748.7718 |
[32m[20221208 14:53:26 @agent_ppo2.py:179][0m |          -0.0566 |           1.7711 |        -760.0119 |
[32m[20221208 14:53:26 @agent_ppo2.py:179][0m |          -0.0622 |           1.6543 |        -784.5548 |
[32m[20221208 14:53:26 @agent_ppo2.py:179][0m |          -0.0643 |           1.5549 |        -777.8948 |
[32m[20221208 14:53:26 @agent_ppo2.py:179][0m |          -0.0662 |           1.4838 |        -791.0568 |
[32m[20221208 14:53:26 @agent_ppo2.py:179][0m |          -0.0726 |           1.4209 |        -804.7153 |
[32m[20221208 14:53:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.71
[32m[20221208 14:53:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.61
[32m[20221208 14:53:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.45
[32m[20221208 14:53:26 @agent_ppo2.py:137][0m Total time:      16.87 min
[32m[20221208 14:53:26 @agent_ppo2.py:139][0m 1363968 total steps have happened
[32m[20221208 14:53:26 @agent_ppo2.py:115][0m #------------------------ Iteration 666 --------------------------#
[32m[20221208 14:53:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |           0.1103 |           2.8457 |        -719.4434 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |           1.0019 |           2.2164 |        -554.0265 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |           0.0533 |           2.0634 |        -593.3269 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |           0.0090 |           1.9415 |        -618.6430 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |          -0.0296 |           1.8427 |        -663.5402 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |          -0.0474 |           1.7848 |        -687.0105 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |          -0.0570 |           1.7425 |        -701.6792 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |          -0.0685 |           1.7133 |        -725.5258 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |          -0.0717 |           1.6728 |        -741.6436 |
[32m[20221208 14:53:27 @agent_ppo2.py:179][0m |          -0.0734 |           1.6373 |        -734.8529 |
[32m[20221208 14:53:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.82
[32m[20221208 14:53:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.39
[32m[20221208 14:53:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.53
[32m[20221208 14:53:28 @agent_ppo2.py:137][0m Total time:      16.89 min
[32m[20221208 14:53:28 @agent_ppo2.py:139][0m 1366016 total steps have happened
[32m[20221208 14:53:28 @agent_ppo2.py:115][0m #------------------------ Iteration 667 --------------------------#
[32m[20221208 14:53:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:28 @agent_ppo2.py:179][0m |           0.0734 |           1.9065 |        -772.7355 |
[32m[20221208 14:53:28 @agent_ppo2.py:179][0m |           0.0684 |           1.4573 |        -658.1079 |
[32m[20221208 14:53:28 @agent_ppo2.py:179][0m |           0.0162 |           1.3329 |        -738.6023 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0077 |           1.2724 |        -759.2729 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0360 |           1.2259 |        -796.0780 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0436 |           1.1639 |        -810.5973 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0463 |           1.1492 |        -825.6906 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0515 |           1.1101 |        -830.7681 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0507 |           1.0871 |        -835.0183 |
[32m[20221208 14:53:29 @agent_ppo2.py:179][0m |          -0.0597 |           1.0635 |        -844.5985 |
[32m[20221208 14:53:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:53:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.80
[32m[20221208 14:53:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.26
[32m[20221208 14:53:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.92
[32m[20221208 14:53:29 @agent_ppo2.py:137][0m Total time:      16.92 min
[32m[20221208 14:53:29 @agent_ppo2.py:139][0m 1368064 total steps have happened
[32m[20221208 14:53:29 @agent_ppo2.py:115][0m #------------------------ Iteration 668 --------------------------#
[32m[20221208 14:53:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |           0.0593 |           1.0388 |        -798.7008 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |           0.1033 |           0.8925 |        -315.9088 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |           0.0568 |           0.8248 |        -289.2727 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |           0.0252 |           0.7895 |        -394.9231 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |           0.0035 |           0.7632 |        -472.2904 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |          -0.0061 |           0.7351 |        -584.2357 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |          -0.0165 |           0.7148 |        -673.2431 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |          -0.0343 |           0.6972 |        -769.9379 |
[32m[20221208 14:53:30 @agent_ppo2.py:179][0m |          -0.0376 |           0.6808 |        -778.2198 |
[32m[20221208 14:53:31 @agent_ppo2.py:179][0m |          -0.0488 |           0.6699 |        -799.4279 |
[32m[20221208 14:53:31 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:53:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.23
[32m[20221208 14:53:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.69
[32m[20221208 14:53:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.69
[32m[20221208 14:53:31 @agent_ppo2.py:137][0m Total time:      16.94 min
[32m[20221208 14:53:31 @agent_ppo2.py:139][0m 1370112 total steps have happened
[32m[20221208 14:53:31 @agent_ppo2.py:115][0m #------------------------ Iteration 669 --------------------------#
[32m[20221208 14:53:31 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:53:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |           0.0834 |           2.5157 |        -711.7642 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |           0.0548 |           2.2478 |        -669.5923 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |           0.0017 |           2.0593 |        -728.8408 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0263 |           1.9119 |        -730.4299 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0354 |           1.8098 |        -750.1561 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0385 |           1.7240 |        -743.3960 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0562 |           1.6606 |        -764.0574 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0637 |           1.5998 |        -774.6003 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0674 |           1.5555 |        -779.7115 |
[32m[20221208 14:53:32 @agent_ppo2.py:179][0m |          -0.0728 |           1.5108 |        -790.6009 |
[32m[20221208 14:53:32 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:53:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.00
[32m[20221208 14:53:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.13
[32m[20221208 14:53:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.69
[32m[20221208 14:53:32 @agent_ppo2.py:137][0m Total time:      16.97 min
[32m[20221208 14:53:32 @agent_ppo2.py:139][0m 1372160 total steps have happened
[32m[20221208 14:53:32 @agent_ppo2.py:115][0m #------------------------ Iteration 670 --------------------------#
[32m[20221208 14:53:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |           0.0892 |           3.0447 |        -758.8228 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |           0.0387 |           2.5082 |        -645.0431 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |           0.0042 |           2.3003 |        -698.6511 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |          -0.0340 |           2.1641 |        -768.5847 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |          -0.0457 |           2.0626 |        -778.4882 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |          -0.0536 |           1.9772 |        -799.1978 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |          -0.0578 |           1.9363 |        -814.4465 |
[32m[20221208 14:53:33 @agent_ppo2.py:179][0m |          -0.0602 |           1.8537 |        -810.9532 |
[32m[20221208 14:53:34 @agent_ppo2.py:179][0m |          -0.0619 |           1.8202 |        -829.0379 |
[32m[20221208 14:53:34 @agent_ppo2.py:179][0m |          -0.0684 |           1.7837 |        -830.0196 |
[32m[20221208 14:53:34 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:53:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.87
[32m[20221208 14:53:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.92
[32m[20221208 14:53:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.33
[32m[20221208 14:53:34 @agent_ppo2.py:137][0m Total time:      17.00 min
[32m[20221208 14:53:34 @agent_ppo2.py:139][0m 1374208 total steps have happened
[32m[20221208 14:53:34 @agent_ppo2.py:115][0m #------------------------ Iteration 671 --------------------------#
[32m[20221208 14:53:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |           0.0661 |           4.1902 |        -763.4072 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |           0.0718 |           3.7739 |        -655.8284 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |           0.0509 |           3.4987 |        -665.9976 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |           0.0195 |           3.3386 |        -694.4528 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |          -0.0025 |           3.2353 |        -752.8215 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |          -0.0295 |           3.1507 |        -795.6417 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |          -0.0352 |           3.0843 |        -818.7293 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |          -0.0406 |           2.9709 |        -821.5554 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |          -0.0447 |           2.9220 |        -836.0772 |
[32m[20221208 14:53:35 @agent_ppo2.py:179][0m |          -0.0521 |           2.8782 |        -838.4348 |
[32m[20221208 14:53:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 109.82
[32m[20221208 14:53:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.01
[32m[20221208 14:53:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.93
[32m[20221208 14:53:36 @agent_ppo2.py:137][0m Total time:      17.02 min
[32m[20221208 14:53:36 @agent_ppo2.py:139][0m 1376256 total steps have happened
[32m[20221208 14:53:36 @agent_ppo2.py:115][0m #------------------------ Iteration 672 --------------------------#
[32m[20221208 14:53:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |           0.0666 |           4.7638 |        -755.2557 |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |           0.0579 |           4.0423 |        -600.7132 |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |           0.0055 |           3.7116 |        -651.7750 |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |          -0.0227 |           3.5292 |        -692.7511 |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |          -0.0404 |           3.3667 |        -730.8892 |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |          -0.0544 |           3.2366 |        -749.3723 |
[32m[20221208 14:53:36 @agent_ppo2.py:179][0m |          -0.0545 |           3.1939 |        -770.0577 |
[32m[20221208 14:53:37 @agent_ppo2.py:179][0m |          -0.0622 |           3.0684 |        -776.3077 |
[32m[20221208 14:53:37 @agent_ppo2.py:179][0m |          -0.0665 |           2.9860 |        -778.3884 |
[32m[20221208 14:53:37 @agent_ppo2.py:179][0m |          -0.0713 |           2.9157 |        -789.8903 |
[32m[20221208 14:53:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:53:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.79
[32m[20221208 14:53:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.55
[32m[20221208 14:53:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.84
[32m[20221208 14:53:37 @agent_ppo2.py:137][0m Total time:      17.05 min
[32m[20221208 14:53:37 @agent_ppo2.py:139][0m 1378304 total steps have happened
[32m[20221208 14:53:37 @agent_ppo2.py:115][0m #------------------------ Iteration 673 --------------------------#
[32m[20221208 14:53:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |           0.1303 |           1.8193 |        -658.7163 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |           0.0793 |           1.4926 |        -507.2426 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |           0.0253 |           1.4030 |        -493.1603 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0098 |           1.3410 |        -534.3792 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0284 |           1.3121 |        -547.7544 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0426 |           1.2769 |        -545.7238 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0496 |           1.2656 |        -566.9180 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0617 |           1.2493 |        -580.3494 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0613 |           1.2327 |        -578.5517 |
[32m[20221208 14:53:38 @agent_ppo2.py:179][0m |          -0.0568 |           1.2181 |        -585.6518 |
[32m[20221208 14:53:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.90
[32m[20221208 14:53:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.57
[32m[20221208 14:53:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.92
[32m[20221208 14:53:39 @agent_ppo2.py:137][0m Total time:      17.07 min
[32m[20221208 14:53:39 @agent_ppo2.py:139][0m 1380352 total steps have happened
[32m[20221208 14:53:39 @agent_ppo2.py:115][0m #------------------------ Iteration 674 --------------------------#
[32m[20221208 14:53:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:39 @agent_ppo2.py:179][0m |           0.0669 |           4.9778 |        -759.0690 |
[32m[20221208 14:53:39 @agent_ppo2.py:179][0m |           0.0512 |           4.3362 |        -687.9914 |
[32m[20221208 14:53:39 @agent_ppo2.py:179][0m |          -0.0033 |           3.9837 |        -718.7756 |
[32m[20221208 14:53:39 @agent_ppo2.py:179][0m |          -0.0320 |           3.7811 |        -782.3838 |
[32m[20221208 14:53:39 @agent_ppo2.py:179][0m |          -0.0417 |           3.6636 |        -793.6709 |
[32m[20221208 14:53:39 @agent_ppo2.py:179][0m |          -0.0293 |           3.5321 |        -787.3627 |
[32m[20221208 14:53:40 @agent_ppo2.py:179][0m |          -0.0428 |           3.4205 |        -787.7076 |
[32m[20221208 14:53:40 @agent_ppo2.py:179][0m |          -0.0610 |           3.3599 |        -821.7221 |
[32m[20221208 14:53:40 @agent_ppo2.py:179][0m |          -0.0630 |           3.2645 |        -834.8076 |
[32m[20221208 14:53:40 @agent_ppo2.py:179][0m |          -0.0652 |           3.2076 |        -847.7871 |
[32m[20221208 14:53:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.72
[32m[20221208 14:53:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.98
[32m[20221208 14:53:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.26
[32m[20221208 14:53:40 @agent_ppo2.py:137][0m Total time:      17.10 min
[32m[20221208 14:53:40 @agent_ppo2.py:139][0m 1382400 total steps have happened
[32m[20221208 14:53:40 @agent_ppo2.py:115][0m #------------------------ Iteration 675 --------------------------#
[32m[20221208 14:53:41 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:53:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |           0.0476 |           5.1011 |        -806.9471 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |           0.0079 |           4.1989 |        -801.4319 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0205 |           3.8729 |        -798.5497 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0081 |           3.7186 |        -744.1189 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0315 |           3.5793 |        -763.7025 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0437 |           3.4698 |        -784.9975 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0436 |           3.3925 |        -790.6218 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0613 |           3.3479 |        -827.3351 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0648 |           3.2705 |        -834.4280 |
[32m[20221208 14:53:41 @agent_ppo2.py:179][0m |          -0.0671 |           3.2127 |        -843.4693 |
[32m[20221208 14:53:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.41
[32m[20221208 14:53:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.68
[32m[20221208 14:53:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.07
[32m[20221208 14:53:42 @agent_ppo2.py:137][0m Total time:      17.12 min
[32m[20221208 14:53:42 @agent_ppo2.py:139][0m 1384448 total steps have happened
[32m[20221208 14:53:42 @agent_ppo2.py:115][0m #------------------------ Iteration 676 --------------------------#
[32m[20221208 14:53:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:42 @agent_ppo2.py:179][0m |           0.0719 |           1.7101 |        -783.9022 |
[32m[20221208 14:53:42 @agent_ppo2.py:179][0m |           0.0768 |           1.4089 |        -442.2096 |
[32m[20221208 14:53:42 @agent_ppo2.py:179][0m |           0.0230 |           1.3025 |        -575.4737 |
[32m[20221208 14:53:42 @agent_ppo2.py:179][0m |           0.0030 |           1.2265 |        -603.8810 |
[32m[20221208 14:53:42 @agent_ppo2.py:179][0m |          -0.0115 |           1.1730 |        -614.6652 |
[32m[20221208 14:53:42 @agent_ppo2.py:179][0m |          -0.0255 |           1.1344 |        -693.2170 |
[32m[20221208 14:53:43 @agent_ppo2.py:179][0m |          -0.0307 |           1.1179 |        -738.8667 |
[32m[20221208 14:53:43 @agent_ppo2.py:179][0m |          -0.0374 |           1.0871 |        -759.0345 |
[32m[20221208 14:53:43 @agent_ppo2.py:179][0m |          -0.0424 |           1.0617 |        -797.6974 |
[32m[20221208 14:53:43 @agent_ppo2.py:179][0m |          -0.0427 |           1.0446 |        -832.4601 |
[32m[20221208 14:53:43 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.17
[32m[20221208 14:53:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.98
[32m[20221208 14:53:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.96
[32m[20221208 14:53:43 @agent_ppo2.py:137][0m Total time:      17.15 min
[32m[20221208 14:53:43 @agent_ppo2.py:139][0m 1386496 total steps have happened
[32m[20221208 14:53:43 @agent_ppo2.py:115][0m #------------------------ Iteration 677 --------------------------#
[32m[20221208 14:53:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |           0.1131 |           3.7877 |        -722.2935 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |           0.1563 |           3.3538 |        -492.0248 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0110 |           3.2387 |        -543.1480 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0368 |           3.1225 |        -600.1227 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0472 |           3.0771 |        -609.7189 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0572 |           2.9898 |        -621.2995 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0698 |           2.9422 |        -659.6639 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0732 |           2.9414 |        -672.6892 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0800 |           2.8793 |        -679.1375 |
[32m[20221208 14:53:44 @agent_ppo2.py:179][0m |          -0.0805 |           2.8531 |        -699.7547 |
[32m[20221208 14:53:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.24
[32m[20221208 14:53:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.63
[32m[20221208 14:53:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.83
[32m[20221208 14:53:45 @agent_ppo2.py:137][0m Total time:      17.17 min
[32m[20221208 14:53:45 @agent_ppo2.py:139][0m 1388544 total steps have happened
[32m[20221208 14:53:45 @agent_ppo2.py:115][0m #------------------------ Iteration 678 --------------------------#
[32m[20221208 14:53:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:45 @agent_ppo2.py:179][0m |           0.1052 |           2.3469 |        -595.7323 |
[32m[20221208 14:53:45 @agent_ppo2.py:179][0m |           0.0820 |           1.9646 |        -427.5241 |
[32m[20221208 14:53:45 @agent_ppo2.py:179][0m |           0.0482 |           1.8435 |        -466.9299 |
[32m[20221208 14:53:45 @agent_ppo2.py:179][0m |          -0.0116 |           1.7562 |        -545.8753 |
[32m[20221208 14:53:45 @agent_ppo2.py:179][0m |          -0.0173 |           1.7103 |        -552.8905 |
[32m[20221208 14:53:45 @agent_ppo2.py:179][0m |          -0.0191 |           1.6646 |        -558.2193 |
[32m[20221208 14:53:46 @agent_ppo2.py:179][0m |          -0.0302 |           1.6256 |        -557.7232 |
[32m[20221208 14:53:46 @agent_ppo2.py:179][0m |          -0.0444 |           1.6040 |        -597.6596 |
[32m[20221208 14:53:46 @agent_ppo2.py:179][0m |          -0.0429 |           1.5697 |        -589.1050 |
[32m[20221208 14:53:46 @agent_ppo2.py:179][0m |          -0.0373 |           1.5460 |        -584.2541 |
[32m[20221208 14:53:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.14
[32m[20221208 14:53:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.56
[32m[20221208 14:53:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.98
[32m[20221208 14:53:46 @agent_ppo2.py:137][0m Total time:      17.20 min
[32m[20221208 14:53:46 @agent_ppo2.py:139][0m 1390592 total steps have happened
[32m[20221208 14:53:46 @agent_ppo2.py:115][0m #------------------------ Iteration 679 --------------------------#
[32m[20221208 14:53:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |           0.1169 |           3.0412 |        -719.4646 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |           0.0496 |           2.4780 |        -642.1652 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |           0.0100 |           2.3119 |        -672.3218 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0293 |           2.1917 |        -771.7665 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0437 |           2.0907 |        -793.3181 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0571 |           2.0536 |        -783.7972 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0644 |           1.9932 |        -807.1298 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0631 |           1.9702 |        -816.6018 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0699 |           1.9278 |        -815.7247 |
[32m[20221208 14:53:47 @agent_ppo2.py:179][0m |          -0.0712 |           1.9098 |        -821.1902 |
[32m[20221208 14:53:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.39
[32m[20221208 14:53:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.16
[32m[20221208 14:53:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.60
[32m[20221208 14:53:48 @agent_ppo2.py:137][0m Total time:      17.22 min
[32m[20221208 14:53:48 @agent_ppo2.py:139][0m 1392640 total steps have happened
[32m[20221208 14:53:48 @agent_ppo2.py:115][0m #------------------------ Iteration 680 --------------------------#
[32m[20221208 14:53:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:48 @agent_ppo2.py:179][0m |           0.1135 |           1.9329 |        -630.1066 |
[32m[20221208 14:53:48 @agent_ppo2.py:179][0m |           0.1157 |           1.5281 |        -367.0944 |
[32m[20221208 14:53:48 @agent_ppo2.py:179][0m |           0.0616 |           1.4091 |        -441.8748 |
[32m[20221208 14:53:48 @agent_ppo2.py:179][0m |           0.0303 |           1.3295 |        -501.2463 |
[32m[20221208 14:53:48 @agent_ppo2.py:179][0m |           0.0286 |           1.2940 |        -502.8480 |
[32m[20221208 14:53:49 @agent_ppo2.py:179][0m |           0.0120 |           1.2598 |        -529.7926 |
[32m[20221208 14:53:49 @agent_ppo2.py:179][0m |           0.0028 |           1.2368 |        -526.6269 |
[32m[20221208 14:53:49 @agent_ppo2.py:179][0m |          -0.0210 |           1.2011 |        -616.2541 |
[32m[20221208 14:53:49 @agent_ppo2.py:179][0m |          -0.0254 |           1.1781 |        -693.8013 |
[32m[20221208 14:53:49 @agent_ppo2.py:179][0m |          -0.0432 |           1.1553 |        -766.1378 |
[32m[20221208 14:53:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.03
[32m[20221208 14:53:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.94
[32m[20221208 14:53:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.60
[32m[20221208 14:53:49 @agent_ppo2.py:137][0m Total time:      17.25 min
[32m[20221208 14:53:49 @agent_ppo2.py:139][0m 1394688 total steps have happened
[32m[20221208 14:53:49 @agent_ppo2.py:115][0m #------------------------ Iteration 681 --------------------------#
[32m[20221208 14:53:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:53:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |           0.0703 |           3.8960 |        -743.8559 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |           0.0237 |           3.4074 |        -625.0557 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0115 |           3.2053 |        -648.6886 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0356 |           3.0935 |        -649.8974 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0506 |           2.9616 |        -680.7018 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0554 |           2.9361 |        -700.6078 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0672 |           2.8506 |        -720.7066 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0675 |           2.8488 |        -726.8583 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0736 |           2.7539 |        -736.3425 |
[32m[20221208 14:53:50 @agent_ppo2.py:179][0m |          -0.0754 |           2.7061 |        -738.1118 |
[32m[20221208 14:53:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.21
[32m[20221208 14:53:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.70
[32m[20221208 14:53:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.54
[32m[20221208 14:53:51 @agent_ppo2.py:137][0m Total time:      17.27 min
[32m[20221208 14:53:51 @agent_ppo2.py:139][0m 1396736 total steps have happened
[32m[20221208 14:53:51 @agent_ppo2.py:115][0m #------------------------ Iteration 682 --------------------------#
[32m[20221208 14:53:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:51 @agent_ppo2.py:179][0m |           0.0914 |           4.0015 |        -744.9666 |
[32m[20221208 14:53:51 @agent_ppo2.py:179][0m |           0.0526 |           3.4255 |        -649.2519 |
[32m[20221208 14:53:51 @agent_ppo2.py:179][0m |          -0.0003 |           3.2429 |        -729.7618 |
[32m[20221208 14:53:51 @agent_ppo2.py:179][0m |          -0.0292 |           3.0757 |        -756.1041 |
[32m[20221208 14:53:51 @agent_ppo2.py:179][0m |          -0.0377 |           2.9535 |        -765.6984 |
[32m[20221208 14:53:52 @agent_ppo2.py:179][0m |          -0.0542 |           2.8917 |        -794.5346 |
[32m[20221208 14:53:52 @agent_ppo2.py:179][0m |          -0.0594 |           2.7942 |        -797.5512 |
[32m[20221208 14:53:52 @agent_ppo2.py:179][0m |          -0.0676 |           2.7417 |        -800.6625 |
[32m[20221208 14:53:52 @agent_ppo2.py:179][0m |          -0.0686 |           2.6513 |        -819.9777 |
[32m[20221208 14:53:52 @agent_ppo2.py:179][0m |          -0.0706 |           2.5756 |        -814.7225 |
[32m[20221208 14:53:52 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.34
[32m[20221208 14:53:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.37
[32m[20221208 14:53:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.38
[32m[20221208 14:53:52 @agent_ppo2.py:137][0m Total time:      17.30 min
[32m[20221208 14:53:52 @agent_ppo2.py:139][0m 1398784 total steps have happened
[32m[20221208 14:53:52 @agent_ppo2.py:115][0m #------------------------ Iteration 683 --------------------------#
[32m[20221208 14:53:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |           0.0758 |           3.4531 |        -820.2963 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |           0.0519 |           3.0594 |        -682.7385 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0002 |           2.8888 |        -704.1525 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0219 |           2.7909 |        -760.9022 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0240 |           2.7488 |        -754.6994 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0416 |           2.6639 |        -778.1228 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0475 |           2.6156 |        -793.0860 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0519 |           2.5567 |        -793.6570 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0514 |           2.5286 |        -812.9885 |
[32m[20221208 14:53:53 @agent_ppo2.py:179][0m |          -0.0588 |           2.4774 |        -811.6260 |
[32m[20221208 14:53:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:53:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.78
[32m[20221208 14:53:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.73
[32m[20221208 14:53:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.39
[32m[20221208 14:53:54 @agent_ppo2.py:137][0m Total time:      17.32 min
[32m[20221208 14:53:54 @agent_ppo2.py:139][0m 1400832 total steps have happened
[32m[20221208 14:53:54 @agent_ppo2.py:115][0m #------------------------ Iteration 684 --------------------------#
[32m[20221208 14:53:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:54 @agent_ppo2.py:179][0m |           0.0937 |           6.3055 |        -778.4972 |
[32m[20221208 14:53:54 @agent_ppo2.py:179][0m |           0.2265 |           5.3617 |        -496.8405 |
[32m[20221208 14:53:54 @agent_ppo2.py:179][0m |           0.0472 |           4.9531 |        -565.1544 |
[32m[20221208 14:53:54 @agent_ppo2.py:179][0m |          -0.0094 |           4.6903 |        -715.6549 |
[32m[20221208 14:53:54 @agent_ppo2.py:179][0m |          -0.0301 |           4.4912 |        -767.5029 |
[32m[20221208 14:53:55 @agent_ppo2.py:179][0m |          -0.0398 |           4.3635 |        -780.5762 |
[32m[20221208 14:53:55 @agent_ppo2.py:179][0m |          -0.0457 |           4.2440 |        -802.1470 |
[32m[20221208 14:53:55 @agent_ppo2.py:179][0m |          -0.0474 |           4.1683 |        -790.0452 |
[32m[20221208 14:53:55 @agent_ppo2.py:179][0m |          -0.0616 |           4.0995 |        -814.1046 |
[32m[20221208 14:53:55 @agent_ppo2.py:179][0m |          -0.0645 |           4.0160 |        -824.8454 |
[32m[20221208 14:53:55 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:53:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.01
[32m[20221208 14:53:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.18
[32m[20221208 14:53:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.32
[32m[20221208 14:53:55 @agent_ppo2.py:137][0m Total time:      17.35 min
[32m[20221208 14:53:55 @agent_ppo2.py:139][0m 1402880 total steps have happened
[32m[20221208 14:53:55 @agent_ppo2.py:115][0m #------------------------ Iteration 685 --------------------------#
[32m[20221208 14:53:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |           0.1659 |           2.8596 |        -707.9775 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |           0.1433 |           2.0700 |        -501.7138 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |           0.0927 |           1.7535 |        -493.7413 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |           0.0544 |           1.5757 |        -508.1795 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |           0.0134 |           1.4779 |        -575.5421 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |          -0.0071 |           1.4094 |        -652.0444 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |          -0.0267 |           1.3422 |        -695.0574 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |          -0.0316 |           1.3019 |        -718.7795 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |          -0.0374 |           1.2741 |        -727.8441 |
[32m[20221208 14:53:56 @agent_ppo2.py:179][0m |          -0.0385 |           1.2246 |        -734.8394 |
[32m[20221208 14:53:56 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:53:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.37
[32m[20221208 14:53:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.28
[32m[20221208 14:53:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.62
[32m[20221208 14:53:57 @agent_ppo2.py:137][0m Total time:      17.37 min
[32m[20221208 14:53:57 @agent_ppo2.py:139][0m 1404928 total steps have happened
[32m[20221208 14:53:57 @agent_ppo2.py:115][0m #------------------------ Iteration 686 --------------------------#
[32m[20221208 14:53:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:53:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:57 @agent_ppo2.py:179][0m |           0.0683 |           3.7415 |        -762.4444 |
[32m[20221208 14:53:57 @agent_ppo2.py:179][0m |           0.0210 |           3.0829 |        -721.2653 |
[32m[20221208 14:53:57 @agent_ppo2.py:179][0m |          -0.0125 |           2.8131 |        -728.3771 |
[32m[20221208 14:53:57 @agent_ppo2.py:179][0m |          -0.0296 |           2.6220 |        -754.3482 |
[32m[20221208 14:53:58 @agent_ppo2.py:179][0m |          -0.0390 |           2.5222 |        -768.1978 |
[32m[20221208 14:53:58 @agent_ppo2.py:179][0m |          -0.0536 |           2.4283 |        -792.7144 |
[32m[20221208 14:53:58 @agent_ppo2.py:179][0m |          -0.0495 |           2.3578 |        -777.5598 |
[32m[20221208 14:53:58 @agent_ppo2.py:179][0m |          -0.0593 |           2.3232 |        -814.5556 |
[32m[20221208 14:53:58 @agent_ppo2.py:179][0m |          -0.0667 |           2.2439 |        -826.1644 |
[32m[20221208 14:53:58 @agent_ppo2.py:179][0m |          -0.0696 |           2.2079 |        -843.7111 |
[32m[20221208 14:53:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:53:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.45
[32m[20221208 14:53:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.88
[32m[20221208 14:53:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.86
[32m[20221208 14:53:58 @agent_ppo2.py:137][0m Total time:      17.40 min
[32m[20221208 14:53:58 @agent_ppo2.py:139][0m 1406976 total steps have happened
[32m[20221208 14:53:58 @agent_ppo2.py:115][0m #------------------------ Iteration 687 --------------------------#
[32m[20221208 14:53:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:53:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |           0.0987 |           2.3520 |        -708.6794 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |           0.0977 |           1.9111 |        -581.5197 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |           0.0361 |           1.7228 |        -549.2366 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0061 |           1.6218 |        -604.0812 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0269 |           1.5518 |        -668.9076 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0388 |           1.4782 |        -727.3211 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0512 |           1.4497 |        -741.1501 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0587 |           1.4173 |        -758.5426 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0640 |           1.3827 |        -743.5447 |
[32m[20221208 14:53:59 @agent_ppo2.py:179][0m |          -0.0690 |           1.3574 |        -771.0350 |
[32m[20221208 14:53:59 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:54:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.62
[32m[20221208 14:54:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.68
[32m[20221208 14:54:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.06
[32m[20221208 14:54:00 @agent_ppo2.py:137][0m Total time:      17.43 min
[32m[20221208 14:54:00 @agent_ppo2.py:139][0m 1409024 total steps have happened
[32m[20221208 14:54:00 @agent_ppo2.py:115][0m #------------------------ Iteration 688 --------------------------#
[32m[20221208 14:54:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:00 @agent_ppo2.py:179][0m |           0.0997 |           5.2270 |        -715.9540 |
[32m[20221208 14:54:00 @agent_ppo2.py:179][0m |           0.0626 |           4.5603 |        -479.9958 |
[32m[20221208 14:54:00 @agent_ppo2.py:179][0m |           0.0139 |           4.3403 |        -562.5498 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0187 |           4.2354 |        -599.1202 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0399 |           4.1335 |        -626.4121 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0467 |           4.0740 |        -647.4851 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0552 |           3.9829 |        -670.8662 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0622 |           3.9849 |        -691.5009 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0675 |           3.8841 |        -697.1385 |
[32m[20221208 14:54:01 @agent_ppo2.py:179][0m |          -0.0708 |           3.8317 |        -713.2844 |
[32m[20221208 14:54:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.61
[32m[20221208 14:54:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.56
[32m[20221208 14:54:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.22
[32m[20221208 14:54:01 @agent_ppo2.py:137][0m Total time:      17.45 min
[32m[20221208 14:54:01 @agent_ppo2.py:139][0m 1411072 total steps have happened
[32m[20221208 14:54:01 @agent_ppo2.py:115][0m #------------------------ Iteration 689 --------------------------#
[32m[20221208 14:54:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |           0.0997 |           6.6482 |        -711.6112 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |           0.0848 |           5.8062 |        -497.5766 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |           0.0275 |           5.4283 |        -604.5826 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0052 |           5.2544 |        -650.4825 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0268 |           5.1226 |        -702.8181 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0401 |           4.9995 |        -731.0246 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0499 |           4.9346 |        -747.8107 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0549 |           4.8067 |        -781.4179 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0571 |           4.7348 |        -787.8501 |
[32m[20221208 14:54:02 @agent_ppo2.py:179][0m |          -0.0614 |           4.7288 |        -794.9573 |
[32m[20221208 14:54:02 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.65
[32m[20221208 14:54:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.36
[32m[20221208 14:54:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.84
[32m[20221208 14:54:03 @agent_ppo2.py:137][0m Total time:      17.48 min
[32m[20221208 14:54:03 @agent_ppo2.py:139][0m 1413120 total steps have happened
[32m[20221208 14:54:03 @agent_ppo2.py:115][0m #------------------------ Iteration 690 --------------------------#
[32m[20221208 14:54:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:03 @agent_ppo2.py:179][0m |           0.1112 |           4.1878 |        -802.8852 |
[32m[20221208 14:54:03 @agent_ppo2.py:179][0m |           0.0913 |           3.6559 |        -585.3996 |
[32m[20221208 14:54:03 @agent_ppo2.py:179][0m |           0.0192 |           3.3165 |        -678.3800 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0244 |           3.1497 |        -735.9685 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0461 |           3.0128 |        -782.1998 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0472 |           2.9218 |        -789.7696 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0621 |           2.8057 |        -802.0185 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0709 |           2.7197 |        -831.6680 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0721 |           2.6340 |        -839.2185 |
[32m[20221208 14:54:04 @agent_ppo2.py:179][0m |          -0.0761 |           2.5514 |        -847.1080 |
[32m[20221208 14:54:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.59
[32m[20221208 14:54:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.25
[32m[20221208 14:54:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.19
[32m[20221208 14:54:04 @agent_ppo2.py:137][0m Total time:      17.50 min
[32m[20221208 14:54:04 @agent_ppo2.py:139][0m 1415168 total steps have happened
[32m[20221208 14:54:04 @agent_ppo2.py:115][0m #------------------------ Iteration 691 --------------------------#
[32m[20221208 14:54:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |           0.4315 |           3.0549 |        -745.2161 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |           0.0709 |           2.5037 |        -652.4276 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |           0.0158 |           2.2075 |        -681.9910 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0008 |           2.0001 |        -729.0955 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0116 |           1.8410 |        -718.4912 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0285 |           1.7268 |        -759.0843 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0423 |           1.6150 |        -783.1232 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0471 |           1.5305 |        -772.8007 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0508 |           1.4628 |        -786.2894 |
[32m[20221208 14:54:05 @agent_ppo2.py:179][0m |          -0.0545 |           1.4347 |        -793.2615 |
[32m[20221208 14:54:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.95
[32m[20221208 14:54:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.07
[32m[20221208 14:54:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.86
[32m[20221208 14:54:06 @agent_ppo2.py:137][0m Total time:      17.53 min
[32m[20221208 14:54:06 @agent_ppo2.py:139][0m 1417216 total steps have happened
[32m[20221208 14:54:06 @agent_ppo2.py:115][0m #------------------------ Iteration 692 --------------------------#
[32m[20221208 14:54:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:06 @agent_ppo2.py:179][0m |           0.0812 |           3.5090 |        -790.3644 |
[32m[20221208 14:54:06 @agent_ppo2.py:179][0m |           0.0810 |           3.0778 |        -664.9904 |
[32m[20221208 14:54:06 @agent_ppo2.py:179][0m |           0.0586 |           2.9107 |        -608.5450 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0034 |           2.7672 |        -704.2722 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0231 |           2.6917 |        -742.9033 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0466 |           2.5998 |        -769.0208 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0582 |           2.5307 |        -810.3126 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0656 |           2.4819 |        -819.2458 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0725 |           2.4161 |        -831.8688 |
[32m[20221208 14:54:07 @agent_ppo2.py:179][0m |          -0.0706 |           2.3770 |        -835.2651 |
[32m[20221208 14:54:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.68
[32m[20221208 14:54:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.08
[32m[20221208 14:54:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.61
[32m[20221208 14:54:07 @agent_ppo2.py:137][0m Total time:      17.55 min
[32m[20221208 14:54:07 @agent_ppo2.py:139][0m 1419264 total steps have happened
[32m[20221208 14:54:07 @agent_ppo2.py:115][0m #------------------------ Iteration 693 --------------------------#
[32m[20221208 14:54:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |           0.1168 |           4.0149 |        -645.3050 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |           0.0573 |           3.3412 |        -480.9558 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |           0.0033 |           3.1223 |        -581.2101 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0244 |           2.9518 |        -614.9363 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0397 |           2.8486 |        -636.4303 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0512 |           2.7464 |        -658.5828 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0567 |           2.6654 |        -666.1669 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0649 |           2.6082 |        -678.3906 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0678 |           2.5406 |        -689.6859 |
[32m[20221208 14:54:08 @agent_ppo2.py:179][0m |          -0.0649 |           2.4924 |        -684.2858 |
[32m[20221208 14:54:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.95
[32m[20221208 14:54:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.86
[32m[20221208 14:54:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.55
[32m[20221208 14:54:09 @agent_ppo2.py:137][0m Total time:      17.58 min
[32m[20221208 14:54:09 @agent_ppo2.py:139][0m 1421312 total steps have happened
[32m[20221208 14:54:09 @agent_ppo2.py:115][0m #------------------------ Iteration 694 --------------------------#
[32m[20221208 14:54:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:09 @agent_ppo2.py:179][0m |           0.0715 |           5.5960 |        -772.6350 |
[32m[20221208 14:54:09 @agent_ppo2.py:179][0m |           0.0427 |           4.3983 |        -649.7149 |
[32m[20221208 14:54:09 @agent_ppo2.py:179][0m |          -0.0090 |           3.9855 |        -677.7128 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0344 |           3.6871 |        -685.9927 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0500 |           3.5382 |        -691.7484 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0627 |           3.3626 |        -711.6245 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0702 |           3.2660 |        -724.6474 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0748 |           3.1825 |        -745.0845 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0837 |           3.1215 |        -761.1254 |
[32m[20221208 14:54:10 @agent_ppo2.py:179][0m |          -0.0847 |           3.0564 |        -758.8981 |
[32m[20221208 14:54:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.58
[32m[20221208 14:54:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.11
[32m[20221208 14:54:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.21
[32m[20221208 14:54:10 @agent_ppo2.py:137][0m Total time:      17.60 min
[32m[20221208 14:54:10 @agent_ppo2.py:139][0m 1423360 total steps have happened
[32m[20221208 14:54:10 @agent_ppo2.py:115][0m #------------------------ Iteration 695 --------------------------#
[32m[20221208 14:54:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |           0.0879 |           4.1543 |        -887.4048 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |           0.0354 |           3.5471 |        -811.0534 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0119 |           3.3240 |        -855.8256 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0302 |           3.1845 |        -840.4289 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0513 |           3.0333 |        -866.6870 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0629 |           2.9591 |        -877.9863 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0681 |           2.9080 |        -889.5655 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0725 |           2.8364 |        -900.5463 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0763 |           2.7862 |        -905.7362 |
[32m[20221208 14:54:11 @agent_ppo2.py:179][0m |          -0.0801 |           2.7180 |        -906.9771 |
[32m[20221208 14:54:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:54:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.21
[32m[20221208 14:54:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.43
[32m[20221208 14:54:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.18
[32m[20221208 14:54:12 @agent_ppo2.py:137][0m Total time:      17.63 min
[32m[20221208 14:54:12 @agent_ppo2.py:139][0m 1425408 total steps have happened
[32m[20221208 14:54:12 @agent_ppo2.py:115][0m #------------------------ Iteration 696 --------------------------#
[32m[20221208 14:54:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:12 @agent_ppo2.py:179][0m |           0.1274 |           5.1539 |        -799.3830 |
[32m[20221208 14:54:12 @agent_ppo2.py:179][0m |           0.1047 |           4.4821 |        -611.6262 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |           0.0118 |           4.1537 |        -753.5385 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0223 |           3.9615 |        -780.9249 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0425 |           3.8071 |        -810.2584 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0551 |           3.7485 |        -819.4847 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0629 |           3.6265 |        -839.0738 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0694 |           3.5487 |        -855.7144 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0797 |           3.4869 |        -876.8530 |
[32m[20221208 14:54:13 @agent_ppo2.py:179][0m |          -0.0801 |           3.4533 |        -883.5327 |
[32m[20221208 14:54:13 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.54
[32m[20221208 14:54:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.95
[32m[20221208 14:54:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.21
[32m[20221208 14:54:13 @agent_ppo2.py:137][0m Total time:      17.65 min
[32m[20221208 14:54:13 @agent_ppo2.py:139][0m 1427456 total steps have happened
[32m[20221208 14:54:13 @agent_ppo2.py:115][0m #------------------------ Iteration 697 --------------------------#
[32m[20221208 14:54:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |           0.0321 |           4.0216 |        -649.9007 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |           0.0044 |           3.6325 |        -591.8511 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0273 |           3.4534 |        -642.3084 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0486 |           3.3538 |        -661.0750 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0531 |           3.2690 |        -677.6398 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0638 |           3.2630 |        -684.4826 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0665 |           3.1407 |        -687.1716 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0677 |           3.1540 |        -696.4796 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0639 |           3.1066 |        -697.7857 |
[32m[20221208 14:54:14 @agent_ppo2.py:179][0m |          -0.0707 |           3.0829 |        -708.2346 |
[32m[20221208 14:54:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.30
[32m[20221208 14:54:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.76
[32m[20221208 14:54:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.88
[32m[20221208 14:54:15 @agent_ppo2.py:137][0m Total time:      17.68 min
[32m[20221208 14:54:15 @agent_ppo2.py:139][0m 1429504 total steps have happened
[32m[20221208 14:54:15 @agent_ppo2.py:115][0m #------------------------ Iteration 698 --------------------------#
[32m[20221208 14:54:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:15 @agent_ppo2.py:179][0m |           0.1133 |           5.6019 |        -779.3472 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |           0.0859 |           4.7094 |        -637.8318 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |           0.0327 |           4.2833 |        -722.4859 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0136 |           4.0216 |        -818.5783 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0320 |           3.8629 |        -868.3115 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0442 |           3.7164 |        -877.3241 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0497 |           3.6211 |        -900.9548 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0534 |           3.5041 |        -926.1314 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0620 |           3.4276 |        -932.1465 |
[32m[20221208 14:54:16 @agent_ppo2.py:179][0m |          -0.0669 |           3.3645 |        -970.0377 |
[32m[20221208 14:54:16 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.74
[32m[20221208 14:54:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.56
[32m[20221208 14:54:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.44
[32m[20221208 14:54:16 @agent_ppo2.py:137][0m Total time:      17.70 min
[32m[20221208 14:54:16 @agent_ppo2.py:139][0m 1431552 total steps have happened
[32m[20221208 14:54:16 @agent_ppo2.py:115][0m #------------------------ Iteration 699 --------------------------#
[32m[20221208 14:54:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |           0.0799 |           5.6814 |        -860.5878 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |           0.0377 |           4.7461 |        -812.0089 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0092 |           4.3996 |        -847.8257 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0344 |           4.2643 |        -896.9724 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0388 |           4.1286 |        -871.7217 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0563 |           4.0262 |        -928.0584 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0685 |           3.9637 |        -929.5572 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0718 |           3.8895 |        -942.2001 |
[32m[20221208 14:54:17 @agent_ppo2.py:179][0m |          -0.0765 |           3.8156 |        -960.4687 |
[32m[20221208 14:54:18 @agent_ppo2.py:179][0m |          -0.0678 |           3.7799 |        -909.4383 |
[32m[20221208 14:54:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.75
[32m[20221208 14:54:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.58
[32m[20221208 14:54:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.97
[32m[20221208 14:54:18 @agent_ppo2.py:137][0m Total time:      17.73 min
[32m[20221208 14:54:18 @agent_ppo2.py:139][0m 1433600 total steps have happened
[32m[20221208 14:54:18 @agent_ppo2.py:115][0m #------------------------ Iteration 700 --------------------------#
[32m[20221208 14:54:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:54:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |           0.1174 |           4.8992 |        -869.6021 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |           0.0961 |           4.2191 |        -709.7960 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |           0.0246 |           3.9553 |        -788.3323 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0084 |           3.7754 |        -828.8813 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0275 |           3.6886 |        -867.3314 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0360 |           3.5728 |        -892.9849 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0366 |           3.4936 |        -871.8855 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0493 |           3.4595 |        -895.3445 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0535 |           3.3712 |        -913.7918 |
[32m[20221208 14:54:19 @agent_ppo2.py:179][0m |          -0.0530 |           3.3005 |        -915.1516 |
[32m[20221208 14:54:19 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.61
[32m[20221208 14:54:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.64
[32m[20221208 14:54:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.60
[32m[20221208 14:54:19 @agent_ppo2.py:137][0m Total time:      17.75 min
[32m[20221208 14:54:19 @agent_ppo2.py:139][0m 1435648 total steps have happened
[32m[20221208 14:54:19 @agent_ppo2.py:115][0m #------------------------ Iteration 701 --------------------------#
[32m[20221208 14:54:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |           0.0743 |           3.1932 |        -907.7362 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |           0.0580 |           2.5870 |        -757.7357 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0034 |           2.2901 |        -867.8898 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0203 |           2.0906 |        -921.5490 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0351 |           1.9588 |        -935.6243 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0466 |           1.8482 |        -961.3751 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0477 |           1.7595 |        -970.1973 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0529 |           1.6868 |        -985.1548 |
[32m[20221208 14:54:20 @agent_ppo2.py:179][0m |          -0.0586 |           1.6313 |       -1013.1035 |
[32m[20221208 14:54:21 @agent_ppo2.py:179][0m |          -0.0529 |           1.5625 |       -1007.7390 |
[32m[20221208 14:54:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.44
[32m[20221208 14:54:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.64
[32m[20221208 14:54:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.19
[32m[20221208 14:54:21 @agent_ppo2.py:137][0m Total time:      17.78 min
[32m[20221208 14:54:21 @agent_ppo2.py:139][0m 1437696 total steps have happened
[32m[20221208 14:54:21 @agent_ppo2.py:115][0m #------------------------ Iteration 702 --------------------------#
[32m[20221208 14:54:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |           0.1025 |           2.4786 |        -762.9123 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |           0.0772 |           1.9690 |        -507.0272 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |           0.0427 |           1.7925 |        -451.3676 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0028 |           1.7172 |        -503.2375 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0187 |           1.6497 |        -530.0030 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0276 |           1.6111 |        -553.1598 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0382 |           1.5689 |        -563.9645 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0408 |           1.5325 |        -575.8947 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0448 |           1.4944 |        -586.8487 |
[32m[20221208 14:54:22 @agent_ppo2.py:179][0m |          -0.0469 |           1.4693 |        -603.8806 |
[32m[20221208 14:54:22 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.74
[32m[20221208 14:54:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.53
[32m[20221208 14:54:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.96
[32m[20221208 14:54:22 @agent_ppo2.py:137][0m Total time:      17.80 min
[32m[20221208 14:54:22 @agent_ppo2.py:139][0m 1439744 total steps have happened
[32m[20221208 14:54:22 @agent_ppo2.py:115][0m #------------------------ Iteration 703 --------------------------#
[32m[20221208 14:54:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |           0.0704 |           2.2047 |        -777.7464 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |           0.0273 |           1.6281 |        -331.4393 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |          -0.0099 |           1.4087 |        -339.2505 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |          -0.0251 |           1.2848 |        -348.8463 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |          -0.0328 |           1.1943 |        -370.4215 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |          -0.0418 |           1.1321 |        -375.7398 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |          -0.0403 |           1.0706 |        -382.8699 |
[32m[20221208 14:54:23 @agent_ppo2.py:179][0m |          -0.0462 |           1.0341 |        -403.4105 |
[32m[20221208 14:54:24 @agent_ppo2.py:179][0m |          -0.0477 |           1.0042 |        -412.5282 |
[32m[20221208 14:54:24 @agent_ppo2.py:179][0m |          -0.0506 |           0.9643 |        -414.3017 |
[32m[20221208 14:54:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.32
[32m[20221208 14:54:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.20
[32m[20221208 14:54:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.23
[32m[20221208 14:54:24 @agent_ppo2.py:137][0m Total time:      17.83 min
[32m[20221208 14:54:24 @agent_ppo2.py:139][0m 1441792 total steps have happened
[32m[20221208 14:54:24 @agent_ppo2.py:115][0m #------------------------ Iteration 704 --------------------------#
[32m[20221208 14:54:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |           0.0713 |           3.1451 |        -967.7072 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |           0.0700 |           2.2562 |        -557.9021 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |           0.0346 |           2.0368 |        -616.6254 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0132 |           1.8919 |        -645.9147 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0407 |           1.8053 |        -682.0934 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0460 |           1.7302 |        -695.5404 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0576 |           1.6449 |        -710.4948 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0627 |           1.5895 |        -736.1356 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0666 |           1.5293 |        -727.4935 |
[32m[20221208 14:54:25 @agent_ppo2.py:179][0m |          -0.0734 |           1.4864 |        -748.1037 |
[32m[20221208 14:54:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.57
[32m[20221208 14:54:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.14
[32m[20221208 14:54:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.77
[32m[20221208 14:54:25 @agent_ppo2.py:137][0m Total time:      17.85 min
[32m[20221208 14:54:25 @agent_ppo2.py:139][0m 1443840 total steps have happened
[32m[20221208 14:54:25 @agent_ppo2.py:115][0m #------------------------ Iteration 705 --------------------------#
[32m[20221208 14:54:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |           0.0917 |           3.1000 |        -718.1693 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |           0.0778 |           2.4159 |        -493.0014 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |          -0.0011 |           2.1517 |        -519.1019 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |          -0.0379 |           2.0181 |        -581.3649 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |          -0.0544 |           1.8863 |        -592.7225 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |          -0.0631 |           1.8332 |        -612.5005 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |          -0.0670 |           1.7685 |        -625.6543 |
[32m[20221208 14:54:26 @agent_ppo2.py:179][0m |          -0.0750 |           1.7212 |        -630.2878 |
[32m[20221208 14:54:27 @agent_ppo2.py:179][0m |          -0.0795 |           1.6691 |        -636.7917 |
[32m[20221208 14:54:27 @agent_ppo2.py:179][0m |          -0.0807 |           1.6352 |        -643.6007 |
[32m[20221208 14:54:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.30
[32m[20221208 14:54:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.84
[32m[20221208 14:54:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.61
[32m[20221208 14:54:27 @agent_ppo2.py:137][0m Total time:      17.88 min
[32m[20221208 14:54:27 @agent_ppo2.py:139][0m 1445888 total steps have happened
[32m[20221208 14:54:27 @agent_ppo2.py:115][0m #------------------------ Iteration 706 --------------------------#
[32m[20221208 14:54:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |           0.1491 |           4.4265 |        -968.0530 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |           0.0563 |           3.7831 |        -794.0779 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |           0.0186 |           3.4588 |        -827.5509 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0196 |           3.2705 |        -913.6709 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0305 |           3.1612 |        -957.2784 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0390 |           3.0541 |        -982.8208 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0484 |           2.9756 |       -1010.1751 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0492 |           2.8910 |       -1012.9725 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0615 |           2.7950 |       -1024.8570 |
[32m[20221208 14:54:28 @agent_ppo2.py:179][0m |          -0.0624 |           2.7551 |       -1050.8530 |
[32m[20221208 14:54:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:54:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.56
[32m[20221208 14:54:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.05
[32m[20221208 14:54:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.78
[32m[20221208 14:54:29 @agent_ppo2.py:137][0m Total time:      17.90 min
[32m[20221208 14:54:29 @agent_ppo2.py:139][0m 1447936 total steps have happened
[32m[20221208 14:54:29 @agent_ppo2.py:115][0m #------------------------ Iteration 707 --------------------------#
[32m[20221208 14:54:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |           0.0970 |           5.1051 |       -1018.6089 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |           0.0933 |           4.2796 |        -842.7708 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |           0.0445 |           4.0132 |        -822.7968 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |          -0.0087 |           3.7248 |        -960.7979 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |          -0.0334 |           3.5761 |       -1014.3307 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |          -0.0484 |           3.4814 |       -1045.5690 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |          -0.0509 |           3.3676 |       -1044.0964 |
[32m[20221208 14:54:29 @agent_ppo2.py:179][0m |          -0.0602 |           3.3104 |       -1068.5574 |
[32m[20221208 14:54:30 @agent_ppo2.py:179][0m |          -0.0637 |           3.2161 |       -1060.1114 |
[32m[20221208 14:54:30 @agent_ppo2.py:179][0m |          -0.0674 |           3.1674 |       -1106.4490 |
[32m[20221208 14:54:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.91
[32m[20221208 14:54:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.50
[32m[20221208 14:54:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.52
[32m[20221208 14:54:30 @agent_ppo2.py:137][0m Total time:      17.93 min
[32m[20221208 14:54:30 @agent_ppo2.py:139][0m 1449984 total steps have happened
[32m[20221208 14:54:30 @agent_ppo2.py:115][0m #------------------------ Iteration 708 --------------------------#
[32m[20221208 14:54:30 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:54:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |           0.1311 |           3.3393 |        -784.1483 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |           0.1240 |           3.0277 |        -359.4519 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |           0.0658 |           2.9093 |        -463.6204 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |           0.0552 |           2.8075 |        -553.6640 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |           0.0276 |           2.7296 |        -595.7008 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |           0.0012 |           2.6845 |        -784.9882 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |          -0.0078 |           2.6338 |        -961.1868 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |          -0.0360 |           2.5910 |       -1055.8631 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |          -0.0384 |           2.5544 |       -1078.3052 |
[32m[20221208 14:54:31 @agent_ppo2.py:179][0m |          -0.0082 |           2.5224 |        -870.4920 |
[32m[20221208 14:54:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:54:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.25
[32m[20221208 14:54:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.44
[32m[20221208 14:54:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.30
[32m[20221208 14:54:32 @agent_ppo2.py:137][0m Total time:      17.95 min
[32m[20221208 14:54:32 @agent_ppo2.py:139][0m 1452032 total steps have happened
[32m[20221208 14:54:32 @agent_ppo2.py:115][0m #------------------------ Iteration 709 --------------------------#
[32m[20221208 14:54:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.0898 |           2.0984 |        -863.9077 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.1782 |           1.5301 |        -352.7204 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.0533 |           1.3768 |        -238.3484 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.0303 |           1.2900 |        -300.8974 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.0217 |           1.2592 |        -333.7060 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.0090 |           1.2133 |        -331.3696 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |           0.0004 |           1.2209 |        -355.7567 |
[32m[20221208 14:54:32 @agent_ppo2.py:179][0m |          -0.0022 |           1.1661 |        -383.8815 |
[32m[20221208 14:54:33 @agent_ppo2.py:179][0m |          -0.0063 |           1.1347 |        -371.7441 |
[32m[20221208 14:54:33 @agent_ppo2.py:179][0m |          -0.0103 |           1.1314 |        -397.9844 |
[32m[20221208 14:54:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:54:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.19
[32m[20221208 14:54:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.63
[32m[20221208 14:54:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.34
[32m[20221208 14:54:33 @agent_ppo2.py:137][0m Total time:      17.98 min
[32m[20221208 14:54:33 @agent_ppo2.py:139][0m 1454080 total steps have happened
[32m[20221208 14:54:33 @agent_ppo2.py:115][0m #------------------------ Iteration 710 --------------------------#
[32m[20221208 14:54:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |           0.1061 |           4.7906 |       -1016.9267 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |           0.0703 |           4.0125 |        -834.6861 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |           0.0347 |           3.7046 |        -653.8783 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0046 |           3.4465 |        -693.1641 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0310 |           3.2609 |        -746.5442 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0376 |           3.1508 |        -772.9940 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0472 |           3.0199 |        -789.8301 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0580 |           2.8928 |        -838.1114 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0608 |           2.7990 |        -855.8656 |
[32m[20221208 14:54:34 @agent_ppo2.py:179][0m |          -0.0651 |           2.7108 |        -829.8712 |
[32m[20221208 14:54:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.44
[32m[20221208 14:54:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.53
[32m[20221208 14:54:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.91
[32m[20221208 14:54:35 @agent_ppo2.py:137][0m Total time:      18.00 min
[32m[20221208 14:54:35 @agent_ppo2.py:139][0m 1456128 total steps have happened
[32m[20221208 14:54:35 @agent_ppo2.py:115][0m #------------------------ Iteration 711 --------------------------#
[32m[20221208 14:54:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |           0.1293 |           4.7200 |        -978.6328 |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |           0.0721 |           3.8020 |        -917.8442 |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |           0.0451 |           3.5129 |        -854.7921 |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |          -0.0056 |           3.3030 |        -971.5232 |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |           0.0001 |           3.1745 |        -996.3953 |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |          -0.0128 |           3.0444 |       -1009.0127 |
[32m[20221208 14:54:35 @agent_ppo2.py:179][0m |          -0.0359 |           2.9720 |       -1051.6348 |
[32m[20221208 14:54:36 @agent_ppo2.py:179][0m |          -0.0402 |           2.8819 |       -1079.8391 |
[32m[20221208 14:54:36 @agent_ppo2.py:179][0m |          -0.0473 |           2.9212 |       -1104.6243 |
[32m[20221208 14:54:36 @agent_ppo2.py:179][0m |          -0.0505 |           2.7885 |       -1140.9529 |
[32m[20221208 14:54:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.84
[32m[20221208 14:54:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.03
[32m[20221208 14:54:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.25
[32m[20221208 14:54:36 @agent_ppo2.py:137][0m Total time:      18.03 min
[32m[20221208 14:54:36 @agent_ppo2.py:139][0m 1458176 total steps have happened
[32m[20221208 14:54:36 @agent_ppo2.py:115][0m #------------------------ Iteration 712 --------------------------#
[32m[20221208 14:54:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |           0.0727 |           4.7183 |        -925.0980 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |           0.0299 |           4.2329 |        -892.8753 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0008 |           4.0488 |        -913.5884 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0245 |           3.9033 |        -987.0331 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0356 |           3.8185 |        -992.6292 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0439 |           3.7915 |       -1020.9939 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0577 |           3.7091 |       -1048.9915 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0662 |           3.6708 |       -1085.8739 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0712 |           3.6169 |       -1095.9697 |
[32m[20221208 14:54:37 @agent_ppo2.py:179][0m |          -0.0735 |           3.5689 |       -1111.8877 |
[32m[20221208 14:54:37 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 109.46
[32m[20221208 14:54:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.59
[32m[20221208 14:54:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.97
[32m[20221208 14:54:38 @agent_ppo2.py:137][0m Total time:      18.05 min
[32m[20221208 14:54:38 @agent_ppo2.py:139][0m 1460224 total steps have happened
[32m[20221208 14:54:38 @agent_ppo2.py:115][0m #------------------------ Iteration 713 --------------------------#
[32m[20221208 14:54:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |           0.0591 |           3.0493 |       -1073.6683 |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |           0.0653 |           2.2765 |        -756.5363 |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |           0.0183 |           1.9744 |        -538.9238 |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |          -0.0071 |           1.7568 |        -607.8096 |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |          -0.0192 |           1.6530 |        -644.9550 |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |          -0.0325 |           1.5850 |        -684.6266 |
[32m[20221208 14:54:38 @agent_ppo2.py:179][0m |          -0.0373 |           1.5190 |        -713.1631 |
[32m[20221208 14:54:39 @agent_ppo2.py:179][0m |          -0.0391 |           1.4548 |        -717.2792 |
[32m[20221208 14:54:39 @agent_ppo2.py:179][0m |          -0.0442 |           1.4334 |        -762.5536 |
[32m[20221208 14:54:39 @agent_ppo2.py:179][0m |          -0.0470 |           1.4149 |        -798.0174 |
[32m[20221208 14:54:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.01
[32m[20221208 14:54:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.70
[32m[20221208 14:54:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.35
[32m[20221208 14:54:39 @agent_ppo2.py:137][0m Total time:      18.08 min
[32m[20221208 14:54:39 @agent_ppo2.py:139][0m 1462272 total steps have happened
[32m[20221208 14:54:39 @agent_ppo2.py:115][0m #------------------------ Iteration 714 --------------------------#
[32m[20221208 14:54:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |           0.1080 |           4.7830 |       -1114.5786 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |           0.0858 |           3.8982 |        -977.1575 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |           0.0246 |           3.5714 |        -955.9472 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0058 |           3.4045 |       -1076.4037 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0256 |           3.3164 |       -1097.7069 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0275 |           3.1963 |       -1111.8420 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0427 |           3.0976 |       -1157.0236 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0485 |           3.0032 |       -1168.3087 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0546 |           2.9745 |       -1201.6990 |
[32m[20221208 14:54:40 @agent_ppo2.py:179][0m |          -0.0573 |           2.9414 |       -1231.2139 |
[32m[20221208 14:54:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.05
[32m[20221208 14:54:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.21
[32m[20221208 14:54:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.68
[32m[20221208 14:54:41 @agent_ppo2.py:137][0m Total time:      18.11 min
[32m[20221208 14:54:41 @agent_ppo2.py:139][0m 1464320 total steps have happened
[32m[20221208 14:54:41 @agent_ppo2.py:115][0m #------------------------ Iteration 715 --------------------------#
[32m[20221208 14:54:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:41 @agent_ppo2.py:179][0m |           0.0654 |           5.7916 |       -1119.9685 |
[32m[20221208 14:54:41 @agent_ppo2.py:179][0m |           0.0408 |           4.9673 |        -958.5836 |
[32m[20221208 14:54:41 @agent_ppo2.py:179][0m |          -0.0086 |           4.6331 |       -1085.5359 |
[32m[20221208 14:54:41 @agent_ppo2.py:179][0m |          -0.0339 |           4.4682 |       -1171.9694 |
[32m[20221208 14:54:41 @agent_ppo2.py:179][0m |          -0.0465 |           4.3485 |       -1199.3732 |
[32m[20221208 14:54:41 @agent_ppo2.py:179][0m |          -0.0493 |           4.2294 |       -1230.6711 |
[32m[20221208 14:54:42 @agent_ppo2.py:179][0m |          -0.0597 |           4.1688 |       -1229.4699 |
[32m[20221208 14:54:42 @agent_ppo2.py:179][0m |          -0.0653 |           4.0957 |       -1261.2244 |
[32m[20221208 14:54:42 @agent_ppo2.py:179][0m |          -0.0669 |           4.0523 |       -1282.8699 |
[32m[20221208 14:54:42 @agent_ppo2.py:179][0m |          -0.0723 |           3.9834 |       -1280.9811 |
[32m[20221208 14:54:42 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.83
[32m[20221208 14:54:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.66
[32m[20221208 14:54:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.75
[32m[20221208 14:54:42 @agent_ppo2.py:137][0m Total time:      18.13 min
[32m[20221208 14:54:42 @agent_ppo2.py:139][0m 1466368 total steps have happened
[32m[20221208 14:54:42 @agent_ppo2.py:115][0m #------------------------ Iteration 716 --------------------------#
[32m[20221208 14:54:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |           0.0812 |           5.4685 |       -1047.5680 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |           0.0374 |           4.4867 |        -826.6584 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0138 |           4.1212 |        -918.9184 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0391 |           3.8941 |        -981.0193 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0483 |           3.7602 |        -998.1635 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0565 |           3.6393 |       -1036.2264 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0624 |           3.5183 |       -1039.2273 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0676 |           3.4838 |       -1068.7027 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0711 |           3.3820 |       -1076.2554 |
[32m[20221208 14:54:43 @agent_ppo2.py:179][0m |          -0.0738 |           3.3052 |       -1080.4029 |
[32m[20221208 14:54:43 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.33
[32m[20221208 14:54:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.92
[32m[20221208 14:54:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.79
[32m[20221208 14:54:44 @agent_ppo2.py:137][0m Total time:      18.16 min
[32m[20221208 14:54:44 @agent_ppo2.py:139][0m 1468416 total steps have happened
[32m[20221208 14:54:44 @agent_ppo2.py:115][0m #------------------------ Iteration 717 --------------------------#
[32m[20221208 14:54:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:44 @agent_ppo2.py:179][0m |           0.0892 |           4.7008 |       -1001.3460 |
[32m[20221208 14:54:44 @agent_ppo2.py:179][0m |           0.1852 |           4.2263 |        -421.2315 |
[32m[20221208 14:54:44 @agent_ppo2.py:179][0m |           0.0680 |           3.9771 |        -429.5129 |
[32m[20221208 14:54:44 @agent_ppo2.py:179][0m |           0.0146 |           3.7784 |        -581.4834 |
[32m[20221208 14:54:44 @agent_ppo2.py:179][0m |          -0.0029 |           3.6601 |        -662.2426 |
[32m[20221208 14:54:45 @agent_ppo2.py:179][0m |          -0.0203 |           3.5500 |        -698.7850 |
[32m[20221208 14:54:45 @agent_ppo2.py:179][0m |          -0.0326 |           3.4429 |        -748.2284 |
[32m[20221208 14:54:45 @agent_ppo2.py:179][0m |          -0.0405 |           3.3610 |        -781.2945 |
[32m[20221208 14:54:45 @agent_ppo2.py:179][0m |          -0.0376 |           3.3297 |        -784.7974 |
[32m[20221208 14:54:45 @agent_ppo2.py:179][0m |          -0.0477 |           3.2263 |        -823.2949 |
[32m[20221208 14:54:45 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:54:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.39
[32m[20221208 14:54:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.70
[32m[20221208 14:54:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.73
[32m[20221208 14:54:45 @agent_ppo2.py:137][0m Total time:      18.18 min
[32m[20221208 14:54:45 @agent_ppo2.py:139][0m 1470464 total steps have happened
[32m[20221208 14:54:45 @agent_ppo2.py:115][0m #------------------------ Iteration 718 --------------------------#
[32m[20221208 14:54:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |           0.1067 |           3.6311 |        -873.0687 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |           0.0529 |           3.0549 |        -491.8224 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |           0.0184 |           2.7635 |        -540.1915 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0041 |           2.6374 |        -583.2070 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0198 |           2.4928 |        -620.3135 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0265 |           2.4545 |        -649.1928 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0396 |           2.3404 |        -694.7548 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0448 |           2.3071 |        -695.6174 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0538 |           2.2064 |        -717.9168 |
[32m[20221208 14:54:46 @agent_ppo2.py:179][0m |          -0.0545 |           2.1994 |        -736.2788 |
[32m[20221208 14:54:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.85
[32m[20221208 14:54:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.22
[32m[20221208 14:54:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.03
[32m[20221208 14:54:47 @agent_ppo2.py:137][0m Total time:      18.21 min
[32m[20221208 14:54:47 @agent_ppo2.py:139][0m 1472512 total steps have happened
[32m[20221208 14:54:47 @agent_ppo2.py:115][0m #------------------------ Iteration 719 --------------------------#
[32m[20221208 14:54:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:47 @agent_ppo2.py:179][0m |           0.0910 |           5.5741 |        -874.2974 |
[32m[20221208 14:54:47 @agent_ppo2.py:179][0m |           0.0505 |           4.5615 |        -732.9528 |
[32m[20221208 14:54:47 @agent_ppo2.py:179][0m |           0.0217 |           4.1256 |        -723.2928 |
[32m[20221208 14:54:47 @agent_ppo2.py:179][0m |          -0.0222 |           3.9462 |        -892.7508 |
[32m[20221208 14:54:47 @agent_ppo2.py:179][0m |          -0.0399 |           3.7460 |        -948.0712 |
[32m[20221208 14:54:48 @agent_ppo2.py:179][0m |          -0.0527 |           3.6590 |        -994.4062 |
[32m[20221208 14:54:48 @agent_ppo2.py:179][0m |          -0.0606 |           3.5825 |       -1028.8662 |
[32m[20221208 14:54:48 @agent_ppo2.py:179][0m |          -0.0663 |           3.4811 |       -1036.9318 |
[32m[20221208 14:54:48 @agent_ppo2.py:179][0m |          -0.0664 |           3.4400 |       -1060.1516 |
[32m[20221208 14:54:48 @agent_ppo2.py:179][0m |          -0.0711 |           3.3802 |       -1064.9018 |
[32m[20221208 14:54:48 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.98
[32m[20221208 14:54:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.70
[32m[20221208 14:54:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.27
[32m[20221208 14:54:48 @agent_ppo2.py:137][0m Total time:      18.23 min
[32m[20221208 14:54:48 @agent_ppo2.py:139][0m 1474560 total steps have happened
[32m[20221208 14:54:48 @agent_ppo2.py:115][0m #------------------------ Iteration 720 --------------------------#
[32m[20221208 14:54:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |           0.1528 |           3.6486 |       -1015.1394 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |           0.2139 |           3.2568 |        -690.7386 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |           0.0629 |           3.1248 |        -848.3477 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |           0.0409 |           3.0061 |        -832.9008 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |           0.0070 |           2.9353 |        -915.4222 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |          -0.0090 |           2.8627 |        -988.8684 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |          -0.0263 |           2.7914 |       -1062.8128 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |          -0.0469 |           2.7522 |       -1148.6680 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |          -0.0429 |           2.7079 |       -1164.4337 |
[32m[20221208 14:54:49 @agent_ppo2.py:179][0m |          -0.0294 |           2.6694 |       -1015.6061 |
[32m[20221208 14:54:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.83
[32m[20221208 14:54:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.82
[32m[20221208 14:54:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.87
[32m[20221208 14:54:50 @agent_ppo2.py:137][0m Total time:      18.26 min
[32m[20221208 14:54:50 @agent_ppo2.py:139][0m 1476608 total steps have happened
[32m[20221208 14:54:50 @agent_ppo2.py:115][0m #------------------------ Iteration 721 --------------------------#
[32m[20221208 14:54:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:50 @agent_ppo2.py:179][0m |           0.0919 |           4.5017 |       -1168.6867 |
[32m[20221208 14:54:50 @agent_ppo2.py:179][0m |           0.0814 |           3.9966 |        -935.5730 |
[32m[20221208 14:54:50 @agent_ppo2.py:179][0m |           0.0401 |           3.7451 |       -1067.7761 |
[32m[20221208 14:54:50 @agent_ppo2.py:179][0m |          -0.0015 |           3.6259 |       -1056.2984 |
[32m[20221208 14:54:51 @agent_ppo2.py:179][0m |          -0.0192 |           3.5111 |       -1077.6085 |
[32m[20221208 14:54:51 @agent_ppo2.py:179][0m |          -0.0291 |           3.4618 |        -974.6665 |
[32m[20221208 14:54:51 @agent_ppo2.py:179][0m |          -0.0420 |           3.3993 |        -942.5449 |
[32m[20221208 14:54:51 @agent_ppo2.py:179][0m |          -0.0533 |           3.3483 |        -950.1872 |
[32m[20221208 14:54:51 @agent_ppo2.py:179][0m |          -0.0558 |           3.3244 |        -958.0037 |
[32m[20221208 14:54:51 @agent_ppo2.py:179][0m |          -0.0617 |           3.2818 |        -959.9308 |
[32m[20221208 14:54:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 109.16
[32m[20221208 14:54:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.63
[32m[20221208 14:54:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.39
[32m[20221208 14:54:51 @agent_ppo2.py:137][0m Total time:      18.28 min
[32m[20221208 14:54:51 @agent_ppo2.py:139][0m 1478656 total steps have happened
[32m[20221208 14:54:51 @agent_ppo2.py:115][0m #------------------------ Iteration 722 --------------------------#
[32m[20221208 14:54:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |           0.0929 |           3.6898 |       -1198.7640 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |           0.0534 |           3.2131 |       -1007.3008 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |           0.0008 |           2.9837 |       -1132.9853 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0139 |           2.8416 |       -1143.0884 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0239 |           2.7448 |       -1132.8474 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0360 |           2.6946 |       -1221.5603 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0495 |           2.6112 |       -1249.4330 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0572 |           2.5692 |       -1287.8302 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0592 |           2.5399 |       -1314.6725 |
[32m[20221208 14:54:52 @agent_ppo2.py:179][0m |          -0.0594 |           2.4581 |       -1316.6243 |
[32m[20221208 14:54:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:54:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.41
[32m[20221208 14:54:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.88
[32m[20221208 14:54:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.24
[32m[20221208 14:54:53 @agent_ppo2.py:137][0m Total time:      18.31 min
[32m[20221208 14:54:53 @agent_ppo2.py:139][0m 1480704 total steps have happened
[32m[20221208 14:54:53 @agent_ppo2.py:115][0m #------------------------ Iteration 723 --------------------------#
[32m[20221208 14:54:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:53 @agent_ppo2.py:179][0m |           0.0778 |           3.8233 |       -1129.3432 |
[32m[20221208 14:54:53 @agent_ppo2.py:179][0m |           0.0809 |           3.4145 |        -757.2440 |
[32m[20221208 14:54:53 @agent_ppo2.py:179][0m |           0.0083 |           3.2134 |        -804.8780 |
[32m[20221208 14:54:53 @agent_ppo2.py:179][0m |          -0.0131 |           3.0732 |        -861.9399 |
[32m[20221208 14:54:54 @agent_ppo2.py:179][0m |          -0.0351 |           3.0026 |        -905.7494 |
[32m[20221208 14:54:54 @agent_ppo2.py:179][0m |          -0.0407 |           2.9441 |        -920.6823 |
[32m[20221208 14:54:54 @agent_ppo2.py:179][0m |          -0.0452 |           2.8709 |        -943.0094 |
[32m[20221208 14:54:54 @agent_ppo2.py:179][0m |          -0.0516 |           2.8396 |        -968.7093 |
[32m[20221208 14:54:54 @agent_ppo2.py:179][0m |          -0.0444 |           2.7993 |        -932.4887 |
[32m[20221208 14:54:54 @agent_ppo2.py:179][0m |          -0.0562 |           2.7825 |        -983.4824 |
[32m[20221208 14:54:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.11
[32m[20221208 14:54:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.96
[32m[20221208 14:54:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.70
[32m[20221208 14:54:54 @agent_ppo2.py:137][0m Total time:      18.33 min
[32m[20221208 14:54:54 @agent_ppo2.py:139][0m 1482752 total steps have happened
[32m[20221208 14:54:54 @agent_ppo2.py:115][0m #------------------------ Iteration 724 --------------------------#
[32m[20221208 14:54:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |           0.0806 |           4.0540 |       -1146.7025 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |           0.1122 |           3.6271 |        -659.9103 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |           0.0117 |           3.4500 |        -757.6756 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0117 |           3.3685 |        -820.7211 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0199 |           3.2804 |        -848.2756 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0386 |           3.2130 |        -898.0214 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0442 |           3.1792 |        -927.4800 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0537 |           3.1309 |        -956.4424 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0537 |           3.1302 |        -978.2142 |
[32m[20221208 14:54:55 @agent_ppo2.py:179][0m |          -0.0588 |           3.1117 |        -985.4169 |
[32m[20221208 14:54:55 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:54:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.94
[32m[20221208 14:54:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.69
[32m[20221208 14:54:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.84
[32m[20221208 14:54:56 @agent_ppo2.py:137][0m Total time:      18.36 min
[32m[20221208 14:54:56 @agent_ppo2.py:139][0m 1484800 total steps have happened
[32m[20221208 14:54:56 @agent_ppo2.py:115][0m #------------------------ Iteration 725 --------------------------#
[32m[20221208 14:54:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:54:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:56 @agent_ppo2.py:179][0m |           0.1097 |           4.4155 |       -1328.9534 |
[32m[20221208 14:54:56 @agent_ppo2.py:179][0m |           0.0905 |           3.9401 |       -1024.9805 |
[32m[20221208 14:54:56 @agent_ppo2.py:179][0m |           0.0478 |           3.7063 |       -1023.8696 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |           0.0075 |           3.5501 |       -1149.8808 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |          -0.0076 |           3.4305 |       -1191.7496 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |          -0.0270 |           3.3219 |       -1256.9679 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |          -0.0328 |           3.2503 |       -1262.3741 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |          -0.0442 |           3.1751 |       -1306.8434 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |          -0.0478 |           3.1108 |       -1327.1663 |
[32m[20221208 14:54:57 @agent_ppo2.py:179][0m |          -0.0560 |           3.0481 |       -1356.6295 |
[32m[20221208 14:54:57 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:54:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.66
[32m[20221208 14:54:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.47
[32m[20221208 14:54:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.63
[32m[20221208 14:54:57 @agent_ppo2.py:137][0m Total time:      18.38 min
[32m[20221208 14:54:57 @agent_ppo2.py:139][0m 1486848 total steps have happened
[32m[20221208 14:54:57 @agent_ppo2.py:115][0m #------------------------ Iteration 726 --------------------------#
[32m[20221208 14:54:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:54:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |           0.1331 |           6.3702 |       -1256.7586 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |           0.1361 |           5.3775 |        -894.3157 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |           0.0678 |           5.0946 |        -945.9061 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |           0.0156 |           4.8153 |       -1094.9260 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |          -0.0110 |           4.6162 |       -1206.9990 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |          -0.0263 |           4.4616 |       -1255.8626 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |          -0.0394 |           4.4169 |       -1301.8920 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |          -0.0482 |           4.2591 |       -1319.7166 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |          -0.0544 |           4.2358 |       -1338.3924 |
[32m[20221208 14:54:58 @agent_ppo2.py:179][0m |          -0.0607 |           4.1347 |       -1362.5199 |
[32m[20221208 14:54:58 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:54:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.36
[32m[20221208 14:54:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.22
[32m[20221208 14:54:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.49
[32m[20221208 14:54:59 @agent_ppo2.py:137][0m Total time:      18.41 min
[32m[20221208 14:54:59 @agent_ppo2.py:139][0m 1488896 total steps have happened
[32m[20221208 14:54:59 @agent_ppo2.py:115][0m #------------------------ Iteration 727 --------------------------#
[32m[20221208 14:54:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:54:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:54:59 @agent_ppo2.py:179][0m |           0.0583 |           3.0937 |       -1302.5722 |
[32m[20221208 14:54:59 @agent_ppo2.py:179][0m |           0.2298 |           2.6597 |        -538.6394 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |           0.0486 |           2.4829 |        -297.9501 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |           0.0177 |           2.3615 |        -387.5278 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |           0.0048 |           2.2677 |        -416.3832 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |          -0.0150 |           2.1783 |        -558.4569 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |          -0.0237 |           2.1273 |        -582.6651 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |          -0.0268 |           2.0815 |        -634.3132 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |          -0.0312 |           2.0005 |        -669.8402 |
[32m[20221208 14:55:00 @agent_ppo2.py:179][0m |          -0.0322 |           1.9377 |        -682.5285 |
[32m[20221208 14:55:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:55:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.90
[32m[20221208 14:55:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.65
[32m[20221208 14:55:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 145.70
[32m[20221208 14:55:00 @agent_ppo2.py:137][0m Total time:      18.44 min
[32m[20221208 14:55:00 @agent_ppo2.py:139][0m 1490944 total steps have happened
[32m[20221208 14:55:00 @agent_ppo2.py:115][0m #------------------------ Iteration 728 --------------------------#
[32m[20221208 14:55:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |           0.0836 |           3.4317 |        -999.9410 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |           0.0282 |           3.1155 |        -591.3110 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |           0.0086 |           2.9734 |        -576.4595 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0258 |           2.8922 |        -646.3208 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0418 |           2.8609 |        -681.3401 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0484 |           2.8008 |        -714.2521 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0530 |           2.8337 |        -716.0078 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0608 |           2.7606 |        -753.3047 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0632 |           2.7615 |        -764.5337 |
[32m[20221208 14:55:01 @agent_ppo2.py:179][0m |          -0.0698 |           2.7038 |        -783.9765 |
[32m[20221208 14:55:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.54
[32m[20221208 14:55:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.50
[32m[20221208 14:55:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.48
[32m[20221208 14:55:02 @agent_ppo2.py:137][0m Total time:      18.46 min
[32m[20221208 14:55:02 @agent_ppo2.py:139][0m 1492992 total steps have happened
[32m[20221208 14:55:02 @agent_ppo2.py:115][0m #------------------------ Iteration 729 --------------------------#
[32m[20221208 14:55:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:02 @agent_ppo2.py:179][0m |           0.1739 |           6.0049 |       -1068.3125 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |           0.0985 |           5.4300 |        -792.4623 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |           0.0718 |           5.2516 |        -931.6708 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |           0.0024 |           5.0868 |       -1058.7189 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |          -0.0265 |           4.9805 |       -1128.7214 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |          -0.0423 |           4.8813 |       -1199.1987 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |          -0.0467 |           4.8147 |       -1213.0505 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |          -0.0525 |           4.7702 |       -1245.7246 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |          -0.0622 |           4.7186 |       -1273.2399 |
[32m[20221208 14:55:03 @agent_ppo2.py:179][0m |          -0.0661 |           4.6466 |       -1297.9118 |
[32m[20221208 14:55:03 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.49
[32m[20221208 14:55:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.13
[32m[20221208 14:55:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.85
[32m[20221208 14:55:03 @agent_ppo2.py:137][0m Total time:      18.49 min
[32m[20221208 14:55:03 @agent_ppo2.py:139][0m 1495040 total steps have happened
[32m[20221208 14:55:03 @agent_ppo2.py:115][0m #------------------------ Iteration 730 --------------------------#
[32m[20221208 14:55:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |           0.0871 |           5.9880 |       -1202.2423 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |           0.2774 |           5.3921 |        -992.1535 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |           0.0157 |           5.0339 |       -1103.1609 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |          -0.0035 |           4.8594 |       -1200.4461 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |          -0.0269 |           4.6656 |       -1255.0344 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |          -0.0366 |           4.4635 |       -1313.7548 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |          -0.0466 |           4.4042 |       -1336.1785 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |          -0.0569 |           4.2690 |       -1386.9898 |
[32m[20221208 14:55:04 @agent_ppo2.py:179][0m |          -0.0593 |           4.1662 |       -1378.6157 |
[32m[20221208 14:55:05 @agent_ppo2.py:179][0m |          -0.0641 |           4.0985 |       -1418.4240 |
[32m[20221208 14:55:05 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.70
[32m[20221208 14:55:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.86
[32m[20221208 14:55:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.84
[32m[20221208 14:55:05 @agent_ppo2.py:137][0m Total time:      18.51 min
[32m[20221208 14:55:05 @agent_ppo2.py:139][0m 1497088 total steps have happened
[32m[20221208 14:55:05 @agent_ppo2.py:115][0m #------------------------ Iteration 731 --------------------------#
[32m[20221208 14:55:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:05 @agent_ppo2.py:179][0m |           0.0977 |           6.2523 |       -1266.8192 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |           0.0650 |           5.6060 |       -1041.3969 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |           0.0287 |           5.3198 |       -1085.3769 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |           0.0018 |           5.1241 |       -1232.9771 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |          -0.0226 |           4.9209 |       -1285.5917 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |          -0.0255 |           4.8073 |       -1294.2587 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |          -0.0408 |           4.6794 |       -1351.7502 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |          -0.0512 |           4.5827 |       -1393.9038 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |          -0.0562 |           4.4659 |       -1397.5264 |
[32m[20221208 14:55:06 @agent_ppo2.py:179][0m |          -0.0615 |           4.4144 |       -1448.4420 |
[32m[20221208 14:55:06 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.90
[32m[20221208 14:55:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.46
[32m[20221208 14:55:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.06
[32m[20221208 14:55:06 @agent_ppo2.py:137][0m Total time:      18.54 min
[32m[20221208 14:55:06 @agent_ppo2.py:139][0m 1499136 total steps have happened
[32m[20221208 14:55:06 @agent_ppo2.py:115][0m #------------------------ Iteration 732 --------------------------#
[32m[20221208 14:55:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |           0.0866 |           4.8593 |       -1192.8945 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |           0.0328 |           4.2320 |        -818.9902 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |          -0.0152 |           4.0387 |        -953.7669 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |          -0.0360 |           3.9191 |        -993.6270 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |          -0.0482 |           3.8653 |       -1027.9651 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |          -0.0609 |           3.7666 |       -1066.0683 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |          -0.0623 |           3.7116 |       -1077.8841 |
[32m[20221208 14:55:07 @agent_ppo2.py:179][0m |          -0.0680 |           3.6649 |       -1124.5211 |
[32m[20221208 14:55:08 @agent_ppo2.py:179][0m |          -0.0734 |           3.6489 |       -1125.5494 |
[32m[20221208 14:55:08 @agent_ppo2.py:179][0m |          -0.0742 |           3.5934 |       -1157.5097 |
[32m[20221208 14:55:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.80
[32m[20221208 14:55:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.19
[32m[20221208 14:55:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.39
[32m[20221208 14:55:08 @agent_ppo2.py:137][0m Total time:      18.56 min
[32m[20221208 14:55:08 @agent_ppo2.py:139][0m 1501184 total steps have happened
[32m[20221208 14:55:08 @agent_ppo2.py:115][0m #------------------------ Iteration 733 --------------------------#
[32m[20221208 14:55:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |           0.0941 |           4.6860 |       -1233.0403 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |           0.1919 |           4.3478 |        -815.6570 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |           0.0508 |           4.1479 |        -996.5875 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |           0.0053 |           4.0464 |       -1118.3261 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |          -0.0136 |           3.9769 |       -1190.0692 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |          -0.0307 |           3.8847 |       -1261.1353 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |          -0.0359 |           3.8246 |       -1302.0383 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |          -0.0479 |           3.7867 |       -1342.0648 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |          -0.0530 |           3.7379 |       -1368.1999 |
[32m[20221208 14:55:09 @agent_ppo2.py:179][0m |          -0.0585 |           3.7230 |       -1404.4132 |
[32m[20221208 14:55:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.66
[32m[20221208 14:55:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.82
[32m[20221208 14:55:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.97
[32m[20221208 14:55:10 @agent_ppo2.py:137][0m Total time:      18.59 min
[32m[20221208 14:55:10 @agent_ppo2.py:139][0m 1503232 total steps have happened
[32m[20221208 14:55:10 @agent_ppo2.py:115][0m #------------------------ Iteration 734 --------------------------#
[32m[20221208 14:55:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |           0.0825 |           3.7017 |       -1223.4138 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |           0.0402 |           3.1672 |       -1089.4631 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |           0.0040 |           3.0086 |       -1228.5625 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |          -0.0119 |           2.8554 |       -1258.2016 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |          -0.0279 |           2.7709 |       -1271.6828 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |          -0.0355 |           2.7173 |       -1308.2971 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |          -0.0478 |           2.6707 |       -1338.0769 |
[32m[20221208 14:55:10 @agent_ppo2.py:179][0m |          -0.0439 |           2.6061 |       -1330.5950 |
[32m[20221208 14:55:11 @agent_ppo2.py:179][0m |          -0.0506 |           2.5728 |       -1352.7801 |
[32m[20221208 14:55:11 @agent_ppo2.py:179][0m |          -0.0489 |           2.5370 |       -1348.9674 |
[32m[20221208 14:55:11 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.29
[32m[20221208 14:55:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.14
[32m[20221208 14:55:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.22
[32m[20221208 14:55:11 @agent_ppo2.py:137][0m Total time:      18.61 min
[32m[20221208 14:55:11 @agent_ppo2.py:139][0m 1505280 total steps have happened
[32m[20221208 14:55:11 @agent_ppo2.py:115][0m #------------------------ Iteration 735 --------------------------#
[32m[20221208 14:55:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |           0.1084 |           3.3303 |       -1300.6852 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |           0.0486 |           2.8657 |        -947.3046 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |           0.0142 |           2.6587 |       -1071.9408 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0027 |           2.5005 |       -1133.3708 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0181 |           2.3807 |       -1211.3084 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0236 |           2.3001 |       -1238.9546 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0227 |           2.2512 |       -1235.9919 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0217 |           2.1888 |       -1204.6952 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0175 |           2.1260 |       -1256.4601 |
[32m[20221208 14:55:12 @agent_ppo2.py:179][0m |          -0.0228 |           2.0822 |       -1282.6746 |
[32m[20221208 14:55:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.72
[32m[20221208 14:55:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.41
[32m[20221208 14:55:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.00
[32m[20221208 14:55:13 @agent_ppo2.py:137][0m Total time:      18.64 min
[32m[20221208 14:55:13 @agent_ppo2.py:139][0m 1507328 total steps have happened
[32m[20221208 14:55:13 @agent_ppo2.py:115][0m #------------------------ Iteration 736 --------------------------#
[32m[20221208 14:55:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |           0.0998 |           3.5129 |       -1073.3278 |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |           0.0722 |           3.2262 |        -514.0461 |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |           0.0252 |           3.0540 |        -537.7069 |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |          -0.0155 |           2.9297 |        -658.5956 |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |          -0.0308 |           2.8433 |        -724.0940 |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |          -0.0339 |           2.8128 |        -738.7017 |
[32m[20221208 14:55:13 @agent_ppo2.py:179][0m |          -0.0399 |           2.7349 |        -740.1194 |
[32m[20221208 14:55:14 @agent_ppo2.py:179][0m |          -0.0519 |           2.7168 |        -795.1823 |
[32m[20221208 14:55:14 @agent_ppo2.py:179][0m |          -0.0569 |           2.6553 |        -824.1426 |
[32m[20221208 14:55:14 @agent_ppo2.py:179][0m |          -0.0626 |           2.6245 |        -843.4075 |
[32m[20221208 14:55:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.33
[32m[20221208 14:55:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.15
[32m[20221208 14:55:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.37
[32m[20221208 14:55:14 @agent_ppo2.py:137][0m Total time:      18.66 min
[32m[20221208 14:55:14 @agent_ppo2.py:139][0m 1509376 total steps have happened
[32m[20221208 14:55:14 @agent_ppo2.py:115][0m #------------------------ Iteration 737 --------------------------#
[32m[20221208 14:55:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |           0.1397 |           5.2075 |       -1274.6774 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |           0.1145 |           4.5474 |        -812.9864 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |           0.0405 |           4.2485 |       -1015.4133 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0032 |           3.9970 |       -1172.0785 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0247 |           3.8249 |       -1280.2189 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0378 |           3.6605 |       -1338.8305 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0415 |           3.5628 |       -1319.0316 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0437 |           3.4370 |       -1296.4750 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0560 |           3.3650 |       -1375.9733 |
[32m[20221208 14:55:15 @agent_ppo2.py:179][0m |          -0.0618 |           3.2752 |       -1434.5545 |
[32m[20221208 14:55:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.45
[32m[20221208 14:55:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.09
[32m[20221208 14:55:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.85
[32m[20221208 14:55:16 @agent_ppo2.py:137][0m Total time:      18.69 min
[32m[20221208 14:55:16 @agent_ppo2.py:139][0m 1511424 total steps have happened
[32m[20221208 14:55:16 @agent_ppo2.py:115][0m #------------------------ Iteration 738 --------------------------#
[32m[20221208 14:55:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |           0.1223 |           4.5746 |       -1244.9663 |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |           0.0822 |           4.1746 |       -1078.3189 |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |           0.0353 |           3.9418 |        -981.5528 |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |          -0.0008 |           3.8796 |       -1109.5271 |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |          -0.0108 |           3.7303 |       -1279.2489 |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |          -0.0266 |           3.6996 |       -1326.1353 |
[32m[20221208 14:55:16 @agent_ppo2.py:179][0m |          -0.0346 |           3.6444 |       -1384.5507 |
[32m[20221208 14:55:17 @agent_ppo2.py:179][0m |          -0.0374 |           3.5936 |       -1476.1375 |
[32m[20221208 14:55:17 @agent_ppo2.py:179][0m |          -0.0352 |           3.5395 |       -1485.4778 |
[32m[20221208 14:55:17 @agent_ppo2.py:179][0m |          -0.0430 |           3.5292 |       -1501.4894 |
[32m[20221208 14:55:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.06
[32m[20221208 14:55:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.19
[32m[20221208 14:55:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.61
[32m[20221208 14:55:17 @agent_ppo2.py:137][0m Total time:      18.71 min
[32m[20221208 14:55:17 @agent_ppo2.py:139][0m 1513472 total steps have happened
[32m[20221208 14:55:17 @agent_ppo2.py:115][0m #------------------------ Iteration 739 --------------------------#
[32m[20221208 14:55:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |           0.1105 |           6.1365 |       -1232.0031 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |           0.0584 |           5.2749 |       -1026.2498 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |           0.0107 |           4.9560 |       -1113.3212 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0180 |           4.8118 |       -1220.6164 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0272 |           4.5929 |       -1269.5050 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0398 |           4.5087 |       -1325.4608 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0500 |           4.4256 |       -1366.4185 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0522 |           4.3306 |       -1398.5769 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0572 |           4.2498 |       -1424.7615 |
[32m[20221208 14:55:18 @agent_ppo2.py:179][0m |          -0.0608 |           4.1672 |       -1460.5652 |
[32m[20221208 14:55:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.40
[32m[20221208 14:55:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.89
[32m[20221208 14:55:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.22
[32m[20221208 14:55:19 @agent_ppo2.py:137][0m Total time:      18.74 min
[32m[20221208 14:55:19 @agent_ppo2.py:139][0m 1515520 total steps have happened
[32m[20221208 14:55:19 @agent_ppo2.py:115][0m #------------------------ Iteration 740 --------------------------#
[32m[20221208 14:55:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:19 @agent_ppo2.py:179][0m |           0.0893 |           6.1614 |       -1300.5676 |
[32m[20221208 14:55:19 @agent_ppo2.py:179][0m |           0.0941 |           5.5000 |       -1016.1428 |
[32m[20221208 14:55:19 @agent_ppo2.py:179][0m |           0.0417 |           5.2216 |       -1101.1726 |
[32m[20221208 14:55:19 @agent_ppo2.py:179][0m |           0.0187 |           5.0422 |       -1143.9132 |
[32m[20221208 14:55:19 @agent_ppo2.py:179][0m |          -0.0086 |           4.8495 |       -1280.3300 |
[32m[20221208 14:55:19 @agent_ppo2.py:179][0m |          -0.0252 |           4.7653 |       -1320.3645 |
[32m[20221208 14:55:20 @agent_ppo2.py:179][0m |          -0.0400 |           4.5953 |       -1390.1888 |
[32m[20221208 14:55:20 @agent_ppo2.py:179][0m |          -0.0491 |           4.5404 |       -1430.8932 |
[32m[20221208 14:55:20 @agent_ppo2.py:179][0m |          -0.0515 |           4.4381 |       -1452.8618 |
[32m[20221208 14:55:20 @agent_ppo2.py:179][0m |          -0.0557 |           4.3214 |       -1480.5190 |
[32m[20221208 14:55:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.66
[32m[20221208 14:55:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.07
[32m[20221208 14:55:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.40
[32m[20221208 14:55:20 @agent_ppo2.py:137][0m Total time:      18.76 min
[32m[20221208 14:55:20 @agent_ppo2.py:139][0m 1517568 total steps have happened
[32m[20221208 14:55:20 @agent_ppo2.py:115][0m #------------------------ Iteration 741 --------------------------#
[32m[20221208 14:55:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |           0.0766 |           3.0735 |       -1003.7583 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |           0.0494 |           2.7417 |        -356.1092 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |           0.0139 |           2.6194 |        -469.2829 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0064 |           2.5762 |        -529.2186 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0158 |           2.5134 |        -546.5393 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0194 |           2.4924 |        -542.0171 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0285 |           2.4539 |        -566.3844 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0366 |           2.4544 |        -596.8742 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0378 |           2.4118 |        -606.3317 |
[32m[20221208 14:55:21 @agent_ppo2.py:179][0m |          -0.0391 |           2.3989 |        -610.7586 |
[32m[20221208 14:55:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:55:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.48
[32m[20221208 14:55:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.79
[32m[20221208 14:55:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.88
[32m[20221208 14:55:22 @agent_ppo2.py:137][0m Total time:      18.79 min
[32m[20221208 14:55:22 @agent_ppo2.py:139][0m 1519616 total steps have happened
[32m[20221208 14:55:22 @agent_ppo2.py:115][0m #------------------------ Iteration 742 --------------------------#
[32m[20221208 14:55:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:22 @agent_ppo2.py:179][0m |           0.0746 |           5.8635 |       -1348.6955 |
[32m[20221208 14:55:22 @agent_ppo2.py:179][0m |           0.0631 |           5.2225 |       -1079.1325 |
[32m[20221208 14:55:22 @agent_ppo2.py:179][0m |           0.0650 |           4.9870 |       -1079.4575 |
[32m[20221208 14:55:22 @agent_ppo2.py:179][0m |           0.0041 |           4.7990 |       -1148.1110 |
[32m[20221208 14:55:22 @agent_ppo2.py:179][0m |          -0.0237 |           4.6615 |       -1244.4572 |
[32m[20221208 14:55:23 @agent_ppo2.py:179][0m |          -0.0358 |           4.5973 |       -1293.8390 |
[32m[20221208 14:55:23 @agent_ppo2.py:179][0m |          -0.0437 |           4.4456 |       -1323.5038 |
[32m[20221208 14:55:23 @agent_ppo2.py:179][0m |          -0.0536 |           4.3969 |       -1371.2980 |
[32m[20221208 14:55:23 @agent_ppo2.py:179][0m |          -0.0540 |           4.3066 |       -1404.7357 |
[32m[20221208 14:55:23 @agent_ppo2.py:179][0m |          -0.0612 |           4.2869 |       -1445.4142 |
[32m[20221208 14:55:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.48
[32m[20221208 14:55:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.54
[32m[20221208 14:55:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.51
[32m[20221208 14:55:23 @agent_ppo2.py:137][0m Total time:      18.82 min
[32m[20221208 14:55:23 @agent_ppo2.py:139][0m 1521664 total steps have happened
[32m[20221208 14:55:23 @agent_ppo2.py:115][0m #------------------------ Iteration 743 --------------------------#
[32m[20221208 14:55:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |           0.1616 |           6.2515 |       -1236.8121 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |           0.1323 |           5.5188 |        -773.7772 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |           0.0661 |           5.1775 |        -934.0262 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |           0.0276 |           4.9638 |       -1191.1621 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |          -0.0039 |           4.7753 |       -1238.0573 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |          -0.0211 |           4.6617 |       -1292.7317 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |          -0.0250 |           4.5702 |       -1343.5580 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |          -0.0338 |           4.4896 |       -1361.5791 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |          -0.0459 |           4.4139 |       -1402.1631 |
[32m[20221208 14:55:24 @agent_ppo2.py:179][0m |          -0.0476 |           4.3290 |       -1430.3262 |
[32m[20221208 14:55:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.61
[32m[20221208 14:55:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.46
[32m[20221208 14:55:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.67
[32m[20221208 14:55:25 @agent_ppo2.py:137][0m Total time:      18.84 min
[32m[20221208 14:55:25 @agent_ppo2.py:139][0m 1523712 total steps have happened
[32m[20221208 14:55:25 @agent_ppo2.py:115][0m #------------------------ Iteration 744 --------------------------#
[32m[20221208 14:55:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:25 @agent_ppo2.py:179][0m |           0.1129 |           6.2082 |       -1266.4768 |
[32m[20221208 14:55:25 @agent_ppo2.py:179][0m |           0.1005 |           5.6348 |       -1021.5140 |
[32m[20221208 14:55:25 @agent_ppo2.py:179][0m |           0.0485 |           5.3280 |       -1169.9622 |
[32m[20221208 14:55:25 @agent_ppo2.py:179][0m |           0.0190 |           5.1101 |       -1172.8218 |
[32m[20221208 14:55:25 @agent_ppo2.py:179][0m |          -0.0147 |           5.0096 |       -1272.8469 |
[32m[20221208 14:55:26 @agent_ppo2.py:179][0m |          -0.0355 |           4.9556 |       -1300.6901 |
[32m[20221208 14:55:26 @agent_ppo2.py:179][0m |          -0.0474 |           4.8435 |       -1337.0229 |
[32m[20221208 14:55:26 @agent_ppo2.py:179][0m |          -0.0518 |           4.7579 |       -1378.6006 |
[32m[20221208 14:55:26 @agent_ppo2.py:179][0m |          -0.0510 |           4.6965 |       -1342.6653 |
[32m[20221208 14:55:26 @agent_ppo2.py:179][0m |          -0.0554 |           4.5974 |       -1371.5681 |
[32m[20221208 14:55:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.25
[32m[20221208 14:55:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.45
[32m[20221208 14:55:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.08
[32m[20221208 14:55:26 @agent_ppo2.py:137][0m Total time:      18.87 min
[32m[20221208 14:55:26 @agent_ppo2.py:139][0m 1525760 total steps have happened
[32m[20221208 14:55:26 @agent_ppo2.py:115][0m #------------------------ Iteration 745 --------------------------#
[32m[20221208 14:55:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |           0.0707 |           6.9320 |       -1273.8797 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |           0.1115 |           6.4020 |       -1004.3975 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |           0.0514 |           6.1216 |       -1012.6031 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0050 |           5.9445 |       -1147.5478 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0301 |           5.7537 |       -1238.5632 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0429 |           5.6706 |       -1272.6577 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0529 |           5.5274 |       -1302.2097 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0584 |           5.4247 |       -1333.9398 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0631 |           5.3136 |       -1353.8297 |
[32m[20221208 14:55:27 @agent_ppo2.py:179][0m |          -0.0655 |           5.2232 |       -1370.0862 |
[32m[20221208 14:55:27 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.66
[32m[20221208 14:55:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.47
[32m[20221208 14:55:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.86
[32m[20221208 14:55:28 @agent_ppo2.py:137][0m Total time:      18.89 min
[32m[20221208 14:55:28 @agent_ppo2.py:139][0m 1527808 total steps have happened
[32m[20221208 14:55:28 @agent_ppo2.py:115][0m #------------------------ Iteration 746 --------------------------#
[32m[20221208 14:55:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:28 @agent_ppo2.py:179][0m |           0.0852 |           6.5150 |       -1222.9319 |
[32m[20221208 14:55:28 @agent_ppo2.py:179][0m |           0.0697 |           5.9198 |       -1056.5887 |
[32m[20221208 14:55:28 @agent_ppo2.py:179][0m |           0.0041 |           5.7344 |       -1230.7610 |
[32m[20221208 14:55:28 @agent_ppo2.py:179][0m |          -0.0204 |           5.5505 |       -1290.1080 |
[32m[20221208 14:55:29 @agent_ppo2.py:179][0m |          -0.0348 |           5.3840 |       -1350.8863 |
[32m[20221208 14:55:29 @agent_ppo2.py:179][0m |          -0.0399 |           5.2803 |       -1349.0263 |
[32m[20221208 14:55:29 @agent_ppo2.py:179][0m |          -0.0460 |           5.2201 |       -1367.2124 |
[32m[20221208 14:55:29 @agent_ppo2.py:179][0m |          -0.0545 |           5.1376 |       -1391.3687 |
[32m[20221208 14:55:29 @agent_ppo2.py:179][0m |          -0.0597 |           5.0890 |       -1419.9530 |
[32m[20221208 14:55:29 @agent_ppo2.py:179][0m |          -0.0637 |           5.0231 |       -1448.2608 |
[32m[20221208 14:55:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.73
[32m[20221208 14:55:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.56
[32m[20221208 14:55:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.77
[32m[20221208 14:55:29 @agent_ppo2.py:137][0m Total time:      18.92 min
[32m[20221208 14:55:29 @agent_ppo2.py:139][0m 1529856 total steps have happened
[32m[20221208 14:55:29 @agent_ppo2.py:115][0m #------------------------ Iteration 747 --------------------------#
[32m[20221208 14:55:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |           0.1297 |           5.6550 |       -1262.8768 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |           0.1201 |           5.0343 |        -840.1605 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |           0.0746 |           4.7775 |        -791.8440 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |           0.0314 |           4.5933 |        -877.4219 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |          -0.0005 |           4.4816 |       -1047.7462 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |          -0.0193 |           4.4049 |       -1141.0367 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |          -0.0328 |           4.2748 |       -1223.6005 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |          -0.0411 |           4.2066 |       -1287.6919 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |          -0.0416 |           4.1676 |       -1296.8112 |
[32m[20221208 14:55:30 @agent_ppo2.py:179][0m |          -0.0484 |           4.0916 |       -1327.2766 |
[32m[20221208 14:55:30 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:55:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.87
[32m[20221208 14:55:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.23
[32m[20221208 14:55:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.15
[32m[20221208 14:55:31 @agent_ppo2.py:137][0m Total time:      18.94 min
[32m[20221208 14:55:31 @agent_ppo2.py:139][0m 1531904 total steps have happened
[32m[20221208 14:55:31 @agent_ppo2.py:115][0m #------------------------ Iteration 748 --------------------------#
[32m[20221208 14:55:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:31 @agent_ppo2.py:179][0m |           0.0985 |           6.0427 |       -1266.8124 |
[32m[20221208 14:55:31 @agent_ppo2.py:179][0m |           0.0343 |           4.8866 |       -1208.0799 |
[32m[20221208 14:55:31 @agent_ppo2.py:179][0m |          -0.0021 |           4.4270 |       -1309.2791 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0102 |           4.1919 |       -1290.2346 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0354 |           4.0025 |       -1333.9403 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0437 |           3.8543 |       -1377.0017 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0486 |           3.8152 |       -1392.8478 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0544 |           3.6958 |       -1424.1336 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0577 |           3.6643 |       -1407.8931 |
[32m[20221208 14:55:32 @agent_ppo2.py:179][0m |          -0.0572 |           3.6061 |       -1436.9914 |
[32m[20221208 14:55:32 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:55:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.93
[32m[20221208 14:55:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.20
[32m[20221208 14:55:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.22
[32m[20221208 14:55:32 @agent_ppo2.py:137][0m Total time:      18.97 min
[32m[20221208 14:55:32 @agent_ppo2.py:139][0m 1533952 total steps have happened
[32m[20221208 14:55:32 @agent_ppo2.py:115][0m #------------------------ Iteration 749 --------------------------#
[32m[20221208 14:55:33 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:55:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |           0.1111 |           6.7067 |       -1159.0989 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |           0.0736 |           5.7007 |       -1012.6263 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |           0.0260 |           5.3199 |       -1106.3411 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |           0.0011 |           5.0322 |       -1182.7736 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |          -0.0272 |           4.8980 |       -1277.0836 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |          -0.0348 |           4.7855 |       -1302.5765 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |          -0.0369 |           4.7026 |       -1336.9560 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |          -0.0393 |           4.6056 |       -1362.0089 |
[32m[20221208 14:55:33 @agent_ppo2.py:179][0m |          -0.0475 |           4.5661 |       -1392.8802 |
[32m[20221208 14:55:34 @agent_ppo2.py:179][0m |          -0.0508 |           4.5266 |       -1413.4149 |
[32m[20221208 14:55:34 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:55:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.32
[32m[20221208 14:55:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.04
[32m[20221208 14:55:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.92
[32m[20221208 14:55:34 @agent_ppo2.py:137][0m Total time:      19.00 min
[32m[20221208 14:55:34 @agent_ppo2.py:139][0m 1536000 total steps have happened
[32m[20221208 14:55:34 @agent_ppo2.py:115][0m #------------------------ Iteration 750 --------------------------#
[32m[20221208 14:55:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |           0.1011 |           6.6824 |       -1350.2618 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |           0.0676 |           5.7882 |       -1064.1074 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |           0.0381 |           5.4824 |       -1209.9007 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0022 |           5.2848 |       -1246.1060 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0255 |           5.1238 |       -1357.5298 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0401 |           5.0247 |       -1375.8991 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0496 |           4.8891 |       -1443.6210 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0553 |           4.7933 |       -1468.2105 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0650 |           4.7226 |       -1487.7827 |
[32m[20221208 14:55:35 @agent_ppo2.py:179][0m |          -0.0608 |           4.6502 |       -1481.3483 |
[32m[20221208 14:55:35 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:55:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.27
[32m[20221208 14:55:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.03
[32m[20221208 14:55:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.10
[32m[20221208 14:55:36 @agent_ppo2.py:137][0m Total time:      19.02 min
[32m[20221208 14:55:36 @agent_ppo2.py:139][0m 1538048 total steps have happened
[32m[20221208 14:55:36 @agent_ppo2.py:115][0m #------------------------ Iteration 751 --------------------------#
[32m[20221208 14:55:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |           0.1917 |           6.4120 |       -1086.5027 |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |           0.0651 |           5.6995 |        -892.9046 |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |           0.0092 |           5.4732 |       -1113.9806 |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |          -0.0213 |           5.3534 |       -1194.7211 |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |          -0.0445 |           5.2412 |       -1278.5316 |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |          -0.0514 |           5.1448 |       -1314.6751 |
[32m[20221208 14:55:36 @agent_ppo2.py:179][0m |          -0.0572 |           5.1010 |       -1337.1868 |
[32m[20221208 14:55:37 @agent_ppo2.py:179][0m |          -0.0604 |           5.0127 |       -1330.2146 |
[32m[20221208 14:55:37 @agent_ppo2.py:179][0m |          -0.0700 |           4.9169 |       -1370.8775 |
[32m[20221208 14:55:37 @agent_ppo2.py:179][0m |          -0.0741 |           4.8542 |       -1398.4742 |
[32m[20221208 14:55:37 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.02
[32m[20221208 14:55:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.20
[32m[20221208 14:55:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.04
[32m[20221208 14:55:37 @agent_ppo2.py:137][0m Total time:      19.05 min
[32m[20221208 14:55:37 @agent_ppo2.py:139][0m 1540096 total steps have happened
[32m[20221208 14:55:37 @agent_ppo2.py:115][0m #------------------------ Iteration 752 --------------------------#
[32m[20221208 14:55:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |           0.1050 |           6.2530 |       -1327.3550 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |           0.0672 |           5.3430 |       -1099.3431 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |           0.0172 |           4.9998 |       -1190.5155 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0200 |           4.7194 |       -1268.7918 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0409 |           4.4973 |       -1343.9895 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0511 |           4.3416 |       -1397.4378 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0562 |           4.2426 |       -1416.2674 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0622 |           4.0623 |       -1444.8440 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0699 |           3.9594 |       -1462.5654 |
[32m[20221208 14:55:38 @agent_ppo2.py:179][0m |          -0.0759 |           3.8553 |       -1481.1108 |
[32m[20221208 14:55:38 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.38
[32m[20221208 14:55:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.48
[32m[20221208 14:55:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.34
[32m[20221208 14:55:39 @agent_ppo2.py:137][0m Total time:      19.07 min
[32m[20221208 14:55:39 @agent_ppo2.py:139][0m 1542144 total steps have happened
[32m[20221208 14:55:39 @agent_ppo2.py:115][0m #------------------------ Iteration 753 --------------------------#
[32m[20221208 14:55:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:39 @agent_ppo2.py:179][0m |           0.0968 |           6.6516 |       -1416.1606 |
[32m[20221208 14:55:39 @agent_ppo2.py:179][0m |           0.1406 |           5.8829 |        -899.2346 |
[32m[20221208 14:55:39 @agent_ppo2.py:179][0m |           0.0658 |           5.5251 |       -1008.7532 |
[32m[20221208 14:55:39 @agent_ppo2.py:179][0m |           0.0224 |           5.3474 |       -1232.8914 |
[32m[20221208 14:55:39 @agent_ppo2.py:179][0m |           0.0041 |           5.1609 |       -1304.5860 |
[32m[20221208 14:55:39 @agent_ppo2.py:179][0m |          -0.0188 |           5.0699 |       -1417.8081 |
[32m[20221208 14:55:40 @agent_ppo2.py:179][0m |          -0.0262 |           4.9529 |       -1455.4004 |
[32m[20221208 14:55:40 @agent_ppo2.py:179][0m |          -0.0392 |           4.9055 |       -1543.3521 |
[32m[20221208 14:55:40 @agent_ppo2.py:179][0m |          -0.0414 |           4.8023 |       -1562.3382 |
[32m[20221208 14:55:40 @agent_ppo2.py:179][0m |          -0.0461 |           4.7812 |       -1595.4714 |
[32m[20221208 14:55:40 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.72
[32m[20221208 14:55:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.37
[32m[20221208 14:55:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.36
[32m[20221208 14:55:40 @agent_ppo2.py:137][0m Total time:      19.10 min
[32m[20221208 14:55:40 @agent_ppo2.py:139][0m 1544192 total steps have happened
[32m[20221208 14:55:40 @agent_ppo2.py:115][0m #------------------------ Iteration 754 --------------------------#
[32m[20221208 14:55:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |           0.0681 |           5.0517 |       -1415.6486 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |           0.0456 |           3.9526 |       -1167.3915 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |           0.0033 |           3.5210 |       -1312.7932 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0235 |           3.2898 |       -1409.0849 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0376 |           3.0826 |       -1442.9364 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0416 |           2.9684 |       -1501.3585 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0437 |           2.8316 |       -1497.6910 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0517 |           2.7400 |       -1547.0428 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0587 |           2.6460 |       -1581.2071 |
[32m[20221208 14:55:41 @agent_ppo2.py:179][0m |          -0.0631 |           2.5536 |       -1611.2235 |
[32m[20221208 14:55:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.81
[32m[20221208 14:55:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.24
[32m[20221208 14:55:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.47
[32m[20221208 14:55:42 @agent_ppo2.py:137][0m Total time:      19.12 min
[32m[20221208 14:55:42 @agent_ppo2.py:139][0m 1546240 total steps have happened
[32m[20221208 14:55:42 @agent_ppo2.py:115][0m #------------------------ Iteration 755 --------------------------#
[32m[20221208 14:55:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:42 @agent_ppo2.py:179][0m |           0.0905 |           6.2472 |       -1338.1611 |
[32m[20221208 14:55:42 @agent_ppo2.py:179][0m |           0.0400 |           5.2712 |       -1185.1061 |
[32m[20221208 14:55:42 @agent_ppo2.py:179][0m |           0.0066 |           4.9766 |       -1231.5223 |
[32m[20221208 14:55:42 @agent_ppo2.py:179][0m |          -0.0259 |           4.7529 |       -1383.6987 |
[32m[20221208 14:55:42 @agent_ppo2.py:179][0m |          -0.0398 |           4.6584 |       -1432.6556 |
[32m[20221208 14:55:43 @agent_ppo2.py:179][0m |          -0.0496 |           4.4699 |       -1458.7294 |
[32m[20221208 14:55:43 @agent_ppo2.py:179][0m |          -0.0512 |           4.3326 |       -1488.7050 |
[32m[20221208 14:55:43 @agent_ppo2.py:179][0m |          -0.0625 |           4.2809 |       -1503.2577 |
[32m[20221208 14:55:43 @agent_ppo2.py:179][0m |          -0.0681 |           4.1759 |       -1530.7657 |
[32m[20221208 14:55:43 @agent_ppo2.py:179][0m |          -0.0676 |           4.1425 |       -1540.2132 |
[32m[20221208 14:55:43 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.80
[32m[20221208 14:55:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.04
[32m[20221208 14:55:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.18
[32m[20221208 14:55:43 @agent_ppo2.py:137][0m Total time:      19.15 min
[32m[20221208 14:55:43 @agent_ppo2.py:139][0m 1548288 total steps have happened
[32m[20221208 14:55:43 @agent_ppo2.py:115][0m #------------------------ Iteration 756 --------------------------#
[32m[20221208 14:55:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |           0.1351 |           6.4209 |       -1209.6439 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |           0.0873 |           5.5192 |       -1038.5453 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |           0.0378 |           5.0704 |       -1103.5181 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |           0.0079 |           4.8288 |       -1229.2703 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |          -0.0037 |           4.6883 |       -1264.7831 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |          -0.0231 |           4.5337 |       -1344.9544 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |          -0.0363 |           4.4238 |       -1423.6575 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |          -0.0422 |           4.3363 |       -1465.4008 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |          -0.0495 |           4.2488 |       -1553.3630 |
[32m[20221208 14:55:44 @agent_ppo2.py:179][0m |          -0.0532 |           4.1928 |       -1571.8149 |
[32m[20221208 14:55:44 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.11
[32m[20221208 14:55:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.54
[32m[20221208 14:55:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.32
[32m[20221208 14:55:45 @agent_ppo2.py:137][0m Total time:      19.17 min
[32m[20221208 14:55:45 @agent_ppo2.py:139][0m 1550336 total steps have happened
[32m[20221208 14:55:45 @agent_ppo2.py:115][0m #------------------------ Iteration 757 --------------------------#
[32m[20221208 14:55:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:45 @agent_ppo2.py:179][0m |           0.1212 |           5.6657 |       -1373.4503 |
[32m[20221208 14:55:45 @agent_ppo2.py:179][0m |           0.0942 |           4.9222 |       -1028.2271 |
[32m[20221208 14:55:45 @agent_ppo2.py:179][0m |           0.0226 |           4.6850 |       -1240.6244 |
[32m[20221208 14:55:45 @agent_ppo2.py:179][0m |          -0.0089 |           4.5584 |       -1347.2887 |
[32m[20221208 14:55:46 @agent_ppo2.py:179][0m |          -0.0329 |           4.4248 |       -1426.1453 |
[32m[20221208 14:55:46 @agent_ppo2.py:179][0m |          -0.0493 |           4.3475 |       -1463.9281 |
[32m[20221208 14:55:46 @agent_ppo2.py:179][0m |          -0.0519 |           4.2096 |       -1494.3335 |
[32m[20221208 14:55:46 @agent_ppo2.py:179][0m |          -0.0554 |           4.1052 |       -1516.6763 |
[32m[20221208 14:55:46 @agent_ppo2.py:179][0m |          -0.0648 |           4.0693 |       -1570.1121 |
[32m[20221208 14:55:46 @agent_ppo2.py:179][0m |          -0.0679 |           3.9818 |       -1575.0800 |
[32m[20221208 14:55:46 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.83
[32m[20221208 14:55:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.76
[32m[20221208 14:55:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.34
[32m[20221208 14:55:46 @agent_ppo2.py:137][0m Total time:      19.20 min
[32m[20221208 14:55:46 @agent_ppo2.py:139][0m 1552384 total steps have happened
[32m[20221208 14:55:46 @agent_ppo2.py:115][0m #------------------------ Iteration 758 --------------------------#
[32m[20221208 14:55:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:55:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |           0.0889 |           8.0910 |       -1402.5526 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |           0.0575 |           7.0019 |       -1222.8381 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0021 |           6.5619 |       -1316.4010 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0278 |           6.2486 |       -1366.4468 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0418 |           6.0785 |       -1452.7324 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0515 |           5.9116 |       -1465.2235 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0580 |           5.7467 |       -1482.5400 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0639 |           5.6685 |       -1494.8472 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0678 |           5.5961 |       -1514.4085 |
[32m[20221208 14:55:47 @agent_ppo2.py:179][0m |          -0.0722 |           5.4943 |       -1546.8974 |
[32m[20221208 14:55:47 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.66
[32m[20221208 14:55:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.15
[32m[20221208 14:55:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.84
[32m[20221208 14:55:48 @agent_ppo2.py:137][0m Total time:      19.23 min
[32m[20221208 14:55:48 @agent_ppo2.py:139][0m 1554432 total steps have happened
[32m[20221208 14:55:48 @agent_ppo2.py:115][0m #------------------------ Iteration 759 --------------------------#
[32m[20221208 14:55:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:48 @agent_ppo2.py:179][0m |           0.1281 |           8.1946 |       -1373.4628 |
[32m[20221208 14:55:48 @agent_ppo2.py:179][0m |           0.1224 |           7.2913 |        -966.7234 |
[32m[20221208 14:55:48 @agent_ppo2.py:179][0m |           0.1153 |           6.9191 |        -912.1678 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |           0.0603 |           6.7076 |        -926.8652 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |           0.0077 |           6.5067 |       -1146.8548 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |          -0.0162 |           6.4477 |       -1253.0447 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |          -0.0319 |           6.2842 |       -1295.0873 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |          -0.0425 |           6.1902 |       -1373.6434 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |          -0.0436 |           6.1289 |       -1382.8442 |
[32m[20221208 14:55:49 @agent_ppo2.py:179][0m |          -0.0432 |           6.0681 |       -1383.5483 |
[32m[20221208 14:55:49 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:55:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.15
[32m[20221208 14:55:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.24
[32m[20221208 14:55:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.25
[32m[20221208 14:55:49 @agent_ppo2.py:137][0m Total time:      19.25 min
[32m[20221208 14:55:49 @agent_ppo2.py:139][0m 1556480 total steps have happened
[32m[20221208 14:55:49 @agent_ppo2.py:115][0m #------------------------ Iteration 760 --------------------------#
[32m[20221208 14:55:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |           0.1318 |           6.4903 |       -1281.8822 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |           0.1347 |           5.4952 |        -782.0551 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |           0.0623 |           5.1800 |       -1021.7567 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |           0.0233 |           4.9400 |       -1194.9246 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |           0.0034 |           4.8019 |       -1259.4993 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |          -0.0242 |           4.6040 |       -1358.3068 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |          -0.0395 |           4.5493 |       -1430.8823 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |          -0.0517 |           4.4055 |       -1456.9962 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |          -0.0496 |           4.3348 |       -1435.4157 |
[32m[20221208 14:55:50 @agent_ppo2.py:179][0m |          -0.0616 |           4.2569 |       -1480.6284 |
[32m[20221208 14:55:50 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.72
[32m[20221208 14:55:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.38
[32m[20221208 14:55:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.80
[32m[20221208 14:55:51 @agent_ppo2.py:137][0m Total time:      19.28 min
[32m[20221208 14:55:51 @agent_ppo2.py:139][0m 1558528 total steps have happened
[32m[20221208 14:55:51 @agent_ppo2.py:115][0m #------------------------ Iteration 761 --------------------------#
[32m[20221208 14:55:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:55:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:51 @agent_ppo2.py:179][0m |           0.0852 |           5.1604 |       -1344.5488 |
[32m[20221208 14:55:51 @agent_ppo2.py:179][0m |           0.0539 |           3.4559 |       -1085.8515 |
[32m[20221208 14:55:51 @agent_ppo2.py:179][0m |           0.0085 |           2.8825 |       -1190.0081 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0294 |           2.5727 |       -1297.2100 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0412 |           2.4120 |       -1333.7019 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0518 |           2.2869 |       -1368.3322 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0548 |           2.1819 |       -1383.0785 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0610 |           2.0965 |       -1400.8299 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0552 |           2.0581 |       -1400.0293 |
[32m[20221208 14:55:52 @agent_ppo2.py:179][0m |          -0.0625 |           1.9947 |       -1426.7312 |
[32m[20221208 14:55:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.41
[32m[20221208 14:55:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.49
[32m[20221208 14:55:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.54
[32m[20221208 14:55:52 @agent_ppo2.py:137][0m Total time:      19.30 min
[32m[20221208 14:55:52 @agent_ppo2.py:139][0m 1560576 total steps have happened
[32m[20221208 14:55:52 @agent_ppo2.py:115][0m #------------------------ Iteration 762 --------------------------#
[32m[20221208 14:55:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |           0.1154 |           5.1286 |       -1412.6101 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |           0.0982 |           4.0818 |       -1082.2963 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |           0.0463 |           3.6688 |       -1144.5438 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0093 |           3.4098 |       -1291.8306 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0313 |           3.1755 |       -1358.3200 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0415 |           2.9989 |       -1405.6043 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0522 |           2.8553 |       -1428.2264 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0587 |           2.7382 |       -1486.4106 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0607 |           2.6437 |       -1486.2533 |
[32m[20221208 14:55:53 @agent_ppo2.py:179][0m |          -0.0695 |           2.5364 |       -1527.4047 |
[32m[20221208 14:55:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.05
[32m[20221208 14:55:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.30
[32m[20221208 14:55:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.48
[32m[20221208 14:55:54 @agent_ppo2.py:137][0m Total time:      19.33 min
[32m[20221208 14:55:54 @agent_ppo2.py:139][0m 1562624 total steps have happened
[32m[20221208 14:55:54 @agent_ppo2.py:115][0m #------------------------ Iteration 763 --------------------------#
[32m[20221208 14:55:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:54 @agent_ppo2.py:179][0m |           0.1839 |           6.9581 |       -1222.6593 |
[32m[20221208 14:55:54 @agent_ppo2.py:179][0m |           0.0417 |           5.8439 |        -827.4017 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0075 |           5.5134 |        -978.4657 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0377 |           5.2524 |       -1037.5932 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0480 |           5.0989 |       -1070.3725 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0588 |           5.0289 |       -1118.6185 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0639 |           4.8889 |       -1130.6452 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0673 |           4.8737 |       -1159.1821 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0750 |           4.7377 |       -1200.8502 |
[32m[20221208 14:55:55 @agent_ppo2.py:179][0m |          -0.0777 |           4.6794 |       -1198.5625 |
[32m[20221208 14:55:55 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.51
[32m[20221208 14:55:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.86
[32m[20221208 14:55:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.18
[32m[20221208 14:55:55 @agent_ppo2.py:137][0m Total time:      19.35 min
[32m[20221208 14:55:55 @agent_ppo2.py:139][0m 1564672 total steps have happened
[32m[20221208 14:55:55 @agent_ppo2.py:115][0m #------------------------ Iteration 764 --------------------------#
[32m[20221208 14:55:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |           0.0807 |           6.6798 |       -1444.3293 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |           0.0984 |           6.0526 |       -1001.3848 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |           0.0382 |           5.8411 |       -1074.2673 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0077 |           5.6786 |       -1292.4915 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0277 |           5.5714 |       -1378.0734 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0344 |           5.4974 |       -1380.3123 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0461 |           5.3936 |       -1409.5204 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0499 |           5.3237 |       -1461.5433 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0557 |           5.2503 |       -1487.8730 |
[32m[20221208 14:55:56 @agent_ppo2.py:179][0m |          -0.0609 |           5.2405 |       -1515.4594 |
[32m[20221208 14:55:56 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:55:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.77
[32m[20221208 14:55:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.65
[32m[20221208 14:55:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.68
[32m[20221208 14:55:57 @agent_ppo2.py:137][0m Total time:      19.38 min
[32m[20221208 14:55:57 @agent_ppo2.py:139][0m 1566720 total steps have happened
[32m[20221208 14:55:57 @agent_ppo2.py:115][0m #------------------------ Iteration 765 --------------------------#
[32m[20221208 14:55:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:57 @agent_ppo2.py:179][0m |           0.1444 |           5.5672 |       -1247.0490 |
[32m[20221208 14:55:57 @agent_ppo2.py:179][0m |           0.0814 |           4.2692 |       -1094.6812 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |           0.0081 |           3.8261 |       -1288.4403 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0238 |           3.6021 |       -1347.6054 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0352 |           3.4059 |       -1368.7562 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0390 |           3.3006 |       -1401.2594 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0558 |           3.2105 |       -1459.9277 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0598 |           3.1098 |       -1479.8646 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0598 |           3.0480 |       -1494.1863 |
[32m[20221208 14:55:58 @agent_ppo2.py:179][0m |          -0.0668 |           2.9708 |       -1506.2109 |
[32m[20221208 14:55:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:55:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.73
[32m[20221208 14:55:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.65
[32m[20221208 14:55:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.11
[32m[20221208 14:55:58 @agent_ppo2.py:137][0m Total time:      19.40 min
[32m[20221208 14:55:58 @agent_ppo2.py:139][0m 1568768 total steps have happened
[32m[20221208 14:55:58 @agent_ppo2.py:115][0m #------------------------ Iteration 766 --------------------------#
[32m[20221208 14:55:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:55:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |           0.0744 |           6.7502 |       -1484.8705 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |           0.1005 |           6.0683 |        -964.8885 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |           0.0369 |           5.7881 |       -1113.1965 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |           0.0009 |           5.6232 |       -1346.8165 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |          -0.0123 |           5.5188 |       -1350.6950 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |          -0.0235 |           5.4056 |       -1459.2975 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |          -0.0382 |           5.3195 |       -1534.3438 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |          -0.0351 |           5.2542 |       -1525.4660 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |          -0.0274 |           5.2150 |       -1456.8667 |
[32m[20221208 14:55:59 @agent_ppo2.py:179][0m |          -0.0432 |           5.1572 |       -1565.0767 |
[32m[20221208 14:55:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.19
[32m[20221208 14:56:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.35
[32m[20221208 14:56:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.79
[32m[20221208 14:56:00 @agent_ppo2.py:137][0m Total time:      19.43 min
[32m[20221208 14:56:00 @agent_ppo2.py:139][0m 1570816 total steps have happened
[32m[20221208 14:56:00 @agent_ppo2.py:115][0m #------------------------ Iteration 767 --------------------------#
[32m[20221208 14:56:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:00 @agent_ppo2.py:179][0m |           0.1346 |           6.0115 |       -1048.7885 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |           0.0589 |           5.0467 |        -758.1465 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |           0.0075 |           4.7864 |        -893.6220 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0142 |           4.6306 |        -962.3921 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0317 |           4.5413 |       -1019.1170 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0399 |           4.4392 |       -1032.0925 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0458 |           4.3599 |       -1086.5389 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0548 |           4.4163 |       -1117.1906 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0611 |           4.2681 |       -1131.9747 |
[32m[20221208 14:56:01 @agent_ppo2.py:179][0m |          -0.0672 |           4.2214 |       -1169.7680 |
[32m[20221208 14:56:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.64
[32m[20221208 14:56:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.96
[32m[20221208 14:56:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.08
[32m[20221208 14:56:01 @agent_ppo2.py:137][0m Total time:      19.45 min
[32m[20221208 14:56:01 @agent_ppo2.py:139][0m 1572864 total steps have happened
[32m[20221208 14:56:01 @agent_ppo2.py:115][0m #------------------------ Iteration 768 --------------------------#
[32m[20221208 14:56:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |           0.0512 |           6.5663 |       -1142.0315 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0065 |           5.8922 |       -1052.4535 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0386 |           5.6169 |       -1161.4855 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0408 |           5.4319 |       -1187.3456 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0579 |           5.3523 |       -1267.2080 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0643 |           5.2589 |       -1308.3487 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0692 |           5.1980 |       -1351.3470 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0719 |           5.2034 |       -1390.8809 |
[32m[20221208 14:56:02 @agent_ppo2.py:179][0m |          -0.0780 |           5.0647 |       -1425.7850 |
[32m[20221208 14:56:03 @agent_ppo2.py:179][0m |          -0.0783 |           5.0513 |       -1432.6529 |
[32m[20221208 14:56:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.27
[32m[20221208 14:56:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.82
[32m[20221208 14:56:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.68
[32m[20221208 14:56:03 @agent_ppo2.py:137][0m Total time:      19.48 min
[32m[20221208 14:56:03 @agent_ppo2.py:139][0m 1574912 total steps have happened
[32m[20221208 14:56:03 @agent_ppo2.py:115][0m #------------------------ Iteration 769 --------------------------#
[32m[20221208 14:56:03 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:56:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |           0.0930 |           6.0669 |       -1388.7667 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |           0.0736 |           5.2987 |        -970.2538 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |           0.0247 |           5.0185 |       -1170.9840 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0132 |           4.7859 |       -1294.8991 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0317 |           4.6305 |       -1402.2416 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0451 |           4.5520 |       -1443.6553 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0476 |           4.4257 |       -1483.1561 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0573 |           4.3519 |       -1530.7157 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0553 |           4.2696 |       -1550.0144 |
[32m[20221208 14:56:04 @agent_ppo2.py:179][0m |          -0.0633 |           4.2391 |       -1550.0546 |
[32m[20221208 14:56:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.60
[32m[20221208 14:56:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.83
[32m[20221208 14:56:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.68
[32m[20221208 14:56:04 @agent_ppo2.py:137][0m Total time:      19.50 min
[32m[20221208 14:56:04 @agent_ppo2.py:139][0m 1576960 total steps have happened
[32m[20221208 14:56:04 @agent_ppo2.py:115][0m #------------------------ Iteration 770 --------------------------#
[32m[20221208 14:56:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |           0.1409 |           5.9602 |       -1304.6052 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |           0.0979 |           5.3809 |        -590.2537 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |           0.0384 |           5.1316 |        -893.9069 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |           0.0060 |           4.9508 |       -1050.9434 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |          -0.0097 |           4.8042 |       -1152.9351 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |          -0.0099 |           4.6889 |       -1130.1074 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |          -0.0297 |           4.5782 |       -1206.4531 |
[32m[20221208 14:56:05 @agent_ppo2.py:179][0m |          -0.0445 |           4.4994 |       -1291.2026 |
[32m[20221208 14:56:06 @agent_ppo2.py:179][0m |          -0.0502 |           4.3831 |       -1337.3505 |
[32m[20221208 14:56:06 @agent_ppo2.py:179][0m |          -0.0540 |           4.2997 |       -1375.5258 |
[32m[20221208 14:56:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.20
[32m[20221208 14:56:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.77
[32m[20221208 14:56:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.84
[32m[20221208 14:56:06 @agent_ppo2.py:137][0m Total time:      19.53 min
[32m[20221208 14:56:06 @agent_ppo2.py:139][0m 1579008 total steps have happened
[32m[20221208 14:56:06 @agent_ppo2.py:115][0m #------------------------ Iteration 771 --------------------------#
[32m[20221208 14:56:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |           0.1113 |           5.8917 |       -1341.0307 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |           0.0840 |           5.3086 |        -828.8191 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |           0.0495 |           5.0325 |       -1083.6996 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |           0.0145 |           4.8321 |       -1186.3977 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |          -0.0066 |           4.7256 |       -1316.0579 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |          -0.0236 |           4.6138 |       -1416.4055 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |          -0.0342 |           4.5252 |       -1470.5219 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |          -0.0402 |           4.4500 |       -1494.5598 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |          -0.0454 |           4.3768 |       -1515.0926 |
[32m[20221208 14:56:07 @agent_ppo2.py:179][0m |          -0.0501 |           4.3174 |       -1554.5868 |
[32m[20221208 14:56:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.73
[32m[20221208 14:56:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.45
[32m[20221208 14:56:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.73
[32m[20221208 14:56:08 @agent_ppo2.py:137][0m Total time:      19.55 min
[32m[20221208 14:56:08 @agent_ppo2.py:139][0m 1581056 total steps have happened
[32m[20221208 14:56:08 @agent_ppo2.py:115][0m #------------------------ Iteration 772 --------------------------#
[32m[20221208 14:56:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |           0.1921 |           7.4479 |       -1522.5488 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |           0.1274 |           6.5598 |        -937.7719 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |           0.0668 |           6.1175 |       -1065.1338 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |           0.0272 |           5.8204 |       -1311.5797 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |           0.0153 |           5.5099 |       -1417.6147 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |          -0.0105 |           5.3320 |       -1549.8176 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |          -0.0263 |           5.1819 |       -1634.3020 |
[32m[20221208 14:56:08 @agent_ppo2.py:179][0m |          -0.0354 |           5.1010 |       -1709.8599 |
[32m[20221208 14:56:09 @agent_ppo2.py:179][0m |          -0.0430 |           4.9338 |       -1753.3578 |
[32m[20221208 14:56:09 @agent_ppo2.py:179][0m |          -0.0515 |           4.8498 |       -1808.2079 |
[32m[20221208 14:56:09 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.76
[32m[20221208 14:56:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.15
[32m[20221208 14:56:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.14
[32m[20221208 14:56:09 @agent_ppo2.py:137][0m Total time:      19.58 min
[32m[20221208 14:56:09 @agent_ppo2.py:139][0m 1583104 total steps have happened
[32m[20221208 14:56:09 @agent_ppo2.py:115][0m #------------------------ Iteration 773 --------------------------#
[32m[20221208 14:56:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.1188 |           6.6705 |       -1579.8575 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.2849 |           6.1141 |        -736.6342 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.1525 |           5.9176 |        -487.9974 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.1159 |           5.7375 |        -651.2200 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.0746 |           5.6930 |        -854.9636 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.0411 |           5.5926 |       -1045.0766 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |           0.0239 |           5.5181 |       -1172.8731 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |          -0.0012 |           5.4678 |       -1301.4202 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |          -0.0120 |           5.4063 |       -1395.6257 |
[32m[20221208 14:56:10 @agent_ppo2.py:179][0m |          -0.0242 |           5.3903 |       -1476.6169 |
[32m[20221208 14:56:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:56:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.93
[32m[20221208 14:56:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 168.69
[32m[20221208 14:56:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.00
[32m[20221208 14:56:11 @agent_ppo2.py:137][0m Total time:      19.60 min
[32m[20221208 14:56:11 @agent_ppo2.py:139][0m 1585152 total steps have happened
[32m[20221208 14:56:11 @agent_ppo2.py:115][0m #------------------------ Iteration 774 --------------------------#
[32m[20221208 14:56:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |           0.1272 |           6.8166 |       -1408.7973 |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |           0.0505 |           5.9985 |       -1133.9641 |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |           0.0024 |           5.4619 |       -1388.4527 |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |          -0.0211 |           5.2266 |       -1447.6398 |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |          -0.0296 |           4.9850 |       -1483.8740 |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |          -0.0338 |           4.8285 |       -1480.9947 |
[32m[20221208 14:56:11 @agent_ppo2.py:179][0m |          -0.0439 |           4.7485 |       -1550.6301 |
[32m[20221208 14:56:12 @agent_ppo2.py:179][0m |          -0.0539 |           4.6102 |       -1598.8524 |
[32m[20221208 14:56:12 @agent_ppo2.py:179][0m |          -0.0602 |           4.5786 |       -1642.6632 |
[32m[20221208 14:56:12 @agent_ppo2.py:179][0m |          -0.0636 |           4.4579 |       -1665.0340 |
[32m[20221208 14:56:12 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:56:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.45
[32m[20221208 14:56:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.96
[32m[20221208 14:56:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.22
[32m[20221208 14:56:12 @agent_ppo2.py:137][0m Total time:      19.63 min
[32m[20221208 14:56:12 @agent_ppo2.py:139][0m 1587200 total steps have happened
[32m[20221208 14:56:12 @agent_ppo2.py:115][0m #------------------------ Iteration 775 --------------------------#
[32m[20221208 14:56:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |           0.0777 |           7.1754 |       -1626.7608 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |           0.0969 |           6.7690 |       -1199.5664 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |           0.0399 |           6.5166 |       -1273.7955 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |           0.0283 |           6.3094 |       -1330.8292 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |          -0.0059 |           6.1285 |       -1444.7354 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |          -0.0220 |           5.9923 |       -1549.5257 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |          -0.0327 |           5.9195 |       -1607.5851 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |          -0.0418 |           5.7917 |       -1687.3879 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |          -0.0465 |           5.7148 |       -1712.7258 |
[32m[20221208 14:56:13 @agent_ppo2.py:179][0m |          -0.0491 |           5.6576 |       -1722.7301 |
[32m[20221208 14:56:13 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.98
[32m[20221208 14:56:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.43
[32m[20221208 14:56:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 152.85
[32m[20221208 14:56:14 @agent_ppo2.py:137][0m Total time:      19.66 min
[32m[20221208 14:56:14 @agent_ppo2.py:139][0m 1589248 total steps have happened
[32m[20221208 14:56:14 @agent_ppo2.py:115][0m #------------------------ Iteration 776 --------------------------#
[32m[20221208 14:56:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:14 @agent_ppo2.py:179][0m |           0.1542 |           7.5308 |       -1433.4623 |
[32m[20221208 14:56:14 @agent_ppo2.py:179][0m |           0.0791 |           6.5712 |       -1102.0361 |
[32m[20221208 14:56:14 @agent_ppo2.py:179][0m |           0.0368 |           6.2016 |       -1291.4431 |
[32m[20221208 14:56:14 @agent_ppo2.py:179][0m |          -0.0106 |           5.9895 |       -1495.8139 |
[32m[20221208 14:56:14 @agent_ppo2.py:179][0m |          -0.0217 |           5.8989 |       -1523.9885 |
[32m[20221208 14:56:14 @agent_ppo2.py:179][0m |          -0.0334 |           5.7249 |       -1583.7661 |
[32m[20221208 14:56:15 @agent_ppo2.py:179][0m |          -0.0483 |           5.5428 |       -1657.3162 |
[32m[20221208 14:56:15 @agent_ppo2.py:179][0m |          -0.0531 |           5.4473 |       -1642.5555 |
[32m[20221208 14:56:15 @agent_ppo2.py:179][0m |          -0.0575 |           5.3750 |       -1689.5839 |
[32m[20221208 14:56:15 @agent_ppo2.py:179][0m |          -0.0636 |           5.3064 |       -1722.4487 |
[32m[20221208 14:56:15 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.46
[32m[20221208 14:56:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.04
[32m[20221208 14:56:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.39
[32m[20221208 14:56:15 @agent_ppo2.py:137][0m Total time:      19.68 min
[32m[20221208 14:56:15 @agent_ppo2.py:139][0m 1591296 total steps have happened
[32m[20221208 14:56:15 @agent_ppo2.py:115][0m #------------------------ Iteration 777 --------------------------#
[32m[20221208 14:56:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |           0.0931 |           7.7398 |       -1547.1925 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |           0.0915 |           6.9374 |       -1256.3577 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |           0.0543 |           6.5210 |       -1226.6607 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |           0.0071 |           6.3001 |       -1316.1916 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |          -0.0169 |           6.1518 |       -1484.3273 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |          -0.0342 |           6.0158 |       -1525.2381 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |          -0.0477 |           5.8882 |       -1621.6632 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |          -0.0559 |           5.7726 |       -1644.4306 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |          -0.0605 |           5.6883 |       -1676.7694 |
[32m[20221208 14:56:16 @agent_ppo2.py:179][0m |          -0.0650 |           5.5960 |       -1718.5337 |
[32m[20221208 14:56:16 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.88
[32m[20221208 14:56:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.70
[32m[20221208 14:56:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.49
[32m[20221208 14:56:17 @agent_ppo2.py:137][0m Total time:      19.71 min
[32m[20221208 14:56:17 @agent_ppo2.py:139][0m 1593344 total steps have happened
[32m[20221208 14:56:17 @agent_ppo2.py:115][0m #------------------------ Iteration 778 --------------------------#
[32m[20221208 14:56:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:17 @agent_ppo2.py:179][0m |           0.1207 |           7.7609 |       -1468.3753 |
[32m[20221208 14:56:17 @agent_ppo2.py:179][0m |           0.1130 |           7.2446 |       -1011.2044 |
[32m[20221208 14:56:17 @agent_ppo2.py:179][0m |           0.0339 |           6.9376 |       -1325.3677 |
[32m[20221208 14:56:17 @agent_ppo2.py:179][0m |          -0.0006 |           6.6773 |       -1452.0437 |
[32m[20221208 14:56:17 @agent_ppo2.py:179][0m |          -0.0113 |           6.5437 |       -1524.9626 |
[32m[20221208 14:56:18 @agent_ppo2.py:179][0m |          -0.0293 |           6.3861 |       -1555.3552 |
[32m[20221208 14:56:18 @agent_ppo2.py:179][0m |          -0.0423 |           6.2430 |       -1622.8359 |
[32m[20221208 14:56:18 @agent_ppo2.py:179][0m |          -0.0510 |           6.1516 |       -1670.6366 |
[32m[20221208 14:56:18 @agent_ppo2.py:179][0m |          -0.0575 |           5.9918 |       -1736.7644 |
[32m[20221208 14:56:18 @agent_ppo2.py:179][0m |          -0.0582 |           5.9372 |       -1756.1909 |
[32m[20221208 14:56:18 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.70
[32m[20221208 14:56:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.27
[32m[20221208 14:56:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.44
[32m[20221208 14:56:18 @agent_ppo2.py:137][0m Total time:      19.73 min
[32m[20221208 14:56:18 @agent_ppo2.py:139][0m 1595392 total steps have happened
[32m[20221208 14:56:18 @agent_ppo2.py:115][0m #------------------------ Iteration 779 --------------------------#
[32m[20221208 14:56:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |           0.1106 |           7.9264 |       -1368.8518 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |           0.0719 |           7.4471 |       -1179.1747 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |           0.0281 |           7.1723 |       -1316.3418 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |           0.0189 |           7.0135 |       -1250.1886 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |          -0.0195 |           6.9134 |       -1394.1435 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |          -0.0346 |           6.7823 |       -1440.1812 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |          -0.0477 |           6.7301 |       -1485.6176 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |          -0.0531 |           6.6290 |       -1537.2157 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |          -0.0511 |           6.5575 |       -1534.6499 |
[32m[20221208 14:56:19 @agent_ppo2.py:179][0m |          -0.0566 |           6.4940 |       -1555.5441 |
[32m[20221208 14:56:19 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.16
[32m[20221208 14:56:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.94
[32m[20221208 14:56:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.06
[32m[20221208 14:56:20 @agent_ppo2.py:137][0m Total time:      19.76 min
[32m[20221208 14:56:20 @agent_ppo2.py:139][0m 1597440 total steps have happened
[32m[20221208 14:56:20 @agent_ppo2.py:115][0m #------------------------ Iteration 780 --------------------------#
[32m[20221208 14:56:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:20 @agent_ppo2.py:179][0m |           0.1565 |           7.6312 |       -1336.9624 |
[32m[20221208 14:56:20 @agent_ppo2.py:179][0m |           0.0989 |           6.7860 |        -966.1588 |
[32m[20221208 14:56:20 @agent_ppo2.py:179][0m |           0.0234 |           6.3377 |       -1247.1833 |
[32m[20221208 14:56:20 @agent_ppo2.py:179][0m |          -0.0085 |           6.0625 |       -1407.5337 |
[32m[20221208 14:56:21 @agent_ppo2.py:179][0m |          -0.0207 |           5.8617 |       -1444.6635 |
[32m[20221208 14:56:21 @agent_ppo2.py:179][0m |          -0.0256 |           5.6944 |       -1457.8848 |
[32m[20221208 14:56:21 @agent_ppo2.py:179][0m |          -0.0363 |           5.5274 |       -1447.3523 |
[32m[20221208 14:56:21 @agent_ppo2.py:179][0m |          -0.0478 |           5.4005 |       -1525.5239 |
[32m[20221208 14:56:21 @agent_ppo2.py:179][0m |          -0.0548 |           5.3566 |       -1550.2284 |
[32m[20221208 14:56:21 @agent_ppo2.py:179][0m |          -0.0531 |           5.2156 |       -1563.7397 |
[32m[20221208 14:56:21 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.19
[32m[20221208 14:56:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.61
[32m[20221208 14:56:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.75
[32m[20221208 14:56:21 @agent_ppo2.py:137][0m Total time:      19.78 min
[32m[20221208 14:56:21 @agent_ppo2.py:139][0m 1599488 total steps have happened
[32m[20221208 14:56:21 @agent_ppo2.py:115][0m #------------------------ Iteration 781 --------------------------#
[32m[20221208 14:56:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |           0.0696 |           7.0227 |       -1377.8523 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |           0.0489 |           5.8758 |       -1187.2175 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |           0.0098 |           5.5498 |       -1299.3115 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0182 |           5.3977 |       -1356.8535 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0420 |           5.2401 |       -1406.1616 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0491 |           5.1572 |       -1419.4703 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0570 |           5.0608 |       -1443.6847 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0645 |           4.9828 |       -1459.6844 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0677 |           4.9265 |       -1476.9194 |
[32m[20221208 14:56:22 @agent_ppo2.py:179][0m |          -0.0656 |           4.8740 |       -1474.3008 |
[32m[20221208 14:56:22 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.94
[32m[20221208 14:56:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.46
[32m[20221208 14:56:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.67
[32m[20221208 14:56:23 @agent_ppo2.py:137][0m Total time:      19.81 min
[32m[20221208 14:56:23 @agent_ppo2.py:139][0m 1601536 total steps have happened
[32m[20221208 14:56:23 @agent_ppo2.py:115][0m #------------------------ Iteration 782 --------------------------#
[32m[20221208 14:56:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:23 @agent_ppo2.py:179][0m |           0.1140 |           7.3786 |       -1293.2118 |
[32m[20221208 14:56:23 @agent_ppo2.py:179][0m |           0.1037 |           6.4638 |       -1001.6187 |
[32m[20221208 14:56:23 @agent_ppo2.py:179][0m |           0.0678 |           6.1201 |       -1088.0030 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |           0.0071 |           5.8280 |       -1226.2305 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |          -0.0144 |           5.6197 |       -1309.7829 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |          -0.0244 |           5.4669 |       -1349.2584 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |          -0.0369 |           5.3509 |       -1382.2090 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |          -0.0495 |           5.2367 |       -1437.3638 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |          -0.0562 |           5.1727 |       -1457.7190 |
[32m[20221208 14:56:24 @agent_ppo2.py:179][0m |          -0.0617 |           5.0814 |       -1477.7715 |
[32m[20221208 14:56:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.88
[32m[20221208 14:56:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.68
[32m[20221208 14:56:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.13
[32m[20221208 14:56:24 @agent_ppo2.py:137][0m Total time:      19.83 min
[32m[20221208 14:56:24 @agent_ppo2.py:139][0m 1603584 total steps have happened
[32m[20221208 14:56:24 @agent_ppo2.py:115][0m #------------------------ Iteration 783 --------------------------#
[32m[20221208 14:56:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:56:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |           0.1326 |           7.5418 |       -1230.9030 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |           0.1467 |           6.7007 |        -834.7795 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |           0.0579 |           6.3484 |       -1014.7151 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |           0.0160 |           6.0467 |       -1178.3189 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |          -0.0067 |           5.8500 |       -1238.9839 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |          -0.0198 |           5.7002 |       -1301.9431 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |          -0.0291 |           5.5619 |       -1314.5288 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |          -0.0406 |           5.4223 |       -1364.8513 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |          -0.0463 |           5.3825 |       -1403.0615 |
[32m[20221208 14:56:25 @agent_ppo2.py:179][0m |          -0.0456 |           5.2998 |       -1397.4810 |
[32m[20221208 14:56:25 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.03
[32m[20221208 14:56:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.57
[32m[20221208 14:56:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.33
[32m[20221208 14:56:26 @agent_ppo2.py:137][0m Total time:      19.86 min
[32m[20221208 14:56:26 @agent_ppo2.py:139][0m 1605632 total steps have happened
[32m[20221208 14:56:26 @agent_ppo2.py:115][0m #------------------------ Iteration 784 --------------------------#
[32m[20221208 14:56:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:26 @agent_ppo2.py:179][0m |           0.0924 |           8.3108 |       -1375.3529 |
[32m[20221208 14:56:26 @agent_ppo2.py:179][0m |           0.0380 |           7.4977 |       -1279.7805 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |           0.0253 |           7.1536 |       -1325.8732 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0047 |           6.8886 |       -1360.9178 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0266 |           6.7260 |       -1422.0564 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0403 |           6.5838 |       -1472.2022 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0494 |           6.4387 |       -1516.9027 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0595 |           6.3238 |       -1546.1603 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0638 |           6.3205 |       -1600.6425 |
[32m[20221208 14:56:27 @agent_ppo2.py:179][0m |          -0.0644 |           6.2244 |       -1591.4383 |
[32m[20221208 14:56:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.63
[32m[20221208 14:56:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.80
[32m[20221208 14:56:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.97
[32m[20221208 14:56:27 @agent_ppo2.py:137][0m Total time:      19.88 min
[32m[20221208 14:56:27 @agent_ppo2.py:139][0m 1607680 total steps have happened
[32m[20221208 14:56:27 @agent_ppo2.py:115][0m #------------------------ Iteration 785 --------------------------#
[32m[20221208 14:56:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |           0.0596 |           6.6552 |       -1295.8967 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |           0.0281 |           5.9309 |       -1230.2743 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |           0.0069 |           5.5273 |       -1323.5903 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |           0.0148 |           5.2731 |       -1296.0161 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |          -0.0193 |           5.0793 |       -1381.7607 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |          -0.0347 |           4.9213 |       -1463.1188 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |          -0.0455 |           4.7533 |       -1524.6971 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |          -0.0518 |           4.6669 |       -1527.9151 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |          -0.0518 |           4.5558 |       -1537.7289 |
[32m[20221208 14:56:28 @agent_ppo2.py:179][0m |          -0.0609 |           4.4502 |       -1576.7510 |
[32m[20221208 14:56:28 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.37
[32m[20221208 14:56:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.99
[32m[20221208 14:56:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.90
[32m[20221208 14:56:29 @agent_ppo2.py:137][0m Total time:      19.91 min
[32m[20221208 14:56:29 @agent_ppo2.py:139][0m 1609728 total steps have happened
[32m[20221208 14:56:29 @agent_ppo2.py:115][0m #------------------------ Iteration 786 --------------------------#
[32m[20221208 14:56:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:29 @agent_ppo2.py:179][0m |           0.0739 |           5.6367 |       -1389.9610 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |           0.0391 |           4.6796 |       -1156.2335 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |           0.0256 |           4.2785 |       -1196.1884 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |           0.0100 |           4.0429 |       -1248.2035 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |          -0.0249 |           3.8864 |       -1346.8519 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |          -0.0451 |           3.7596 |       -1417.6149 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |          -0.0526 |           3.7128 |       -1447.6557 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |          -0.0592 |           3.5996 |       -1474.9146 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |          -0.0645 |           3.5533 |       -1530.4720 |
[32m[20221208 14:56:30 @agent_ppo2.py:179][0m |          -0.0648 |           3.5022 |       -1522.0656 |
[32m[20221208 14:56:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 157.05
[32m[20221208 14:56:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 168.62
[32m[20221208 14:56:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.77
[32m[20221208 14:56:30 @agent_ppo2.py:137][0m Total time:      19.94 min
[32m[20221208 14:56:30 @agent_ppo2.py:139][0m 1611776 total steps have happened
[32m[20221208 14:56:30 @agent_ppo2.py:115][0m #------------------------ Iteration 787 --------------------------#
[32m[20221208 14:56:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |           0.1235 |           7.8814 |       -1393.8723 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |           0.1094 |           7.2482 |        -992.5389 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |           0.0483 |           7.0383 |       -1106.0774 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |           0.0083 |           6.8110 |       -1263.0742 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |          -0.0196 |           6.6026 |       -1365.1997 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |          -0.0320 |           6.4986 |       -1415.1851 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |          -0.0412 |           6.4314 |       -1471.3815 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |          -0.0493 |           6.3435 |       -1517.3949 |
[32m[20221208 14:56:31 @agent_ppo2.py:179][0m |          -0.0503 |           6.2663 |       -1531.5556 |
[32m[20221208 14:56:32 @agent_ppo2.py:179][0m |          -0.0514 |           6.1888 |       -1546.4194 |
[32m[20221208 14:56:32 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.05
[32m[20221208 14:56:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.68
[32m[20221208 14:56:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.19
[32m[20221208 14:56:32 @agent_ppo2.py:137][0m Total time:      19.96 min
[32m[20221208 14:56:32 @agent_ppo2.py:139][0m 1613824 total steps have happened
[32m[20221208 14:56:32 @agent_ppo2.py:115][0m #------------------------ Iteration 788 --------------------------#
[32m[20221208 14:56:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:32 @agent_ppo2.py:179][0m |           0.0862 |           8.7167 |       -1381.2639 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |           0.0617 |           7.5595 |       -1204.9473 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |           0.0080 |           7.0627 |       -1366.3450 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0158 |           6.7257 |       -1454.4657 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0321 |           6.5498 |       -1476.1575 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0472 |           6.3613 |       -1507.1371 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0550 |           6.2141 |       -1553.9457 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0600 |           6.0282 |       -1568.9996 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0633 |           5.9878 |       -1590.0915 |
[32m[20221208 14:56:33 @agent_ppo2.py:179][0m |          -0.0680 |           5.8429 |       -1605.2036 |
[32m[20221208 14:56:33 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.98
[32m[20221208 14:56:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.68
[32m[20221208 14:56:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.59
[32m[20221208 14:56:33 @agent_ppo2.py:137][0m Total time:      19.99 min
[32m[20221208 14:56:33 @agent_ppo2.py:139][0m 1615872 total steps have happened
[32m[20221208 14:56:33 @agent_ppo2.py:115][0m #------------------------ Iteration 789 --------------------------#
[32m[20221208 14:56:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |           0.1130 |           8.4780 |       -1278.8638 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |           0.2075 |           8.0439 |        -747.9047 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |           0.1083 |           7.8628 |        -860.0052 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |           0.0410 |           7.7086 |       -1101.3291 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |           0.0106 |           7.4719 |       -1194.8992 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |          -0.0019 |           7.3778 |       -1233.0666 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |          -0.0172 |           7.3343 |       -1300.4558 |
[32m[20221208 14:56:34 @agent_ppo2.py:179][0m |          -0.0323 |           7.2073 |       -1353.1073 |
[32m[20221208 14:56:35 @agent_ppo2.py:179][0m |          -0.0328 |           7.0809 |       -1369.5988 |
[32m[20221208 14:56:35 @agent_ppo2.py:179][0m |          -0.0453 |           7.0460 |       -1390.1987 |
[32m[20221208 14:56:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.93
[32m[20221208 14:56:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.24
[32m[20221208 14:56:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.75
[32m[20221208 14:56:35 @agent_ppo2.py:137][0m Total time:      20.01 min
[32m[20221208 14:56:35 @agent_ppo2.py:139][0m 1617920 total steps have happened
[32m[20221208 14:56:35 @agent_ppo2.py:115][0m #------------------------ Iteration 790 --------------------------#
[32m[20221208 14:56:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |           0.0748 |           5.7930 |       -1111.3424 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |           0.0420 |           4.7513 |        -904.5360 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0079 |           4.4012 |        -992.3352 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0356 |           4.1503 |       -1092.0683 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0466 |           4.0215 |       -1122.8604 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0570 |           3.9273 |       -1182.2079 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0593 |           3.8301 |       -1195.0109 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0643 |           3.7156 |       -1253.9262 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0690 |           3.6981 |       -1277.2540 |
[32m[20221208 14:56:36 @agent_ppo2.py:179][0m |          -0.0703 |           3.6477 |       -1279.1129 |
[32m[20221208 14:56:36 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.94
[32m[20221208 14:56:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.72
[32m[20221208 14:56:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.08
[32m[20221208 14:56:37 @agent_ppo2.py:137][0m Total time:      20.04 min
[32m[20221208 14:56:37 @agent_ppo2.py:139][0m 1619968 total steps have happened
[32m[20221208 14:56:37 @agent_ppo2.py:115][0m #------------------------ Iteration 791 --------------------------#
[32m[20221208 14:56:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:56:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |           0.4900 |           7.2262 |       -1322.3038 |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |           0.0943 |           6.2362 |       -1060.2757 |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |           0.0269 |           5.8060 |       -1276.4247 |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |          -0.0083 |           5.4407 |       -1354.4228 |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |          -0.0287 |           5.2232 |       -1431.6253 |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |          -0.0382 |           5.0713 |       -1488.8122 |
[32m[20221208 14:56:37 @agent_ppo2.py:179][0m |          -0.0481 |           4.8681 |       -1532.1151 |
[32m[20221208 14:56:38 @agent_ppo2.py:179][0m |          -0.0512 |           4.7402 |       -1557.3251 |
[32m[20221208 14:56:38 @agent_ppo2.py:179][0m |          -0.0545 |           4.6174 |       -1594.0497 |
[32m[20221208 14:56:38 @agent_ppo2.py:179][0m |          -0.0600 |           4.5280 |       -1629.7373 |
[32m[20221208 14:56:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.68
[32m[20221208 14:56:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.49
[32m[20221208 14:56:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.54
[32m[20221208 14:56:38 @agent_ppo2.py:137][0m Total time:      20.06 min
[32m[20221208 14:56:38 @agent_ppo2.py:139][0m 1622016 total steps have happened
[32m[20221208 14:56:38 @agent_ppo2.py:115][0m #------------------------ Iteration 792 --------------------------#
[32m[20221208 14:56:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |           0.1666 |           6.1777 |       -1064.3383 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |           0.0467 |           5.5778 |        -849.2411 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0100 |           5.3371 |       -1050.2622 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0346 |           5.1579 |       -1110.8619 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0517 |           5.0457 |       -1147.0781 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0607 |           4.9368 |       -1179.6702 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0620 |           4.8698 |       -1205.4659 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0651 |           4.7838 |       -1216.0717 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0713 |           4.7437 |       -1250.3468 |
[32m[20221208 14:56:39 @agent_ppo2.py:179][0m |          -0.0758 |           4.6821 |       -1268.5720 |
[32m[20221208 14:56:39 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.31
[32m[20221208 14:56:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.00
[32m[20221208 14:56:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.11
[32m[20221208 14:56:40 @agent_ppo2.py:137][0m Total time:      20.09 min
[32m[20221208 14:56:40 @agent_ppo2.py:139][0m 1624064 total steps have happened
[32m[20221208 14:56:40 @agent_ppo2.py:115][0m #------------------------ Iteration 793 --------------------------#
[32m[20221208 14:56:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:40 @agent_ppo2.py:179][0m |           0.1301 |           5.9751 |       -1195.0628 |
[32m[20221208 14:56:40 @agent_ppo2.py:179][0m |           0.0893 |           5.4946 |        -681.9869 |
[32m[20221208 14:56:40 @agent_ppo2.py:179][0m |           0.0405 |           5.2884 |        -950.0857 |
[32m[20221208 14:56:40 @agent_ppo2.py:179][0m |           0.0239 |           5.1685 |       -1038.0398 |
[32m[20221208 14:56:40 @agent_ppo2.py:179][0m |           0.0013 |           5.0568 |       -1095.3131 |
[32m[20221208 14:56:40 @agent_ppo2.py:179][0m |          -0.0194 |           4.9977 |       -1240.5729 |
[32m[20221208 14:56:41 @agent_ppo2.py:179][0m |          -0.0315 |           4.8961 |       -1314.7794 |
[32m[20221208 14:56:41 @agent_ppo2.py:179][0m |          -0.0409 |           4.8345 |       -1377.4511 |
[32m[20221208 14:56:41 @agent_ppo2.py:179][0m |          -0.0476 |           4.7583 |       -1415.1224 |
[32m[20221208 14:56:41 @agent_ppo2.py:179][0m |          -0.0505 |           4.7310 |       -1461.0383 |
[32m[20221208 14:56:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.42
[32m[20221208 14:56:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.53
[32m[20221208 14:56:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.64
[32m[20221208 14:56:41 @agent_ppo2.py:137][0m Total time:      20.11 min
[32m[20221208 14:56:41 @agent_ppo2.py:139][0m 1626112 total steps have happened
[32m[20221208 14:56:41 @agent_ppo2.py:115][0m #------------------------ Iteration 794 --------------------------#
[32m[20221208 14:56:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |           0.0906 |           6.8684 |       -1519.2331 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |           0.0546 |           5.9710 |       -1367.4648 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |           0.0037 |           5.5944 |       -1517.6666 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0148 |           5.4209 |       -1524.4631 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0360 |           5.3417 |       -1588.8556 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0436 |           5.2121 |       -1625.3955 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0511 |           5.1522 |       -1644.7816 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0616 |           5.1181 |       -1701.6226 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0663 |           5.0510 |       -1721.5134 |
[32m[20221208 14:56:42 @agent_ppo2.py:179][0m |          -0.0690 |           5.0509 |       -1756.4366 |
[32m[20221208 14:56:42 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.65
[32m[20221208 14:56:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.18
[32m[20221208 14:56:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.06
[32m[20221208 14:56:43 @agent_ppo2.py:137][0m Total time:      20.14 min
[32m[20221208 14:56:43 @agent_ppo2.py:139][0m 1628160 total steps have happened
[32m[20221208 14:56:43 @agent_ppo2.py:115][0m #------------------------ Iteration 795 --------------------------#
[32m[20221208 14:56:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:43 @agent_ppo2.py:179][0m |           0.1163 |           7.7166 |       -1535.9647 |
[32m[20221208 14:56:43 @agent_ppo2.py:179][0m |           0.0958 |           6.2537 |       -1144.1788 |
[32m[20221208 14:56:43 @agent_ppo2.py:179][0m |           0.0420 |           5.5909 |       -1239.0607 |
[32m[20221208 14:56:43 @agent_ppo2.py:179][0m |           0.0338 |           5.2816 |       -1252.6790 |
[32m[20221208 14:56:43 @agent_ppo2.py:179][0m |          -0.0181 |           4.9757 |       -1443.8641 |
[32m[20221208 14:56:43 @agent_ppo2.py:179][0m |          -0.0360 |           4.7335 |       -1532.5254 |
[32m[20221208 14:56:44 @agent_ppo2.py:179][0m |          -0.0493 |           4.6055 |       -1604.1557 |
[32m[20221208 14:56:44 @agent_ppo2.py:179][0m |          -0.0559 |           4.4362 |       -1665.0799 |
[32m[20221208 14:56:44 @agent_ppo2.py:179][0m |          -0.0637 |           4.3239 |       -1691.6454 |
[32m[20221208 14:56:44 @agent_ppo2.py:179][0m |          -0.0686 |           4.2211 |       -1728.7057 |
[32m[20221208 14:56:44 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.70
[32m[20221208 14:56:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 193.61
[32m[20221208 14:56:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.98
[32m[20221208 14:56:44 @agent_ppo2.py:137][0m Total time:      20.16 min
[32m[20221208 14:56:44 @agent_ppo2.py:139][0m 1630208 total steps have happened
[32m[20221208 14:56:44 @agent_ppo2.py:115][0m #------------------------ Iteration 796 --------------------------#
[32m[20221208 14:56:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |           0.1384 |           8.1293 |       -1495.0681 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |           0.0598 |           7.4284 |       -1240.9843 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |           0.0443 |           7.0968 |       -1448.2035 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |           0.0159 |           6.8656 |       -1538.2731 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |          -0.0206 |           6.6765 |       -1682.9913 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |          -0.0373 |           6.4782 |       -1750.1867 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |          -0.0444 |           6.3354 |       -1795.8432 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |          -0.0424 |           6.2086 |       -1805.5865 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |          -0.0505 |           6.1010 |       -1817.9342 |
[32m[20221208 14:56:45 @agent_ppo2.py:179][0m |          -0.0591 |           6.0240 |       -1886.8618 |
[32m[20221208 14:56:45 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.75
[32m[20221208 14:56:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.89
[32m[20221208 14:56:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.92
[32m[20221208 14:56:46 @agent_ppo2.py:137][0m Total time:      20.19 min
[32m[20221208 14:56:46 @agent_ppo2.py:139][0m 1632256 total steps have happened
[32m[20221208 14:56:46 @agent_ppo2.py:115][0m #------------------------ Iteration 797 --------------------------#
[32m[20221208 14:56:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:46 @agent_ppo2.py:179][0m |           0.0973 |           7.6787 |       -1550.3442 |
[32m[20221208 14:56:46 @agent_ppo2.py:179][0m |           0.1206 |           7.1798 |       -1292.5376 |
[32m[20221208 14:56:46 @agent_ppo2.py:179][0m |           0.0525 |           6.9105 |       -1276.5210 |
[32m[20221208 14:56:46 @agent_ppo2.py:179][0m |           0.0212 |           6.7119 |       -1474.6290 |
[32m[20221208 14:56:46 @agent_ppo2.py:179][0m |           0.0043 |           6.5537 |       -1531.9931 |
[32m[20221208 14:56:47 @agent_ppo2.py:179][0m |          -0.0191 |           6.4450 |       -1654.5581 |
[32m[20221208 14:56:47 @agent_ppo2.py:179][0m |          -0.0284 |           6.3588 |       -1724.8268 |
[32m[20221208 14:56:47 @agent_ppo2.py:179][0m |          -0.0257 |           6.3172 |       -1693.3360 |
[32m[20221208 14:56:47 @agent_ppo2.py:179][0m |          -0.0402 |           6.2239 |       -1793.6326 |
[32m[20221208 14:56:47 @agent_ppo2.py:179][0m |          -0.0454 |           6.1673 |       -1844.6430 |
[32m[20221208 14:56:47 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.30
[32m[20221208 14:56:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.02
[32m[20221208 14:56:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.86
[32m[20221208 14:56:47 @agent_ppo2.py:137][0m Total time:      20.22 min
[32m[20221208 14:56:47 @agent_ppo2.py:139][0m 1634304 total steps have happened
[32m[20221208 14:56:47 @agent_ppo2.py:115][0m #------------------------ Iteration 798 --------------------------#
[32m[20221208 14:56:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |           0.0734 |           7.2726 |       -1685.9902 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |           0.0750 |           6.5470 |       -1493.5496 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |           0.0285 |           6.1432 |       -1557.0295 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0051 |           5.9188 |       -1661.8807 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0262 |           5.7361 |       -1739.2048 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0339 |           5.5739 |       -1757.4035 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0414 |           5.4381 |       -1802.2499 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0551 |           5.3114 |       -1839.8360 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0596 |           5.1937 |       -1868.8779 |
[32m[20221208 14:56:48 @agent_ppo2.py:179][0m |          -0.0623 |           5.1344 |       -1881.4545 |
[32m[20221208 14:56:48 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.09
[32m[20221208 14:56:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.49
[32m[20221208 14:56:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.83
[32m[20221208 14:56:49 @agent_ppo2.py:137][0m Total time:      20.24 min
[32m[20221208 14:56:49 @agent_ppo2.py:139][0m 1636352 total steps have happened
[32m[20221208 14:56:49 @agent_ppo2.py:115][0m #------------------------ Iteration 799 --------------------------#
[32m[20221208 14:56:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:49 @agent_ppo2.py:179][0m |           0.1378 |           8.0252 |       -1493.9037 |
[32m[20221208 14:56:49 @agent_ppo2.py:179][0m |           0.1171 |           7.5605 |        -990.2327 |
[32m[20221208 14:56:49 @agent_ppo2.py:179][0m |           0.0779 |           7.2839 |       -1126.7708 |
[32m[20221208 14:56:49 @agent_ppo2.py:179][0m |           0.0476 |           7.1319 |       -1261.2045 |
[32m[20221208 14:56:50 @agent_ppo2.py:179][0m |           0.0138 |           7.0969 |       -1412.9948 |
[32m[20221208 14:56:50 @agent_ppo2.py:179][0m |          -0.0046 |           6.9939 |       -1547.2909 |
[32m[20221208 14:56:50 @agent_ppo2.py:179][0m |          -0.0165 |           6.8899 |       -1632.6195 |
[32m[20221208 14:56:50 @agent_ppo2.py:179][0m |          -0.0106 |           6.8192 |       -1627.9773 |
[32m[20221208 14:56:50 @agent_ppo2.py:179][0m |          -0.0261 |           6.7733 |       -1698.6068 |
[32m[20221208 14:56:50 @agent_ppo2.py:179][0m |          -0.0318 |           6.7256 |       -1721.3220 |
[32m[20221208 14:56:50 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.89
[32m[20221208 14:56:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.32
[32m[20221208 14:56:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.84
[32m[20221208 14:56:50 @agent_ppo2.py:137][0m Total time:      20.27 min
[32m[20221208 14:56:50 @agent_ppo2.py:139][0m 1638400 total steps have happened
[32m[20221208 14:56:50 @agent_ppo2.py:115][0m #------------------------ Iteration 800 --------------------------#
[32m[20221208 14:56:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |           0.0756 |           7.3055 |       -1565.6453 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |           0.0428 |           6.7652 |       -1429.8933 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |           0.0112 |           6.5090 |       -1509.6260 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0206 |           6.3626 |       -1632.4506 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0337 |           6.2909 |       -1705.7995 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0409 |           6.1547 |       -1718.2180 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0514 |           6.1134 |       -1751.3658 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0562 |           6.0510 |       -1815.7532 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0499 |           5.9860 |       -1798.5225 |
[32m[20221208 14:56:51 @agent_ppo2.py:179][0m |          -0.0595 |           5.9063 |       -1831.8456 |
[32m[20221208 14:56:51 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:56:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.29
[32m[20221208 14:56:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.54
[32m[20221208 14:56:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.06
[32m[20221208 14:56:52 @agent_ppo2.py:137][0m Total time:      20.29 min
[32m[20221208 14:56:52 @agent_ppo2.py:139][0m 1640448 total steps have happened
[32m[20221208 14:56:52 @agent_ppo2.py:115][0m #------------------------ Iteration 801 --------------------------#
[32m[20221208 14:56:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:52 @agent_ppo2.py:179][0m |           0.0918 |           7.6266 |       -1694.4624 |
[32m[20221208 14:56:52 @agent_ppo2.py:179][0m |           0.0544 |           6.9758 |       -1505.9287 |
[32m[20221208 14:56:52 @agent_ppo2.py:179][0m |           0.0195 |           6.7113 |       -1635.8169 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0077 |           6.4734 |       -1684.3660 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0335 |           6.3262 |       -1763.7264 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0464 |           6.1788 |       -1805.1584 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0506 |           6.0919 |       -1844.4569 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0586 |           5.9725 |       -1884.5542 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0614 |           5.8654 |       -1898.6359 |
[32m[20221208 14:56:53 @agent_ppo2.py:179][0m |          -0.0603 |           5.7434 |       -1913.8587 |
[32m[20221208 14:56:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.85
[32m[20221208 14:56:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.13
[32m[20221208 14:56:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.74
[32m[20221208 14:56:53 @agent_ppo2.py:137][0m Total time:      20.32 min
[32m[20221208 14:56:53 @agent_ppo2.py:139][0m 1642496 total steps have happened
[32m[20221208 14:56:53 @agent_ppo2.py:115][0m #------------------------ Iteration 802 --------------------------#
[32m[20221208 14:56:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |           0.0835 |           6.9026 |       -1448.8183 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |           0.0497 |           5.9509 |       -1286.1342 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0028 |           5.6351 |       -1466.5279 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0267 |           5.3125 |       -1548.7780 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0352 |           5.1550 |       -1589.1951 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0433 |           5.0217 |       -1610.7070 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0525 |           4.9178 |       -1625.1542 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0604 |           4.8061 |       -1669.2984 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0622 |           4.7310 |       -1704.6901 |
[32m[20221208 14:56:54 @agent_ppo2.py:179][0m |          -0.0665 |           4.6786 |       -1729.6199 |
[32m[20221208 14:56:54 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:56:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.03
[32m[20221208 14:56:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.61
[32m[20221208 14:56:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.60
[32m[20221208 14:56:55 @agent_ppo2.py:137][0m Total time:      20.34 min
[32m[20221208 14:56:55 @agent_ppo2.py:139][0m 1644544 total steps have happened
[32m[20221208 14:56:55 @agent_ppo2.py:115][0m #------------------------ Iteration 803 --------------------------#
[32m[20221208 14:56:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:55 @agent_ppo2.py:179][0m |           0.0945 |           7.6022 |       -1806.2817 |
[32m[20221208 14:56:55 @agent_ppo2.py:179][0m |           0.0892 |           7.0597 |       -1143.2686 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |           0.0524 |           6.7864 |       -1219.6810 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |           0.0319 |           6.5622 |       -1362.4745 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |           0.0072 |           6.4004 |       -1561.3975 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |          -0.0016 |           6.2714 |       -1625.9351 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |          -0.0190 |           6.1629 |       -1733.4940 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |          -0.0330 |           6.0584 |       -1873.6503 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |          -0.0405 |           5.9962 |       -1939.4569 |
[32m[20221208 14:56:56 @agent_ppo2.py:179][0m |          -0.0428 |           5.9224 |       -2004.0765 |
[32m[20221208 14:56:56 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.12
[32m[20221208 14:56:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.97
[32m[20221208 14:56:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 157.87
[32m[20221208 14:56:56 @agent_ppo2.py:137][0m Total time:      20.37 min
[32m[20221208 14:56:56 @agent_ppo2.py:139][0m 1646592 total steps have happened
[32m[20221208 14:56:56 @agent_ppo2.py:115][0m #------------------------ Iteration 804 --------------------------#
[32m[20221208 14:56:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:56:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |           0.0959 |           7.8418 |       -1615.0225 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |           0.1406 |           7.3248 |       -1139.9755 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |           0.0570 |           7.1483 |       -1500.3574 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |           0.0140 |           7.0068 |       -1576.7262 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |          -0.0097 |           6.8722 |       -1743.9498 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |          -0.0246 |           6.8115 |       -1784.2357 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |          -0.0298 |           6.7443 |       -1821.7620 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |          -0.0398 |           6.6897 |       -1877.7038 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |          -0.0482 |           6.6606 |       -1913.6817 |
[32m[20221208 14:56:57 @agent_ppo2.py:179][0m |          -0.0471 |           6.5625 |       -1913.8675 |
[32m[20221208 14:56:57 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:56:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.26
[32m[20221208 14:56:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.48
[32m[20221208 14:56:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.99
[32m[20221208 14:56:58 @agent_ppo2.py:137][0m Total time:      20.39 min
[32m[20221208 14:56:58 @agent_ppo2.py:139][0m 1648640 total steps have happened
[32m[20221208 14:56:58 @agent_ppo2.py:115][0m #------------------------ Iteration 805 --------------------------#
[32m[20221208 14:56:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:56:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:56:58 @agent_ppo2.py:179][0m |           0.1023 |           7.5192 |       -1678.1021 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |           0.1881 |           7.0532 |       -1192.0091 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |           0.0478 |           6.8283 |       -1361.2389 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |           0.0121 |           6.6907 |       -1552.1761 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |          -0.0080 |           6.5208 |       -1679.1753 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |          -0.0148 |           6.4429 |       -1728.1102 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |          -0.0286 |           6.3176 |       -1796.9238 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |          -0.0354 |           6.2354 |       -1844.4559 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |          -0.0427 |           6.1380 |       -1905.6969 |
[32m[20221208 14:56:59 @agent_ppo2.py:179][0m |          -0.0410 |           6.1022 |       -1875.2969 |
[32m[20221208 14:56:59 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:56:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.40
[32m[20221208 14:56:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.67
[32m[20221208 14:56:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.39
[32m[20221208 14:56:59 @agent_ppo2.py:137][0m Total time:      20.42 min
[32m[20221208 14:56:59 @agent_ppo2.py:139][0m 1650688 total steps have happened
[32m[20221208 14:56:59 @agent_ppo2.py:115][0m #------------------------ Iteration 806 --------------------------#
[32m[20221208 14:57:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |           0.2867 |           6.8103 |       -1204.4189 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |           0.1013 |           6.4074 |        -750.8925 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |           0.0308 |           6.3013 |        -954.2338 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |          -0.0014 |           6.1237 |       -1112.2558 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |          -0.0198 |           6.0362 |       -1188.5340 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |          -0.0349 |           6.0033 |       -1268.6926 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |          -0.0394 |           5.9212 |       -1280.1454 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |          -0.0450 |           5.8645 |       -1329.4922 |
[32m[20221208 14:57:00 @agent_ppo2.py:179][0m |          -0.0499 |           5.8008 |       -1347.8231 |
[32m[20221208 14:57:01 @agent_ppo2.py:179][0m |          -0.0536 |           5.7292 |       -1375.0380 |
[32m[20221208 14:57:01 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.93
[32m[20221208 14:57:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.42
[32m[20221208 14:57:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.11
[32m[20221208 14:57:01 @agent_ppo2.py:137][0m Total time:      20.45 min
[32m[20221208 14:57:01 @agent_ppo2.py:139][0m 1652736 total steps have happened
[32m[20221208 14:57:01 @agent_ppo2.py:115][0m #------------------------ Iteration 807 --------------------------#
[32m[20221208 14:57:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |           0.0619 |           7.3263 |       -1710.9260 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |           0.0503 |           6.8553 |       -1400.2115 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |           0.0154 |           6.6590 |       -1547.4727 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0112 |           6.5026 |       -1667.5114 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0253 |           6.3868 |       -1738.8746 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0393 |           6.2822 |       -1831.5722 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0493 |           6.2568 |       -1898.3498 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0522 |           6.1524 |       -1926.3268 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0519 |           6.1240 |       -1917.1840 |
[32m[20221208 14:57:02 @agent_ppo2.py:179][0m |          -0.0577 |           6.2236 |       -1986.8547 |
[32m[20221208 14:57:02 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.44
[32m[20221208 14:57:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.77
[32m[20221208 14:57:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 168.06
[32m[20221208 14:57:03 @agent_ppo2.py:137][0m Total time:      20.47 min
[32m[20221208 14:57:03 @agent_ppo2.py:139][0m 1654784 total steps have happened
[32m[20221208 14:57:03 @agent_ppo2.py:115][0m #------------------------ Iteration 808 --------------------------#
[32m[20221208 14:57:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |           0.0992 |           6.9554 |       -1575.5578 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |           0.1255 |           6.3335 |       -1140.1445 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |           0.0480 |           6.0630 |       -1225.7050 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |           0.0237 |           5.8146 |       -1368.0732 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |          -0.0057 |           5.6642 |       -1513.8497 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |          -0.0216 |           5.5322 |       -1605.2101 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |          -0.0307 |           5.4140 |       -1700.6644 |
[32m[20221208 14:57:03 @agent_ppo2.py:179][0m |          -0.0386 |           5.3015 |       -1742.2412 |
[32m[20221208 14:57:04 @agent_ppo2.py:179][0m |          -0.0460 |           5.2487 |       -1806.3991 |
[32m[20221208 14:57:04 @agent_ppo2.py:179][0m |          -0.0468 |           5.1303 |       -1798.6576 |
[32m[20221208 14:57:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:57:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.02
[32m[20221208 14:57:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.16
[32m[20221208 14:57:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.40
[32m[20221208 14:57:04 @agent_ppo2.py:137][0m Total time:      20.50 min
[32m[20221208 14:57:04 @agent_ppo2.py:139][0m 1656832 total steps have happened
[32m[20221208 14:57:04 @agent_ppo2.py:115][0m #------------------------ Iteration 809 --------------------------#
[32m[20221208 14:57:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |           0.1384 |           8.4203 |       -1455.3319 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |           0.0661 |           7.8518 |       -1238.5685 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |           0.0123 |           7.5898 |       -1455.9066 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0016 |           7.3689 |       -1518.9257 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0242 |           7.1693 |       -1566.4225 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0404 |           7.0852 |       -1622.6019 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0514 |           7.0159 |       -1665.9784 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0559 |           6.8886 |       -1696.4854 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0590 |           6.7954 |       -1725.1379 |
[32m[20221208 14:57:05 @agent_ppo2.py:179][0m |          -0.0594 |           6.7292 |       -1735.0250 |
[32m[20221208 14:57:05 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.35
[32m[20221208 14:57:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.32
[32m[20221208 14:57:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.88
[32m[20221208 14:57:06 @agent_ppo2.py:137][0m Total time:      20.52 min
[32m[20221208 14:57:06 @agent_ppo2.py:139][0m 1658880 total steps have happened
[32m[20221208 14:57:06 @agent_ppo2.py:115][0m #------------------------ Iteration 810 --------------------------#
[32m[20221208 14:57:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |           0.1629 |           7.8374 |       -1499.8658 |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |           0.0571 |           6.8383 |       -1197.5651 |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |           0.0068 |           6.4932 |       -1419.5888 |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |          -0.0145 |           6.2965 |       -1535.2684 |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |          -0.0338 |           6.1133 |       -1600.2317 |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |          -0.0364 |           6.0057 |       -1646.2029 |
[32m[20221208 14:57:06 @agent_ppo2.py:179][0m |          -0.0467 |           5.8538 |       -1706.8215 |
[32m[20221208 14:57:07 @agent_ppo2.py:179][0m |          -0.0531 |           5.7599 |       -1735.9066 |
[32m[20221208 14:57:07 @agent_ppo2.py:179][0m |          -0.0581 |           5.7332 |       -1764.5496 |
[32m[20221208 14:57:07 @agent_ppo2.py:179][0m |          -0.0615 |           5.6215 |       -1771.1427 |
[32m[20221208 14:57:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.18
[32m[20221208 14:57:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.83
[32m[20221208 14:57:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.72
[32m[20221208 14:57:07 @agent_ppo2.py:137][0m Total time:      20.55 min
[32m[20221208 14:57:07 @agent_ppo2.py:139][0m 1660928 total steps have happened
[32m[20221208 14:57:07 @agent_ppo2.py:115][0m #------------------------ Iteration 811 --------------------------#
[32m[20221208 14:57:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |           0.1421 |           7.7163 |       -1496.2368 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |           0.1393 |           7.0146 |        -783.9913 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |           0.0774 |           6.7989 |        -914.1866 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |           0.0543 |           6.6254 |       -1140.5105 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |           0.0271 |           6.5479 |       -1200.7850 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |          -0.0005 |           6.4199 |       -1385.9668 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |          -0.0136 |           6.3852 |       -1458.6410 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |          -0.0249 |           6.2750 |       -1516.7674 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |          -0.0371 |           6.2004 |       -1575.0165 |
[32m[20221208 14:57:08 @agent_ppo2.py:179][0m |          -0.0442 |           6.1667 |       -1644.6573 |
[32m[20221208 14:57:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.79
[32m[20221208 14:57:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.83
[32m[20221208 14:57:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.82
[32m[20221208 14:57:09 @agent_ppo2.py:137][0m Total time:      20.57 min
[32m[20221208 14:57:09 @agent_ppo2.py:139][0m 1662976 total steps have happened
[32m[20221208 14:57:09 @agent_ppo2.py:115][0m #------------------------ Iteration 812 --------------------------#
[32m[20221208 14:57:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:09 @agent_ppo2.py:179][0m |           0.1196 |           8.0684 |       -1331.6415 |
[32m[20221208 14:57:09 @agent_ppo2.py:179][0m |           0.1187 |           7.6382 |        -955.3351 |
[32m[20221208 14:57:09 @agent_ppo2.py:179][0m |           0.0388 |           7.3643 |       -1175.7538 |
[32m[20221208 14:57:09 @agent_ppo2.py:179][0m |           0.0153 |           7.2531 |       -1319.3315 |
[32m[20221208 14:57:09 @agent_ppo2.py:179][0m |           0.0008 |           7.1135 |       -1296.6779 |
[32m[20221208 14:57:09 @agent_ppo2.py:179][0m |          -0.0205 |           6.9940 |       -1414.0403 |
[32m[20221208 14:57:10 @agent_ppo2.py:179][0m |          -0.0219 |           6.8981 |       -1417.8105 |
[32m[20221208 14:57:10 @agent_ppo2.py:179][0m |          -0.0370 |           6.8613 |       -1466.0587 |
[32m[20221208 14:57:10 @agent_ppo2.py:179][0m |          -0.0432 |           6.8046 |       -1505.8023 |
[32m[20221208 14:57:10 @agent_ppo2.py:179][0m |          -0.0490 |           6.6828 |       -1544.3788 |
[32m[20221208 14:57:10 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.50
[32m[20221208 14:57:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.09
[32m[20221208 14:57:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.93
[32m[20221208 14:57:10 @agent_ppo2.py:137][0m Total time:      20.60 min
[32m[20221208 14:57:10 @agent_ppo2.py:139][0m 1665024 total steps have happened
[32m[20221208 14:57:10 @agent_ppo2.py:115][0m #------------------------ Iteration 813 --------------------------#
[32m[20221208 14:57:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |           0.1176 |           8.4392 |       -1351.9768 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |           0.1245 |           7.7402 |        -828.1058 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |           0.0855 |           7.4536 |        -838.8182 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |           0.0487 |           7.2690 |        -939.7131 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |           0.0192 |           7.0885 |       -1062.9592 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |          -0.0076 |           7.0110 |       -1188.4961 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |          -0.0194 |           6.9009 |       -1296.0003 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |          -0.0303 |           6.7994 |       -1347.0303 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |          -0.0365 |           6.6719 |       -1381.3106 |
[32m[20221208 14:57:11 @agent_ppo2.py:179][0m |          -0.0452 |           6.6045 |       -1439.1935 |
[32m[20221208 14:57:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.41
[32m[20221208 14:57:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.64
[32m[20221208 14:57:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.05
[32m[20221208 14:57:12 @agent_ppo2.py:137][0m Total time:      20.62 min
[32m[20221208 14:57:12 @agent_ppo2.py:139][0m 1667072 total steps have happened
[32m[20221208 14:57:12 @agent_ppo2.py:115][0m #------------------------ Iteration 814 --------------------------#
[32m[20221208 14:57:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:12 @agent_ppo2.py:179][0m |           0.1181 |           9.0459 |       -1120.5940 |
[32m[20221208 14:57:12 @agent_ppo2.py:179][0m |           0.1009 |           8.4666 |        -793.9468 |
[32m[20221208 14:57:12 @agent_ppo2.py:179][0m |           0.0624 |           8.1865 |        -934.4666 |
[32m[20221208 14:57:12 @agent_ppo2.py:179][0m |           0.0224 |           8.0360 |       -1058.6152 |
[32m[20221208 14:57:12 @agent_ppo2.py:179][0m |          -0.0056 |           7.9386 |       -1163.2953 |
[32m[20221208 14:57:13 @agent_ppo2.py:179][0m |          -0.0151 |           7.8758 |       -1216.8291 |
[32m[20221208 14:57:13 @agent_ppo2.py:179][0m |          -0.0253 |           7.7973 |       -1233.0238 |
[32m[20221208 14:57:13 @agent_ppo2.py:179][0m |          -0.0329 |           7.7107 |       -1268.3643 |
[32m[20221208 14:57:13 @agent_ppo2.py:179][0m |          -0.0385 |           7.6602 |       -1292.0159 |
[32m[20221208 14:57:13 @agent_ppo2.py:179][0m |          -0.0453 |           7.6133 |       -1283.2362 |
[32m[20221208 14:57:13 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.00
[32m[20221208 14:57:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.00
[32m[20221208 14:57:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 170.50
[32m[20221208 14:57:13 @agent_ppo2.py:137][0m Total time:      20.65 min
[32m[20221208 14:57:13 @agent_ppo2.py:139][0m 1669120 total steps have happened
[32m[20221208 14:57:13 @agent_ppo2.py:115][0m #------------------------ Iteration 815 --------------------------#
[32m[20221208 14:57:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |           0.1509 |           8.1440 |       -1145.8906 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |           0.0584 |           7.8065 |        -999.2797 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |           0.0433 |           7.7077 |       -1046.6560 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |           0.0276 |           7.6195 |       -1096.1582 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |          -0.0041 |           7.5020 |       -1170.5605 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |          -0.0138 |           7.4151 |       -1223.7772 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |          -0.0138 |           7.4059 |       -1198.1280 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |          -0.0260 |           7.3368 |       -1224.2647 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |          -0.0358 |           7.3015 |       -1268.3812 |
[32m[20221208 14:57:14 @agent_ppo2.py:179][0m |          -0.0385 |           7.2702 |       -1277.9766 |
[32m[20221208 14:57:14 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 169.13
[32m[20221208 14:57:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.13
[32m[20221208 14:57:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.32
[32m[20221208 14:57:15 @agent_ppo2.py:137][0m Total time:      20.67 min
[32m[20221208 14:57:15 @agent_ppo2.py:139][0m 1671168 total steps have happened
[32m[20221208 14:57:15 @agent_ppo2.py:115][0m #------------------------ Iteration 816 --------------------------#
[32m[20221208 14:57:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:15 @agent_ppo2.py:179][0m |           0.1132 |           7.9256 |       -1034.6639 |
[32m[20221208 14:57:15 @agent_ppo2.py:179][0m |           0.1551 |           7.4130 |        -455.5180 |
[32m[20221208 14:57:15 @agent_ppo2.py:179][0m |           0.0960 |           7.1318 |        -469.8230 |
[32m[20221208 14:57:15 @agent_ppo2.py:179][0m |           0.0541 |           6.8917 |        -714.3499 |
[32m[20221208 14:57:16 @agent_ppo2.py:179][0m |           0.0403 |           6.7938 |        -814.9521 |
[32m[20221208 14:57:16 @agent_ppo2.py:179][0m |           0.0150 |           6.6589 |        -908.8464 |
[32m[20221208 14:57:16 @agent_ppo2.py:179][0m |          -0.0075 |           6.5864 |        -995.7549 |
[32m[20221208 14:57:16 @agent_ppo2.py:179][0m |          -0.0186 |           6.5213 |       -1068.4712 |
[32m[20221208 14:57:16 @agent_ppo2.py:179][0m |          -0.0251 |           6.4592 |       -1112.0004 |
[32m[20221208 14:57:16 @agent_ppo2.py:179][0m |          -0.0294 |           6.4116 |       -1139.3709 |
[32m[20221208 14:57:16 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 157.71
[32m[20221208 14:57:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.37
[32m[20221208 14:57:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.03
[32m[20221208 14:57:16 @agent_ppo2.py:137][0m Total time:      20.70 min
[32m[20221208 14:57:16 @agent_ppo2.py:139][0m 1673216 total steps have happened
[32m[20221208 14:57:16 @agent_ppo2.py:115][0m #------------------------ Iteration 817 --------------------------#
[32m[20221208 14:57:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |           0.1001 |           8.1420 |        -927.3807 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |           0.1231 |           7.2926 |        -442.2776 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |           0.0768 |           6.9791 |        -662.0328 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |           0.0326 |           6.7873 |        -719.4191 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |           0.0185 |           6.5957 |        -782.3388 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |          -0.0079 |           6.4222 |        -869.7990 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |          -0.0203 |           6.2485 |        -904.0549 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |          -0.0274 |           6.1631 |        -942.8857 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |          -0.0354 |           6.0107 |        -981.2571 |
[32m[20221208 14:57:17 @agent_ppo2.py:179][0m |          -0.0413 |           5.9184 |       -1014.4328 |
[32m[20221208 14:57:17 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.11
[32m[20221208 14:57:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.70
[32m[20221208 14:57:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.79
[32m[20221208 14:57:18 @agent_ppo2.py:137][0m Total time:      20.73 min
[32m[20221208 14:57:18 @agent_ppo2.py:139][0m 1675264 total steps have happened
[32m[20221208 14:57:18 @agent_ppo2.py:115][0m #------------------------ Iteration 818 --------------------------#
[32m[20221208 14:57:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:18 @agent_ppo2.py:179][0m |           0.0829 |           8.6318 |       -1036.0223 |
[32m[20221208 14:57:18 @agent_ppo2.py:179][0m |           0.0455 |           8.0125 |        -990.1341 |
[32m[20221208 14:57:18 @agent_ppo2.py:179][0m |           0.0061 |           7.7411 |       -1021.4201 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0269 |           7.5834 |       -1089.0533 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0310 |           7.4438 |       -1104.4070 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0380 |           7.3392 |       -1106.0887 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0437 |           7.2658 |       -1123.9594 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0481 |           7.1807 |       -1124.8161 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0518 |           7.0932 |       -1128.2836 |
[32m[20221208 14:57:19 @agent_ppo2.py:179][0m |          -0.0579 |           7.0588 |       -1158.7588 |
[32m[20221208 14:57:19 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:57:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.38
[32m[20221208 14:57:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.79
[32m[20221208 14:57:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.29
[32m[20221208 14:57:19 @agent_ppo2.py:137][0m Total time:      20.75 min
[32m[20221208 14:57:19 @agent_ppo2.py:139][0m 1677312 total steps have happened
[32m[20221208 14:57:19 @agent_ppo2.py:115][0m #------------------------ Iteration 819 --------------------------#
[32m[20221208 14:57:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |           0.0840 |           7.3461 |        -978.2474 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |           0.1192 |           6.4389 |        -775.1147 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |           0.0524 |           5.9764 |        -777.0078 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |           0.0153 |           5.6924 |        -844.0679 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |          -0.0082 |           5.4654 |        -946.6734 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |          -0.0226 |           5.3417 |       -1006.3641 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |          -0.0270 |           5.1758 |       -1012.7809 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |          -0.0411 |           5.0599 |       -1054.1412 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |          -0.0451 |           4.9278 |       -1081.8182 |
[32m[20221208 14:57:20 @agent_ppo2.py:179][0m |          -0.0470 |           4.8204 |       -1081.2102 |
[32m[20221208 14:57:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.95
[32m[20221208 14:57:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.59
[32m[20221208 14:57:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.95
[32m[20221208 14:57:21 @agent_ppo2.py:137][0m Total time:      20.78 min
[32m[20221208 14:57:21 @agent_ppo2.py:139][0m 1679360 total steps have happened
[32m[20221208 14:57:21 @agent_ppo2.py:115][0m #------------------------ Iteration 820 --------------------------#
[32m[20221208 14:57:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:21 @agent_ppo2.py:179][0m |           0.0874 |           7.2329 |        -820.3808 |
[32m[20221208 14:57:21 @agent_ppo2.py:179][0m |           0.0310 |           6.4905 |        -723.7337 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0129 |           6.2001 |        -814.1021 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0333 |           6.0162 |        -864.9431 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0444 |           5.9031 |        -924.1591 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0534 |           5.8216 |        -933.6054 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0605 |           5.7069 |        -948.9991 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0633 |           5.6246 |        -966.5203 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0695 |           5.5293 |        -982.6945 |
[32m[20221208 14:57:22 @agent_ppo2.py:179][0m |          -0.0717 |           5.5022 |        -999.0479 |
[32m[20221208 14:57:22 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.82
[32m[20221208 14:57:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.99
[32m[20221208 14:57:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 173.20
[32m[20221208 14:57:22 @agent_ppo2.py:137][0m Total time:      20.80 min
[32m[20221208 14:57:22 @agent_ppo2.py:139][0m 1681408 total steps have happened
[32m[20221208 14:57:22 @agent_ppo2.py:115][0m #------------------------ Iteration 821 --------------------------#
[32m[20221208 14:57:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |           0.1189 |           7.7056 |        -988.0543 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |           0.1101 |           6.9614 |        -652.3626 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |           0.0434 |           6.6279 |        -753.5319 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |           0.0043 |           6.4239 |        -857.4845 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |          -0.0186 |           6.2921 |        -910.3523 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |          -0.0319 |           6.1875 |        -946.0741 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |          -0.0422 |           6.0540 |        -982.1747 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |          -0.0479 |           6.0472 |       -1018.6511 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |          -0.0536 |           5.9028 |       -1018.5158 |
[32m[20221208 14:57:23 @agent_ppo2.py:179][0m |          -0.0595 |           5.8586 |       -1052.8118 |
[32m[20221208 14:57:23 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:57:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.62
[32m[20221208 14:57:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.15
[32m[20221208 14:57:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.09
[32m[20221208 14:57:24 @agent_ppo2.py:137][0m Total time:      20.83 min
[32m[20221208 14:57:24 @agent_ppo2.py:139][0m 1683456 total steps have happened
[32m[20221208 14:57:24 @agent_ppo2.py:115][0m #------------------------ Iteration 822 --------------------------#
[32m[20221208 14:57:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:24 @agent_ppo2.py:179][0m |           0.1208 |           6.9143 |        -859.5811 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |           0.0752 |           6.0284 |        -744.3666 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |           0.0375 |           5.7013 |        -818.0867 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0033 |           5.5237 |        -931.4488 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0195 |           5.4079 |        -997.9349 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0319 |           5.2804 |       -1042.7263 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0322 |           5.1661 |       -1038.6599 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0470 |           5.1052 |       -1068.4989 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0569 |           5.0229 |       -1116.2114 |
[32m[20221208 14:57:25 @agent_ppo2.py:179][0m |          -0.0612 |           5.0222 |       -1144.3953 |
[32m[20221208 14:57:25 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.59
[32m[20221208 14:57:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.87
[32m[20221208 14:57:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.67
[32m[20221208 14:57:25 @agent_ppo2.py:137][0m Total time:      20.85 min
[32m[20221208 14:57:25 @agent_ppo2.py:139][0m 1685504 total steps have happened
[32m[20221208 14:57:25 @agent_ppo2.py:115][0m #------------------------ Iteration 823 --------------------------#
[32m[20221208 14:57:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |           0.0814 |           8.3785 |       -1012.1416 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |           0.0912 |           7.8420 |        -902.1137 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |           0.0768 |           7.6192 |        -831.7085 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |           0.0410 |           7.5091 |        -796.4933 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |           0.0042 |           7.4427 |        -908.5328 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |          -0.0146 |           7.3260 |        -949.6091 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |          -0.0247 |           7.2811 |        -987.4127 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |          -0.0287 |           7.2363 |       -1000.5806 |
[32m[20221208 14:57:26 @agent_ppo2.py:179][0m |          -0.0358 |           7.1978 |       -1024.7148 |
[32m[20221208 14:57:27 @agent_ppo2.py:179][0m |          -0.0208 |           7.1476 |       -1007.8034 |
[32m[20221208 14:57:27 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.13
[32m[20221208 14:57:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.25
[32m[20221208 14:57:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.03
[32m[20221208 14:57:27 @agent_ppo2.py:137][0m Total time:      20.88 min
[32m[20221208 14:57:27 @agent_ppo2.py:139][0m 1687552 total steps have happened
[32m[20221208 14:57:27 @agent_ppo2.py:115][0m #------------------------ Iteration 824 --------------------------#
[32m[20221208 14:57:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |           0.0916 |           7.5805 |        -934.1482 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |           0.0890 |           6.7307 |        -729.9877 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |           0.0535 |           6.2448 |        -688.7086 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |           0.0116 |           5.9544 |        -814.2818 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |          -0.0141 |           5.7517 |        -887.6787 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |          -0.0229 |           5.6150 |        -938.9617 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |          -0.0337 |           5.4872 |        -962.7213 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |          -0.0418 |           5.3694 |        -989.9563 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |          -0.0479 |           5.2976 |       -1014.4968 |
[32m[20221208 14:57:28 @agent_ppo2.py:179][0m |          -0.0531 |           5.2178 |       -1043.4326 |
[32m[20221208 14:57:28 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.51
[32m[20221208 14:57:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.37
[32m[20221208 14:57:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.84
[32m[20221208 14:57:29 @agent_ppo2.py:137][0m Total time:      20.90 min
[32m[20221208 14:57:29 @agent_ppo2.py:139][0m 1689600 total steps have happened
[32m[20221208 14:57:29 @agent_ppo2.py:115][0m #------------------------ Iteration 825 --------------------------#
[32m[20221208 14:57:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |           0.1590 |           8.7702 |        -862.2224 |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |           0.1658 |           7.7278 |        -563.6511 |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |           0.0705 |           7.2088 |        -653.5859 |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |           0.0128 |           6.9901 |        -793.4547 |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |          -0.0130 |           6.7839 |        -829.8891 |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |          -0.0334 |           6.6317 |        -867.8499 |
[32m[20221208 14:57:29 @agent_ppo2.py:179][0m |          -0.0491 |           6.5534 |        -904.1429 |
[32m[20221208 14:57:30 @agent_ppo2.py:179][0m |          -0.0504 |           6.4278 |        -916.9974 |
[32m[20221208 14:57:30 @agent_ppo2.py:179][0m |          -0.0572 |           6.3336 |        -936.7563 |
[32m[20221208 14:57:30 @agent_ppo2.py:179][0m |          -0.0625 |           6.3079 |        -941.1117 |
[32m[20221208 14:57:30 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:57:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.22
[32m[20221208 14:57:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.76
[32m[20221208 14:57:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.12
[32m[20221208 14:57:30 @agent_ppo2.py:137][0m Total time:      20.93 min
[32m[20221208 14:57:30 @agent_ppo2.py:139][0m 1691648 total steps have happened
[32m[20221208 14:57:30 @agent_ppo2.py:115][0m #------------------------ Iteration 826 --------------------------#
[32m[20221208 14:57:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |           0.0799 |           8.7333 |        -881.4039 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |           0.0861 |           7.6968 |        -726.7745 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |           0.0238 |           7.2686 |        -807.5082 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0123 |           7.0351 |        -888.1350 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0290 |           6.8207 |        -926.0916 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0335 |           6.7180 |        -952.3746 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0359 |           6.5638 |        -966.8320 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0449 |           6.4526 |        -991.5544 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0499 |           6.4220 |       -1002.9218 |
[32m[20221208 14:57:31 @agent_ppo2.py:179][0m |          -0.0527 |           6.3311 |       -1017.6621 |
[32m[20221208 14:57:31 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.97
[32m[20221208 14:57:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.01
[32m[20221208 14:57:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.87
[32m[20221208 14:57:32 @agent_ppo2.py:137][0m Total time:      20.96 min
[32m[20221208 14:57:32 @agent_ppo2.py:139][0m 1693696 total steps have happened
[32m[20221208 14:57:32 @agent_ppo2.py:115][0m #------------------------ Iteration 827 --------------------------#
[32m[20221208 14:57:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:32 @agent_ppo2.py:179][0m |           0.0638 |           7.7890 |        -880.2800 |
[32m[20221208 14:57:32 @agent_ppo2.py:179][0m |           0.0448 |           6.5107 |        -777.8905 |
[32m[20221208 14:57:32 @agent_ppo2.py:179][0m |           0.0051 |           6.1097 |        -867.5566 |
[32m[20221208 14:57:32 @agent_ppo2.py:179][0m |          -0.0186 |           5.8390 |        -900.4242 |
[32m[20221208 14:57:32 @agent_ppo2.py:179][0m |          -0.0329 |           5.6609 |        -938.2988 |
[32m[20221208 14:57:32 @agent_ppo2.py:179][0m |          -0.0430 |           5.5059 |        -967.0739 |
[32m[20221208 14:57:33 @agent_ppo2.py:179][0m |          -0.0483 |           5.4309 |        -994.8587 |
[32m[20221208 14:57:33 @agent_ppo2.py:179][0m |          -0.0577 |           5.3158 |       -1000.3375 |
[32m[20221208 14:57:33 @agent_ppo2.py:179][0m |          -0.0610 |           5.2116 |       -1029.5831 |
[32m[20221208 14:57:33 @agent_ppo2.py:179][0m |          -0.0584 |           5.1874 |       -1028.2586 |
[32m[20221208 14:57:33 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:57:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.31
[32m[20221208 14:57:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.17
[32m[20221208 14:57:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 160.91
[32m[20221208 14:57:33 @agent_ppo2.py:137][0m Total time:      20.98 min
[32m[20221208 14:57:33 @agent_ppo2.py:139][0m 1695744 total steps have happened
[32m[20221208 14:57:33 @agent_ppo2.py:115][0m #------------------------ Iteration 828 --------------------------#
[32m[20221208 14:57:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |           0.0827 |           8.9852 |        -830.1720 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |           0.1951 |           8.4710 |        -717.1013 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |           0.0420 |           8.2413 |        -767.7196 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |           0.0164 |           8.0521 |        -833.7731 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |          -0.0026 |           7.9520 |        -855.3883 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |          -0.0241 |           7.8423 |        -933.1151 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |          -0.0304 |           7.7270 |        -966.8607 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |          -0.0394 |           7.5978 |        -993.4797 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |          -0.0458 |           7.4942 |        -999.2185 |
[32m[20221208 14:57:34 @agent_ppo2.py:179][0m |          -0.0376 |           7.4032 |       -1007.9667 |
[32m[20221208 14:57:34 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.01
[32m[20221208 14:57:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.13
[32m[20221208 14:57:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.28
[32m[20221208 14:57:35 @agent_ppo2.py:137][0m Total time:      21.01 min
[32m[20221208 14:57:35 @agent_ppo2.py:139][0m 1697792 total steps have happened
[32m[20221208 14:57:35 @agent_ppo2.py:115][0m #------------------------ Iteration 829 --------------------------#
[32m[20221208 14:57:35 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:57:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:35 @agent_ppo2.py:179][0m |           0.1060 |           9.3099 |        -820.1768 |
[32m[20221208 14:57:35 @agent_ppo2.py:179][0m |           0.1175 |           8.6868 |        -591.7059 |
[32m[20221208 14:57:35 @agent_ppo2.py:179][0m |           0.0535 |           8.3561 |        -674.6979 |
[32m[20221208 14:57:35 @agent_ppo2.py:179][0m |           0.0074 |           8.2497 |        -777.7868 |
[32m[20221208 14:57:36 @agent_ppo2.py:179][0m |          -0.0145 |           8.1630 |        -809.3116 |
[32m[20221208 14:57:36 @agent_ppo2.py:179][0m |          -0.0274 |           8.0337 |        -853.7856 |
[32m[20221208 14:57:36 @agent_ppo2.py:179][0m |          -0.0397 |           7.9855 |        -892.7519 |
[32m[20221208 14:57:36 @agent_ppo2.py:179][0m |          -0.0435 |           7.9139 |        -897.8689 |
[32m[20221208 14:57:36 @agent_ppo2.py:179][0m |          -0.0521 |           7.8516 |        -925.2427 |
[32m[20221208 14:57:36 @agent_ppo2.py:179][0m |          -0.0558 |           7.8432 |        -942.4043 |
[32m[20221208 14:57:36 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:57:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.93
[32m[20221208 14:57:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 168.91
[32m[20221208 14:57:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.69
[32m[20221208 14:57:36 @agent_ppo2.py:137][0m Total time:      21.03 min
[32m[20221208 14:57:36 @agent_ppo2.py:139][0m 1699840 total steps have happened
[32m[20221208 14:57:36 @agent_ppo2.py:115][0m #------------------------ Iteration 830 --------------------------#
[32m[20221208 14:57:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |           0.0935 |           8.9110 |        -900.9174 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |           0.0886 |           8.3062 |        -593.9517 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |           0.0289 |           7.9438 |        -715.8944 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0023 |           7.7385 |        -797.5267 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0211 |           7.5923 |        -851.7233 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0303 |           7.5233 |        -884.2099 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0380 |           7.3822 |        -911.8547 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0454 |           7.2488 |        -941.7827 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0503 |           7.1433 |        -959.7675 |
[32m[20221208 14:57:37 @agent_ppo2.py:179][0m |          -0.0556 |           7.0961 |        -973.5593 |
[32m[20221208 14:57:37 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:57:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.12
[32m[20221208 14:57:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.39
[32m[20221208 14:57:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.95
[32m[20221208 14:57:38 @agent_ppo2.py:137][0m Total time:      21.06 min
[32m[20221208 14:57:38 @agent_ppo2.py:139][0m 1701888 total steps have happened
[32m[20221208 14:57:38 @agent_ppo2.py:115][0m #------------------------ Iteration 831 --------------------------#
[32m[20221208 14:57:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:38 @agent_ppo2.py:179][0m |           0.0529 |           7.5999 |        -861.5326 |
[32m[20221208 14:57:38 @agent_ppo2.py:179][0m |           0.0264 |           7.1650 |        -776.4161 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0091 |           6.9749 |        -842.6952 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0317 |           6.9233 |        -890.2021 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0334 |           6.7393 |        -869.8135 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0413 |           6.6600 |        -880.9185 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0565 |           6.5912 |        -932.9934 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0657 |           6.5119 |        -957.4388 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0663 |           6.4487 |        -960.7633 |
[32m[20221208 14:57:39 @agent_ppo2.py:179][0m |          -0.0695 |           6.4094 |        -986.4435 |
[32m[20221208 14:57:39 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.15
[32m[20221208 14:57:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.30
[32m[20221208 14:57:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.58
[32m[20221208 14:57:39 @agent_ppo2.py:137][0m Total time:      21.09 min
[32m[20221208 14:57:39 @agent_ppo2.py:139][0m 1703936 total steps have happened
[32m[20221208 14:57:39 @agent_ppo2.py:115][0m #------------------------ Iteration 832 --------------------------#
[32m[20221208 14:57:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |           0.0839 |           6.8497 |        -846.6500 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |           0.0282 |           5.6307 |        -810.1803 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0044 |           5.2035 |        -874.1851 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0194 |           4.9224 |        -901.0518 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0226 |           4.7062 |        -894.0666 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0324 |           4.5249 |        -912.7933 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0366 |           4.4172 |        -943.1001 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0510 |           4.3402 |        -973.0137 |
[32m[20221208 14:57:40 @agent_ppo2.py:179][0m |          -0.0528 |           4.2272 |        -994.6069 |
[32m[20221208 14:57:41 @agent_ppo2.py:179][0m |          -0.0534 |           4.1522 |        -991.1273 |
[32m[20221208 14:57:41 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:57:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.45
[32m[20221208 14:57:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.51
[32m[20221208 14:57:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 159.97
[32m[20221208 14:57:41 @agent_ppo2.py:137][0m Total time:      21.11 min
[32m[20221208 14:57:41 @agent_ppo2.py:139][0m 1705984 total steps have happened
[32m[20221208 14:57:41 @agent_ppo2.py:115][0m #------------------------ Iteration 833 --------------------------#
[32m[20221208 14:57:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |           0.1150 |           9.0182 |        -739.2814 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |           0.1253 |           8.0382 |        -538.7836 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |           0.0545 |           7.5740 |        -605.4097 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |           0.0183 |           7.2664 |        -628.7364 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |           0.0007 |           7.0498 |        -692.2919 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |          -0.0196 |           6.8729 |        -730.3257 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |          -0.0307 |           6.7578 |        -767.1082 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |          -0.0291 |           6.6009 |        -781.8485 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |          -0.0387 |           6.4661 |        -820.4401 |
[32m[20221208 14:57:42 @agent_ppo2.py:179][0m |          -0.0453 |           6.3798 |        -823.6620 |
[32m[20221208 14:57:42 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 148.18
[32m[20221208 14:57:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.92
[32m[20221208 14:57:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.94
[32m[20221208 14:57:42 @agent_ppo2.py:137][0m Total time:      21.14 min
[32m[20221208 14:57:42 @agent_ppo2.py:139][0m 1708032 total steps have happened
[32m[20221208 14:57:42 @agent_ppo2.py:115][0m #------------------------ Iteration 834 --------------------------#
[32m[20221208 14:57:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |           0.1351 |           9.9893 |        -773.0681 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |           0.2331 |           8.5333 |        -402.3321 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |           0.0959 |           7.9948 |        -434.5418 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |           0.0317 |           7.6880 |        -594.0413 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |          -0.0033 |           7.4528 |        -683.7746 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |          -0.0164 |           7.3067 |        -720.2371 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |          -0.0317 |           7.1659 |        -743.9943 |
[32m[20221208 14:57:43 @agent_ppo2.py:179][0m |          -0.0427 |           7.0058 |        -773.2161 |
[32m[20221208 14:57:44 @agent_ppo2.py:179][0m |          -0.0533 |           6.8891 |        -796.5885 |
[32m[20221208 14:57:44 @agent_ppo2.py:179][0m |          -0.0537 |           6.7592 |        -800.3867 |
[32m[20221208 14:57:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:57:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.27
[32m[20221208 14:57:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.48
[32m[20221208 14:57:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.60
[32m[20221208 14:57:44 @agent_ppo2.py:137][0m Total time:      21.16 min
[32m[20221208 14:57:44 @agent_ppo2.py:139][0m 1710080 total steps have happened
[32m[20221208 14:57:44 @agent_ppo2.py:115][0m #------------------------ Iteration 835 --------------------------#
[32m[20221208 14:57:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |           0.0686 |           7.2005 |        -814.0983 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |           0.0726 |           6.3432 |        -743.6048 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |           0.0224 |           5.7726 |        -779.3588 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |           0.0019 |           5.4283 |        -804.0706 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |          -0.0271 |           5.1959 |        -846.4157 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |          -0.0418 |           4.9962 |        -864.3903 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |          -0.0460 |           4.8183 |        -874.2914 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |          -0.0530 |           4.6601 |        -889.9199 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |          -0.0572 |           4.5591 |        -902.2583 |
[32m[20221208 14:57:45 @agent_ppo2.py:179][0m |          -0.0594 |           4.4186 |        -915.2416 |
[32m[20221208 14:57:45 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:57:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.07
[32m[20221208 14:57:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.05
[32m[20221208 14:57:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.49
[32m[20221208 14:57:46 @agent_ppo2.py:137][0m Total time:      21.19 min
[32m[20221208 14:57:46 @agent_ppo2.py:139][0m 1712128 total steps have happened
[32m[20221208 14:57:46 @agent_ppo2.py:115][0m #------------------------ Iteration 836 --------------------------#
[32m[20221208 14:57:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |           0.0874 |           8.8106 |        -711.0678 |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |           0.1496 |           7.9668 |        -443.6349 |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |           0.0582 |           7.5996 |        -588.1259 |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |           0.0122 |           7.4258 |        -679.6564 |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |           0.0059 |           7.2834 |        -695.7167 |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |          -0.0198 |           7.1684 |        -737.5548 |
[32m[20221208 14:57:46 @agent_ppo2.py:179][0m |          -0.0295 |           7.0950 |        -770.8934 |
[32m[20221208 14:57:47 @agent_ppo2.py:179][0m |          -0.0354 |           7.0190 |        -773.7078 |
[32m[20221208 14:57:47 @agent_ppo2.py:179][0m |          -0.0454 |           6.9777 |        -792.3167 |
[32m[20221208 14:57:47 @agent_ppo2.py:179][0m |          -0.0505 |           6.9083 |        -811.3213 |
[32m[20221208 14:57:47 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.00
[32m[20221208 14:57:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.92
[32m[20221208 14:57:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.13
[32m[20221208 14:57:47 @agent_ppo2.py:137][0m Total time:      21.21 min
[32m[20221208 14:57:47 @agent_ppo2.py:139][0m 1714176 total steps have happened
[32m[20221208 14:57:47 @agent_ppo2.py:115][0m #------------------------ Iteration 837 --------------------------#
[32m[20221208 14:57:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |           0.1536 |          10.7305 |        -669.5690 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |           0.0718 |           9.7227 |        -451.4376 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |           0.0226 |           9.3401 |        -581.9551 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0014 |           9.1132 |        -642.9581 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0049 |           8.9094 |        -590.7477 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0269 |           8.7589 |        -675.1264 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0325 |           8.6641 |        -688.8322 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0461 |           8.5271 |        -714.8172 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0490 |           8.4296 |        -721.8725 |
[32m[20221208 14:57:48 @agent_ppo2.py:179][0m |          -0.0484 |           8.2922 |        -737.2347 |
[32m[20221208 14:57:48 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 155.88
[32m[20221208 14:57:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.92
[32m[20221208 14:57:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.28
[32m[20221208 14:57:49 @agent_ppo2.py:137][0m Total time:      21.24 min
[32m[20221208 14:57:49 @agent_ppo2.py:139][0m 1716224 total steps have happened
[32m[20221208 14:57:49 @agent_ppo2.py:115][0m #------------------------ Iteration 838 --------------------------#
[32m[20221208 14:57:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:49 @agent_ppo2.py:179][0m |           0.0678 |           8.4284 |        -731.3929 |
[32m[20221208 14:57:49 @agent_ppo2.py:179][0m |           0.0456 |           7.6857 |        -690.0618 |
[32m[20221208 14:57:49 @agent_ppo2.py:179][0m |           0.0311 |           7.3429 |        -655.4299 |
[32m[20221208 14:57:49 @agent_ppo2.py:179][0m |          -0.0160 |           7.1466 |        -734.7546 |
[32m[20221208 14:57:49 @agent_ppo2.py:179][0m |          -0.0356 |           6.9410 |        -770.5137 |
[32m[20221208 14:57:49 @agent_ppo2.py:179][0m |          -0.0477 |           6.8178 |        -786.9072 |
[32m[20221208 14:57:50 @agent_ppo2.py:179][0m |          -0.0502 |           6.7605 |        -792.9862 |
[32m[20221208 14:57:50 @agent_ppo2.py:179][0m |          -0.0587 |           6.6028 |        -801.3633 |
[32m[20221208 14:57:50 @agent_ppo2.py:179][0m |          -0.0633 |           6.5552 |        -814.5099 |
[32m[20221208 14:57:50 @agent_ppo2.py:179][0m |          -0.0641 |           6.4557 |        -811.6823 |
[32m[20221208 14:57:50 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.03
[32m[20221208 14:57:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.82
[32m[20221208 14:57:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.36
[32m[20221208 14:57:50 @agent_ppo2.py:137][0m Total time:      21.26 min
[32m[20221208 14:57:50 @agent_ppo2.py:139][0m 1718272 total steps have happened
[32m[20221208 14:57:50 @agent_ppo2.py:115][0m #------------------------ Iteration 839 --------------------------#
[32m[20221208 14:57:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |           0.0813 |           7.9802 |        -721.5688 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |           0.0552 |           7.1380 |        -624.1653 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |           0.0037 |           6.7524 |        -673.7276 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0135 |           6.4679 |        -691.0028 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0368 |           6.2651 |        -732.8249 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0463 |           6.1233 |        -755.9736 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0513 |           5.9823 |        -778.2236 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0555 |           5.8963 |        -780.5173 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0625 |           5.8211 |        -792.2998 |
[32m[20221208 14:57:51 @agent_ppo2.py:179][0m |          -0.0678 |           5.6734 |        -808.4185 |
[32m[20221208 14:57:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.46
[32m[20221208 14:57:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.43
[32m[20221208 14:57:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.30
[32m[20221208 14:57:52 @agent_ppo2.py:137][0m Total time:      21.29 min
[32m[20221208 14:57:52 @agent_ppo2.py:139][0m 1720320 total steps have happened
[32m[20221208 14:57:52 @agent_ppo2.py:115][0m #------------------------ Iteration 840 --------------------------#
[32m[20221208 14:57:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:52 @agent_ppo2.py:179][0m |           3.9438 |           6.8563 |        -707.9083 |
[32m[20221208 14:57:52 @agent_ppo2.py:179][0m |           0.0898 |           5.9744 |        -590.0161 |
[32m[20221208 14:57:52 @agent_ppo2.py:179][0m |           0.0339 |           5.4500 |        -654.0319 |
[32m[20221208 14:57:52 @agent_ppo2.py:179][0m |          -0.0142 |           5.1438 |        -748.6827 |
[32m[20221208 14:57:52 @agent_ppo2.py:179][0m |          -0.0366 |           4.9514 |        -770.8047 |
[32m[20221208 14:57:53 @agent_ppo2.py:179][0m |          -0.0473 |           4.7777 |        -792.1228 |
[32m[20221208 14:57:53 @agent_ppo2.py:179][0m |          -0.0577 |           4.6773 |        -797.7815 |
[32m[20221208 14:57:53 @agent_ppo2.py:179][0m |          -0.0646 |           4.5694 |        -816.1252 |
[32m[20221208 14:57:53 @agent_ppo2.py:179][0m |          -0.0598 |           4.4980 |        -810.3212 |
[32m[20221208 14:57:53 @agent_ppo2.py:179][0m |          -0.0626 |           4.3632 |        -825.1746 |
[32m[20221208 14:57:53 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:57:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.75
[32m[20221208 14:57:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.05
[32m[20221208 14:57:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.30
[32m[20221208 14:57:53 @agent_ppo2.py:137][0m Total time:      21.32 min
[32m[20221208 14:57:53 @agent_ppo2.py:139][0m 1722368 total steps have happened
[32m[20221208 14:57:53 @agent_ppo2.py:115][0m #------------------------ Iteration 841 --------------------------#
[32m[20221208 14:57:54 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:57:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |           0.0900 |           7.9309 |        -768.4644 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |           0.0560 |           6.9271 |        -636.1679 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |           0.0032 |           6.4631 |        -716.8907 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0201 |           6.1701 |        -746.3968 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0327 |           5.9500 |        -769.3362 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0438 |           5.7748 |        -799.3880 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0519 |           5.6569 |        -812.2369 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0596 |           5.4936 |        -834.4758 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0592 |           5.4215 |        -849.9967 |
[32m[20221208 14:57:54 @agent_ppo2.py:179][0m |          -0.0641 |           5.2982 |        -846.3217 |
[32m[20221208 14:57:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.31
[32m[20221208 14:57:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.45
[32m[20221208 14:57:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 159.42
[32m[20221208 14:57:55 @agent_ppo2.py:137][0m Total time:      21.34 min
[32m[20221208 14:57:55 @agent_ppo2.py:139][0m 1724416 total steps have happened
[32m[20221208 14:57:55 @agent_ppo2.py:115][0m #------------------------ Iteration 842 --------------------------#
[32m[20221208 14:57:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:55 @agent_ppo2.py:179][0m |           0.1057 |           7.5519 |        -797.2031 |
[32m[20221208 14:57:55 @agent_ppo2.py:179][0m |           0.0831 |           6.8625 |        -580.8296 |
[32m[20221208 14:57:55 @agent_ppo2.py:179][0m |           0.0271 |           6.6127 |        -688.1102 |
[32m[20221208 14:57:55 @agent_ppo2.py:179][0m |          -0.0027 |           6.4570 |        -772.2651 |
[32m[20221208 14:57:56 @agent_ppo2.py:179][0m |          -0.0192 |           6.3622 |        -792.4327 |
[32m[20221208 14:57:56 @agent_ppo2.py:179][0m |          -0.0333 |           6.2020 |        -828.5586 |
[32m[20221208 14:57:56 @agent_ppo2.py:179][0m |          -0.0411 |           6.2226 |        -848.9969 |
[32m[20221208 14:57:56 @agent_ppo2.py:179][0m |          -0.0462 |           6.1164 |        -860.4894 |
[32m[20221208 14:57:56 @agent_ppo2.py:179][0m |          -0.0531 |           5.9589 |        -879.8416 |
[32m[20221208 14:57:56 @agent_ppo2.py:179][0m |          -0.0566 |           5.9056 |        -884.1843 |
[32m[20221208 14:57:56 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.57
[32m[20221208 14:57:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.32
[32m[20221208 14:57:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.91
[32m[20221208 14:57:56 @agent_ppo2.py:137][0m Total time:      21.37 min
[32m[20221208 14:57:56 @agent_ppo2.py:139][0m 1726464 total steps have happened
[32m[20221208 14:57:56 @agent_ppo2.py:115][0m #------------------------ Iteration 843 --------------------------#
[32m[20221208 14:57:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:57:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |           0.0817 |           6.9265 |        -759.6782 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |           0.0798 |           6.3534 |        -645.0921 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |           0.0390 |           6.0439 |        -598.9488 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0045 |           5.8503 |        -632.7870 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0249 |           5.6410 |        -665.1631 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0310 |           5.5139 |        -696.4729 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0445 |           5.4016 |        -730.8633 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0350 |           5.2606 |        -738.6308 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0501 |           5.1396 |        -763.8886 |
[32m[20221208 14:57:57 @agent_ppo2.py:179][0m |          -0.0597 |           5.0508 |        -783.2031 |
[32m[20221208 14:57:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:57:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.98
[32m[20221208 14:57:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.30
[32m[20221208 14:57:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.68
[32m[20221208 14:57:58 @agent_ppo2.py:137][0m Total time:      21.39 min
[32m[20221208 14:57:58 @agent_ppo2.py:139][0m 1728512 total steps have happened
[32m[20221208 14:57:58 @agent_ppo2.py:115][0m #------------------------ Iteration 844 --------------------------#
[32m[20221208 14:57:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:57:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:57:58 @agent_ppo2.py:179][0m |           0.0897 |           9.3351 |        -714.6189 |
[32m[20221208 14:57:58 @agent_ppo2.py:179][0m |           0.1017 |           8.6089 |        -490.1792 |
[32m[20221208 14:57:58 @agent_ppo2.py:179][0m |           0.0399 |           8.2420 |        -578.7248 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0041 |           8.0319 |        -672.3848 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0064 |           7.9203 |        -682.5379 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0274 |           7.8421 |        -723.5595 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0424 |           7.7834 |        -741.1444 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0499 |           7.6317 |        -758.2937 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0545 |           7.6219 |        -758.6352 |
[32m[20221208 14:57:59 @agent_ppo2.py:179][0m |          -0.0521 |           7.5172 |        -765.5208 |
[32m[20221208 14:57:59 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:57:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.37
[32m[20221208 14:57:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.54
[32m[20221208 14:57:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.54
[32m[20221208 14:57:59 @agent_ppo2.py:137][0m Total time:      21.42 min
[32m[20221208 14:57:59 @agent_ppo2.py:139][0m 1730560 total steps have happened
[32m[20221208 14:57:59 @agent_ppo2.py:115][0m #------------------------ Iteration 845 --------------------------#
[32m[20221208 14:58:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |           0.1986 |           6.9154 |        -460.1255 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |           0.1436 |           6.4088 |        -189.0855 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |           0.0809 |           6.1920 |        -301.4745 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |           0.0405 |           6.0588 |        -414.0658 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |           0.0187 |           5.9178 |        -455.0840 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |          -0.0111 |           5.8377 |        -500.5560 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |          -0.0231 |           5.7906 |        -527.2701 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |          -0.0300 |           5.6780 |        -549.8840 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |          -0.0427 |           5.6340 |        -571.6877 |
[32m[20221208 14:58:00 @agent_ppo2.py:179][0m |          -0.0500 |           5.5878 |        -587.7168 |
[32m[20221208 14:58:00 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.96
[32m[20221208 14:58:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.40
[32m[20221208 14:58:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.95
[32m[20221208 14:58:01 @agent_ppo2.py:137][0m Total time:      21.44 min
[32m[20221208 14:58:01 @agent_ppo2.py:139][0m 1732608 total steps have happened
[32m[20221208 14:58:01 @agent_ppo2.py:115][0m #------------------------ Iteration 846 --------------------------#
[32m[20221208 14:58:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:01 @agent_ppo2.py:179][0m |           0.0743 |           6.6903 |        -619.9989 |
[32m[20221208 14:58:01 @agent_ppo2.py:179][0m |           0.0374 |           5.3009 |        -566.7645 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0118 |           4.8886 |        -621.4549 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0171 |           4.6068 |        -618.3909 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0412 |           4.3846 |        -667.7687 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0492 |           4.2427 |        -685.1577 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0541 |           4.0904 |        -694.3348 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0606 |           3.9609 |        -710.9674 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0647 |           3.8420 |        -715.6161 |
[32m[20221208 14:58:02 @agent_ppo2.py:179][0m |          -0.0661 |           3.7625 |        -726.3969 |
[32m[20221208 14:58:02 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:58:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.65
[32m[20221208 14:58:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.81
[32m[20221208 14:58:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.16
[32m[20221208 14:58:02 @agent_ppo2.py:137][0m Total time:      21.47 min
[32m[20221208 14:58:02 @agent_ppo2.py:139][0m 1734656 total steps have happened
[32m[20221208 14:58:02 @agent_ppo2.py:115][0m #------------------------ Iteration 847 --------------------------#
[32m[20221208 14:58:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |           0.0859 |           7.7772 |        -713.4092 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |           0.1327 |           7.0653 |        -420.1556 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |           0.0679 |           6.8026 |        -449.2231 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |           0.0340 |           6.6290 |        -536.9696 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |           0.0040 |           6.5823 |        -596.6294 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |          -0.0162 |           6.4617 |        -638.1685 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |          -0.0257 |           6.3940 |        -657.6874 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |          -0.0369 |           6.3894 |        -679.8353 |
[32m[20221208 14:58:03 @agent_ppo2.py:179][0m |          -0.0399 |           6.2885 |        -697.5716 |
[32m[20221208 14:58:04 @agent_ppo2.py:179][0m |          -0.0413 |           6.2580 |        -707.7084 |
[32m[20221208 14:58:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.19
[32m[20221208 14:58:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.01
[32m[20221208 14:58:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.23
[32m[20221208 14:58:04 @agent_ppo2.py:137][0m Total time:      21.49 min
[32m[20221208 14:58:04 @agent_ppo2.py:139][0m 1736704 total steps have happened
[32m[20221208 14:58:04 @agent_ppo2.py:115][0m #------------------------ Iteration 848 --------------------------#
[32m[20221208 14:58:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:04 @agent_ppo2.py:179][0m |           0.2691 |           8.3761 |        -575.2752 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |           1.2868 |           7.5761 |        -418.5132 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |           0.0521 |           7.2128 |        -444.0169 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |           0.0223 |           6.9744 |        -467.1221 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |           0.0031 |           6.8126 |        -504.5335 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |          -0.0138 |           6.7049 |        -541.1592 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |          -0.0229 |           6.5257 |        -558.2972 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |          -0.0351 |           6.5224 |        -578.1350 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |          -0.0387 |           6.3452 |        -594.0261 |
[32m[20221208 14:58:05 @agent_ppo2.py:179][0m |          -0.0475 |           6.2275 |        -615.7162 |
[32m[20221208 14:58:05 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.11
[32m[20221208 14:58:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.61
[32m[20221208 14:58:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 184.11
[32m[20221208 14:58:05 @agent_ppo2.py:137][0m Total time:      21.52 min
[32m[20221208 14:58:05 @agent_ppo2.py:139][0m 1738752 total steps have happened
[32m[20221208 14:58:05 @agent_ppo2.py:115][0m #------------------------ Iteration 849 --------------------------#
[32m[20221208 14:58:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |           0.0787 |           6.8634 |        -672.5022 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |           0.0472 |           5.7865 |        -607.4654 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |           0.0068 |           5.4690 |        -634.8739 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |          -0.0240 |           5.2125 |        -672.1992 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |          -0.0414 |           5.0406 |        -671.4716 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |          -0.0499 |           4.8969 |        -694.2267 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |          -0.0556 |           4.7713 |        -699.6892 |
[32m[20221208 14:58:06 @agent_ppo2.py:179][0m |          -0.0674 |           4.6927 |        -722.1489 |
[32m[20221208 14:58:07 @agent_ppo2.py:179][0m |          -0.0635 |           4.5782 |        -720.6039 |
[32m[20221208 14:58:07 @agent_ppo2.py:179][0m |          -0.0681 |           4.5457 |        -738.7343 |
[32m[20221208 14:58:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.66
[32m[20221208 14:58:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.26
[32m[20221208 14:58:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.28
[32m[20221208 14:58:07 @agent_ppo2.py:137][0m Total time:      21.55 min
[32m[20221208 14:58:07 @agent_ppo2.py:139][0m 1740800 total steps have happened
[32m[20221208 14:58:07 @agent_ppo2.py:115][0m #------------------------ Iteration 850 --------------------------#
[32m[20221208 14:58:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |           0.0923 |           7.2360 |        -683.5552 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |           0.6309 |           6.0465 |        -557.2379 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |           0.0321 |           5.5682 |        -565.7080 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0046 |           5.3279 |        -602.7244 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0355 |           5.1116 |        -632.5686 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0492 |           4.9607 |        -637.0165 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0530 |           4.8281 |        -654.9834 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0612 |           4.7911 |        -646.7425 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0680 |           4.6595 |        -671.3752 |
[32m[20221208 14:58:08 @agent_ppo2.py:179][0m |          -0.0710 |           4.5562 |        -673.9606 |
[32m[20221208 14:58:08 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.83
[32m[20221208 14:58:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.28
[32m[20221208 14:58:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.95
[32m[20221208 14:58:09 @agent_ppo2.py:137][0m Total time:      21.57 min
[32m[20221208 14:58:09 @agent_ppo2.py:139][0m 1742848 total steps have happened
[32m[20221208 14:58:09 @agent_ppo2.py:115][0m #------------------------ Iteration 851 --------------------------#
[32m[20221208 14:58:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |           0.0867 |           8.2559 |        -663.4236 |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |           0.0784 |           7.1504 |        -545.9557 |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |           0.0288 |           6.7388 |        -548.3702 |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |          -0.0017 |           6.5464 |        -597.0215 |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |          -0.0256 |           6.4122 |        -651.0253 |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |          -0.0377 |           6.3739 |        -689.3845 |
[32m[20221208 14:58:09 @agent_ppo2.py:179][0m |          -0.0435 |           6.1533 |        -695.4208 |
[32m[20221208 14:58:10 @agent_ppo2.py:179][0m |          -0.0484 |           6.0849 |        -711.3413 |
[32m[20221208 14:58:10 @agent_ppo2.py:179][0m |          -0.0517 |           6.0200 |        -717.3074 |
[32m[20221208 14:58:10 @agent_ppo2.py:179][0m |          -0.0581 |           5.9148 |        -733.7434 |
[32m[20221208 14:58:10 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.91
[32m[20221208 14:58:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.39
[32m[20221208 14:58:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.46
[32m[20221208 14:58:10 @agent_ppo2.py:137][0m Total time:      21.60 min
[32m[20221208 14:58:10 @agent_ppo2.py:139][0m 1744896 total steps have happened
[32m[20221208 14:58:10 @agent_ppo2.py:115][0m #------------------------ Iteration 852 --------------------------#
[32m[20221208 14:58:10 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:58:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |           0.0909 |           8.5605 |        -621.1721 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |           0.0424 |           7.3134 |        -562.4580 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0052 |           6.7097 |        -621.8030 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0159 |           6.4114 |        -643.7628 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0341 |           6.0638 |        -667.4185 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0454 |           5.8549 |        -690.3191 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0546 |           5.6595 |        -707.9187 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0615 |           5.5506 |        -720.7996 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0669 |           5.3873 |        -727.7066 |
[32m[20221208 14:58:11 @agent_ppo2.py:179][0m |          -0.0696 |           5.2920 |        -740.8109 |
[32m[20221208 14:58:11 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.84
[32m[20221208 14:58:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.86
[32m[20221208 14:58:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.54
[32m[20221208 14:58:12 @agent_ppo2.py:137][0m Total time:      21.62 min
[32m[20221208 14:58:12 @agent_ppo2.py:139][0m 1746944 total steps have happened
[32m[20221208 14:58:12 @agent_ppo2.py:115][0m #------------------------ Iteration 853 --------------------------#
[32m[20221208 14:58:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:12 @agent_ppo2.py:179][0m |           0.0562 |           7.4776 |        -759.8030 |
[32m[20221208 14:58:12 @agent_ppo2.py:179][0m |           0.0325 |           6.3242 |        -647.9866 |
[32m[20221208 14:58:12 @agent_ppo2.py:179][0m |          -0.0113 |           5.8601 |        -691.3851 |
[32m[20221208 14:58:12 @agent_ppo2.py:179][0m |          -0.0328 |           5.5829 |        -723.5335 |
[32m[20221208 14:58:12 @agent_ppo2.py:179][0m |          -0.0438 |           5.3813 |        -728.4171 |
[32m[20221208 14:58:12 @agent_ppo2.py:179][0m |          -0.0573 |           5.2260 |        -743.0699 |
[32m[20221208 14:58:13 @agent_ppo2.py:179][0m |          -0.0631 |           5.1135 |        -762.2251 |
[32m[20221208 14:58:13 @agent_ppo2.py:179][0m |          -0.0695 |           4.9293 |        -781.1396 |
[32m[20221208 14:58:13 @agent_ppo2.py:179][0m |          -0.0742 |           4.8307 |        -783.3149 |
[32m[20221208 14:58:13 @agent_ppo2.py:179][0m |          -0.0771 |           4.7203 |        -787.2093 |
[32m[20221208 14:58:13 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:58:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.86
[32m[20221208 14:58:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.30
[32m[20221208 14:58:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.22
[32m[20221208 14:58:13 @agent_ppo2.py:137][0m Total time:      21.65 min
[32m[20221208 14:58:13 @agent_ppo2.py:139][0m 1748992 total steps have happened
[32m[20221208 14:58:13 @agent_ppo2.py:115][0m #------------------------ Iteration 854 --------------------------#
[32m[20221208 14:58:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |           0.0687 |          11.9583 |        -699.5054 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |           0.0533 |          10.9066 |        -604.2587 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |           0.0157 |          10.4816 |        -632.7742 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0167 |          10.2204 |        -687.7496 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0327 |          10.0402 |        -720.8117 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0398 |           9.8678 |        -742.0995 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0459 |           9.7254 |        -751.6832 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0455 |           9.6536 |        -764.2553 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0509 |           9.6807 |        -778.3972 |
[32m[20221208 14:58:14 @agent_ppo2.py:179][0m |          -0.0563 |           9.4905 |        -787.0910 |
[32m[20221208 14:58:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.17
[32m[20221208 14:58:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.59
[32m[20221208 14:58:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.94
[32m[20221208 14:58:15 @agent_ppo2.py:137][0m Total time:      21.67 min
[32m[20221208 14:58:15 @agent_ppo2.py:139][0m 1751040 total steps have happened
[32m[20221208 14:58:15 @agent_ppo2.py:115][0m #------------------------ Iteration 855 --------------------------#
[32m[20221208 14:58:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:15 @agent_ppo2.py:179][0m |           0.1105 |           8.0034 |        -691.6999 |
[32m[20221208 14:58:15 @agent_ppo2.py:179][0m |           0.0814 |           7.0009 |        -515.6466 |
[32m[20221208 14:58:15 @agent_ppo2.py:179][0m |           0.0211 |           6.4294 |        -614.2934 |
[32m[20221208 14:58:15 @agent_ppo2.py:179][0m |          -0.0137 |           6.1831 |        -680.2628 |
[32m[20221208 14:58:15 @agent_ppo2.py:179][0m |          -0.0288 |           5.9918 |        -700.1711 |
[32m[20221208 14:58:16 @agent_ppo2.py:179][0m |          -0.0319 |           5.7115 |        -700.4769 |
[32m[20221208 14:58:16 @agent_ppo2.py:179][0m |          -0.0440 |           5.5705 |        -746.7811 |
[32m[20221208 14:58:16 @agent_ppo2.py:179][0m |          -0.0509 |           5.4813 |        -755.4291 |
[32m[20221208 14:58:16 @agent_ppo2.py:179][0m |          -0.0553 |           5.3388 |        -775.0507 |
[32m[20221208 14:58:16 @agent_ppo2.py:179][0m |          -0.0606 |           5.2284 |        -800.7370 |
[32m[20221208 14:58:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:58:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.86
[32m[20221208 14:58:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.83
[32m[20221208 14:58:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.21
[32m[20221208 14:58:16 @agent_ppo2.py:137][0m Total time:      21.70 min
[32m[20221208 14:58:16 @agent_ppo2.py:139][0m 1753088 total steps have happened
[32m[20221208 14:58:16 @agent_ppo2.py:115][0m #------------------------ Iteration 856 --------------------------#
[32m[20221208 14:58:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |           0.0604 |           6.1204 |        -739.3047 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |           0.0336 |           5.4960 |        -679.6337 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0046 |           5.2862 |        -731.6822 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0260 |           5.1231 |        -763.2251 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0367 |           5.0338 |        -762.4057 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0519 |           4.9819 |        -775.5612 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0582 |           4.8952 |        -798.8647 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0622 |           4.9211 |        -816.5593 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0679 |           4.8103 |        -834.2775 |
[32m[20221208 14:58:17 @agent_ppo2.py:179][0m |          -0.0665 |           4.7630 |        -834.1559 |
[32m[20221208 14:58:17 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.85
[32m[20221208 14:58:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.01
[32m[20221208 14:58:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.27
[32m[20221208 14:58:18 @agent_ppo2.py:137][0m Total time:      21.72 min
[32m[20221208 14:58:18 @agent_ppo2.py:139][0m 1755136 total steps have happened
[32m[20221208 14:58:18 @agent_ppo2.py:115][0m #------------------------ Iteration 857 --------------------------#
[32m[20221208 14:58:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:18 @agent_ppo2.py:179][0m |           0.0690 |           8.4684 |        -718.9725 |
[32m[20221208 14:58:18 @agent_ppo2.py:179][0m |           0.0821 |           7.0465 |        -605.9964 |
[32m[20221208 14:58:18 @agent_ppo2.py:179][0m |           0.0137 |           6.2523 |        -663.1164 |
[32m[20221208 14:58:18 @agent_ppo2.py:179][0m |          -0.0119 |           5.8073 |        -708.1667 |
[32m[20221208 14:58:18 @agent_ppo2.py:179][0m |          -0.0312 |           5.5137 |        -743.1531 |
[32m[20221208 14:58:19 @agent_ppo2.py:179][0m |          -0.0439 |           5.2950 |        -765.1922 |
[32m[20221208 14:58:19 @agent_ppo2.py:179][0m |          -0.0474 |           5.1988 |        -776.3677 |
[32m[20221208 14:58:19 @agent_ppo2.py:179][0m |          -0.0543 |           4.9814 |        -791.7010 |
[32m[20221208 14:58:19 @agent_ppo2.py:179][0m |          -0.0591 |           4.9017 |        -814.7394 |
[32m[20221208 14:58:19 @agent_ppo2.py:179][0m |          -0.0605 |           4.7610 |        -819.7785 |
[32m[20221208 14:58:19 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.89
[32m[20221208 14:58:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.83
[32m[20221208 14:58:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 183.01
[32m[20221208 14:58:19 @agent_ppo2.py:137][0m Total time:      21.75 min
[32m[20221208 14:58:19 @agent_ppo2.py:139][0m 1757184 total steps have happened
[32m[20221208 14:58:19 @agent_ppo2.py:115][0m #------------------------ Iteration 858 --------------------------#
[32m[20221208 14:58:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |           0.1310 |           5.1852 |        -670.4791 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |           0.0590 |           4.3125 |        -619.6645 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |           0.0318 |           4.0098 |        -644.1627 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0082 |           3.8694 |        -691.9911 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0228 |           3.7506 |        -697.7968 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0444 |           3.6490 |        -743.0152 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0467 |           3.5739 |        -753.2039 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0562 |           3.5228 |        -766.7920 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0636 |           3.4611 |        -779.6094 |
[32m[20221208 14:58:20 @agent_ppo2.py:179][0m |          -0.0654 |           3.3971 |        -783.4080 |
[32m[20221208 14:58:20 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:58:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.68
[32m[20221208 14:58:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.29
[32m[20221208 14:58:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 180.55
[32m[20221208 14:58:21 @agent_ppo2.py:137][0m Total time:      21.77 min
[32m[20221208 14:58:21 @agent_ppo2.py:139][0m 1759232 total steps have happened
[32m[20221208 14:58:21 @agent_ppo2.py:115][0m #------------------------ Iteration 859 --------------------------#
[32m[20221208 14:58:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:21 @agent_ppo2.py:179][0m |           0.0819 |           7.7635 |        -716.4708 |
[32m[20221208 14:58:21 @agent_ppo2.py:179][0m |           0.0685 |           6.9002 |        -613.0467 |
[32m[20221208 14:58:21 @agent_ppo2.py:179][0m |           0.0024 |           6.5182 |        -662.7081 |
[32m[20221208 14:58:21 @agent_ppo2.py:179][0m |          -0.0226 |           6.2527 |        -711.1173 |
[32m[20221208 14:58:22 @agent_ppo2.py:179][0m |          -0.0379 |           5.9591 |        -734.3201 |
[32m[20221208 14:58:22 @agent_ppo2.py:179][0m |          -0.0494 |           5.7400 |        -754.9952 |
[32m[20221208 14:58:22 @agent_ppo2.py:179][0m |          -0.0578 |           5.5616 |        -776.4470 |
[32m[20221208 14:58:22 @agent_ppo2.py:179][0m |          -0.0591 |           5.4119 |        -781.9227 |
[32m[20221208 14:58:22 @agent_ppo2.py:179][0m |          -0.0628 |           5.2134 |        -792.5824 |
[32m[20221208 14:58:22 @agent_ppo2.py:179][0m |          -0.0678 |           5.1166 |        -795.7623 |
[32m[20221208 14:58:22 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.90
[32m[20221208 14:58:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.93
[32m[20221208 14:58:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.42
[32m[20221208 14:58:22 @agent_ppo2.py:137][0m Total time:      21.80 min
[32m[20221208 14:58:22 @agent_ppo2.py:139][0m 1761280 total steps have happened
[32m[20221208 14:58:22 @agent_ppo2.py:115][0m #------------------------ Iteration 860 --------------------------#
[32m[20221208 14:58:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |           0.0894 |          11.4237 |        -729.4873 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |           0.0628 |          10.4265 |        -618.2949 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |           0.0117 |          10.0066 |        -659.5715 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0168 |           9.7427 |        -688.8145 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0343 |           9.5247 |        -720.7851 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0423 |           9.3101 |        -745.4778 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0461 |           9.2235 |        -754.5326 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0528 |           9.0763 |        -758.7609 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0605 |           8.9682 |        -781.9206 |
[32m[20221208 14:58:23 @agent_ppo2.py:179][0m |          -0.0622 |           8.9446 |        -797.3820 |
[32m[20221208 14:58:23 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.75
[32m[20221208 14:58:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.47
[32m[20221208 14:58:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.86
[32m[20221208 14:58:24 @agent_ppo2.py:137][0m Total time:      21.83 min
[32m[20221208 14:58:24 @agent_ppo2.py:139][0m 1763328 total steps have happened
[32m[20221208 14:58:24 @agent_ppo2.py:115][0m #------------------------ Iteration 861 --------------------------#
[32m[20221208 14:58:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:24 @agent_ppo2.py:179][0m |           0.1660 |           8.2443 |        -676.7783 |
[32m[20221208 14:58:24 @agent_ppo2.py:179][0m |           0.0614 |           7.3716 |        -570.1086 |
[32m[20221208 14:58:24 @agent_ppo2.py:179][0m |           0.0191 |           7.0445 |        -620.9234 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0177 |           6.7843 |        -665.2053 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0374 |           6.6370 |        -704.3474 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0474 |           6.5075 |        -720.7294 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0575 |           6.5588 |        -735.9755 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0643 |           6.3868 |        -750.4804 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0698 |           6.2990 |        -764.3297 |
[32m[20221208 14:58:25 @agent_ppo2.py:179][0m |          -0.0730 |           6.1832 |        -778.3733 |
[32m[20221208 14:58:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:58:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.74
[32m[20221208 14:58:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.34
[32m[20221208 14:58:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 166.77
[32m[20221208 14:58:25 @agent_ppo2.py:137][0m Total time:      21.85 min
[32m[20221208 14:58:25 @agent_ppo2.py:139][0m 1765376 total steps have happened
[32m[20221208 14:58:25 @agent_ppo2.py:115][0m #------------------------ Iteration 862 --------------------------#
[32m[20221208 14:58:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |           0.0992 |           7.5598 |        -792.9251 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |           0.1188 |           5.6586 |        -522.8146 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |           0.0313 |           5.0832 |        -642.4101 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |           0.0116 |           4.6601 |        -677.2577 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |          -0.0323 |           4.3723 |        -757.6386 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |          -0.0500 |           4.1677 |        -782.3404 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |          -0.0550 |           4.0419 |        -795.4605 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |          -0.0647 |           3.8974 |        -813.8069 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |          -0.0679 |           3.8220 |        -838.7571 |
[32m[20221208 14:58:26 @agent_ppo2.py:179][0m |          -0.0724 |           3.6944 |        -846.6961 |
[32m[20221208 14:58:26 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.18
[32m[20221208 14:58:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.35
[32m[20221208 14:58:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.35
[32m[20221208 14:58:27 @agent_ppo2.py:137][0m Total time:      21.88 min
[32m[20221208 14:58:27 @agent_ppo2.py:139][0m 1767424 total steps have happened
[32m[20221208 14:58:27 @agent_ppo2.py:115][0m #------------------------ Iteration 863 --------------------------#
[32m[20221208 14:58:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:27 @agent_ppo2.py:179][0m |           0.0853 |          10.7090 |        -636.4080 |
[32m[20221208 14:58:27 @agent_ppo2.py:179][0m |           0.1211 |           9.4797 |        -360.3176 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |           0.0623 |           8.9041 |        -405.3121 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |           0.0182 |           8.6010 |        -450.1476 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |          -0.0003 |           8.3399 |        -506.1216 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |          -0.0118 |           8.1982 |        -525.2752 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |          -0.0237 |           7.9509 |        -564.6812 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |          -0.0282 |           7.8591 |        -575.7515 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |          -0.0361 |           7.7538 |        -596.5288 |
[32m[20221208 14:58:28 @agent_ppo2.py:179][0m |          -0.0409 |           7.6203 |        -610.4067 |
[32m[20221208 14:58:28 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.21
[32m[20221208 14:58:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 193.94
[32m[20221208 14:58:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.30
[32m[20221208 14:58:28 @agent_ppo2.py:137][0m Total time:      21.90 min
[32m[20221208 14:58:28 @agent_ppo2.py:139][0m 1769472 total steps have happened
[32m[20221208 14:58:28 @agent_ppo2.py:115][0m #------------------------ Iteration 864 --------------------------#
[32m[20221208 14:58:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |           0.0821 |           8.2188 |        -651.3735 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |           0.0514 |           7.5876 |        -556.6244 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0023 |           7.2183 |        -620.6441 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0218 |           6.9944 |        -643.1662 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0345 |           6.8753 |        -667.3597 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0404 |           6.7517 |        -673.4079 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0402 |           6.6261 |        -660.7342 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0563 |           6.5401 |        -696.8545 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0612 |           6.4589 |        -710.6451 |
[32m[20221208 14:58:29 @agent_ppo2.py:179][0m |          -0.0602 |           6.3948 |        -719.0369 |
[32m[20221208 14:58:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.45
[32m[20221208 14:58:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.00
[32m[20221208 14:58:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.40
[32m[20221208 14:58:30 @agent_ppo2.py:137][0m Total time:      21.93 min
[32m[20221208 14:58:30 @agent_ppo2.py:139][0m 1771520 total steps have happened
[32m[20221208 14:58:30 @agent_ppo2.py:115][0m #------------------------ Iteration 865 --------------------------#
[32m[20221208 14:58:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:30 @agent_ppo2.py:179][0m |           0.0691 |           7.7510 |        -710.0673 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |           0.0368 |           7.1880 |        -654.2984 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0067 |           6.9394 |        -694.0576 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0265 |           6.7500 |        -704.1035 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0455 |           6.6489 |        -735.2984 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0533 |           6.5492 |        -755.9774 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0594 |           6.4820 |        -771.6637 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0622 |           6.4158 |        -784.0440 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0675 |           6.3433 |        -792.2400 |
[32m[20221208 14:58:31 @agent_ppo2.py:179][0m |          -0.0657 |           6.2714 |        -798.2277 |
[32m[20221208 14:58:31 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 148.36
[32m[20221208 14:58:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.88
[32m[20221208 14:58:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.65
[32m[20221208 14:58:31 @agent_ppo2.py:137][0m Total time:      21.95 min
[32m[20221208 14:58:31 @agent_ppo2.py:139][0m 1773568 total steps have happened
[32m[20221208 14:58:31 @agent_ppo2.py:115][0m #------------------------ Iteration 866 --------------------------#
[32m[20221208 14:58:32 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:58:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |           0.1979 |           9.6482 |        -685.7584 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |           0.1031 |           8.4277 |        -400.5757 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |           0.0797 |           7.8552 |        -332.4062 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |           0.0387 |           7.5157 |        -440.5137 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |           0.0137 |           7.2473 |        -532.4211 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |          -0.0098 |           7.0888 |        -597.2544 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |          -0.0234 |           6.8669 |        -633.1127 |
[32m[20221208 14:58:32 @agent_ppo2.py:179][0m |          -0.0328 |           6.7220 |        -658.9837 |
[32m[20221208 14:58:33 @agent_ppo2.py:179][0m |          -0.0377 |           6.5655 |        -675.2738 |
[32m[20221208 14:58:33 @agent_ppo2.py:179][0m |          -0.0480 |           6.4375 |        -700.9883 |
[32m[20221208 14:58:33 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 161.51
[32m[20221208 14:58:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.31
[32m[20221208 14:58:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.94
[32m[20221208 14:58:33 @agent_ppo2.py:137][0m Total time:      21.98 min
[32m[20221208 14:58:33 @agent_ppo2.py:139][0m 1775616 total steps have happened
[32m[20221208 14:58:33 @agent_ppo2.py:115][0m #------------------------ Iteration 867 --------------------------#
[32m[20221208 14:58:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |           0.0745 |           8.7503 |        -693.4730 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |           0.0709 |           8.0353 |        -543.7625 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |           0.0121 |           7.6908 |        -628.9038 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0168 |           7.4727 |        -679.5909 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0329 |           7.2539 |        -683.4763 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0407 |           7.0815 |        -706.6915 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0486 |           6.9964 |        -730.8363 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0537 |           6.8098 |        -736.8908 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0528 |           6.6914 |        -742.6647 |
[32m[20221208 14:58:34 @agent_ppo2.py:179][0m |          -0.0590 |           6.6162 |        -759.1608 |
[32m[20221208 14:58:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:58:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.09
[32m[20221208 14:58:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.84
[32m[20221208 14:58:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.16
[32m[20221208 14:58:35 @agent_ppo2.py:137][0m Total time:      22.00 min
[32m[20221208 14:58:35 @agent_ppo2.py:139][0m 1777664 total steps have happened
[32m[20221208 14:58:35 @agent_ppo2.py:115][0m #------------------------ Iteration 868 --------------------------#
[32m[20221208 14:58:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |           0.0849 |           6.5671 |        -667.2132 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |           0.0852 |           5.3100 |        -534.8546 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |           0.0435 |           4.8844 |        -484.8847 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |          -0.0013 |           4.5618 |        -508.8859 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |          -0.0216 |           4.3110 |        -584.9396 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |          -0.0343 |           4.1355 |        -607.5130 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |          -0.0436 |           3.9861 |        -634.4019 |
[32m[20221208 14:58:35 @agent_ppo2.py:179][0m |          -0.0495 |           3.8320 |        -648.8799 |
[32m[20221208 14:58:36 @agent_ppo2.py:179][0m |          -0.0553 |           3.7036 |        -659.6790 |
[32m[20221208 14:58:36 @agent_ppo2.py:179][0m |          -0.0613 |           3.6684 |        -684.5926 |
[32m[20221208 14:58:36 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.32
[32m[20221208 14:58:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 190.59
[32m[20221208 14:58:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.47
[32m[20221208 14:58:36 @agent_ppo2.py:137][0m Total time:      22.03 min
[32m[20221208 14:58:36 @agent_ppo2.py:139][0m 1779712 total steps have happened
[32m[20221208 14:58:36 @agent_ppo2.py:115][0m #------------------------ Iteration 869 --------------------------#
[32m[20221208 14:58:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |           0.0885 |           7.3510 |        -658.2738 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |           0.0499 |           6.4298 |        -550.8087 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |           0.0272 |           6.1395 |        -572.3069 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0050 |           5.9697 |        -639.4109 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0212 |           5.7698 |        -683.2318 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0292 |           5.6757 |        -707.8684 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0365 |           5.5786 |        -727.3979 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0441 |           5.5153 |        -743.3535 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0442 |           5.4148 |        -746.2394 |
[32m[20221208 14:58:37 @agent_ppo2.py:179][0m |          -0.0407 |           5.3649 |        -737.9919 |
[32m[20221208 14:58:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:58:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.77
[32m[20221208 14:58:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.61
[32m[20221208 14:58:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.25
[32m[20221208 14:58:38 @agent_ppo2.py:137][0m Total time:      22.05 min
[32m[20221208 14:58:38 @agent_ppo2.py:139][0m 1781760 total steps have happened
[32m[20221208 14:58:38 @agent_ppo2.py:115][0m #------------------------ Iteration 870 --------------------------#
[32m[20221208 14:58:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |           0.0865 |           9.5448 |        -647.4182 |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |           0.0463 |           8.0199 |        -581.3733 |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |           0.0063 |           7.3626 |        -658.9840 |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |          -0.0232 |           7.0074 |        -699.1433 |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |          -0.0430 |           6.7505 |        -726.2870 |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |          -0.0486 |           6.4828 |        -735.5870 |
[32m[20221208 14:58:38 @agent_ppo2.py:179][0m |          -0.0455 |           6.2925 |        -713.4745 |
[32m[20221208 14:58:39 @agent_ppo2.py:179][0m |          -0.0598 |           6.1918 |        -757.4245 |
[32m[20221208 14:58:39 @agent_ppo2.py:179][0m |          -0.0676 |           6.0348 |        -769.5532 |
[32m[20221208 14:58:39 @agent_ppo2.py:179][0m |          -0.0683 |           5.8685 |        -775.6879 |
[32m[20221208 14:58:39 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 148.49
[32m[20221208 14:58:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.37
[32m[20221208 14:58:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.71
[32m[20221208 14:58:39 @agent_ppo2.py:137][0m Total time:      22.08 min
[32m[20221208 14:58:39 @agent_ppo2.py:139][0m 1783808 total steps have happened
[32m[20221208 14:58:39 @agent_ppo2.py:115][0m #------------------------ Iteration 871 --------------------------#
[32m[20221208 14:58:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |           0.0688 |           7.5679 |        -722.0293 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |           0.0619 |           6.5883 |        -602.3363 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |           0.0460 |           6.2482 |        -482.0138 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |           0.0016 |           6.0466 |        -516.4576 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |          -0.0182 |           5.8805 |        -573.9378 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |          -0.0292 |           5.7340 |        -589.5087 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |          -0.0409 |           5.7705 |        -628.1973 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |          -0.0503 |           5.6050 |        -647.0328 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |          -0.0494 |           5.5146 |        -658.4757 |
[32m[20221208 14:58:40 @agent_ppo2.py:179][0m |          -0.0547 |           5.4724 |        -664.0519 |
[32m[20221208 14:58:40 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.10
[32m[20221208 14:58:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.73
[32m[20221208 14:58:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.79
[32m[20221208 14:58:41 @agent_ppo2.py:137][0m Total time:      22.11 min
[32m[20221208 14:58:41 @agent_ppo2.py:139][0m 1785856 total steps have happened
[32m[20221208 14:58:41 @agent_ppo2.py:115][0m #------------------------ Iteration 872 --------------------------#
[32m[20221208 14:58:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:41 @agent_ppo2.py:179][0m |           0.0790 |           9.1054 |        -677.6324 |
[32m[20221208 14:58:41 @agent_ppo2.py:179][0m |           0.0822 |           8.4247 |        -500.3753 |
[32m[20221208 14:58:41 @agent_ppo2.py:179][0m |           0.0365 |           8.1363 |        -569.2984 |
[32m[20221208 14:58:41 @agent_ppo2.py:179][0m |           0.0123 |           7.8864 |        -656.9690 |
[32m[20221208 14:58:41 @agent_ppo2.py:179][0m |          -0.0160 |           7.7774 |        -729.7662 |
[32m[20221208 14:58:41 @agent_ppo2.py:179][0m |          -0.0256 |           7.6769 |        -737.2455 |
[32m[20221208 14:58:42 @agent_ppo2.py:179][0m |          -0.0318 |           7.5320 |        -760.0282 |
[32m[20221208 14:58:42 @agent_ppo2.py:179][0m |          -0.0442 |           7.4382 |        -777.8693 |
[32m[20221208 14:58:42 @agent_ppo2.py:179][0m |          -0.0499 |           7.3383 |        -802.0868 |
[32m[20221208 14:58:42 @agent_ppo2.py:179][0m |          -0.0556 |           7.2687 |        -806.4642 |
[32m[20221208 14:58:42 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.16
[32m[20221208 14:58:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.91
[32m[20221208 14:58:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.42
[32m[20221208 14:58:42 @agent_ppo2.py:137][0m Total time:      22.13 min
[32m[20221208 14:58:42 @agent_ppo2.py:139][0m 1787904 total steps have happened
[32m[20221208 14:58:42 @agent_ppo2.py:115][0m #------------------------ Iteration 873 --------------------------#
[32m[20221208 14:58:43 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:58:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |           0.0799 |           8.9032 |        -667.4390 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |           0.0887 |           7.7613 |        -462.9518 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |           0.0285 |           7.2235 |        -477.0347 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0141 |           6.9681 |        -587.2688 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0285 |           6.8440 |        -623.7919 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0440 |           6.6977 |        -659.7457 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0533 |           6.5528 |        -684.5439 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0602 |           6.4122 |        -688.2089 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0605 |           6.3383 |        -702.8790 |
[32m[20221208 14:58:43 @agent_ppo2.py:179][0m |          -0.0635 |           6.2104 |        -711.4713 |
[32m[20221208 14:58:43 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:58:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.34
[32m[20221208 14:58:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.88
[32m[20221208 14:58:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.61
[32m[20221208 14:58:44 @agent_ppo2.py:137][0m Total time:      22.16 min
[32m[20221208 14:58:44 @agent_ppo2.py:139][0m 1789952 total steps have happened
[32m[20221208 14:58:44 @agent_ppo2.py:115][0m #------------------------ Iteration 874 --------------------------#
[32m[20221208 14:58:44 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:58:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:44 @agent_ppo2.py:179][0m |           0.1420 |           7.5155 |        -778.4023 |
[32m[20221208 14:58:44 @agent_ppo2.py:179][0m |           0.0867 |           6.4330 |        -581.5739 |
[32m[20221208 14:58:44 @agent_ppo2.py:179][0m |           0.0319 |           5.9326 |        -681.0171 |
[32m[20221208 14:58:44 @agent_ppo2.py:179][0m |           0.0180 |           5.6137 |        -738.9457 |
[32m[20221208 14:58:45 @agent_ppo2.py:179][0m |          -0.0083 |           5.3942 |        -779.8363 |
[32m[20221208 14:58:45 @agent_ppo2.py:179][0m |          -0.0246 |           5.1741 |        -807.3751 |
[32m[20221208 14:58:45 @agent_ppo2.py:179][0m |          -0.0309 |           4.9707 |        -834.4854 |
[32m[20221208 14:58:45 @agent_ppo2.py:179][0m |          -0.0372 |           4.8457 |        -844.1059 |
[32m[20221208 14:58:45 @agent_ppo2.py:179][0m |          -0.0437 |           4.7064 |        -873.2950 |
[32m[20221208 14:58:45 @agent_ppo2.py:179][0m |          -0.0490 |           4.5724 |        -877.6692 |
[32m[20221208 14:58:45 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.75
[32m[20221208 14:58:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.98
[32m[20221208 14:58:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.00
[32m[20221208 14:58:45 @agent_ppo2.py:137][0m Total time:      22.18 min
[32m[20221208 14:58:45 @agent_ppo2.py:139][0m 1792000 total steps have happened
[32m[20221208 14:58:45 @agent_ppo2.py:115][0m #------------------------ Iteration 875 --------------------------#
[32m[20221208 14:58:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |           0.0955 |           4.0228 |        -597.4001 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |           0.0397 |           3.4904 |        -508.7438 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |           0.0070 |           3.3528 |        -553.2489 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0049 |           3.2672 |        -561.0385 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0165 |           3.2291 |        -579.6598 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0206 |           3.1816 |        -597.1504 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0233 |           3.1414 |        -633.3297 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0345 |           3.1392 |        -662.7445 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0336 |           3.0689 |        -682.1982 |
[32m[20221208 14:58:46 @agent_ppo2.py:179][0m |          -0.0378 |           3.0866 |        -709.9462 |
[32m[20221208 14:58:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:58:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.31
[32m[20221208 14:58:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.45
[32m[20221208 14:58:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 153.88
[32m[20221208 14:58:47 @agent_ppo2.py:137][0m Total time:      22.21 min
[32m[20221208 14:58:47 @agent_ppo2.py:139][0m 1794048 total steps have happened
[32m[20221208 14:58:47 @agent_ppo2.py:115][0m #------------------------ Iteration 876 --------------------------#
[32m[20221208 14:58:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:58:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:47 @agent_ppo2.py:179][0m |           0.1091 |           7.0797 |        -887.7889 |
[32m[20221208 14:58:47 @agent_ppo2.py:179][0m |           0.0460 |           5.1347 |        -843.6063 |
[32m[20221208 14:58:47 @agent_ppo2.py:179][0m |           0.0064 |           4.5436 |        -830.1313 |
[32m[20221208 14:58:47 @agent_ppo2.py:179][0m |          -0.0263 |           4.2678 |        -852.7753 |
[32m[20221208 14:58:48 @agent_ppo2.py:179][0m |          -0.0423 |           4.1082 |        -885.4979 |
[32m[20221208 14:58:48 @agent_ppo2.py:179][0m |          -0.0543 |           3.9706 |        -907.9235 |
[32m[20221208 14:58:48 @agent_ppo2.py:179][0m |          -0.0543 |           3.8642 |        -919.7994 |
[32m[20221208 14:58:48 @agent_ppo2.py:179][0m |          -0.0623 |           3.7875 |        -942.9167 |
[32m[20221208 14:58:48 @agent_ppo2.py:179][0m |          -0.0646 |           3.7589 |        -944.4348 |
[32m[20221208 14:58:48 @agent_ppo2.py:179][0m |          -0.0672 |           3.6852 |        -953.4096 |
[32m[20221208 14:58:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:58:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.52
[32m[20221208 14:58:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 168.52
[32m[20221208 14:58:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.12
[32m[20221208 14:58:48 @agent_ppo2.py:137][0m Total time:      22.23 min
[32m[20221208 14:58:48 @agent_ppo2.py:139][0m 1796096 total steps have happened
[32m[20221208 14:58:48 @agent_ppo2.py:115][0m #------------------------ Iteration 877 --------------------------#
[32m[20221208 14:58:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |           0.0994 |           8.3072 |        -925.8296 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |           0.0734 |           7.4652 |        -603.8211 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |           0.0242 |           7.0772 |        -708.5539 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0056 |           6.8499 |        -797.2428 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0279 |           6.6504 |        -880.1115 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0382 |           6.4946 |        -884.3820 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0458 |           6.4106 |        -918.3813 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0552 |           6.2696 |        -957.9936 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0596 |           6.2376 |        -977.8386 |
[32m[20221208 14:58:49 @agent_ppo2.py:179][0m |          -0.0660 |           6.0857 |       -1005.4844 |
[32m[20221208 14:58:49 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 109.92
[32m[20221208 14:58:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.56
[32m[20221208 14:58:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.41
[32m[20221208 14:58:50 @agent_ppo2.py:137][0m Total time:      22.26 min
[32m[20221208 14:58:50 @agent_ppo2.py:139][0m 1798144 total steps have happened
[32m[20221208 14:58:50 @agent_ppo2.py:115][0m #------------------------ Iteration 878 --------------------------#
[32m[20221208 14:58:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:50 @agent_ppo2.py:179][0m |           0.1104 |          10.7180 |        -896.3914 |
[32m[20221208 14:58:50 @agent_ppo2.py:179][0m |           0.1241 |           9.4410 |        -560.4614 |
[32m[20221208 14:58:50 @agent_ppo2.py:179][0m |           0.0535 |           8.9485 |        -658.6691 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |           0.0156 |           8.7050 |        -748.1985 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |          -0.0112 |           8.4843 |        -816.9027 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |          -0.0208 |           8.3693 |        -822.1798 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |          -0.0355 |           8.2309 |        -894.1122 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |          -0.0347 |           8.0939 |        -894.9493 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |          -0.0488 |           8.0607 |        -933.9326 |
[32m[20221208 14:58:51 @agent_ppo2.py:179][0m |          -0.0545 |           7.9915 |        -952.7493 |
[32m[20221208 14:58:51 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.78
[32m[20221208 14:58:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.47
[32m[20221208 14:58:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.97
[32m[20221208 14:58:51 @agent_ppo2.py:137][0m Total time:      22.28 min
[32m[20221208 14:58:51 @agent_ppo2.py:139][0m 1800192 total steps have happened
[32m[20221208 14:58:51 @agent_ppo2.py:115][0m #------------------------ Iteration 879 --------------------------#
[32m[20221208 14:58:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |           0.0946 |           9.2539 |       -1020.3482 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |           0.0483 |           7.5952 |        -909.1752 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |           0.0008 |           6.9050 |        -914.8426 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0200 |           6.5099 |        -974.4352 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0368 |           6.2368 |        -987.7144 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0474 |           5.9908 |       -1010.1303 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0536 |           5.8115 |       -1025.8833 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0497 |           5.6345 |       -1019.1496 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0598 |           5.5651 |       -1025.0658 |
[32m[20221208 14:58:52 @agent_ppo2.py:179][0m |          -0.0608 |           5.3761 |       -1033.2782 |
[32m[20221208 14:58:52 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:58:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.63
[32m[20221208 14:58:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.77
[32m[20221208 14:58:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.69
[32m[20221208 14:58:53 @agent_ppo2.py:137][0m Total time:      22.31 min
[32m[20221208 14:58:53 @agent_ppo2.py:139][0m 1802240 total steps have happened
[32m[20221208 14:58:53 @agent_ppo2.py:115][0m #------------------------ Iteration 880 --------------------------#
[32m[20221208 14:58:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:53 @agent_ppo2.py:179][0m |           0.1034 |           8.0528 |        -882.1424 |
[32m[20221208 14:58:53 @agent_ppo2.py:179][0m |           0.0533 |           6.9063 |        -714.3809 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |           0.0066 |           6.4569 |        -780.2211 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0214 |           6.1823 |        -836.2907 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0424 |           6.0064 |        -883.1230 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0534 |           5.8780 |        -912.0936 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0604 |           5.7271 |        -920.1958 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0667 |           5.6592 |        -930.4446 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0664 |           5.5632 |        -955.2968 |
[32m[20221208 14:58:54 @agent_ppo2.py:179][0m |          -0.0731 |           5.5105 |        -962.3247 |
[32m[20221208 14:58:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.83
[32m[20221208 14:58:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 191.63
[32m[20221208 14:58:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.18
[32m[20221208 14:58:54 @agent_ppo2.py:137][0m Total time:      22.34 min
[32m[20221208 14:58:54 @agent_ppo2.py:139][0m 1804288 total steps have happened
[32m[20221208 14:58:54 @agent_ppo2.py:115][0m #------------------------ Iteration 881 --------------------------#
[32m[20221208 14:58:55 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:58:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |           0.1499 |           4.4269 |        -823.1427 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |           0.1848 |           3.6313 |        -279.5289 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |           0.0783 |           3.3304 |        -274.2713 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |           0.0410 |           3.1845 |        -365.3520 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |           0.0157 |           3.0508 |        -413.7355 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |           0.0024 |           2.9269 |        -453.6181 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |          -0.0103 |           2.8935 |        -494.6190 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |          -0.0169 |           2.8473 |        -527.7803 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |          -0.0213 |           2.7611 |        -558.8888 |
[32m[20221208 14:58:55 @agent_ppo2.py:179][0m |          -0.0264 |           2.7263 |        -556.3322 |
[32m[20221208 14:58:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:58:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.83
[32m[20221208 14:58:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.00
[32m[20221208 14:58:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.61
[32m[20221208 14:58:56 @agent_ppo2.py:137][0m Total time:      22.36 min
[32m[20221208 14:58:56 @agent_ppo2.py:139][0m 1806336 total steps have happened
[32m[20221208 14:58:56 @agent_ppo2.py:115][0m #------------------------ Iteration 882 --------------------------#
[32m[20221208 14:58:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:56 @agent_ppo2.py:179][0m |           0.1465 |           5.6047 |        -956.9479 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |           0.0784 |           4.5193 |        -701.0745 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |           0.0389 |           4.0522 |        -615.9578 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0048 |           3.7262 |        -692.3757 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0306 |           3.5504 |        -746.9587 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0443 |           3.3540 |        -785.3060 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0520 |           3.2999 |        -818.9042 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0594 |           3.1744 |        -830.3555 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0677 |           3.1122 |        -858.1390 |
[32m[20221208 14:58:57 @agent_ppo2.py:179][0m |          -0.0679 |           3.0195 |        -878.8573 |
[32m[20221208 14:58:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.37
[32m[20221208 14:58:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.91
[32m[20221208 14:58:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.61
[32m[20221208 14:58:57 @agent_ppo2.py:137][0m Total time:      22.39 min
[32m[20221208 14:58:57 @agent_ppo2.py:139][0m 1808384 total steps have happened
[32m[20221208 14:58:57 @agent_ppo2.py:115][0m #------------------------ Iteration 883 --------------------------#
[32m[20221208 14:58:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |           0.1105 |           5.6127 |        -823.7628 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |           0.0589 |           4.2245 |        -547.4513 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |           0.0135 |           3.8621 |        -624.3321 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |          -0.0127 |           3.6542 |        -665.3038 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |          -0.0397 |           3.4927 |        -690.3595 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |          -0.0534 |           3.4034 |        -743.7463 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |          -0.0586 |           3.2964 |        -741.8155 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |          -0.0627 |           3.2388 |        -750.8263 |
[32m[20221208 14:58:58 @agent_ppo2.py:179][0m |          -0.0656 |           3.1316 |        -757.5558 |
[32m[20221208 14:58:59 @agent_ppo2.py:179][0m |          -0.0762 |           3.0828 |        -789.7054 |
[32m[20221208 14:58:59 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:58:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.89
[32m[20221208 14:58:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.55
[32m[20221208 14:58:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.36
[32m[20221208 14:58:59 @agent_ppo2.py:137][0m Total time:      22.41 min
[32m[20221208 14:58:59 @agent_ppo2.py:139][0m 1810432 total steps have happened
[32m[20221208 14:58:59 @agent_ppo2.py:115][0m #------------------------ Iteration 884 --------------------------#
[32m[20221208 14:58:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:58:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |           0.1006 |           4.9607 |        -884.4090 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |           0.0910 |           4.4409 |        -592.1558 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |           0.0189 |           4.1674 |        -692.0623 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0132 |           4.0002 |        -778.6362 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0315 |           3.8710 |        -837.5502 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0491 |           3.7185 |        -871.3278 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0580 |           3.6454 |        -897.4030 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0678 |           3.5826 |        -922.5407 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0706 |           3.5321 |        -946.8781 |
[32m[20221208 14:59:00 @agent_ppo2.py:179][0m |          -0.0739 |           3.5269 |        -972.8203 |
[32m[20221208 14:59:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.16
[32m[20221208 14:59:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.22
[32m[20221208 14:59:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.03
[32m[20221208 14:59:00 @agent_ppo2.py:137][0m Total time:      22.44 min
[32m[20221208 14:59:00 @agent_ppo2.py:139][0m 1812480 total steps have happened
[32m[20221208 14:59:00 @agent_ppo2.py:115][0m #------------------------ Iteration 885 --------------------------#
[32m[20221208 14:59:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.1348 |           3.0597 |        -847.0303 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.1629 |           2.2894 |        -170.3579 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.1038 |           2.1579 |        -162.5454 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.0907 |           2.0893 |        -206.6750 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.0801 |           2.0586 |        -224.2584 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.0758 |           2.0334 |        -263.4064 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.0787 |           2.0051 |        -220.6716 |
[32m[20221208 14:59:01 @agent_ppo2.py:179][0m |           0.0686 |           1.9750 |        -307.0498 |
[32m[20221208 14:59:02 @agent_ppo2.py:179][0m |           0.0676 |           1.9939 |        -317.8450 |
[32m[20221208 14:59:02 @agent_ppo2.py:179][0m |           0.0522 |           1.9706 |        -501.7297 |
[32m[20221208 14:59:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:59:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.27
[32m[20221208 14:59:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.44
[32m[20221208 14:59:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.02
[32m[20221208 14:59:02 @agent_ppo2.py:137][0m Total time:      22.46 min
[32m[20221208 14:59:02 @agent_ppo2.py:139][0m 1814528 total steps have happened
[32m[20221208 14:59:02 @agent_ppo2.py:115][0m #------------------------ Iteration 886 --------------------------#
[32m[20221208 14:59:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |           0.1268 |           5.2802 |        -896.3391 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |           0.0608 |           3.9601 |        -739.5473 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0004 |           3.6141 |        -712.9475 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0314 |           3.4643 |        -693.9768 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0489 |           3.2160 |        -711.8402 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0611 |           3.1296 |        -738.3385 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0663 |           3.0675 |        -746.5798 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0702 |           2.9978 |        -759.5533 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0695 |           2.9034 |        -736.9604 |
[32m[20221208 14:59:03 @agent_ppo2.py:179][0m |          -0.0790 |           2.8415 |        -768.5850 |
[32m[20221208 14:59:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:59:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.98
[32m[20221208 14:59:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.45
[32m[20221208 14:59:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.43
[32m[20221208 14:59:04 @agent_ppo2.py:137][0m Total time:      22.49 min
[32m[20221208 14:59:04 @agent_ppo2.py:139][0m 1816576 total steps have happened
[32m[20221208 14:59:04 @agent_ppo2.py:115][0m #------------------------ Iteration 887 --------------------------#
[32m[20221208 14:59:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |           0.1031 |           5.5885 |        -911.0672 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |           0.0431 |           4.5880 |        -506.0441 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |          -0.0070 |           4.2660 |        -562.2832 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |          -0.0125 |           4.1163 |        -577.7765 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |          -0.0319 |           3.9782 |        -595.3507 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |          -0.0408 |           3.8666 |        -607.2800 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |          -0.0486 |           3.8079 |        -643.5157 |
[32m[20221208 14:59:04 @agent_ppo2.py:179][0m |          -0.0589 |           3.7379 |        -688.1171 |
[32m[20221208 14:59:05 @agent_ppo2.py:179][0m |          -0.0608 |           3.7308 |        -721.9125 |
[32m[20221208 14:59:05 @agent_ppo2.py:179][0m |          -0.0620 |           3.6384 |        -785.4415 |
[32m[20221208 14:59:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.64
[32m[20221208 14:59:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.56
[32m[20221208 14:59:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.17
[32m[20221208 14:59:05 @agent_ppo2.py:137][0m Total time:      22.51 min
[32m[20221208 14:59:05 @agent_ppo2.py:139][0m 1818624 total steps have happened
[32m[20221208 14:59:05 @agent_ppo2.py:115][0m #------------------------ Iteration 888 --------------------------#
[32m[20221208 14:59:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |           0.1073 |           7.8841 |       -1045.2882 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |           0.0586 |           6.3549 |        -779.9285 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |           0.0102 |           5.8384 |        -865.0507 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0221 |           5.5101 |        -918.5073 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0371 |           5.2880 |        -941.4683 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0507 |           5.0574 |        -968.8062 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0545 |           4.8788 |        -993.5642 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0635 |           4.7541 |       -1027.4660 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0699 |           4.6281 |       -1032.4571 |
[32m[20221208 14:59:06 @agent_ppo2.py:179][0m |          -0.0749 |           4.5291 |       -1052.1833 |
[32m[20221208 14:59:06 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.65
[32m[20221208 14:59:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.34
[32m[20221208 14:59:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 32.25
[32m[20221208 14:59:07 @agent_ppo2.py:137][0m Total time:      22.54 min
[32m[20221208 14:59:07 @agent_ppo2.py:139][0m 1820672 total steps have happened
[32m[20221208 14:59:07 @agent_ppo2.py:115][0m #------------------------ Iteration 889 --------------------------#
[32m[20221208 14:59:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |           0.2260 |           3.7251 |        -985.7747 |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |           0.1082 |           2.9587 |        -486.3151 |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |           0.0454 |           2.7097 |        -528.4261 |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |           0.0042 |           2.5467 |        -636.0598 |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |          -0.0070 |           2.4256 |        -635.6531 |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |          -0.0251 |           2.3241 |        -685.1273 |
[32m[20221208 14:59:07 @agent_ppo2.py:179][0m |          -0.0367 |           2.2482 |        -719.2035 |
[32m[20221208 14:59:08 @agent_ppo2.py:179][0m |          -0.0423 |           2.1935 |        -728.4643 |
[32m[20221208 14:59:08 @agent_ppo2.py:179][0m |          -0.0482 |           2.1305 |        -739.1254 |
[32m[20221208 14:59:08 @agent_ppo2.py:179][0m |          -0.0519 |           2.0964 |        -765.1840 |
[32m[20221208 14:59:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:59:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.88
[32m[20221208 14:59:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.69
[32m[20221208 14:59:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.70
[32m[20221208 14:59:08 @agent_ppo2.py:137][0m Total time:      22.56 min
[32m[20221208 14:59:08 @agent_ppo2.py:139][0m 1822720 total steps have happened
[32m[20221208 14:59:08 @agent_ppo2.py:115][0m #------------------------ Iteration 890 --------------------------#
[32m[20221208 14:59:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |           0.1171 |           7.1343 |       -1008.9088 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |           0.1113 |           6.0163 |        -569.6820 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |           0.0504 |           5.6548 |        -605.0069 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |           0.0145 |           5.3893 |        -711.4822 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |          -0.0079 |           5.1948 |        -754.7300 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |          -0.0180 |           5.0048 |        -799.7294 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |          -0.0287 |           4.9491 |        -811.6976 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |          -0.0450 |           4.7986 |        -837.8701 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |          -0.0499 |           4.6931 |        -878.4010 |
[32m[20221208 14:59:09 @agent_ppo2.py:179][0m |          -0.0539 |           4.6003 |        -877.4332 |
[32m[20221208 14:59:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.39
[32m[20221208 14:59:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.96
[32m[20221208 14:59:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.29
[32m[20221208 14:59:10 @agent_ppo2.py:137][0m Total time:      22.59 min
[32m[20221208 14:59:10 @agent_ppo2.py:139][0m 1824768 total steps have happened
[32m[20221208 14:59:10 @agent_ppo2.py:115][0m #------------------------ Iteration 891 --------------------------#
[32m[20221208 14:59:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:10 @agent_ppo2.py:179][0m |           0.1151 |           5.0899 |        -991.5351 |
[32m[20221208 14:59:10 @agent_ppo2.py:179][0m |           0.0593 |           4.2265 |        -808.8588 |
[32m[20221208 14:59:10 @agent_ppo2.py:179][0m |           0.0315 |           3.9433 |        -759.0357 |
[32m[20221208 14:59:10 @agent_ppo2.py:179][0m |          -0.0056 |           3.7578 |        -872.0785 |
[32m[20221208 14:59:10 @agent_ppo2.py:179][0m |          -0.0153 |           3.6128 |        -921.6085 |
[32m[20221208 14:59:10 @agent_ppo2.py:179][0m |          -0.0255 |           3.4907 |        -843.5817 |
[32m[20221208 14:59:11 @agent_ppo2.py:179][0m |          -0.0488 |           3.4477 |        -978.9891 |
[32m[20221208 14:59:11 @agent_ppo2.py:179][0m |          -0.0552 |           3.3690 |       -1004.7707 |
[32m[20221208 14:59:11 @agent_ppo2.py:179][0m |          -0.0526 |           3.2943 |       -1044.3999 |
[32m[20221208 14:59:11 @agent_ppo2.py:179][0m |          -0.0542 |           3.2827 |       -1004.5977 |
[32m[20221208 14:59:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:59:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.61
[32m[20221208 14:59:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.36
[32m[20221208 14:59:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.84
[32m[20221208 14:59:11 @agent_ppo2.py:137][0m Total time:      22.61 min
[32m[20221208 14:59:11 @agent_ppo2.py:139][0m 1826816 total steps have happened
[32m[20221208 14:59:11 @agent_ppo2.py:115][0m #------------------------ Iteration 892 --------------------------#
[32m[20221208 14:59:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |           0.1022 |           6.4449 |        -904.5624 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |           0.1072 |           5.4311 |        -576.9156 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |           0.0308 |           4.9209 |        -651.2886 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |           0.0095 |           4.6272 |        -796.3472 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |          -0.0177 |           4.4171 |        -919.1926 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |          -0.0140 |           4.3046 |        -897.4921 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |          -0.0074 |           4.1544 |        -754.5143 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |          -0.0333 |           4.1379 |        -800.2551 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |          -0.0485 |           4.0170 |        -867.4174 |
[32m[20221208 14:59:12 @agent_ppo2.py:179][0m |          -0.0538 |           3.9706 |        -915.3116 |
[32m[20221208 14:59:12 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.35
[32m[20221208 14:59:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.41
[32m[20221208 14:59:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.99
[32m[20221208 14:59:13 @agent_ppo2.py:137][0m Total time:      22.64 min
[32m[20221208 14:59:13 @agent_ppo2.py:139][0m 1828864 total steps have happened
[32m[20221208 14:59:13 @agent_ppo2.py:115][0m #------------------------ Iteration 893 --------------------------#
[32m[20221208 14:59:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:13 @agent_ppo2.py:179][0m |           0.0738 |           5.1089 |       -1111.6261 |
[32m[20221208 14:59:13 @agent_ppo2.py:179][0m |           0.0514 |           4.3187 |        -972.9400 |
[32m[20221208 14:59:13 @agent_ppo2.py:179][0m |           0.0137 |           3.9608 |        -938.2106 |
[32m[20221208 14:59:13 @agent_ppo2.py:179][0m |          -0.0246 |           3.7182 |       -1080.3363 |
[32m[20221208 14:59:13 @agent_ppo2.py:179][0m |          -0.0358 |           3.5594 |       -1081.8623 |
[32m[20221208 14:59:13 @agent_ppo2.py:179][0m |          -0.0445 |           3.4052 |       -1095.0099 |
[32m[20221208 14:59:14 @agent_ppo2.py:179][0m |          -0.0374 |           3.2534 |        -946.6249 |
[32m[20221208 14:59:14 @agent_ppo2.py:179][0m |          -0.0477 |           3.1754 |       -1043.9480 |
[32m[20221208 14:59:14 @agent_ppo2.py:179][0m |          -0.0588 |           3.1080 |       -1129.2041 |
[32m[20221208 14:59:14 @agent_ppo2.py:179][0m |          -0.0692 |           3.0318 |       -1183.6358 |
[32m[20221208 14:59:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.15
[32m[20221208 14:59:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.79
[32m[20221208 14:59:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.44
[32m[20221208 14:59:14 @agent_ppo2.py:137][0m Total time:      22.66 min
[32m[20221208 14:59:14 @agent_ppo2.py:139][0m 1830912 total steps have happened
[32m[20221208 14:59:14 @agent_ppo2.py:115][0m #------------------------ Iteration 894 --------------------------#
[32m[20221208 14:59:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |           0.0797 |           4.4593 |        -840.0408 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |           0.0690 |           3.6853 |        -556.9494 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |           0.0212 |           3.3415 |        -646.9163 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0201 |           3.1558 |        -703.9425 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0406 |           3.0443 |        -747.9976 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0481 |           2.9558 |        -760.5694 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0620 |           2.8550 |        -789.8497 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0596 |           2.7700 |        -790.5226 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0680 |           2.7273 |        -819.8841 |
[32m[20221208 14:59:15 @agent_ppo2.py:179][0m |          -0.0725 |           2.6392 |        -812.4492 |
[32m[20221208 14:59:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.48
[32m[20221208 14:59:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.49
[32m[20221208 14:59:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.66
[32m[20221208 14:59:16 @agent_ppo2.py:137][0m Total time:      22.69 min
[32m[20221208 14:59:16 @agent_ppo2.py:139][0m 1832960 total steps have happened
[32m[20221208 14:59:16 @agent_ppo2.py:115][0m #------------------------ Iteration 895 --------------------------#
[32m[20221208 14:59:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:16 @agent_ppo2.py:179][0m |           0.0722 |           2.8363 |       -1069.0818 |
[32m[20221208 14:59:16 @agent_ppo2.py:179][0m |           0.0674 |           2.3794 |        -784.5390 |
[32m[20221208 14:59:16 @agent_ppo2.py:179][0m |           0.0375 |           2.1695 |        -782.6942 |
[32m[20221208 14:59:16 @agent_ppo2.py:179][0m |          -0.0167 |           2.0459 |        -925.8081 |
[32m[20221208 14:59:16 @agent_ppo2.py:179][0m |          -0.0232 |           1.9881 |        -924.8319 |
[32m[20221208 14:59:17 @agent_ppo2.py:179][0m |          -0.0388 |           1.9236 |        -975.0893 |
[32m[20221208 14:59:17 @agent_ppo2.py:179][0m |          -0.0479 |           1.8783 |       -1007.0480 |
[32m[20221208 14:59:17 @agent_ppo2.py:179][0m |          -0.0612 |           1.8337 |       -1011.4835 |
[32m[20221208 14:59:17 @agent_ppo2.py:179][0m |          -0.0672 |           1.8157 |       -1033.2198 |
[32m[20221208 14:59:17 @agent_ppo2.py:179][0m |          -0.0673 |           1.7638 |       -1028.1755 |
[32m[20221208 14:59:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:59:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.09
[32m[20221208 14:59:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.04
[32m[20221208 14:59:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.06
[32m[20221208 14:59:17 @agent_ppo2.py:137][0m Total time:      22.72 min
[32m[20221208 14:59:17 @agent_ppo2.py:139][0m 1835008 total steps have happened
[32m[20221208 14:59:17 @agent_ppo2.py:115][0m #------------------------ Iteration 896 --------------------------#
[32m[20221208 14:59:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |           0.0717 |           7.6857 |       -1132.3349 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |           0.0497 |           6.6203 |       -1031.7680 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0104 |           6.1180 |       -1089.4593 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0353 |           5.7925 |       -1135.9279 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0534 |           5.5576 |       -1180.6563 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0646 |           5.3951 |       -1206.5238 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0682 |           5.2373 |       -1211.4893 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0720 |           5.0565 |       -1234.2913 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0785 |           4.9757 |       -1242.6938 |
[32m[20221208 14:59:18 @agent_ppo2.py:179][0m |          -0.0823 |           4.8681 |       -1272.9012 |
[32m[20221208 14:59:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.89
[32m[20221208 14:59:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.81
[32m[20221208 14:59:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.84
[32m[20221208 14:59:19 @agent_ppo2.py:137][0m Total time:      22.74 min
[32m[20221208 14:59:19 @agent_ppo2.py:139][0m 1837056 total steps have happened
[32m[20221208 14:59:19 @agent_ppo2.py:115][0m #------------------------ Iteration 897 --------------------------#
[32m[20221208 14:59:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:19 @agent_ppo2.py:179][0m |           0.1054 |           4.6871 |        -762.4298 |
[32m[20221208 14:59:19 @agent_ppo2.py:179][0m |           0.0590 |           3.7430 |        -595.8412 |
[32m[20221208 14:59:19 @agent_ppo2.py:179][0m |          -0.0061 |           3.3462 |        -679.7755 |
[32m[20221208 14:59:19 @agent_ppo2.py:179][0m |          -0.0342 |           3.0967 |        -747.9408 |
[32m[20221208 14:59:19 @agent_ppo2.py:179][0m |          -0.0480 |           2.9673 |        -788.3827 |
[32m[20221208 14:59:20 @agent_ppo2.py:179][0m |          -0.0553 |           2.8452 |        -811.0346 |
[32m[20221208 14:59:20 @agent_ppo2.py:179][0m |          -0.0713 |           2.7445 |        -854.6640 |
[32m[20221208 14:59:20 @agent_ppo2.py:179][0m |          -0.0786 |           2.6735 |        -869.9537 |
[32m[20221208 14:59:20 @agent_ppo2.py:179][0m |          -0.0820 |           2.6127 |        -876.8241 |
[32m[20221208 14:59:20 @agent_ppo2.py:179][0m |          -0.0852 |           2.5902 |        -894.0568 |
[32m[20221208 14:59:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:59:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.46
[32m[20221208 14:59:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.18
[32m[20221208 14:59:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 166.95
[32m[20221208 14:59:20 @agent_ppo2.py:137][0m Total time:      22.77 min
[32m[20221208 14:59:20 @agent_ppo2.py:139][0m 1839104 total steps have happened
[32m[20221208 14:59:20 @agent_ppo2.py:115][0m #------------------------ Iteration 898 --------------------------#
[32m[20221208 14:59:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |           0.1424 |           5.3478 |        -976.7600 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |           0.1148 |           4.6319 |        -460.4541 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |           0.0587 |           4.3476 |        -548.3284 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |           0.0220 |           4.1168 |        -610.1089 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |           0.0064 |           4.0007 |        -626.9521 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |          -0.0011 |           3.9062 |        -657.7945 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |          -0.0125 |           3.8117 |        -689.5687 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |          -0.0177 |           3.7702 |        -705.9823 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |          -0.0185 |           3.6650 |        -733.0482 |
[32m[20221208 14:59:21 @agent_ppo2.py:179][0m |          -0.0262 |           3.6029 |        -765.9264 |
[32m[20221208 14:59:21 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.18
[32m[20221208 14:59:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.41
[32m[20221208 14:59:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.11
[32m[20221208 14:59:22 @agent_ppo2.py:137][0m Total time:      22.79 min
[32m[20221208 14:59:22 @agent_ppo2.py:139][0m 1841152 total steps have happened
[32m[20221208 14:59:22 @agent_ppo2.py:115][0m #------------------------ Iteration 899 --------------------------#
[32m[20221208 14:59:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:22 @agent_ppo2.py:179][0m |           0.1006 |           4.9499 |       -1083.8243 |
[32m[20221208 14:59:22 @agent_ppo2.py:179][0m |           0.0428 |           3.7494 |        -937.8253 |
[32m[20221208 14:59:22 @agent_ppo2.py:179][0m |          -0.0099 |           3.4031 |        -974.5830 |
[32m[20221208 14:59:22 @agent_ppo2.py:179][0m |          -0.0382 |           3.1778 |       -1000.8473 |
[32m[20221208 14:59:23 @agent_ppo2.py:179][0m |          -0.0503 |           3.0570 |       -1004.5860 |
[32m[20221208 14:59:23 @agent_ppo2.py:179][0m |          -0.0607 |           2.9262 |       -1007.3048 |
[32m[20221208 14:59:23 @agent_ppo2.py:179][0m |          -0.0648 |           2.8374 |       -1009.0942 |
[32m[20221208 14:59:23 @agent_ppo2.py:179][0m |          -0.0716 |           2.7753 |       -1040.7381 |
[32m[20221208 14:59:23 @agent_ppo2.py:179][0m |          -0.0738 |           2.6883 |       -1067.5852 |
[32m[20221208 14:59:23 @agent_ppo2.py:179][0m |          -0.0827 |           2.6428 |       -1060.0254 |
[32m[20221208 14:59:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.36
[32m[20221208 14:59:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.31
[32m[20221208 14:59:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.82
[32m[20221208 14:59:23 @agent_ppo2.py:137][0m Total time:      22.82 min
[32m[20221208 14:59:23 @agent_ppo2.py:139][0m 1843200 total steps have happened
[32m[20221208 14:59:23 @agent_ppo2.py:115][0m #------------------------ Iteration 900 --------------------------#
[32m[20221208 14:59:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |           0.1683 |           4.3744 |       -1082.4395 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |           0.0734 |           3.5924 |        -851.5952 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |           0.0263 |           3.3708 |        -941.6658 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0148 |           3.2046 |        -984.7473 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0322 |           3.0091 |       -1009.1383 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0492 |           2.9552 |       -1060.4103 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0560 |           2.8453 |       -1075.5721 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0595 |           2.7742 |       -1083.3896 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0632 |           2.7252 |       -1113.4963 |
[32m[20221208 14:59:24 @agent_ppo2.py:179][0m |          -0.0714 |           2.6655 |       -1116.1786 |
[32m[20221208 14:59:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.45
[32m[20221208 14:59:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.10
[32m[20221208 14:59:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.31
[32m[20221208 14:59:25 @agent_ppo2.py:137][0m Total time:      22.84 min
[32m[20221208 14:59:25 @agent_ppo2.py:139][0m 1845248 total steps have happened
[32m[20221208 14:59:25 @agent_ppo2.py:115][0m #------------------------ Iteration 901 --------------------------#
[32m[20221208 14:59:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:25 @agent_ppo2.py:179][0m |           0.1667 |           5.1503 |       -1155.9323 |
[32m[20221208 14:59:25 @agent_ppo2.py:179][0m |           0.0635 |           4.1252 |        -910.7714 |
[32m[20221208 14:59:25 @agent_ppo2.py:179][0m |          -0.0098 |           3.8154 |        -995.5437 |
[32m[20221208 14:59:25 @agent_ppo2.py:179][0m |          -0.0373 |           3.5879 |       -1013.8950 |
[32m[20221208 14:59:26 @agent_ppo2.py:179][0m |          -0.0520 |           3.4404 |       -1058.5352 |
[32m[20221208 14:59:26 @agent_ppo2.py:179][0m |          -0.0646 |           3.3149 |       -1075.3722 |
[32m[20221208 14:59:26 @agent_ppo2.py:179][0m |          -0.0710 |           3.2009 |       -1090.5348 |
[32m[20221208 14:59:26 @agent_ppo2.py:179][0m |          -0.0837 |           3.0901 |       -1090.2571 |
[32m[20221208 14:59:26 @agent_ppo2.py:179][0m |          -0.0837 |           3.0404 |       -1108.9897 |
[32m[20221208 14:59:26 @agent_ppo2.py:179][0m |          -0.0873 |           2.9650 |       -1140.1107 |
[32m[20221208 14:59:26 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:59:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.02
[32m[20221208 14:59:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.49
[32m[20221208 14:59:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.04
[32m[20221208 14:59:26 @agent_ppo2.py:137][0m Total time:      22.87 min
[32m[20221208 14:59:26 @agent_ppo2.py:139][0m 1847296 total steps have happened
[32m[20221208 14:59:26 @agent_ppo2.py:115][0m #------------------------ Iteration 902 --------------------------#
[32m[20221208 14:59:27 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |           0.0971 |           9.6901 |       -1169.7310 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |           0.0654 |           7.8455 |       -1028.1868 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |           0.0081 |           7.1713 |       -1132.5214 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0232 |           6.7815 |       -1241.7330 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0436 |           6.4267 |       -1237.8672 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0543 |           6.2700 |       -1295.6942 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0610 |           6.0315 |       -1339.3249 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0679 |           5.9159 |       -1354.3943 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0744 |           5.7428 |       -1375.7721 |
[32m[20221208 14:59:27 @agent_ppo2.py:179][0m |          -0.0788 |           5.6512 |       -1392.9136 |
[32m[20221208 14:59:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.30
[32m[20221208 14:59:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.90
[32m[20221208 14:59:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.73
[32m[20221208 14:59:28 @agent_ppo2.py:137][0m Total time:      22.89 min
[32m[20221208 14:59:28 @agent_ppo2.py:139][0m 1849344 total steps have happened
[32m[20221208 14:59:28 @agent_ppo2.py:115][0m #------------------------ Iteration 903 --------------------------#
[32m[20221208 14:59:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:28 @agent_ppo2.py:179][0m |           0.1249 |           4.1851 |       -1198.0389 |
[32m[20221208 14:59:28 @agent_ppo2.py:179][0m |           0.1083 |           3.6573 |        -867.8500 |
[32m[20221208 14:59:28 @agent_ppo2.py:179][0m |           0.0823 |           3.4299 |        -879.0274 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |           0.0544 |           3.3359 |        -801.7423 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |           0.0157 |           3.1889 |        -884.2823 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |           0.0063 |           3.1193 |        -950.8324 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |          -0.0048 |           3.0593 |       -1013.9058 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |          -0.0148 |           2.9932 |       -1037.3706 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |          -0.0219 |           2.9660 |       -1079.9308 |
[32m[20221208 14:59:29 @agent_ppo2.py:179][0m |          -0.0263 |           2.9140 |       -1096.7899 |
[32m[20221208 14:59:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.63
[32m[20221208 14:59:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.57
[32m[20221208 14:59:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.05
[32m[20221208 14:59:29 @agent_ppo2.py:137][0m Total time:      22.92 min
[32m[20221208 14:59:29 @agent_ppo2.py:139][0m 1851392 total steps have happened
[32m[20221208 14:59:29 @agent_ppo2.py:115][0m #------------------------ Iteration 904 --------------------------#
[32m[20221208 14:59:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |           0.1029 |           7.3837 |       -1183.7224 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |           0.0423 |           6.4566 |        -761.9589 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |           0.0032 |           6.2170 |        -893.8870 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0291 |           6.0359 |        -930.5739 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0375 |           5.7900 |        -988.6830 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0482 |           5.6753 |       -1010.1768 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0601 |           5.5661 |       -1027.7713 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0693 |           5.4788 |       -1076.0223 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0683 |           5.4481 |       -1093.4363 |
[32m[20221208 14:59:30 @agent_ppo2.py:179][0m |          -0.0748 |           5.3600 |       -1091.1477 |
[32m[20221208 14:59:30 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:59:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.75
[32m[20221208 14:59:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 204.79
[32m[20221208 14:59:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.47
[32m[20221208 14:59:31 @agent_ppo2.py:137][0m Total time:      22.94 min
[32m[20221208 14:59:31 @agent_ppo2.py:139][0m 1853440 total steps have happened
[32m[20221208 14:59:31 @agent_ppo2.py:115][0m #------------------------ Iteration 905 --------------------------#
[32m[20221208 14:59:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:59:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:31 @agent_ppo2.py:179][0m |           0.1313 |           9.8589 |       -1201.9046 |
[32m[20221208 14:59:31 @agent_ppo2.py:179][0m |           0.1317 |           8.5231 |        -956.7298 |
[32m[20221208 14:59:31 @agent_ppo2.py:179][0m |           0.0417 |           7.8857 |        -993.9913 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0009 |           7.3816 |       -1105.3013 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0280 |           7.0770 |       -1161.3198 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0406 |           6.7925 |       -1213.2264 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0493 |           6.6132 |       -1256.2689 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0520 |           6.4209 |       -1259.1665 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0618 |           6.2405 |       -1292.1783 |
[32m[20221208 14:59:32 @agent_ppo2.py:179][0m |          -0.0724 |           6.1229 |       -1322.9028 |
[32m[20221208 14:59:32 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.91
[32m[20221208 14:59:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.88
[32m[20221208 14:59:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.68
[32m[20221208 14:59:32 @agent_ppo2.py:137][0m Total time:      22.97 min
[32m[20221208 14:59:32 @agent_ppo2.py:139][0m 1855488 total steps have happened
[32m[20221208 14:59:32 @agent_ppo2.py:115][0m #------------------------ Iteration 906 --------------------------#
[32m[20221208 14:59:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |           0.0648 |           7.3212 |       -1335.4184 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |           0.0553 |           5.9278 |       -1117.3077 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |           0.0239 |           5.4374 |       -1148.5355 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0105 |           5.1305 |       -1221.9403 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0398 |           4.9414 |       -1281.6716 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0553 |           4.7696 |       -1297.1870 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0658 |           4.6284 |       -1353.5089 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0767 |           4.4919 |       -1363.1995 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0766 |           4.4166 |       -1375.5469 |
[32m[20221208 14:59:33 @agent_ppo2.py:179][0m |          -0.0813 |           4.3303 |       -1399.3967 |
[32m[20221208 14:59:33 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:59:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.73
[32m[20221208 14:59:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.38
[32m[20221208 14:59:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.95
[32m[20221208 14:59:34 @agent_ppo2.py:137][0m Total time:      22.99 min
[32m[20221208 14:59:34 @agent_ppo2.py:139][0m 1857536 total steps have happened
[32m[20221208 14:59:34 @agent_ppo2.py:115][0m #------------------------ Iteration 907 --------------------------#
[32m[20221208 14:59:34 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:34 @agent_ppo2.py:179][0m |           0.1071 |           7.4460 |       -1220.9509 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |           0.1352 |           6.4047 |        -885.6681 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |           0.1836 |           6.0480 |        -861.8476 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |           0.0241 |           5.8290 |       -1056.5912 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |          -0.0120 |           5.6355 |       -1170.9752 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |          -0.0325 |           5.4618 |       -1205.5473 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |          -0.0424 |           5.3223 |       -1235.4742 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |          -0.0537 |           5.1909 |       -1262.4158 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |          -0.0599 |           5.0646 |       -1277.9699 |
[32m[20221208 14:59:35 @agent_ppo2.py:179][0m |          -0.0644 |           4.9726 |       -1297.3240 |
[32m[20221208 14:59:35 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:59:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.53
[32m[20221208 14:59:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.10
[32m[20221208 14:59:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.48
[32m[20221208 14:59:35 @agent_ppo2.py:137][0m Total time:      23.02 min
[32m[20221208 14:59:35 @agent_ppo2.py:139][0m 1859584 total steps have happened
[32m[20221208 14:59:35 @agent_ppo2.py:115][0m #------------------------ Iteration 908 --------------------------#
[32m[20221208 14:59:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |           0.1179 |           6.9922 |       -1153.4143 |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |           0.0713 |           5.8499 |        -954.4598 |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |           0.0203 |           5.4630 |       -1048.2978 |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |          -0.0044 |           5.1899 |       -1066.8792 |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |          -0.0233 |           5.0127 |       -1113.9863 |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |          -0.0384 |           4.8631 |       -1147.9961 |
[32m[20221208 14:59:36 @agent_ppo2.py:179][0m |          -0.0456 |           4.7710 |       -1174.0803 |
[32m[20221208 14:59:37 @agent_ppo2.py:179][0m |          -0.0558 |           4.6496 |       -1207.5845 |
[32m[20221208 14:59:37 @agent_ppo2.py:179][0m |          -0.0595 |           4.5676 |       -1225.1808 |
[32m[20221208 14:59:37 @agent_ppo2.py:179][0m |          -0.0614 |           4.4700 |       -1245.5534 |
[32m[20221208 14:59:37 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:59:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.02
[32m[20221208 14:59:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.95
[32m[20221208 14:59:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.51
[32m[20221208 14:59:37 @agent_ppo2.py:137][0m Total time:      23.05 min
[32m[20221208 14:59:37 @agent_ppo2.py:139][0m 1861632 total steps have happened
[32m[20221208 14:59:37 @agent_ppo2.py:115][0m #------------------------ Iteration 909 --------------------------#
[32m[20221208 14:59:38 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:59:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |           0.3213 |           9.1467 |       -1107.5187 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |           0.0839 |           7.5690 |        -953.2772 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |           0.0427 |           7.0181 |        -978.6175 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |           0.0125 |           6.6138 |        -988.6466 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |          -0.0047 |           6.3005 |       -1022.5019 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |          -0.0280 |           6.0837 |       -1093.3535 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |          -0.0326 |           5.8906 |       -1119.4629 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |          -0.0454 |           5.7337 |       -1168.5277 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |          -0.0468 |           5.6227 |       -1151.6108 |
[32m[20221208 14:59:38 @agent_ppo2.py:179][0m |          -0.0555 |           5.5441 |       -1174.7238 |
[32m[20221208 14:59:38 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:59:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.19
[32m[20221208 14:59:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.77
[32m[20221208 14:59:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.86
[32m[20221208 14:59:39 @agent_ppo2.py:137][0m Total time:      23.07 min
[32m[20221208 14:59:39 @agent_ppo2.py:139][0m 1863680 total steps have happened
[32m[20221208 14:59:39 @agent_ppo2.py:115][0m #------------------------ Iteration 910 --------------------------#
[32m[20221208 14:59:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:39 @agent_ppo2.py:179][0m |           0.1133 |           6.7973 |       -1165.1283 |
[32m[20221208 14:59:39 @agent_ppo2.py:179][0m |           0.0547 |           5.5558 |       -1072.9419 |
[32m[20221208 14:59:39 @agent_ppo2.py:179][0m |           0.0049 |           5.0556 |       -1124.0948 |
[32m[20221208 14:59:39 @agent_ppo2.py:179][0m |          -0.0246 |           4.7329 |       -1170.1182 |
[32m[20221208 14:59:39 @agent_ppo2.py:179][0m |          -0.0380 |           4.5267 |       -1189.3759 |
[32m[20221208 14:59:40 @agent_ppo2.py:179][0m |          -0.0459 |           4.3160 |       -1227.9519 |
[32m[20221208 14:59:40 @agent_ppo2.py:179][0m |          -0.0540 |           4.1630 |       -1246.7279 |
[32m[20221208 14:59:40 @agent_ppo2.py:179][0m |          -0.0626 |           4.0566 |       -1225.1436 |
[32m[20221208 14:59:40 @agent_ppo2.py:179][0m |          -0.0663 |           3.9590 |       -1262.5750 |
[32m[20221208 14:59:40 @agent_ppo2.py:179][0m |          -0.0715 |           3.8647 |       -1273.2909 |
[32m[20221208 14:59:40 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:59:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.45
[32m[20221208 14:59:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.11
[32m[20221208 14:59:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.36
[32m[20221208 14:59:40 @agent_ppo2.py:137][0m Total time:      23.10 min
[32m[20221208 14:59:40 @agent_ppo2.py:139][0m 1865728 total steps have happened
[32m[20221208 14:59:40 @agent_ppo2.py:115][0m #------------------------ Iteration 911 --------------------------#
[32m[20221208 14:59:41 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |           0.1171 |           8.4465 |       -1179.2791 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |           0.0872 |           6.5975 |        -896.7790 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |           0.0298 |           5.8832 |       -1050.2190 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0072 |           5.3271 |       -1134.0438 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0289 |           4.9843 |       -1186.9643 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0408 |           4.6597 |       -1217.1301 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0479 |           4.4843 |       -1238.2637 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0550 |           4.2688 |       -1256.1597 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0593 |           4.0876 |       -1278.3933 |
[32m[20221208 14:59:41 @agent_ppo2.py:179][0m |          -0.0676 |           3.9486 |       -1307.5829 |
[32m[20221208 14:59:41 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:59:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.43
[32m[20221208 14:59:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.61
[32m[20221208 14:59:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.57
[32m[20221208 14:59:42 @agent_ppo2.py:137][0m Total time:      23.13 min
[32m[20221208 14:59:42 @agent_ppo2.py:139][0m 1867776 total steps have happened
[32m[20221208 14:59:42 @agent_ppo2.py:115][0m #------------------------ Iteration 912 --------------------------#
[32m[20221208 14:59:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:42 @agent_ppo2.py:179][0m |           0.1124 |           7.7549 |       -1092.2308 |
[32m[20221208 14:59:42 @agent_ppo2.py:179][0m |           0.0912 |           6.3639 |        -811.3260 |
[32m[20221208 14:59:42 @agent_ppo2.py:179][0m |           0.0344 |           5.8675 |        -858.3281 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0075 |           5.5171 |        -991.5443 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0304 |           5.3108 |       -1055.8275 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0441 |           5.1747 |       -1089.9780 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0545 |           5.0189 |       -1139.4377 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0584 |           4.8421 |       -1112.5934 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0685 |           4.7521 |       -1157.3763 |
[32m[20221208 14:59:43 @agent_ppo2.py:179][0m |          -0.0731 |           4.6903 |       -1167.4052 |
[32m[20221208 14:59:43 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:59:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.30
[32m[20221208 14:59:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.16
[32m[20221208 14:59:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.76
[32m[20221208 14:59:43 @agent_ppo2.py:137][0m Total time:      23.15 min
[32m[20221208 14:59:43 @agent_ppo2.py:139][0m 1869824 total steps have happened
[32m[20221208 14:59:43 @agent_ppo2.py:115][0m #------------------------ Iteration 913 --------------------------#
[32m[20221208 14:59:44 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |           0.0873 |           8.6710 |       -1159.0141 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |           0.2161 |           7.8378 |        -782.9429 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |           0.0920 |           7.4990 |        -750.8605 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |           0.0407 |           7.3074 |        -838.6478 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |           0.0098 |           7.2231 |        -925.7649 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |          -0.0103 |           7.0810 |        -998.5703 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |          -0.0275 |           7.0688 |       -1039.6246 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |          -0.0386 |           6.9995 |       -1070.8283 |
[32m[20221208 14:59:44 @agent_ppo2.py:179][0m |          -0.0481 |           6.9465 |       -1114.9915 |
[32m[20221208 14:59:45 @agent_ppo2.py:179][0m |          -0.0569 |           6.8833 |       -1151.5062 |
[32m[20221208 14:59:45 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:59:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.66
[32m[20221208 14:59:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.31
[32m[20221208 14:59:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.12
[32m[20221208 14:59:45 @agent_ppo2.py:137][0m Total time:      23.18 min
[32m[20221208 14:59:45 @agent_ppo2.py:139][0m 1871872 total steps have happened
[32m[20221208 14:59:45 @agent_ppo2.py:115][0m #------------------------ Iteration 914 --------------------------#
[32m[20221208 14:59:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:45 @agent_ppo2.py:179][0m |           0.0569 |           8.6289 |       -1198.8594 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |           0.0484 |           7.7666 |       -1051.7468 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0037 |           7.3460 |       -1110.4317 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0321 |           7.0636 |       -1214.2260 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0476 |           6.8949 |       -1235.4092 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0558 |           6.7141 |       -1264.2597 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0637 |           6.6039 |       -1281.3273 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0654 |           6.4933 |       -1301.5359 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0658 |           6.3669 |       -1312.0139 |
[32m[20221208 14:59:46 @agent_ppo2.py:179][0m |          -0.0655 |           6.2732 |       -1341.8899 |
[32m[20221208 14:59:46 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:59:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.95
[32m[20221208 14:59:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 223.35
[32m[20221208 14:59:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.04
[32m[20221208 14:59:46 @agent_ppo2.py:137][0m Total time:      23.20 min
[32m[20221208 14:59:46 @agent_ppo2.py:139][0m 1873920 total steps have happened
[32m[20221208 14:59:46 @agent_ppo2.py:115][0m #------------------------ Iteration 915 --------------------------#
[32m[20221208 14:59:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |           0.0898 |           6.5277 |       -1082.9513 |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |           0.1127 |           5.7080 |        -781.0950 |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |           0.0299 |           5.4178 |        -780.8297 |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |          -0.0219 |           5.2119 |        -897.5795 |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |          -0.0390 |           5.1116 |        -936.9801 |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |          -0.0527 |           4.9809 |        -990.7924 |
[32m[20221208 14:59:47 @agent_ppo2.py:179][0m |          -0.0644 |           4.8463 |       -1003.1285 |
[32m[20221208 14:59:48 @agent_ppo2.py:179][0m |          -0.0700 |           4.7655 |       -1034.1525 |
[32m[20221208 14:59:48 @agent_ppo2.py:179][0m |          -0.0734 |           4.7002 |       -1044.8903 |
[32m[20221208 14:59:48 @agent_ppo2.py:179][0m |          -0.0809 |           4.6217 |       -1072.1481 |
[32m[20221208 14:59:48 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:59:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.81
[32m[20221208 14:59:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.23
[32m[20221208 14:59:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.96
[32m[20221208 14:59:48 @agent_ppo2.py:137][0m Total time:      23.23 min
[32m[20221208 14:59:48 @agent_ppo2.py:139][0m 1875968 total steps have happened
[32m[20221208 14:59:48 @agent_ppo2.py:115][0m #------------------------ Iteration 916 --------------------------#
[32m[20221208 14:59:49 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |           0.0688 |           6.8450 |       -1037.7291 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |           0.1863 |           5.4905 |        -855.3550 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |           0.0123 |           4.8476 |        -893.8843 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0106 |           4.4601 |        -929.4339 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0292 |           4.2082 |       -1063.3783 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0335 |           3.9751 |       -1099.7750 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0421 |           3.8166 |       -1116.2530 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0507 |           3.6661 |       -1134.3062 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0504 |           3.4798 |       -1138.0509 |
[32m[20221208 14:59:49 @agent_ppo2.py:179][0m |          -0.0528 |           3.3900 |       -1139.9419 |
[32m[20221208 14:59:49 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:59:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.16
[32m[20221208 14:59:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.64
[32m[20221208 14:59:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 189.93
[32m[20221208 14:59:50 @agent_ppo2.py:137][0m Total time:      23.26 min
[32m[20221208 14:59:50 @agent_ppo2.py:139][0m 1878016 total steps have happened
[32m[20221208 14:59:50 @agent_ppo2.py:115][0m #------------------------ Iteration 917 --------------------------#
[32m[20221208 14:59:50 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:50 @agent_ppo2.py:179][0m |           0.0756 |           7.0056 |       -1157.5386 |
[32m[20221208 14:59:50 @agent_ppo2.py:179][0m |           0.0782 |           5.8386 |        -897.3653 |
[32m[20221208 14:59:50 @agent_ppo2.py:179][0m |           0.0144 |           5.3755 |        -869.0479 |
[32m[20221208 14:59:50 @agent_ppo2.py:179][0m |          -0.0211 |           5.0195 |        -967.1798 |
[32m[20221208 14:59:50 @agent_ppo2.py:179][0m |          -0.0418 |           4.8131 |       -1028.8406 |
[32m[20221208 14:59:51 @agent_ppo2.py:179][0m |          -0.0522 |           4.6993 |       -1048.4090 |
[32m[20221208 14:59:51 @agent_ppo2.py:179][0m |          -0.0586 |           4.5621 |       -1036.5921 |
[32m[20221208 14:59:51 @agent_ppo2.py:179][0m |          -0.0588 |           4.4368 |       -1053.6089 |
[32m[20221208 14:59:51 @agent_ppo2.py:179][0m |          -0.0654 |           4.3490 |       -1061.9073 |
[32m[20221208 14:59:51 @agent_ppo2.py:179][0m |          -0.0714 |           4.2643 |       -1076.9193 |
[32m[20221208 14:59:51 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:59:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.08
[32m[20221208 14:59:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.72
[32m[20221208 14:59:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.32
[32m[20221208 14:59:51 @agent_ppo2.py:137][0m Total time:      23.28 min
[32m[20221208 14:59:51 @agent_ppo2.py:139][0m 1880064 total steps have happened
[32m[20221208 14:59:51 @agent_ppo2.py:115][0m #------------------------ Iteration 918 --------------------------#
[32m[20221208 14:59:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |           0.1637 |           6.2672 |        -917.8425 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |           0.1055 |           5.2779 |        -463.9961 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |           0.0287 |           4.9508 |        -581.8455 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0047 |           4.6919 |        -679.7147 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0307 |           4.5491 |        -721.9624 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0476 |           4.3848 |        -744.3149 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0521 |           4.2997 |        -751.5304 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0530 |           4.1561 |        -753.5354 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0682 |           4.1334 |        -785.9864 |
[32m[20221208 14:59:52 @agent_ppo2.py:179][0m |          -0.0727 |           4.0422 |        -818.2062 |
[32m[20221208 14:59:52 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:59:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.16
[32m[20221208 14:59:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.89
[32m[20221208 14:59:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.06
[32m[20221208 14:59:53 @agent_ppo2.py:137][0m Total time:      23.31 min
[32m[20221208 14:59:53 @agent_ppo2.py:139][0m 1882112 total steps have happened
[32m[20221208 14:59:53 @agent_ppo2.py:115][0m #------------------------ Iteration 919 --------------------------#
[32m[20221208 14:59:53 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:53 @agent_ppo2.py:179][0m |           0.1238 |           5.3080 |        -880.2629 |
[32m[20221208 14:59:53 @agent_ppo2.py:179][0m |           0.0626 |           4.5241 |        -645.8489 |
[32m[20221208 14:59:53 @agent_ppo2.py:179][0m |           0.0106 |           4.2341 |        -772.3974 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0021 |           4.0229 |        -753.4542 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0200 |           3.8570 |        -821.9318 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0398 |           3.7694 |        -848.2997 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0513 |           3.6618 |        -881.8820 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0626 |           3.5555 |        -912.0172 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0705 |           3.4918 |        -941.2488 |
[32m[20221208 14:59:54 @agent_ppo2.py:179][0m |          -0.0711 |           3.4123 |        -931.5062 |
[32m[20221208 14:59:54 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:59:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.69
[32m[20221208 14:59:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.57
[32m[20221208 14:59:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.28
[32m[20221208 14:59:54 @agent_ppo2.py:137][0m Total time:      23.33 min
[32m[20221208 14:59:54 @agent_ppo2.py:139][0m 1884160 total steps have happened
[32m[20221208 14:59:54 @agent_ppo2.py:115][0m #------------------------ Iteration 920 --------------------------#
[32m[20221208 14:59:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |           0.1183 |           6.6383 |       -1161.8579 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |           0.0545 |           5.0796 |        -952.3690 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |           0.0077 |           4.4511 |       -1064.7127 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0212 |           4.1985 |       -1091.1347 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0228 |           3.9719 |       -1089.0835 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0457 |           3.8112 |       -1126.8333 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0568 |           3.6555 |       -1190.8079 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0569 |           3.5820 |       -1223.2930 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0657 |           3.4969 |       -1217.5046 |
[32m[20221208 14:59:55 @agent_ppo2.py:179][0m |          -0.0662 |           3.4515 |       -1249.6499 |
[32m[20221208 14:59:55 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:59:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.87
[32m[20221208 14:59:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.77
[32m[20221208 14:59:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.95
[32m[20221208 14:59:56 @agent_ppo2.py:137][0m Total time:      23.36 min
[32m[20221208 14:59:56 @agent_ppo2.py:139][0m 1886208 total steps have happened
[32m[20221208 14:59:56 @agent_ppo2.py:115][0m #------------------------ Iteration 921 --------------------------#
[32m[20221208 14:59:56 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:59:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:56 @agent_ppo2.py:179][0m |           0.0885 |           6.5611 |       -1329.8386 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |           0.0906 |           4.9759 |       -1024.3568 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |           0.0282 |           4.4651 |        -999.9267 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0210 |           4.3224 |       -1154.3290 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0400 |           3.9629 |       -1192.8592 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0469 |           3.7504 |       -1216.3891 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0551 |           3.6097 |       -1240.2879 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0593 |           3.4830 |       -1259.5904 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0670 |           3.4097 |       -1287.4015 |
[32m[20221208 14:59:57 @agent_ppo2.py:179][0m |          -0.0718 |           3.3173 |       -1309.9702 |
[32m[20221208 14:59:57 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:59:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.11
[32m[20221208 14:59:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.21
[32m[20221208 14:59:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.03
[32m[20221208 14:59:57 @agent_ppo2.py:137][0m Total time:      23.39 min
[32m[20221208 14:59:57 @agent_ppo2.py:139][0m 1888256 total steps have happened
[32m[20221208 14:59:57 @agent_ppo2.py:115][0m #------------------------ Iteration 922 --------------------------#
[32m[20221208 14:59:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:59:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |           0.1391 |           8.9875 |       -1109.1729 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |           0.0942 |           7.9130 |        -672.0888 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |           0.0145 |           7.5496 |        -751.9067 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |          -0.0185 |           7.3066 |        -805.8363 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |          -0.0366 |           7.0636 |        -837.9866 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |          -0.0460 |           6.8991 |        -875.2444 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |          -0.0537 |           6.8680 |        -909.5630 |
[32m[20221208 14:59:58 @agent_ppo2.py:179][0m |          -0.0579 |           6.6793 |        -907.1596 |
[32m[20221208 14:59:59 @agent_ppo2.py:179][0m |          -0.0634 |           6.6481 |        -929.5882 |
[32m[20221208 14:59:59 @agent_ppo2.py:179][0m |          -0.0631 |           6.5599 |        -927.4020 |
[32m[20221208 14:59:59 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:59:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.80
[32m[20221208 14:59:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.17
[32m[20221208 14:59:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.30
[32m[20221208 14:59:59 @agent_ppo2.py:137][0m Total time:      23.41 min
[32m[20221208 14:59:59 @agent_ppo2.py:139][0m 1890304 total steps have happened
[32m[20221208 14:59:59 @agent_ppo2.py:115][0m #------------------------ Iteration 923 --------------------------#
[32m[20221208 14:59:59 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |           0.1117 |           9.6171 |       -1206.0450 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |           0.0728 |           8.1230 |        -969.9261 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |           0.0245 |           7.4032 |       -1034.9610 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0087 |           6.9438 |       -1077.5060 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0327 |           6.6596 |       -1142.3203 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0491 |           6.3707 |       -1181.1394 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0570 |           6.2122 |       -1220.6205 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0612 |           6.0461 |       -1222.1382 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0668 |           5.8897 |       -1246.2476 |
[32m[20221208 15:00:00 @agent_ppo2.py:179][0m |          -0.0726 |           5.8017 |       -1263.2703 |
[32m[20221208 15:00:00 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:00:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.70
[32m[20221208 15:00:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.48
[32m[20221208 15:00:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.33
[32m[20221208 15:00:01 @agent_ppo2.py:137][0m Total time:      23.44 min
[32m[20221208 15:00:01 @agent_ppo2.py:139][0m 1892352 total steps have happened
[32m[20221208 15:00:01 @agent_ppo2.py:115][0m #------------------------ Iteration 924 --------------------------#
[32m[20221208 15:00:01 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 15:00:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:01 @agent_ppo2.py:179][0m |           0.1376 |           2.9864 |        -981.9020 |
[32m[20221208 15:00:01 @agent_ppo2.py:179][0m |           0.1974 |           2.2907 |        -188.4618 |
[32m[20221208 15:00:01 @agent_ppo2.py:179][0m |           0.1933 |           2.0504 |        -257.9420 |
[32m[20221208 15:00:01 @agent_ppo2.py:179][0m |           0.1526 |           1.9561 |        -180.8822 |
[32m[20221208 15:00:01 @agent_ppo2.py:179][0m |           0.1152 |           1.8892 |        -382.4381 |
[32m[20221208 15:00:01 @agent_ppo2.py:179][0m |           0.1358 |           1.8360 |        -359.1252 |
[32m[20221208 15:00:02 @agent_ppo2.py:179][0m |           0.1209 |           1.8356 |        -331.0508 |
[32m[20221208 15:00:02 @agent_ppo2.py:179][0m |           0.0945 |           1.7740 |        -555.7623 |
[32m[20221208 15:00:02 @agent_ppo2.py:179][0m |           0.0770 |           1.7538 |        -654.5892 |
[32m[20221208 15:00:02 @agent_ppo2.py:179][0m |           0.0486 |           1.7306 |        -756.8404 |
[32m[20221208 15:00:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:00:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.92
[32m[20221208 15:00:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.25
[32m[20221208 15:00:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.17
[32m[20221208 15:00:02 @agent_ppo2.py:137][0m Total time:      23.46 min
[32m[20221208 15:00:02 @agent_ppo2.py:139][0m 1894400 total steps have happened
[32m[20221208 15:00:02 @agent_ppo2.py:115][0m #------------------------ Iteration 925 --------------------------#
[32m[20221208 15:00:03 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |           0.0880 |           6.3202 |       -1154.7625 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |           0.0818 |           5.4270 |        -962.7646 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |           0.0235 |           4.9736 |       -1047.3208 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0026 |           4.7161 |        -999.3240 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0403 |           4.4702 |       -1067.4436 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0457 |           4.4058 |       -1054.0777 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0657 |           4.2461 |       -1104.6282 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0717 |           4.1332 |       -1110.0283 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0787 |           4.0282 |       -1144.6439 |
[32m[20221208 15:00:03 @agent_ppo2.py:179][0m |          -0.0839 |           3.9470 |       -1154.2155 |
[32m[20221208 15:00:03 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:00:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.16
[32m[20221208 15:00:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.01
[32m[20221208 15:00:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.27
[32m[20221208 15:00:04 @agent_ppo2.py:137][0m Total time:      23.49 min
[32m[20221208 15:00:04 @agent_ppo2.py:139][0m 1896448 total steps have happened
[32m[20221208 15:00:04 @agent_ppo2.py:115][0m #------------------------ Iteration 926 --------------------------#
[32m[20221208 15:00:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:04 @agent_ppo2.py:179][0m |           0.1092 |           3.7648 |       -1002.8388 |
[32m[20221208 15:00:04 @agent_ppo2.py:179][0m |           0.1858 |           2.9379 |        -613.6854 |
[32m[20221208 15:00:04 @agent_ppo2.py:179][0m |           0.0954 |           2.6551 |        -569.1499 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |           0.0297 |           2.4908 |        -665.8236 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |          -0.0027 |           2.3508 |        -710.6056 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |          -0.0153 |           2.2835 |        -763.8287 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |          -0.0244 |           2.2301 |        -781.8013 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |          -0.0357 |           2.1364 |        -805.5800 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |          -0.0372 |           2.0803 |        -825.8979 |
[32m[20221208 15:00:05 @agent_ppo2.py:179][0m |          -0.0456 |           2.0278 |        -863.6871 |
[32m[20221208 15:00:05 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:00:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.34
[32m[20221208 15:00:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.01
[32m[20221208 15:00:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.15
[32m[20221208 15:00:05 @agent_ppo2.py:137][0m Total time:      23.52 min
[32m[20221208 15:00:05 @agent_ppo2.py:139][0m 1898496 total steps have happened
[32m[20221208 15:00:05 @agent_ppo2.py:115][0m #------------------------ Iteration 927 --------------------------#
[32m[20221208 15:00:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |           0.1065 |           6.9450 |       -1142.5704 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |           0.0958 |           5.7675 |        -881.0167 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |           0.0230 |           5.2783 |        -999.9561 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0196 |           5.0033 |       -1098.5548 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0377 |           4.7755 |       -1141.7545 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0570 |           4.6251 |       -1189.6194 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0623 |           4.5229 |       -1189.3816 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0701 |           4.3710 |       -1221.2044 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0744 |           4.2677 |       -1253.0430 |
[32m[20221208 15:00:06 @agent_ppo2.py:179][0m |          -0.0780 |           4.1855 |       -1259.0151 |
[32m[20221208 15:00:07 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 15:00:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.82
[32m[20221208 15:00:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.09
[32m[20221208 15:00:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.96
[32m[20221208 15:00:07 @agent_ppo2.py:137][0m Total time:      23.54 min
[32m[20221208 15:00:07 @agent_ppo2.py:139][0m 1900544 total steps have happened
[32m[20221208 15:00:07 @agent_ppo2.py:115][0m #------------------------ Iteration 928 --------------------------#
[32m[20221208 15:00:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:07 @agent_ppo2.py:179][0m |           0.1321 |           6.1542 |        -976.0198 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |           0.1034 |           4.8906 |        -635.4082 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |           0.0447 |           4.3394 |        -719.2726 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |           0.0091 |           4.0842 |        -822.8882 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |          -0.0194 |           3.9005 |        -882.6169 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |          -0.0365 |           3.7366 |        -916.4272 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |          -0.0478 |           3.6391 |        -952.3503 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |          -0.0605 |           3.5236 |        -980.9371 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |          -0.0638 |           3.4647 |        -987.6802 |
[32m[20221208 15:00:08 @agent_ppo2.py:179][0m |          -0.0702 |           3.3877 |       -1019.7133 |
[32m[20221208 15:00:08 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:00:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.52
[32m[20221208 15:00:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.05
[32m[20221208 15:00:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.05
[32m[20221208 15:00:08 @agent_ppo2.py:137][0m Total time:      23.57 min
[32m[20221208 15:00:08 @agent_ppo2.py:139][0m 1902592 total steps have happened
[32m[20221208 15:00:08 @agent_ppo2.py:115][0m #------------------------ Iteration 929 --------------------------#
[32m[20221208 15:00:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |           0.1164 |           8.4706 |       -1065.0185 |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |           0.1160 |           7.0422 |        -730.9798 |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |           0.0124 |           6.4485 |        -912.8629 |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |          -0.0173 |           6.1352 |        -974.2152 |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |          -0.0385 |           5.9324 |       -1022.6547 |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |          -0.0463 |           5.7447 |       -1068.0496 |
[32m[20221208 15:00:09 @agent_ppo2.py:179][0m |          -0.0619 |           5.5977 |       -1076.0024 |
[32m[20221208 15:00:10 @agent_ppo2.py:179][0m |          -0.0634 |           5.4321 |       -1093.7844 |
[32m[20221208 15:00:10 @agent_ppo2.py:179][0m |          -0.0700 |           5.3436 |       -1104.8819 |
[32m[20221208 15:00:10 @agent_ppo2.py:179][0m |          -0.0735 |           5.2602 |       -1132.7005 |
[32m[20221208 15:00:10 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:00:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.80
[32m[20221208 15:00:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.23
[32m[20221208 15:00:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.14
[32m[20221208 15:00:10 @agent_ppo2.py:137][0m Total time:      23.60 min
[32m[20221208 15:00:10 @agent_ppo2.py:139][0m 1904640 total steps have happened
[32m[20221208 15:00:10 @agent_ppo2.py:115][0m #------------------------ Iteration 930 --------------------------#
[32m[20221208 15:00:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |           0.1045 |           9.1952 |       -1063.8729 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |           0.0721 |           7.7356 |        -727.6716 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0005 |           7.3554 |        -766.4085 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0267 |           7.0933 |        -862.1853 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0441 |           6.8946 |        -899.0927 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0562 |           6.7590 |        -931.4099 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0635 |           6.6111 |        -952.8278 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0685 |           6.4945 |        -989.1741 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0692 |           6.4133 |        -981.7360 |
[32m[20221208 15:00:11 @agent_ppo2.py:179][0m |          -0.0718 |           6.4128 |        -993.4821 |
[32m[20221208 15:00:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:00:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.10
[32m[20221208 15:00:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.08
[32m[20221208 15:00:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.16
[32m[20221208 15:00:12 @agent_ppo2.py:137][0m Total time:      23.62 min
[32m[20221208 15:00:12 @agent_ppo2.py:139][0m 1906688 total steps have happened
[32m[20221208 15:00:12 @agent_ppo2.py:115][0m #------------------------ Iteration 931 --------------------------#
[32m[20221208 15:00:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:12 @agent_ppo2.py:179][0m |           0.0847 |           6.9663 |       -1225.9568 |
[32m[20221208 15:00:12 @agent_ppo2.py:179][0m |           0.0644 |           5.4827 |       -1069.1836 |
[32m[20221208 15:00:12 @agent_ppo2.py:179][0m |          -0.0048 |           5.0168 |       -1172.4323 |
[32m[20221208 15:00:12 @agent_ppo2.py:179][0m |          -0.0356 |           4.7416 |       -1221.3235 |
[32m[20221208 15:00:12 @agent_ppo2.py:179][0m |          -0.0494 |           4.5566 |       -1266.8151 |
[32m[20221208 15:00:13 @agent_ppo2.py:179][0m |          -0.0591 |           4.3869 |       -1309.4864 |
[32m[20221208 15:00:13 @agent_ppo2.py:179][0m |          -0.0633 |           4.2663 |       -1316.1741 |
[32m[20221208 15:00:13 @agent_ppo2.py:179][0m |          -0.0706 |           4.1563 |       -1323.7813 |
[32m[20221208 15:00:13 @agent_ppo2.py:179][0m |          -0.0755 |           4.0960 |       -1356.9912 |
[32m[20221208 15:00:13 @agent_ppo2.py:179][0m |          -0.0758 |           3.9958 |       -1350.7481 |
[32m[20221208 15:00:13 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.02
[32m[20221208 15:00:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.41
[32m[20221208 15:00:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.87
[32m[20221208 15:00:13 @agent_ppo2.py:137][0m Total time:      23.65 min
[32m[20221208 15:00:13 @agent_ppo2.py:139][0m 1908736 total steps have happened
[32m[20221208 15:00:13 @agent_ppo2.py:115][0m #------------------------ Iteration 932 --------------------------#
[32m[20221208 15:00:14 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |           0.0431 |           6.8270 |        -967.8320 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0000 |           5.8480 |        -866.1242 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0268 |           5.4075 |        -921.3999 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0421 |           5.1240 |        -934.1103 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0586 |           4.9378 |       -1000.4769 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0689 |           4.8066 |       -1037.6033 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0737 |           4.7287 |       -1051.2671 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0767 |           4.5564 |       -1074.0585 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0817 |           4.5439 |       -1113.3229 |
[32m[20221208 15:00:14 @agent_ppo2.py:179][0m |          -0.0825 |           4.4162 |       -1096.3693 |
[32m[20221208 15:00:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:00:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.99
[32m[20221208 15:00:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.44
[32m[20221208 15:00:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 162.14
[32m[20221208 15:00:15 @agent_ppo2.py:137][0m Total time:      23.67 min
[32m[20221208 15:00:15 @agent_ppo2.py:139][0m 1910784 total steps have happened
[32m[20221208 15:00:15 @agent_ppo2.py:115][0m #------------------------ Iteration 933 --------------------------#
[32m[20221208 15:00:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:15 @agent_ppo2.py:179][0m |           0.1075 |           6.5735 |       -1216.7717 |
[32m[20221208 15:00:15 @agent_ppo2.py:179][0m |           0.0760 |           5.6263 |        -895.3900 |
[32m[20221208 15:00:15 @agent_ppo2.py:179][0m |           0.0190 |           5.3556 |       -1012.2041 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0092 |           5.1614 |       -1096.7623 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0239 |           5.0057 |       -1115.6855 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0364 |           4.9226 |       -1183.5021 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0466 |           4.8363 |       -1204.9476 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0579 |           4.7190 |       -1277.2457 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0579 |           4.6807 |       -1284.7650 |
[32m[20221208 15:00:16 @agent_ppo2.py:179][0m |          -0.0664 |           4.6150 |       -1308.0699 |
[32m[20221208 15:00:16 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 15:00:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.63
[32m[20221208 15:00:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.33
[32m[20221208 15:00:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.86
[32m[20221208 15:00:16 @agent_ppo2.py:137][0m Total time:      23.70 min
[32m[20221208 15:00:16 @agent_ppo2.py:139][0m 1912832 total steps have happened
[32m[20221208 15:00:16 @agent_ppo2.py:115][0m #------------------------ Iteration 934 --------------------------#
[32m[20221208 15:00:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |           0.1316 |           8.5053 |       -1175.6635 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |           0.1200 |           6.5916 |        -850.8395 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |           0.0277 |           6.0177 |       -1031.7084 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |          -0.0019 |           5.6619 |       -1184.1262 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |          -0.0212 |           5.4088 |       -1197.4963 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |          -0.0422 |           5.2535 |       -1283.6430 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |          -0.0539 |           5.0830 |       -1337.1569 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |          -0.0639 |           4.9667 |       -1394.7933 |
[32m[20221208 15:00:17 @agent_ppo2.py:179][0m |          -0.0704 |           4.8596 |       -1432.6256 |
[32m[20221208 15:00:18 @agent_ppo2.py:179][0m |          -0.0744 |           4.7524 |       -1450.8094 |
[32m[20221208 15:00:18 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.98
[32m[20221208 15:00:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.40
[32m[20221208 15:00:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.30
[32m[20221208 15:00:18 @agent_ppo2.py:137][0m Total time:      23.73 min
[32m[20221208 15:00:18 @agent_ppo2.py:139][0m 1914880 total steps have happened
[32m[20221208 15:00:18 @agent_ppo2.py:115][0m #------------------------ Iteration 935 --------------------------#
[32m[20221208 15:00:18 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 15:00:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.3633 |           3.6003 |       -1132.6575 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.1538 |           2.8628 |        -491.3241 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.1094 |           2.6759 |        -553.3429 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0755 |           2.4813 |        -640.0699 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0638 |           2.4299 |        -687.6247 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0540 |           2.3364 |        -723.1117 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0436 |           2.2559 |        -767.6329 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0361 |           2.2323 |        -800.2630 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0455 |           2.2034 |        -818.2346 |
[32m[20221208 15:00:19 @agent_ppo2.py:179][0m |           0.0423 |           2.1684 |        -757.3673 |
[32m[20221208 15:00:19 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:00:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.28
[32m[20221208 15:00:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.00
[32m[20221208 15:00:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.82
[32m[20221208 15:00:20 @agent_ppo2.py:137][0m Total time:      23.75 min
[32m[20221208 15:00:20 @agent_ppo2.py:139][0m 1916928 total steps have happened
[32m[20221208 15:00:20 @agent_ppo2.py:115][0m #------------------------ Iteration 936 --------------------------#
[32m[20221208 15:00:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:20 @agent_ppo2.py:179][0m |           0.1087 |           6.5018 |       -1296.1697 |
[32m[20221208 15:00:20 @agent_ppo2.py:179][0m |           0.0710 |           5.0693 |        -809.0444 |
[32m[20221208 15:00:20 @agent_ppo2.py:179][0m |           0.0146 |           4.5051 |        -919.6982 |
[32m[20221208 15:00:20 @agent_ppo2.py:179][0m |          -0.0186 |           4.1386 |       -1027.2043 |
[32m[20221208 15:00:20 @agent_ppo2.py:179][0m |          -0.0368 |           3.9410 |       -1089.6821 |
[32m[20221208 15:00:20 @agent_ppo2.py:179][0m |          -0.0489 |           3.7667 |       -1167.0733 |
[32m[20221208 15:00:21 @agent_ppo2.py:179][0m |          -0.0620 |           3.5904 |       -1210.4806 |
[32m[20221208 15:00:21 @agent_ppo2.py:179][0m |          -0.0681 |           3.5028 |       -1229.9083 |
[32m[20221208 15:00:21 @agent_ppo2.py:179][0m |          -0.0732 |           3.3724 |       -1260.9290 |
[32m[20221208 15:00:21 @agent_ppo2.py:179][0m |          -0.0786 |           3.2857 |       -1284.7116 |
[32m[20221208 15:00:21 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:00:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.34
[32m[20221208 15:00:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 168.82
[32m[20221208 15:00:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.06
[32m[20221208 15:00:21 @agent_ppo2.py:137][0m Total time:      23.78 min
[32m[20221208 15:00:21 @agent_ppo2.py:139][0m 1918976 total steps have happened
[32m[20221208 15:00:21 @agent_ppo2.py:115][0m #------------------------ Iteration 937 --------------------------#
[32m[20221208 15:00:22 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:00:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |           0.1127 |           7.8625 |       -1507.5152 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |           0.1016 |           6.6169 |       -1133.1929 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |           0.0995 |           6.0406 |        -892.3892 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |           0.0204 |           5.6910 |       -1051.5254 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |          -0.0226 |           5.5142 |       -1146.6094 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |          -0.0374 |           5.3056 |       -1179.3233 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |          -0.0509 |           5.1577 |       -1237.0994 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |          -0.0497 |           5.0139 |       -1267.6331 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |          -0.0570 |           4.9263 |       -1277.2604 |
[32m[20221208 15:00:22 @agent_ppo2.py:179][0m |          -0.0621 |           4.8603 |       -1316.1884 |
[32m[20221208 15:00:22 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 15:00:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.03
[32m[20221208 15:00:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.08
[32m[20221208 15:00:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.02
[32m[20221208 15:00:23 @agent_ppo2.py:137][0m Total time:      23.81 min
[32m[20221208 15:00:23 @agent_ppo2.py:139][0m 1921024 total steps have happened
[32m[20221208 15:00:23 @agent_ppo2.py:115][0m #------------------------ Iteration 938 --------------------------#
[32m[20221208 15:00:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:23 @agent_ppo2.py:179][0m |           0.0811 |          11.0300 |       -1557.5832 |
[32m[20221208 15:00:23 @agent_ppo2.py:179][0m |           0.0646 |           9.6719 |       -1251.0273 |
[32m[20221208 15:00:23 @agent_ppo2.py:179][0m |           0.0318 |           9.1256 |       -1264.0473 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0150 |           8.8061 |       -1370.4802 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0372 |           8.6443 |       -1445.0326 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0514 |           8.4361 |       -1482.0279 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0586 |           8.2380 |       -1523.7158 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0689 |           8.1399 |       -1549.2882 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0743 |           8.0689 |       -1602.0924 |
[32m[20221208 15:00:24 @agent_ppo2.py:179][0m |          -0.0777 |           7.9967 |       -1609.0282 |
[32m[20221208 15:00:24 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:00:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.02
[32m[20221208 15:00:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.30
[32m[20221208 15:00:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.93
[32m[20221208 15:00:24 @agent_ppo2.py:137][0m Total time:      23.83 min
[32m[20221208 15:00:24 @agent_ppo2.py:139][0m 1923072 total steps have happened
[32m[20221208 15:00:24 @agent_ppo2.py:115][0m #------------------------ Iteration 939 --------------------------#
[32m[20221208 15:00:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |           0.0839 |           7.0841 |       -1394.8674 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |           0.0899 |           6.2484 |        -984.7747 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |           0.0146 |           5.9204 |       -1160.4367 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0207 |           5.7243 |       -1236.3212 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0328 |           5.5797 |       -1285.0031 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0467 |           5.4188 |       -1306.2959 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0542 |           5.3346 |       -1342.3756 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0596 |           5.2209 |       -1368.2530 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0680 |           5.1815 |       -1421.9610 |
[32m[20221208 15:00:25 @agent_ppo2.py:179][0m |          -0.0764 |           5.1020 |       -1440.8271 |
[32m[20221208 15:00:25 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.48
[32m[20221208 15:00:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.41
[32m[20221208 15:00:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.39
[32m[20221208 15:00:26 @agent_ppo2.py:137][0m Total time:      23.86 min
[32m[20221208 15:00:26 @agent_ppo2.py:139][0m 1925120 total steps have happened
[32m[20221208 15:00:26 @agent_ppo2.py:115][0m #------------------------ Iteration 940 --------------------------#
[32m[20221208 15:00:26 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:26 @agent_ppo2.py:179][0m |           0.1007 |           8.4107 |       -1442.6864 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |           0.0632 |           7.2034 |       -1250.0747 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0018 |           6.7855 |       -1467.5482 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0242 |           6.4753 |       -1543.7623 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0427 |           6.2756 |       -1599.1690 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0514 |           6.1465 |       -1641.4455 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0566 |           6.0075 |       -1638.3090 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0667 |           5.8464 |       -1675.5477 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0723 |           5.7052 |       -1705.2020 |
[32m[20221208 15:00:27 @agent_ppo2.py:179][0m |          -0.0740 |           5.6611 |       -1722.0484 |
[32m[20221208 15:00:27 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:00:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.83
[32m[20221208 15:00:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.00
[32m[20221208 15:00:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.94
[32m[20221208 15:00:27 @agent_ppo2.py:137][0m Total time:      23.89 min
[32m[20221208 15:00:27 @agent_ppo2.py:139][0m 1927168 total steps have happened
[32m[20221208 15:00:27 @agent_ppo2.py:115][0m #------------------------ Iteration 941 --------------------------#
[32m[20221208 15:00:28 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |           0.0975 |           6.5412 |       -1561.6254 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |           0.0763 |           5.1643 |       -1277.2117 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |           0.0185 |           4.5315 |       -1247.3828 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |          -0.0176 |           4.1386 |       -1396.0066 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |          -0.0393 |           3.8571 |       -1421.2667 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |          -0.0522 |           3.6695 |       -1484.9485 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |          -0.0588 |           3.4597 |       -1508.0050 |
[32m[20221208 15:00:28 @agent_ppo2.py:179][0m |          -0.0624 |           3.3085 |       -1536.2936 |
[32m[20221208 15:00:29 @agent_ppo2.py:179][0m |          -0.0657 |           3.1434 |       -1544.3931 |
[32m[20221208 15:00:29 @agent_ppo2.py:179][0m |          -0.0704 |           3.0500 |       -1576.4064 |
[32m[20221208 15:00:29 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:00:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.77
[32m[20221208 15:00:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.53
[32m[20221208 15:00:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.49
[32m[20221208 15:00:29 @agent_ppo2.py:137][0m Total time:      23.91 min
[32m[20221208 15:00:29 @agent_ppo2.py:139][0m 1929216 total steps have happened
[32m[20221208 15:00:29 @agent_ppo2.py:115][0m #------------------------ Iteration 942 --------------------------#
[32m[20221208 15:00:29 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |           0.1179 |           6.7236 |       -1336.9855 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |           0.0783 |           5.4070 |        -974.4783 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |           0.0447 |           4.9250 |       -1116.9825 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |           0.0003 |           4.6811 |       -1151.8715 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |          -0.0373 |           4.4595 |       -1207.3434 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |          -0.0555 |           4.2859 |       -1255.7133 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |          -0.0627 |           4.1608 |       -1290.2820 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |          -0.0692 |           4.0677 |       -1310.1937 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |          -0.0710 |           4.0272 |       -1329.5187 |
[32m[20221208 15:00:30 @agent_ppo2.py:179][0m |          -0.0769 |           3.9803 |       -1372.3531 |
[32m[20221208 15:00:30 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:00:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.99
[32m[20221208 15:00:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.73
[32m[20221208 15:00:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.34
[32m[20221208 15:00:31 @agent_ppo2.py:137][0m Total time:      23.94 min
[32m[20221208 15:00:31 @agent_ppo2.py:139][0m 1931264 total steps have happened
[32m[20221208 15:00:31 @agent_ppo2.py:115][0m #------------------------ Iteration 943 --------------------------#
[32m[20221208 15:00:31 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:31 @agent_ppo2.py:179][0m |           0.0688 |           6.4006 |       -1354.1961 |
[32m[20221208 15:00:31 @agent_ppo2.py:179][0m |           0.0551 |           5.5005 |       -1265.6306 |
[32m[20221208 15:00:31 @agent_ppo2.py:179][0m |           0.0927 |           5.0957 |       -1180.3625 |
[32m[20221208 15:00:31 @agent_ppo2.py:179][0m |          -0.0031 |           4.8159 |       -1315.3265 |
[32m[20221208 15:00:31 @agent_ppo2.py:179][0m |          -0.0333 |           4.6308 |       -1443.5435 |
[32m[20221208 15:00:32 @agent_ppo2.py:179][0m |          -0.0514 |           4.4750 |       -1500.8544 |
[32m[20221208 15:00:32 @agent_ppo2.py:179][0m |          -0.0582 |           4.3381 |       -1537.2383 |
[32m[20221208 15:00:32 @agent_ppo2.py:179][0m |          -0.0655 |           4.1983 |       -1584.0593 |
[32m[20221208 15:00:32 @agent_ppo2.py:179][0m |          -0.0677 |           4.0979 |       -1625.6571 |
[32m[20221208 15:00:32 @agent_ppo2.py:179][0m |          -0.0753 |           4.0328 |       -1650.4305 |
[32m[20221208 15:00:32 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.35
[32m[20221208 15:00:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.11
[32m[20221208 15:00:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.15
[32m[20221208 15:00:32 @agent_ppo2.py:137][0m Total time:      23.97 min
[32m[20221208 15:00:32 @agent_ppo2.py:139][0m 1933312 total steps have happened
[32m[20221208 15:00:32 @agent_ppo2.py:115][0m #------------------------ Iteration 944 --------------------------#
[32m[20221208 15:00:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |           0.1397 |           6.8311 |       -1433.8079 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |           0.1303 |           5.3359 |        -967.6115 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |           0.0360 |           4.8467 |       -1093.6057 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0061 |           4.5056 |       -1264.4220 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0266 |           4.2917 |       -1344.4168 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0402 |           4.1525 |       -1403.1635 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0518 |           4.0229 |       -1441.0431 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0621 |           3.9363 |       -1507.8840 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0657 |           3.8450 |       -1510.6888 |
[32m[20221208 15:00:33 @agent_ppo2.py:179][0m |          -0.0685 |           3.8115 |       -1538.4749 |
[32m[20221208 15:00:33 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.28
[32m[20221208 15:00:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.24
[32m[20221208 15:00:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.70
[32m[20221208 15:00:34 @agent_ppo2.py:137][0m Total time:      23.99 min
[32m[20221208 15:00:34 @agent_ppo2.py:139][0m 1935360 total steps have happened
[32m[20221208 15:00:34 @agent_ppo2.py:115][0m #------------------------ Iteration 945 --------------------------#
[32m[20221208 15:00:34 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:34 @agent_ppo2.py:179][0m |           0.0723 |           7.5232 |       -1396.7222 |
[32m[20221208 15:00:34 @agent_ppo2.py:179][0m |           0.0562 |           6.6814 |       -1125.2524 |
[32m[20221208 15:00:34 @agent_ppo2.py:179][0m |          -0.0114 |           6.3262 |       -1198.6223 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0329 |           6.1818 |       -1253.8470 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0497 |           6.0231 |       -1309.1664 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0563 |           5.8693 |       -1329.4390 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0626 |           5.8161 |       -1332.0166 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0675 |           5.7337 |       -1357.5139 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0759 |           5.6793 |       -1377.1636 |
[32m[20221208 15:00:35 @agent_ppo2.py:179][0m |          -0.0785 |           5.6563 |       -1391.3558 |
[32m[20221208 15:00:35 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:00:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.32
[32m[20221208 15:00:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.64
[32m[20221208 15:00:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.55
[32m[20221208 15:00:35 @agent_ppo2.py:137][0m Total time:      24.02 min
[32m[20221208 15:00:35 @agent_ppo2.py:139][0m 1937408 total steps have happened
[32m[20221208 15:00:35 @agent_ppo2.py:115][0m #------------------------ Iteration 946 --------------------------#
[32m[20221208 15:00:36 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |           0.0974 |           6.0912 |       -1396.7717 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |           0.0748 |           5.2169 |        -827.9010 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |           0.0117 |           4.8464 |        -951.0475 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0081 |           4.6394 |        -978.4623 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0337 |           4.4929 |       -1085.5227 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0490 |           4.3527 |       -1115.4474 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0583 |           4.2452 |       -1170.9792 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0659 |           4.1688 |       -1226.2231 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0689 |           4.0864 |       -1226.9936 |
[32m[20221208 15:00:36 @agent_ppo2.py:179][0m |          -0.0752 |           4.0178 |       -1241.8475 |
[32m[20221208 15:00:36 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.25
[32m[20221208 15:00:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.46
[32m[20221208 15:00:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.27
[32m[20221208 15:00:37 @agent_ppo2.py:137][0m Total time:      24.04 min
[32m[20221208 15:00:37 @agent_ppo2.py:139][0m 1939456 total steps have happened
[32m[20221208 15:00:37 @agent_ppo2.py:115][0m #------------------------ Iteration 947 --------------------------#
[32m[20221208 15:00:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:37 @agent_ppo2.py:179][0m |           0.0688 |           6.3846 |       -1410.3169 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |           0.0858 |           5.6158 |        -874.7833 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |           0.0104 |           5.1670 |       -1044.4224 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0252 |           4.8933 |       -1147.7796 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0428 |           4.6738 |       -1177.3393 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0495 |           4.4670 |       -1215.4003 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0576 |           4.3300 |       -1242.2119 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0495 |           4.2187 |       -1203.3930 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0597 |           4.0613 |       -1293.1283 |
[32m[20221208 15:00:38 @agent_ppo2.py:179][0m |          -0.0658 |           3.9753 |       -1283.2046 |
[32m[20221208 15:00:38 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:00:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.48
[32m[20221208 15:00:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.02
[32m[20221208 15:00:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.02
[32m[20221208 15:00:38 @agent_ppo2.py:137][0m Total time:      24.07 min
[32m[20221208 15:00:38 @agent_ppo2.py:139][0m 1941504 total steps have happened
[32m[20221208 15:00:38 @agent_ppo2.py:115][0m #------------------------ Iteration 948 --------------------------#
[32m[20221208 15:00:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |           0.1219 |           8.5941 |       -1464.7935 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |           0.1004 |           6.7566 |       -1039.0784 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |           0.0133 |           6.1555 |       -1248.1924 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |          -0.0138 |           5.8215 |       -1355.8156 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |          -0.0330 |           5.5682 |       -1409.9735 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |          -0.0453 |           5.4064 |       -1472.6628 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |          -0.0611 |           5.2344 |       -1535.8182 |
[32m[20221208 15:00:39 @agent_ppo2.py:179][0m |          -0.0625 |           5.1227 |       -1544.8263 |
[32m[20221208 15:00:40 @agent_ppo2.py:179][0m |          -0.0689 |           5.0149 |       -1546.7529 |
[32m[20221208 15:00:40 @agent_ppo2.py:179][0m |          -0.0728 |           4.9078 |       -1600.4853 |
[32m[20221208 15:00:40 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:00:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.30
[32m[20221208 15:00:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.31
[32m[20221208 15:00:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.17
[32m[20221208 15:00:40 @agent_ppo2.py:137][0m Total time:      24.10 min
[32m[20221208 15:00:40 @agent_ppo2.py:139][0m 1943552 total steps have happened
[32m[20221208 15:00:40 @agent_ppo2.py:115][0m #------------------------ Iteration 949 --------------------------#
[32m[20221208 15:00:41 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 15:00:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |           0.1011 |           5.1823 |       -1376.2774 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |           0.0736 |           3.9450 |        -768.6694 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |           0.0490 |           3.5841 |        -721.9742 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |           0.0147 |           3.3904 |        -817.6933 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |          -0.0037 |           3.2333 |        -904.5488 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |          -0.0205 |           3.1344 |        -996.3703 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |          -0.0263 |           3.0524 |       -1019.2931 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |          -0.0320 |           2.9522 |       -1066.7604 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |          -0.0409 |           2.9027 |       -1107.0583 |
[32m[20221208 15:00:41 @agent_ppo2.py:179][0m |          -0.0447 |           2.8522 |       -1119.5856 |
[32m[20221208 15:00:41 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:00:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.59
[32m[20221208 15:00:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.03
[32m[20221208 15:00:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.61
[32m[20221208 15:00:42 @agent_ppo2.py:137][0m Total time:      24.12 min
[32m[20221208 15:00:42 @agent_ppo2.py:139][0m 1945600 total steps have happened
[32m[20221208 15:00:42 @agent_ppo2.py:115][0m #------------------------ Iteration 950 --------------------------#
[32m[20221208 15:00:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:42 @agent_ppo2.py:179][0m |           0.0984 |           7.5114 |       -1344.3164 |
[32m[20221208 15:00:42 @agent_ppo2.py:179][0m |           0.1760 |           7.0081 |        -773.0168 |
[32m[20221208 15:00:42 @agent_ppo2.py:179][0m |           0.0298 |           6.8043 |        -997.6131 |
[32m[20221208 15:00:42 @agent_ppo2.py:179][0m |          -0.0005 |           6.5551 |       -1187.7955 |
[32m[20221208 15:00:42 @agent_ppo2.py:179][0m |          -0.0214 |           6.4824 |       -1298.5973 |
[32m[20221208 15:00:43 @agent_ppo2.py:179][0m |          -0.0296 |           6.3441 |       -1368.5605 |
[32m[20221208 15:00:43 @agent_ppo2.py:179][0m |          -0.0382 |           6.2675 |       -1402.7655 |
[32m[20221208 15:00:43 @agent_ppo2.py:179][0m |          -0.0478 |           6.2046 |       -1451.4671 |
[32m[20221208 15:00:43 @agent_ppo2.py:179][0m |          -0.0543 |           6.0965 |       -1497.1426 |
[32m[20221208 15:00:43 @agent_ppo2.py:179][0m |          -0.0582 |           6.0570 |       -1499.6974 |
[32m[20221208 15:00:43 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:00:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.84
[32m[20221208 15:00:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.52
[32m[20221208 15:00:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.94
[32m[20221208 15:00:43 @agent_ppo2.py:137][0m Total time:      24.15 min
[32m[20221208 15:00:43 @agent_ppo2.py:139][0m 1947648 total steps have happened
[32m[20221208 15:00:43 @agent_ppo2.py:115][0m #------------------------ Iteration 951 --------------------------#
[32m[20221208 15:00:44 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |           0.2544 |           3.8707 |        -812.6153 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |           0.0409 |           3.2301 |        -598.8238 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |           0.0222 |           3.0355 |        -595.4375 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0015 |           2.9165 |        -632.3647 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0213 |           2.8111 |        -691.1581 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0362 |           2.7657 |        -761.6900 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0362 |           2.7253 |        -777.5013 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0456 |           2.6769 |        -815.6637 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0499 |           2.6412 |        -853.1422 |
[32m[20221208 15:00:44 @agent_ppo2.py:179][0m |          -0.0606 |           2.5928 |        -881.1000 |
[32m[20221208 15:00:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:00:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.03
[32m[20221208 15:00:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.74
[32m[20221208 15:00:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.32
[32m[20221208 15:00:45 @agent_ppo2.py:137][0m Total time:      24.17 min
[32m[20221208 15:00:45 @agent_ppo2.py:139][0m 1949696 total steps have happened
[32m[20221208 15:00:45 @agent_ppo2.py:115][0m #------------------------ Iteration 952 --------------------------#
[32m[20221208 15:00:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:45 @agent_ppo2.py:179][0m |           0.0862 |           5.4862 |       -1416.5693 |
[32m[20221208 15:00:45 @agent_ppo2.py:179][0m |           0.0389 |           4.6914 |       -1119.4533 |
[32m[20221208 15:00:45 @agent_ppo2.py:179][0m |          -0.0084 |           4.3040 |       -1224.6060 |
[32m[20221208 15:00:45 @agent_ppo2.py:179][0m |          -0.0353 |           4.0568 |       -1288.9388 |
[32m[20221208 15:00:46 @agent_ppo2.py:179][0m |          -0.0530 |           3.8629 |       -1340.5418 |
[32m[20221208 15:00:46 @agent_ppo2.py:179][0m |          -0.0636 |           3.7524 |       -1401.0284 |
[32m[20221208 15:00:46 @agent_ppo2.py:179][0m |          -0.0695 |           3.5744 |       -1418.2438 |
[32m[20221208 15:00:46 @agent_ppo2.py:179][0m |          -0.0775 |           3.4629 |       -1491.1263 |
[32m[20221208 15:00:46 @agent_ppo2.py:179][0m |          -0.0780 |           3.3762 |       -1491.6445 |
[32m[20221208 15:00:46 @agent_ppo2.py:179][0m |          -0.0841 |           3.2878 |       -1535.9019 |
[32m[20221208 15:00:46 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.88
[32m[20221208 15:00:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 205.61
[32m[20221208 15:00:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.13
[32m[20221208 15:00:46 @agent_ppo2.py:137][0m Total time:      24.20 min
[32m[20221208 15:00:46 @agent_ppo2.py:139][0m 1951744 total steps have happened
[32m[20221208 15:00:46 @agent_ppo2.py:115][0m #------------------------ Iteration 953 --------------------------#
[32m[20221208 15:00:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |           0.1122 |           5.0078 |       -1084.7700 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |           0.0500 |           4.3025 |        -860.5810 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |           0.0161 |           4.1882 |        -950.0377 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0115 |           4.0174 |        -974.7724 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0307 |           3.9139 |       -1087.1441 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0426 |           3.9867 |       -1120.7490 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0479 |           3.8692 |       -1159.0265 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0560 |           3.7811 |       -1204.4486 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0615 |           3.7294 |       -1227.6779 |
[32m[20221208 15:00:47 @agent_ppo2.py:179][0m |          -0.0637 |           3.7603 |       -1253.7153 |
[32m[20221208 15:00:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:00:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.45
[32m[20221208 15:00:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.55
[32m[20221208 15:00:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.96
[32m[20221208 15:00:48 @agent_ppo2.py:137][0m Total time:      24.23 min
[32m[20221208 15:00:48 @agent_ppo2.py:139][0m 1953792 total steps have happened
[32m[20221208 15:00:48 @agent_ppo2.py:115][0m #------------------------ Iteration 954 --------------------------#
[32m[20221208 15:00:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:48 @agent_ppo2.py:179][0m |           0.1078 |           9.6848 |       -1612.1584 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |           0.0837 |           8.3610 |       -1337.8696 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |           0.0417 |           7.8332 |       -1345.7701 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0048 |           7.4250 |       -1575.7392 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0226 |           7.1656 |       -1706.3394 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0421 |           6.8719 |       -1784.3053 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0496 |           6.6234 |       -1801.1310 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0530 |           6.4952 |       -1835.7136 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0575 |           6.3391 |       -1858.4053 |
[32m[20221208 15:00:49 @agent_ppo2.py:179][0m |          -0.0670 |           6.2275 |       -1946.7299 |
[32m[20221208 15:00:49 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:00:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.98
[32m[20221208 15:00:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.25
[32m[20221208 15:00:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.32
[32m[20221208 15:00:49 @agent_ppo2.py:137][0m Total time:      24.25 min
[32m[20221208 15:00:49 @agent_ppo2.py:139][0m 1955840 total steps have happened
[32m[20221208 15:00:49 @agent_ppo2.py:115][0m #------------------------ Iteration 955 --------------------------#
[32m[20221208 15:00:50 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |           0.0940 |           7.9733 |       -1439.6801 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |           0.0956 |           6.8189 |       -1015.6857 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |           0.0319 |           6.3467 |       -1262.4363 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |          -0.0131 |           6.0270 |       -1436.2971 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |          -0.0269 |           5.8203 |       -1537.3080 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |          -0.0431 |           5.6107 |       -1616.9843 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |          -0.0555 |           5.4272 |       -1659.7846 |
[32m[20221208 15:00:50 @agent_ppo2.py:179][0m |          -0.0538 |           5.2835 |       -1671.7459 |
[32m[20221208 15:00:51 @agent_ppo2.py:179][0m |          -0.0606 |           5.1523 |       -1716.4888 |
[32m[20221208 15:00:51 @agent_ppo2.py:179][0m |          -0.0678 |           5.0263 |       -1736.7786 |
[32m[20221208 15:00:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:00:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.70
[32m[20221208 15:00:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.35
[32m[20221208 15:00:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 166.95
[32m[20221208 15:00:51 @agent_ppo2.py:137][0m Total time:      24.28 min
[32m[20221208 15:00:51 @agent_ppo2.py:139][0m 1957888 total steps have happened
[32m[20221208 15:00:51 @agent_ppo2.py:115][0m #------------------------ Iteration 956 --------------------------#
[32m[20221208 15:00:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:00:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |           0.1061 |           7.5744 |       -1753.6911 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |           0.0848 |           6.2500 |       -1390.7140 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |           0.0126 |           5.7254 |       -1551.8606 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0268 |           5.4734 |       -1669.7812 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0472 |           5.2801 |       -1744.6992 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0574 |           5.1165 |       -1796.0736 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0649 |           4.9969 |       -1843.7970 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0705 |           4.9173 |       -1878.6361 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0780 |           4.8027 |       -1899.8007 |
[32m[20221208 15:00:52 @agent_ppo2.py:179][0m |          -0.0801 |           4.7111 |       -1927.9637 |
[32m[20221208 15:00:52 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.02
[32m[20221208 15:00:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.77
[32m[20221208 15:00:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.53
[32m[20221208 15:00:53 @agent_ppo2.py:137][0m Total time:      24.31 min
[32m[20221208 15:00:53 @agent_ppo2.py:139][0m 1959936 total steps have happened
[32m[20221208 15:00:53 @agent_ppo2.py:115][0m #------------------------ Iteration 957 --------------------------#
[32m[20221208 15:00:53 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 15:00:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:53 @agent_ppo2.py:179][0m |           0.1362 |           4.3558 |        -891.3346 |
[32m[20221208 15:00:53 @agent_ppo2.py:179][0m |           0.1305 |           3.9639 |        -401.5005 |
[32m[20221208 15:00:53 @agent_ppo2.py:179][0m |           0.0985 |           3.8305 |        -478.9561 |
[32m[20221208 15:00:53 @agent_ppo2.py:179][0m |           0.0836 |           3.7881 |        -545.1595 |
[32m[20221208 15:00:53 @agent_ppo2.py:179][0m |           0.0742 |           3.6809 |        -572.9281 |
[32m[20221208 15:00:54 @agent_ppo2.py:179][0m |           0.0684 |           3.6308 |        -613.0000 |
[32m[20221208 15:00:54 @agent_ppo2.py:179][0m |           0.0647 |           3.5984 |        -622.0050 |
[32m[20221208 15:00:54 @agent_ppo2.py:179][0m |           0.0641 |           3.5801 |        -622.2838 |
[32m[20221208 15:00:54 @agent_ppo2.py:179][0m |           0.0597 |           3.6017 |        -625.3119 |
[32m[20221208 15:00:54 @agent_ppo2.py:179][0m |           0.0590 |           3.5968 |        -641.1730 |
[32m[20221208 15:00:54 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:00:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.10
[32m[20221208 15:00:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.94
[32m[20221208 15:00:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.96
[32m[20221208 15:00:54 @agent_ppo2.py:137][0m Total time:      24.33 min
[32m[20221208 15:00:54 @agent_ppo2.py:139][0m 1961984 total steps have happened
[32m[20221208 15:00:54 @agent_ppo2.py:115][0m #------------------------ Iteration 958 --------------------------#
[32m[20221208 15:00:55 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |           0.1219 |           6.8925 |       -1763.6563 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |           0.1182 |           5.7857 |        -948.1857 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |           0.0408 |           5.2646 |       -1158.5180 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |           0.0182 |           4.9226 |       -1359.1520 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |          -0.0163 |           4.6433 |       -1423.1662 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |          -0.0322 |           4.4817 |       -1533.3230 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |          -0.0449 |           4.3261 |       -1557.4371 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |          -0.0567 |           4.2098 |       -1649.0739 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |          -0.0470 |           4.0965 |       -1658.9885 |
[32m[20221208 15:00:55 @agent_ppo2.py:179][0m |          -0.0629 |           3.9774 |       -1705.4411 |
[32m[20221208 15:00:55 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.14
[32m[20221208 15:00:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.59
[32m[20221208 15:00:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.72
[32m[20221208 15:00:56 @agent_ppo2.py:137][0m Total time:      24.36 min
[32m[20221208 15:00:56 @agent_ppo2.py:139][0m 1964032 total steps have happened
[32m[20221208 15:00:56 @agent_ppo2.py:115][0m #------------------------ Iteration 959 --------------------------#
[32m[20221208 15:00:56 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:56 @agent_ppo2.py:179][0m |           0.1311 |           8.3272 |       -1561.7466 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |           0.0944 |           7.4506 |       -1203.7453 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |           0.0284 |           7.0424 |       -1430.1820 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0005 |           6.6864 |       -1588.8095 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0254 |           6.5135 |       -1677.5555 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0352 |           6.3573 |       -1723.5238 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0416 |           6.2353 |       -1750.8135 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0520 |           6.1535 |       -1836.1649 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0597 |           5.9696 |       -1877.6075 |
[32m[20221208 15:00:57 @agent_ppo2.py:179][0m |          -0.0619 |           5.9913 |       -1908.4023 |
[32m[20221208 15:00:57 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 15:00:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.95
[32m[20221208 15:00:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.39
[32m[20221208 15:00:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.85
[32m[20221208 15:00:58 @agent_ppo2.py:137][0m Total time:      24.39 min
[32m[20221208 15:00:58 @agent_ppo2.py:139][0m 1966080 total steps have happened
[32m[20221208 15:00:58 @agent_ppo2.py:115][0m #------------------------ Iteration 960 --------------------------#
[32m[20221208 15:00:58 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:00:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:00:58 @agent_ppo2.py:179][0m |           0.2945 |           7.8183 |       -1443.5647 |
[32m[20221208 15:00:58 @agent_ppo2.py:179][0m |           0.2052 |           6.7287 |        -788.9771 |
[32m[20221208 15:00:58 @agent_ppo2.py:179][0m |           0.0648 |           6.2138 |       -1076.0109 |
[32m[20221208 15:00:58 @agent_ppo2.py:179][0m |           0.0221 |           5.9209 |       -1371.7428 |
[32m[20221208 15:00:58 @agent_ppo2.py:179][0m |          -0.0026 |           5.6924 |       -1493.4001 |
[32m[20221208 15:00:58 @agent_ppo2.py:179][0m |          -0.0087 |           5.5140 |       -1511.4440 |
[32m[20221208 15:00:59 @agent_ppo2.py:179][0m |          -0.0247 |           5.3684 |       -1497.3161 |
[32m[20221208 15:00:59 @agent_ppo2.py:179][0m |          -0.0464 |           5.3109 |       -1593.3707 |
[32m[20221208 15:00:59 @agent_ppo2.py:179][0m |          -0.0421 |           5.1852 |       -1554.7911 |
[32m[20221208 15:00:59 @agent_ppo2.py:179][0m |          -0.0486 |           5.1047 |       -1619.5888 |
[32m[20221208 15:00:59 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:00:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.74
[32m[20221208 15:00:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.15
[32m[20221208 15:00:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.97
[32m[20221208 15:00:59 @agent_ppo2.py:137][0m Total time:      24.42 min
[32m[20221208 15:00:59 @agent_ppo2.py:139][0m 1968128 total steps have happened
[32m[20221208 15:00:59 @agent_ppo2.py:115][0m #------------------------ Iteration 961 --------------------------#
[32m[20221208 15:01:00 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |           0.1093 |           5.1562 |       -1147.7819 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |           0.0580 |           4.4752 |        -893.2781 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |           0.0169 |           4.2196 |       -1070.0645 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0132 |           4.1146 |       -1195.6956 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0298 |           3.9366 |       -1260.0393 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0382 |           3.9037 |       -1304.0665 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0471 |           3.8050 |       -1366.1739 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0471 |           3.7146 |       -1347.8072 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0511 |           3.6750 |       -1369.1387 |
[32m[20221208 15:01:00 @agent_ppo2.py:179][0m |          -0.0575 |           3.5915 |       -1408.0780 |
[32m[20221208 15:01:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.46
[32m[20221208 15:01:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.41
[32m[20221208 15:01:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.03
[32m[20221208 15:01:01 @agent_ppo2.py:137][0m Total time:      24.44 min
[32m[20221208 15:01:01 @agent_ppo2.py:139][0m 1970176 total steps have happened
[32m[20221208 15:01:01 @agent_ppo2.py:115][0m #------------------------ Iteration 962 --------------------------#
[32m[20221208 15:01:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:01 @agent_ppo2.py:179][0m |           0.1008 |           8.6369 |       -1760.3361 |
[32m[20221208 15:01:01 @agent_ppo2.py:179][0m |           0.1541 |           7.7296 |       -1002.9884 |
[32m[20221208 15:01:01 @agent_ppo2.py:179][0m |           0.0584 |           7.1483 |       -1285.8705 |
[32m[20221208 15:01:01 @agent_ppo2.py:179][0m |           0.0137 |           6.7951 |       -1538.3669 |
[32m[20221208 15:01:02 @agent_ppo2.py:179][0m |          -0.0132 |           6.4794 |       -1715.2654 |
[32m[20221208 15:01:02 @agent_ppo2.py:179][0m |          -0.0293 |           6.2566 |       -1711.9588 |
[32m[20221208 15:01:02 @agent_ppo2.py:179][0m |          -0.0441 |           6.1944 |       -1812.4039 |
[32m[20221208 15:01:02 @agent_ppo2.py:179][0m |          -0.0516 |           6.0140 |       -1870.7264 |
[32m[20221208 15:01:02 @agent_ppo2.py:179][0m |          -0.0573 |           5.9165 |       -1909.1771 |
[32m[20221208 15:01:02 @agent_ppo2.py:179][0m |          -0.0660 |           5.7639 |       -1955.4855 |
[32m[20221208 15:01:02 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:01:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 155.03
[32m[20221208 15:01:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.53
[32m[20221208 15:01:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.89
[32m[20221208 15:01:02 @agent_ppo2.py:137][0m Total time:      24.47 min
[32m[20221208 15:01:02 @agent_ppo2.py:139][0m 1972224 total steps have happened
[32m[20221208 15:01:02 @agent_ppo2.py:115][0m #------------------------ Iteration 963 --------------------------#
[32m[20221208 15:01:03 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |           0.0873 |           6.0315 |       -1581.3286 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |           0.1003 |           5.2334 |        -969.4287 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |           0.0966 |           4.8301 |        -807.5432 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |           0.0610 |           4.5208 |        -806.3305 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |           0.0240 |           4.3537 |        -960.6650 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |           0.0007 |           4.2018 |       -1104.5302 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |          -0.0167 |           4.0028 |       -1220.4219 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |          -0.0350 |           3.9040 |       -1311.4376 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |          -0.0333 |           3.7964 |       -1313.1787 |
[32m[20221208 15:01:03 @agent_ppo2.py:179][0m |          -0.0407 |           3.7522 |       -1356.1719 |
[32m[20221208 15:01:03 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:01:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.61
[32m[20221208 15:01:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.49
[32m[20221208 15:01:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.28
[32m[20221208 15:01:04 @agent_ppo2.py:137][0m Total time:      24.49 min
[32m[20221208 15:01:04 @agent_ppo2.py:139][0m 1974272 total steps have happened
[32m[20221208 15:01:04 @agent_ppo2.py:115][0m #------------------------ Iteration 964 --------------------------#
[32m[20221208 15:01:04 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:01:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:04 @agent_ppo2.py:179][0m |           0.1662 |          10.0218 |       -1447.0629 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |           0.1125 |           8.1417 |       -1056.0022 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |           0.0781 |           7.5092 |       -1136.5323 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |           0.0248 |           7.1173 |       -1247.5778 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |          -0.0136 |           6.8258 |       -1501.9578 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |          -0.0354 |           6.6318 |       -1609.3867 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |          -0.0438 |           6.4350 |       -1648.7933 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |          -0.0528 |           6.3608 |       -1718.7187 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |          -0.0606 |           6.1950 |       -1755.5150 |
[32m[20221208 15:01:05 @agent_ppo2.py:179][0m |          -0.0607 |           6.1579 |       -1752.4860 |
[32m[20221208 15:01:05 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:01:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.22
[32m[20221208 15:01:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.78
[32m[20221208 15:01:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.43
[32m[20221208 15:01:05 @agent_ppo2.py:137][0m Total time:      24.52 min
[32m[20221208 15:01:05 @agent_ppo2.py:139][0m 1976320 total steps have happened
[32m[20221208 15:01:05 @agent_ppo2.py:115][0m #------------------------ Iteration 965 --------------------------#
[32m[20221208 15:01:06 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |           0.0773 |           6.2531 |       -1429.3575 |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |           0.0330 |           5.6354 |       -1237.9934 |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |           0.0300 |           5.4215 |       -1184.6090 |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |          -0.0033 |           5.2932 |       -1278.9855 |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |          -0.0385 |           5.1678 |       -1393.2263 |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |          -0.0518 |           5.0862 |       -1477.1124 |
[32m[20221208 15:01:06 @agent_ppo2.py:179][0m |          -0.0617 |           4.9666 |       -1509.7726 |
[32m[20221208 15:01:07 @agent_ppo2.py:179][0m |          -0.0657 |           4.9462 |       -1540.6862 |
[32m[20221208 15:01:07 @agent_ppo2.py:179][0m |          -0.0743 |           4.8766 |       -1581.8858 |
[32m[20221208 15:01:07 @agent_ppo2.py:179][0m |          -0.0758 |           4.8875 |       -1615.3475 |
[32m[20221208 15:01:07 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:01:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.54
[32m[20221208 15:01:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.74
[32m[20221208 15:01:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.90
[32m[20221208 15:01:07 @agent_ppo2.py:137][0m Total time:      24.55 min
[32m[20221208 15:01:07 @agent_ppo2.py:139][0m 1978368 total steps have happened
[32m[20221208 15:01:07 @agent_ppo2.py:115][0m #------------------------ Iteration 966 --------------------------#
[32m[20221208 15:01:08 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:01:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0784 |           6.2415 |       -1297.1725 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0495 |           5.4455 |        -977.1437 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0227 |           5.1165 |       -1030.8887 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0569 |           4.9689 |        -883.0348 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0369 |           4.8488 |        -777.1126 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0202 |           4.7730 |        -836.9604 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0129 |           4.7322 |        -887.2403 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0074 |           4.6452 |        -904.9920 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |           0.0027 |           4.5884 |        -953.1785 |
[32m[20221208 15:01:08 @agent_ppo2.py:179][0m |          -0.0013 |           4.5314 |        -979.4555 |
[32m[20221208 15:01:08 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:01:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.66
[32m[20221208 15:01:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.86
[32m[20221208 15:01:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.55
[32m[20221208 15:01:09 @agent_ppo2.py:137][0m Total time:      24.57 min
[32m[20221208 15:01:09 @agent_ppo2.py:139][0m 1980416 total steps have happened
[32m[20221208 15:01:09 @agent_ppo2.py:115][0m #------------------------ Iteration 967 --------------------------#
[32m[20221208 15:01:09 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:09 @agent_ppo2.py:179][0m |           0.0959 |           7.6650 |       -1632.2931 |
[32m[20221208 15:01:09 @agent_ppo2.py:179][0m |           0.0616 |           6.7161 |       -1240.6532 |
[32m[20221208 15:01:09 @agent_ppo2.py:179][0m |           0.0028 |           6.2828 |       -1524.9047 |
[32m[20221208 15:01:09 @agent_ppo2.py:179][0m |          -0.0201 |           6.0491 |       -1639.3607 |
[32m[20221208 15:01:10 @agent_ppo2.py:179][0m |          -0.0341 |           5.8238 |       -1676.5422 |
[32m[20221208 15:01:10 @agent_ppo2.py:179][0m |          -0.0422 |           5.6908 |       -1709.2774 |
[32m[20221208 15:01:10 @agent_ppo2.py:179][0m |          -0.0472 |           5.5778 |       -1770.0670 |
[32m[20221208 15:01:10 @agent_ppo2.py:179][0m |          -0.0536 |           5.5013 |       -1762.5983 |
[32m[20221208 15:01:10 @agent_ppo2.py:179][0m |          -0.0527 |           5.3851 |       -1773.2564 |
[32m[20221208 15:01:10 @agent_ppo2.py:179][0m |          -0.0565 |           5.2854 |       -1829.1156 |
[32m[20221208 15:01:10 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:01:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.18
[32m[20221208 15:01:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.61
[32m[20221208 15:01:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.09
[32m[20221208 15:01:10 @agent_ppo2.py:137][0m Total time:      24.60 min
[32m[20221208 15:01:10 @agent_ppo2.py:139][0m 1982464 total steps have happened
[32m[20221208 15:01:10 @agent_ppo2.py:115][0m #------------------------ Iteration 968 --------------------------#
[32m[20221208 15:01:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |           0.1693 |           5.6392 |       -1353.0637 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |           0.1616 |           4.9816 |        -729.6500 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |           0.0952 |           4.7033 |        -845.0914 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |           0.0371 |           4.4812 |       -1135.1301 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |          -0.0038 |           4.3557 |       -1327.7248 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |          -0.0198 |           4.2243 |       -1439.2594 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |          -0.0315 |           4.1118 |       -1466.7698 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |          -0.0429 |           4.0538 |       -1521.2622 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |          -0.0518 |           3.9856 |       -1572.7730 |
[32m[20221208 15:01:11 @agent_ppo2.py:179][0m |          -0.0588 |           3.9057 |       -1646.7700 |
[32m[20221208 15:01:11 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:01:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.28
[32m[20221208 15:01:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.76
[32m[20221208 15:01:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.72
[32m[20221208 15:01:12 @agent_ppo2.py:137][0m Total time:      24.63 min
[32m[20221208 15:01:12 @agent_ppo2.py:139][0m 1984512 total steps have happened
[32m[20221208 15:01:12 @agent_ppo2.py:115][0m #------------------------ Iteration 969 --------------------------#
[32m[20221208 15:01:12 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:12 @agent_ppo2.py:179][0m |           0.0788 |           8.9230 |       -1703.7040 |
[32m[20221208 15:01:12 @agent_ppo2.py:179][0m |           0.0780 |           7.4583 |       -1318.0172 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |           0.0145 |           6.9349 |       -1469.2049 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0210 |           6.6212 |       -1597.8572 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0309 |           6.4160 |       -1669.7779 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0479 |           6.2403 |       -1676.0585 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0552 |           6.1473 |       -1699.0583 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0594 |           5.9871 |       -1743.8159 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0647 |           5.9381 |       -1739.1113 |
[32m[20221208 15:01:13 @agent_ppo2.py:179][0m |          -0.0712 |           5.8881 |       -1800.5641 |
[32m[20221208 15:01:13 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:01:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.81
[32m[20221208 15:01:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.03
[32m[20221208 15:01:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.20
[32m[20221208 15:01:13 @agent_ppo2.py:137][0m Total time:      24.65 min
[32m[20221208 15:01:13 @agent_ppo2.py:139][0m 1986560 total steps have happened
[32m[20221208 15:01:13 @agent_ppo2.py:115][0m #------------------------ Iteration 970 --------------------------#
[32m[20221208 15:01:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |           0.0854 |           9.5649 |       -1601.4284 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |           0.0815 |           8.6391 |       -1358.5762 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |           0.0395 |           8.1522 |       -1463.5587 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |           0.0050 |           7.7666 |       -1594.6354 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |          -0.0186 |           7.5259 |       -1737.1672 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |          -0.0305 |           7.3473 |       -1797.5796 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |          -0.0356 |           7.2419 |       -1824.3410 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |          -0.0402 |           7.0786 |       -1835.8462 |
[32m[20221208 15:01:14 @agent_ppo2.py:179][0m |          -0.0509 |           6.9685 |       -1922.2647 |
[32m[20221208 15:01:15 @agent_ppo2.py:179][0m |          -0.0591 |           6.9217 |       -1992.7649 |
[32m[20221208 15:01:15 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:01:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.19
[32m[20221208 15:01:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.36
[32m[20221208 15:01:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.55
[32m[20221208 15:01:15 @agent_ppo2.py:137][0m Total time:      24.68 min
[32m[20221208 15:01:15 @agent_ppo2.py:139][0m 1988608 total steps have happened
[32m[20221208 15:01:15 @agent_ppo2.py:115][0m #------------------------ Iteration 971 --------------------------#
[32m[20221208 15:01:15 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:01:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |           0.1478 |           4.7717 |       -1301.6566 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |           0.0491 |           3.9862 |       -1060.3929 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |           0.0020 |           3.7647 |       -1201.5407 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0256 |           3.5728 |       -1350.4681 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0388 |           3.5188 |       -1388.8710 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0505 |           3.3729 |       -1441.9724 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0473 |           3.2927 |       -1418.9653 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0510 |           3.2307 |       -1446.0782 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0596 |           3.1877 |       -1503.2566 |
[32m[20221208 15:01:16 @agent_ppo2.py:179][0m |          -0.0649 |           3.1308 |       -1539.4378 |
[32m[20221208 15:01:16 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.14
[32m[20221208 15:01:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.96
[32m[20221208 15:01:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.28
[32m[20221208 15:01:17 @agent_ppo2.py:137][0m Total time:      24.70 min
[32m[20221208 15:01:17 @agent_ppo2.py:139][0m 1990656 total steps have happened
[32m[20221208 15:01:17 @agent_ppo2.py:115][0m #------------------------ Iteration 972 --------------------------#
[32m[20221208 15:01:17 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:01:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:17 @agent_ppo2.py:179][0m |           0.1708 |           7.6789 |       -1484.6418 |
[32m[20221208 15:01:17 @agent_ppo2.py:179][0m |           0.1324 |           6.8310 |        -624.0370 |
[32m[20221208 15:01:17 @agent_ppo2.py:179][0m |           0.0634 |           6.6023 |        -737.7849 |
[32m[20221208 15:01:17 @agent_ppo2.py:179][0m |           0.0067 |           6.3651 |       -1114.9931 |
[32m[20221208 15:01:17 @agent_ppo2.py:179][0m |          -0.0166 |           6.2974 |       -1222.0202 |
[32m[20221208 15:01:17 @agent_ppo2.py:179][0m |          -0.0248 |           6.2394 |       -1269.9099 |
[32m[20221208 15:01:18 @agent_ppo2.py:179][0m |          -0.0430 |           6.0456 |       -1360.2277 |
[32m[20221208 15:01:18 @agent_ppo2.py:179][0m |          -0.0522 |           6.0324 |       -1455.2268 |
[32m[20221208 15:01:18 @agent_ppo2.py:179][0m |          -0.0543 |           5.9603 |       -1469.6032 |
[32m[20221208 15:01:18 @agent_ppo2.py:179][0m |          -0.0636 |           5.8710 |       -1510.3377 |
[32m[20221208 15:01:18 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 15:01:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 127.06
[32m[20221208 15:01:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 168.43
[32m[20221208 15:01:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.70
[32m[20221208 15:01:18 @agent_ppo2.py:137][0m Total time:      24.73 min
[32m[20221208 15:01:18 @agent_ppo2.py:139][0m 1992704 total steps have happened
[32m[20221208 15:01:18 @agent_ppo2.py:115][0m #------------------------ Iteration 973 --------------------------#
[32m[20221208 15:01:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |           0.0969 |           4.6655 |       -1224.8382 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |           0.0114 |           4.0397 |        -970.3210 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0212 |           3.7468 |       -1074.3917 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0346 |           3.5866 |       -1150.4230 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0445 |           3.5313 |       -1172.5516 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0541 |           3.4390 |       -1213.2114 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0591 |           3.3087 |       -1248.8116 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0578 |           3.2925 |       -1266.9255 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0658 |           3.2246 |       -1290.7145 |
[32m[20221208 15:01:19 @agent_ppo2.py:179][0m |          -0.0667 |           3.1750 |       -1290.4980 |
[32m[20221208 15:01:19 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.32
[32m[20221208 15:01:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.93
[32m[20221208 15:01:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.63
[32m[20221208 15:01:20 @agent_ppo2.py:137][0m Total time:      24.76 min
[32m[20221208 15:01:20 @agent_ppo2.py:139][0m 1994752 total steps have happened
[32m[20221208 15:01:20 @agent_ppo2.py:115][0m #------------------------ Iteration 974 --------------------------#
[32m[20221208 15:01:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:20 @agent_ppo2.py:179][0m |           0.1307 |          10.3660 |       -1795.4378 |
[32m[20221208 15:01:20 @agent_ppo2.py:179][0m |           0.0908 |           9.5255 |       -1366.7898 |
[32m[20221208 15:01:20 @agent_ppo2.py:179][0m |           0.0401 |           9.1045 |       -1562.6384 |
[32m[20221208 15:01:20 @agent_ppo2.py:179][0m |           0.0028 |           8.9082 |       -1718.5538 |
[32m[20221208 15:01:20 @agent_ppo2.py:179][0m |          -0.0087 |           8.7142 |       -1833.8803 |
[32m[20221208 15:01:21 @agent_ppo2.py:179][0m |          -0.0190 |           8.5174 |       -1760.2394 |
[32m[20221208 15:01:21 @agent_ppo2.py:179][0m |          -0.0361 |           8.4254 |       -1947.2049 |
[32m[20221208 15:01:21 @agent_ppo2.py:179][0m |          -0.0461 |           8.3497 |       -2014.7795 |
[32m[20221208 15:01:21 @agent_ppo2.py:179][0m |          -0.0550 |           8.1962 |       -2098.6023 |
[32m[20221208 15:01:21 @agent_ppo2.py:179][0m |          -0.0506 |           8.2042 |       -2049.4078 |
[32m[20221208 15:01:21 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.85
[32m[20221208 15:01:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.46
[32m[20221208 15:01:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 160.25
[32m[20221208 15:01:21 @agent_ppo2.py:137][0m Total time:      24.78 min
[32m[20221208 15:01:21 @agent_ppo2.py:139][0m 1996800 total steps have happened
[32m[20221208 15:01:21 @agent_ppo2.py:115][0m #------------------------ Iteration 975 --------------------------#
[32m[20221208 15:01:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |           0.1363 |           9.6920 |       -1790.7767 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |           0.1380 |           8.1341 |       -1221.3346 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |           0.0797 |           7.7763 |       -1273.3350 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |           0.0409 |           7.4653 |       -1558.2525 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |           0.0033 |           7.2772 |       -1686.8533 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |          -0.0136 |           7.0388 |       -1810.7893 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |          -0.0331 |           6.9296 |       -1920.5273 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |          -0.0454 |           6.7819 |       -2004.2162 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |          -0.0506 |           6.6615 |       -2061.4812 |
[32m[20221208 15:01:22 @agent_ppo2.py:179][0m |          -0.0486 |           6.6052 |       -2050.2352 |
[32m[20221208 15:01:22 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 15:01:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.81
[32m[20221208 15:01:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.78
[32m[20221208 15:01:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.09
[32m[20221208 15:01:23 @agent_ppo2.py:137][0m Total time:      24.81 min
[32m[20221208 15:01:23 @agent_ppo2.py:139][0m 1998848 total steps have happened
[32m[20221208 15:01:23 @agent_ppo2.py:115][0m #------------------------ Iteration 976 --------------------------#
[32m[20221208 15:01:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:23 @agent_ppo2.py:179][0m |           0.1474 |           7.3069 |       -1300.6829 |
[32m[20221208 15:01:23 @agent_ppo2.py:179][0m |           0.0413 |           6.5407 |       -1012.5914 |
[32m[20221208 15:01:23 @agent_ppo2.py:179][0m |           0.0133 |           6.2200 |       -1112.0244 |
[32m[20221208 15:01:23 @agent_ppo2.py:179][0m |          -0.0147 |           5.9134 |       -1153.2873 |
[32m[20221208 15:01:24 @agent_ppo2.py:179][0m |          -0.0326 |           5.6958 |       -1270.5201 |
[32m[20221208 15:01:24 @agent_ppo2.py:179][0m |          -0.0435 |           5.5786 |       -1311.8974 |
[32m[20221208 15:01:24 @agent_ppo2.py:179][0m |          -0.0482 |           5.5678 |       -1366.9893 |
[32m[20221208 15:01:24 @agent_ppo2.py:179][0m |          -0.0549 |           5.3507 |       -1386.9043 |
[32m[20221208 15:01:24 @agent_ppo2.py:179][0m |          -0.0566 |           5.3122 |       -1405.0077 |
[32m[20221208 15:01:24 @agent_ppo2.py:179][0m |          -0.0594 |           5.2620 |       -1439.8953 |
[32m[20221208 15:01:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.36
[32m[20221208 15:01:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.45
[32m[20221208 15:01:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.64
[32m[20221208 15:01:24 @agent_ppo2.py:137][0m Total time:      24.83 min
[32m[20221208 15:01:24 @agent_ppo2.py:139][0m 2000896 total steps have happened
[32m[20221208 15:01:24 @agent_ppo2.py:115][0m #------------------------ Iteration 977 --------------------------#
[32m[20221208 15:01:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |           0.1054 |           6.0565 |       -1646.8797 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |           0.0775 |           4.7888 |       -1395.8590 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |           0.0054 |           4.3615 |       -1514.7811 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0203 |           4.1023 |       -1601.9216 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0452 |           3.8870 |       -1747.3493 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0537 |           3.7377 |       -1809.6987 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0611 |           3.6149 |       -1824.5942 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0659 |           3.5016 |       -1872.9480 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0767 |           3.4398 |       -1958.2491 |
[32m[20221208 15:01:25 @agent_ppo2.py:179][0m |          -0.0799 |           3.3917 |       -2017.4989 |
[32m[20221208 15:01:25 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.59
[32m[20221208 15:01:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.68
[32m[20221208 15:01:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.06
[32m[20221208 15:01:26 @agent_ppo2.py:137][0m Total time:      24.86 min
[32m[20221208 15:01:26 @agent_ppo2.py:139][0m 2002944 total steps have happened
[32m[20221208 15:01:26 @agent_ppo2.py:115][0m #------------------------ Iteration 978 --------------------------#
[32m[20221208 15:01:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:26 @agent_ppo2.py:179][0m |           0.1072 |           7.6388 |       -1609.6775 |
[32m[20221208 15:01:26 @agent_ppo2.py:179][0m |           0.1323 |           6.9234 |        -970.3902 |
[32m[20221208 15:01:26 @agent_ppo2.py:179][0m |           0.0961 |           6.6077 |        -782.0237 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |           0.0448 |           6.3764 |       -1200.3329 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |           0.0047 |           6.1620 |       -1460.8192 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |          -0.0131 |           6.0174 |       -1596.1819 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |          -0.0174 |           5.8931 |       -1787.7508 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |          -0.0285 |           5.7567 |       -1618.8840 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |          -0.0385 |           5.6325 |       -1709.4715 |
[32m[20221208 15:01:27 @agent_ppo2.py:179][0m |          -0.0383 |           5.5770 |       -1721.2789 |
[32m[20221208 15:01:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:01:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.59
[32m[20221208 15:01:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.99
[32m[20221208 15:01:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.50
[32m[20221208 15:01:27 @agent_ppo2.py:137][0m Total time:      24.88 min
[32m[20221208 15:01:27 @agent_ppo2.py:139][0m 2004992 total steps have happened
[32m[20221208 15:01:27 @agent_ppo2.py:115][0m #------------------------ Iteration 979 --------------------------#
[32m[20221208 15:01:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |           0.1066 |           7.3906 |       -1656.6639 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |           0.0393 |           6.4264 |       -1382.4183 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0032 |           5.9688 |       -1581.3186 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0260 |           5.6692 |       -1678.2480 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0419 |           5.4423 |       -1742.9212 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0533 |           5.2511 |       -1796.1791 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0620 |           5.0914 |       -1854.3989 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0650 |           4.9457 |       -1895.5462 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0713 |           4.8481 |       -1946.4337 |
[32m[20221208 15:01:28 @agent_ppo2.py:179][0m |          -0.0757 |           4.7280 |       -1960.1357 |
[32m[20221208 15:01:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:01:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.01
[32m[20221208 15:01:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.42
[32m[20221208 15:01:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.03
[32m[20221208 15:01:29 @agent_ppo2.py:137][0m Total time:      24.91 min
[32m[20221208 15:01:29 @agent_ppo2.py:139][0m 2007040 total steps have happened
[32m[20221208 15:01:29 @agent_ppo2.py:115][0m #------------------------ Iteration 980 --------------------------#
[32m[20221208 15:01:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:29 @agent_ppo2.py:179][0m |           0.0673 |           4.1378 |       -1179.9893 |
[32m[20221208 15:01:29 @agent_ppo2.py:179][0m |           0.0502 |           3.7052 |        -543.3871 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0147 |           3.5217 |        -940.6496 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0238 |           3.4245 |        -963.1432 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0408 |           3.3766 |       -1066.8163 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0486 |           3.3174 |       -1106.3370 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0464 |           3.2807 |       -1135.3104 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0384 |           3.2509 |       -1067.5658 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |           0.0129 |           3.2388 |        -709.1412 |
[32m[20221208 15:01:30 @agent_ppo2.py:179][0m |          -0.0387 |           3.2019 |       -1032.1718 |
[32m[20221208 15:01:30 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:01:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.93
[32m[20221208 15:01:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.32
[32m[20221208 15:01:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.97
[32m[20221208 15:01:30 @agent_ppo2.py:137][0m Total time:      24.93 min
[32m[20221208 15:01:30 @agent_ppo2.py:139][0m 2009088 total steps have happened
[32m[20221208 15:01:30 @agent_ppo2.py:115][0m #------------------------ Iteration 981 --------------------------#
[32m[20221208 15:01:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |           0.0839 |           7.0718 |       -2109.7908 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |           0.0613 |           6.1990 |       -1563.3213 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |           0.0144 |           5.7826 |       -1659.4171 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |           0.0132 |           5.4768 |       -1681.2534 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |          -0.0321 |           5.3316 |       -1829.9006 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |          -0.0527 |           5.1825 |       -1951.5728 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |          -0.0573 |           5.0457 |       -1966.1162 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |          -0.0585 |           4.9456 |       -2012.0483 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |          -0.0574 |           4.8645 |       -1968.3516 |
[32m[20221208 15:01:31 @agent_ppo2.py:179][0m |          -0.0671 |           4.8004 |       -2049.3769 |
[32m[20221208 15:01:31 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 134.29
[32m[20221208 15:01:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.00
[32m[20221208 15:01:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.24
[32m[20221208 15:01:32 @agent_ppo2.py:137][0m Total time:      24.96 min
[32m[20221208 15:01:32 @agent_ppo2.py:139][0m 2011136 total steps have happened
[32m[20221208 15:01:32 @agent_ppo2.py:115][0m #------------------------ Iteration 982 --------------------------#
[32m[20221208 15:01:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:32 @agent_ppo2.py:179][0m |           0.1057 |           8.4023 |       -2079.4579 |
[32m[20221208 15:01:32 @agent_ppo2.py:179][0m |           0.0932 |           7.3446 |       -1247.3472 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |           0.0600 |           6.8185 |       -1328.4031 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |           0.0231 |           6.4954 |       -1547.5710 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |          -0.0059 |           6.2087 |       -1707.2679 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |          -0.0180 |           6.0407 |       -1860.7547 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |          -0.0330 |           5.9206 |       -1969.3096 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |          -0.0422 |           5.7462 |       -2050.3801 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |          -0.0475 |           5.6268 |       -2083.6226 |
[32m[20221208 15:01:33 @agent_ppo2.py:179][0m |          -0.0517 |           5.5018 |       -2182.1972 |
[32m[20221208 15:01:33 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.52
[32m[20221208 15:01:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.41
[32m[20221208 15:01:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.71
[32m[20221208 15:01:33 @agent_ppo2.py:137][0m Total time:      24.99 min
[32m[20221208 15:01:33 @agent_ppo2.py:139][0m 2013184 total steps have happened
[32m[20221208 15:01:33 @agent_ppo2.py:115][0m #------------------------ Iteration 983 --------------------------#
[32m[20221208 15:01:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |           0.1215 |           8.8662 |       -1959.5505 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |           0.1019 |           7.0388 |       -1310.6458 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |           0.0448 |           6.3835 |       -1345.1042 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |           0.0212 |           6.0096 |       -1359.5003 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |          -0.0220 |           5.8322 |       -1511.7447 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |          -0.0475 |           5.6493 |       -1688.3146 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |          -0.0587 |           5.5262 |       -1710.7808 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |          -0.0673 |           5.4231 |       -1770.6178 |
[32m[20221208 15:01:34 @agent_ppo2.py:179][0m |          -0.0677 |           5.2477 |       -1792.2058 |
[32m[20221208 15:01:35 @agent_ppo2.py:179][0m |          -0.0724 |           5.1793 |       -1858.0331 |
[32m[20221208 15:01:35 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.00
[32m[20221208 15:01:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 183.82
[32m[20221208 15:01:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.55
[32m[20221208 15:01:35 @agent_ppo2.py:137][0m Total time:      25.01 min
[32m[20221208 15:01:35 @agent_ppo2.py:139][0m 2015232 total steps have happened
[32m[20221208 15:01:35 @agent_ppo2.py:115][0m #------------------------ Iteration 984 --------------------------#
[32m[20221208 15:01:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:35 @agent_ppo2.py:179][0m |           0.1295 |           8.6837 |       -1544.8445 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |           0.1046 |           7.7026 |        -981.8690 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |           0.0441 |           7.3002 |       -1091.3204 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |           0.0104 |           7.0370 |       -1249.4800 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |          -0.0135 |           6.7959 |       -1430.8928 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |          -0.0276 |           6.6536 |       -1515.4756 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |          -0.0378 |           6.4746 |       -1575.2834 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |          -0.0427 |           6.3617 |       -1626.6868 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |          -0.0484 |           6.2719 |       -1651.2266 |
[32m[20221208 15:01:36 @agent_ppo2.py:179][0m |          -0.0550 |           6.2334 |       -1695.4048 |
[32m[20221208 15:01:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:01:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.32
[32m[20221208 15:01:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.80
[32m[20221208 15:01:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 162.54
[32m[20221208 15:01:36 @agent_ppo2.py:137][0m Total time:      25.04 min
[32m[20221208 15:01:36 @agent_ppo2.py:139][0m 2017280 total steps have happened
[32m[20221208 15:01:36 @agent_ppo2.py:115][0m #------------------------ Iteration 985 --------------------------#
[32m[20221208 15:01:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |           0.0799 |           9.7290 |       -1981.0183 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |           0.1474 |           8.7775 |       -1186.7569 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |           0.1408 |           8.2763 |       -1080.2117 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |           0.0831 |           7.9695 |       -1117.0573 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |           0.0420 |           7.7163 |       -1430.9456 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |           0.0132 |           7.5594 |       -1657.4317 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |          -0.0025 |           7.3754 |       -1758.0810 |
[32m[20221208 15:01:37 @agent_ppo2.py:179][0m |          -0.0162 |           7.2719 |       -1885.5437 |
[32m[20221208 15:01:38 @agent_ppo2.py:179][0m |          -0.0249 |           7.1404 |       -1922.6220 |
[32m[20221208 15:01:38 @agent_ppo2.py:179][0m |          -0.0373 |           6.9726 |       -1976.2089 |
[32m[20221208 15:01:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.76
[32m[20221208 15:01:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.98
[32m[20221208 15:01:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.58
[32m[20221208 15:01:38 @agent_ppo2.py:137][0m Total time:      25.06 min
[32m[20221208 15:01:38 @agent_ppo2.py:139][0m 2019328 total steps have happened
[32m[20221208 15:01:38 @agent_ppo2.py:115][0m #------------------------ Iteration 986 --------------------------#
[32m[20221208 15:01:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |           0.0714 |           6.7253 |       -1612.5783 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |           0.0523 |           6.1093 |       -1272.5198 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0015 |           5.9048 |       -1451.3018 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0038 |           5.7104 |       -1435.5688 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0287 |           5.5645 |       -1554.9082 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0428 |           5.4837 |       -1624.5029 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0500 |           5.3848 |       -1686.7548 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0547 |           5.3267 |       -1751.4843 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0571 |           5.2687 |       -1755.1972 |
[32m[20221208 15:01:39 @agent_ppo2.py:179][0m |          -0.0620 |           5.1652 |       -1790.5467 |
[32m[20221208 15:01:39 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.89
[32m[20221208 15:01:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.42
[32m[20221208 15:01:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.31
[32m[20221208 15:01:40 @agent_ppo2.py:137][0m Total time:      25.09 min
[32m[20221208 15:01:40 @agent_ppo2.py:139][0m 2021376 total steps have happened
[32m[20221208 15:01:40 @agent_ppo2.py:115][0m #------------------------ Iteration 987 --------------------------#
[32m[20221208 15:01:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |           0.1647 |           8.6501 |       -1836.2923 |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |           0.1240 |           7.9452 |       -1037.7335 |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |           0.0622 |           7.6198 |       -1575.1121 |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |           0.0292 |           7.4463 |       -1676.8213 |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |           0.0005 |           7.3472 |       -1839.7250 |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |          -0.0169 |           7.1881 |       -1933.4685 |
[32m[20221208 15:01:40 @agent_ppo2.py:179][0m |          -0.0320 |           7.1111 |       -1980.3305 |
[32m[20221208 15:01:41 @agent_ppo2.py:179][0m |          -0.0406 |           7.0504 |       -2018.8667 |
[32m[20221208 15:01:41 @agent_ppo2.py:179][0m |          -0.0435 |           6.9454 |       -2032.3249 |
[32m[20221208 15:01:41 @agent_ppo2.py:179][0m |          -0.0444 |           6.9056 |       -2077.6386 |
[32m[20221208 15:01:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.96
[32m[20221208 15:01:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.29
[32m[20221208 15:01:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.93
[32m[20221208 15:01:41 @agent_ppo2.py:137][0m Total time:      25.11 min
[32m[20221208 15:01:41 @agent_ppo2.py:139][0m 2023424 total steps have happened
[32m[20221208 15:01:41 @agent_ppo2.py:115][0m #------------------------ Iteration 988 --------------------------#
[32m[20221208 15:01:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |           0.0826 |           6.7165 |       -1747.6017 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |           0.0629 |           5.6853 |       -1405.2286 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |           0.0157 |           5.2313 |       -1574.6340 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0181 |           4.9585 |       -1713.1827 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0360 |           4.7501 |       -1821.8943 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0470 |           4.6116 |       -1829.7535 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0512 |           4.5121 |       -1896.3321 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0534 |           4.3480 |       -1935.5199 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0632 |           4.2309 |       -1988.7537 |
[32m[20221208 15:01:42 @agent_ppo2.py:179][0m |          -0.0652 |           4.1627 |       -2009.1715 |
[32m[20221208 15:01:42 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.49
[32m[20221208 15:01:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.03
[32m[20221208 15:01:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.02
[32m[20221208 15:01:43 @agent_ppo2.py:137][0m Total time:      25.14 min
[32m[20221208 15:01:43 @agent_ppo2.py:139][0m 2025472 total steps have happened
[32m[20221208 15:01:43 @agent_ppo2.py:115][0m #------------------------ Iteration 989 --------------------------#
[32m[20221208 15:01:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:43 @agent_ppo2.py:179][0m |           0.0872 |           7.7444 |       -1593.3000 |
[32m[20221208 15:01:43 @agent_ppo2.py:179][0m |           0.0720 |           7.0272 |        -988.9687 |
[32m[20221208 15:01:43 @agent_ppo2.py:179][0m |           0.0170 |           6.8446 |       -1242.7483 |
[32m[20221208 15:01:43 @agent_ppo2.py:179][0m |           0.0052 |           6.7721 |       -1348.2067 |
[32m[20221208 15:01:43 @agent_ppo2.py:179][0m |          -0.0215 |           6.6355 |       -1470.8177 |
[32m[20221208 15:01:43 @agent_ppo2.py:179][0m |          -0.0418 |           6.5171 |       -1543.9443 |
[32m[20221208 15:01:44 @agent_ppo2.py:179][0m |          -0.0504 |           6.4879 |       -1620.1892 |
[32m[20221208 15:01:44 @agent_ppo2.py:179][0m |          -0.0557 |           6.3903 |       -1657.8437 |
[32m[20221208 15:01:44 @agent_ppo2.py:179][0m |          -0.0611 |           6.3443 |       -1683.7665 |
[32m[20221208 15:01:44 @agent_ppo2.py:179][0m |          -0.0644 |           6.2780 |       -1736.4605 |
[32m[20221208 15:01:44 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:01:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.88
[32m[20221208 15:01:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.20
[32m[20221208 15:01:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.70
[32m[20221208 15:01:44 @agent_ppo2.py:137][0m Total time:      25.16 min
[32m[20221208 15:01:44 @agent_ppo2.py:139][0m 2027520 total steps have happened
[32m[20221208 15:01:44 @agent_ppo2.py:115][0m #------------------------ Iteration 990 --------------------------#
[32m[20221208 15:01:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |           0.0957 |           7.5340 |       -1727.3174 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |           0.0753 |           7.0517 |       -1417.2877 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |           0.0271 |           6.8452 |       -1696.8128 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |           0.0055 |           6.7215 |       -1747.8998 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |          -0.0092 |           6.5834 |       -1866.2504 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |          -0.0297 |           6.4806 |       -1966.8812 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |          -0.0420 |           6.4284 |       -2032.9241 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |          -0.0492 |           6.3244 |       -2113.8028 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |          -0.0521 |           6.2677 |       -2136.5598 |
[32m[20221208 15:01:45 @agent_ppo2.py:179][0m |          -0.0535 |           6.2551 |       -2169.6898 |
[32m[20221208 15:01:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:01:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.72
[32m[20221208 15:01:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.31
[32m[20221208 15:01:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 158.60
[32m[20221208 15:01:46 @agent_ppo2.py:137][0m Total time:      25.19 min
[32m[20221208 15:01:46 @agent_ppo2.py:139][0m 2029568 total steps have happened
[32m[20221208 15:01:46 @agent_ppo2.py:115][0m #------------------------ Iteration 991 --------------------------#
[32m[20221208 15:01:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:46 @agent_ppo2.py:179][0m |           0.1141 |           7.9653 |       -1665.7375 |
[32m[20221208 15:01:46 @agent_ppo2.py:179][0m |           0.1563 |           7.3112 |        -844.7388 |
[32m[20221208 15:01:46 @agent_ppo2.py:179][0m |           0.0976 |           6.9645 |        -945.8822 |
[32m[20221208 15:01:46 @agent_ppo2.py:179][0m |           0.0503 |           6.6821 |       -1221.7943 |
[32m[20221208 15:01:46 @agent_ppo2.py:179][0m |           0.0287 |           6.4336 |       -1412.5578 |
[32m[20221208 15:01:46 @agent_ppo2.py:179][0m |           0.0053 |           6.3212 |       -1499.8928 |
[32m[20221208 15:01:47 @agent_ppo2.py:179][0m |          -0.0156 |           6.1245 |       -1675.4829 |
[32m[20221208 15:01:47 @agent_ppo2.py:179][0m |          -0.0255 |           5.9884 |       -1757.0658 |
[32m[20221208 15:01:47 @agent_ppo2.py:179][0m |          -0.0147 |           5.9016 |       -1778.7989 |
[32m[20221208 15:01:47 @agent_ppo2.py:179][0m |          -0.0288 |           5.8086 |       -1814.7752 |
[32m[20221208 15:01:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:01:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.73
[32m[20221208 15:01:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.58
[32m[20221208 15:01:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.56
[32m[20221208 15:01:47 @agent_ppo2.py:137][0m Total time:      25.21 min
[32m[20221208 15:01:47 @agent_ppo2.py:139][0m 2031616 total steps have happened
[32m[20221208 15:01:47 @agent_ppo2.py:115][0m #------------------------ Iteration 992 --------------------------#
[32m[20221208 15:01:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |           0.0932 |           5.0414 |       -1288.4084 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |           0.0384 |           4.2358 |        -973.6506 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0114 |           4.1234 |       -1158.8607 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0361 |           3.9370 |       -1225.6348 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0512 |           3.8129 |       -1315.0318 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0562 |           3.7228 |       -1369.8193 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0699 |           3.7017 |       -1454.8888 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0739 |           3.6651 |       -1513.5688 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0721 |           3.6203 |       -1521.6836 |
[32m[20221208 15:01:48 @agent_ppo2.py:179][0m |          -0.0750 |           3.5502 |       -1545.9408 |
[32m[20221208 15:01:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:01:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.44
[32m[20221208 15:01:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.89
[32m[20221208 15:01:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 142.24
[32m[20221208 15:01:49 @agent_ppo2.py:137][0m Total time:      25.24 min
[32m[20221208 15:01:49 @agent_ppo2.py:139][0m 2033664 total steps have happened
[32m[20221208 15:01:49 @agent_ppo2.py:115][0m #------------------------ Iteration 993 --------------------------#
[32m[20221208 15:01:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:49 @agent_ppo2.py:179][0m |           0.1166 |           5.7368 |       -1484.2497 |
[32m[20221208 15:01:49 @agent_ppo2.py:179][0m |           0.0568 |           4.4635 |        -832.5004 |
[32m[20221208 15:01:49 @agent_ppo2.py:179][0m |           0.0008 |           3.9789 |       -1046.5506 |
[32m[20221208 15:01:49 @agent_ppo2.py:179][0m |          -0.0281 |           3.7222 |       -1198.6625 |
[32m[20221208 15:01:49 @agent_ppo2.py:179][0m |          -0.0435 |           3.5652 |       -1274.3367 |
[32m[20221208 15:01:50 @agent_ppo2.py:179][0m |          -0.0532 |           3.4297 |       -1336.3576 |
[32m[20221208 15:01:50 @agent_ppo2.py:179][0m |          -0.0610 |           3.3662 |       -1390.7841 |
[32m[20221208 15:01:50 @agent_ppo2.py:179][0m |          -0.0665 |           3.2833 |       -1423.6071 |
[32m[20221208 15:01:50 @agent_ppo2.py:179][0m |          -0.0685 |           3.2293 |       -1464.3055 |
[32m[20221208 15:01:50 @agent_ppo2.py:179][0m |          -0.0739 |           3.1988 |       -1496.6329 |
[32m[20221208 15:01:50 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.78
[32m[20221208 15:01:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.20
[32m[20221208 15:01:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.07
[32m[20221208 15:01:50 @agent_ppo2.py:137][0m Total time:      25.27 min
[32m[20221208 15:01:50 @agent_ppo2.py:139][0m 2035712 total steps have happened
[32m[20221208 15:01:50 @agent_ppo2.py:115][0m #------------------------ Iteration 994 --------------------------#
[32m[20221208 15:01:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.1383 |           7.3326 |       -1763.7974 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.1341 |           6.2878 |        -814.2153 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0975 |           6.0829 |        -626.6013 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0678 |           5.8586 |        -697.0807 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0599 |           5.7434 |        -828.9096 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0379 |           5.6263 |        -996.0608 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0249 |           5.6139 |       -1147.9443 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0200 |           5.4953 |       -1225.3318 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |           0.0073 |           5.4252 |       -1333.2059 |
[32m[20221208 15:01:51 @agent_ppo2.py:179][0m |          -0.0011 |           5.3754 |       -1390.2218 |
[32m[20221208 15:01:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.11
[32m[20221208 15:01:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.18
[32m[20221208 15:01:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.42
[32m[20221208 15:01:52 @agent_ppo2.py:137][0m Total time:      25.29 min
[32m[20221208 15:01:52 @agent_ppo2.py:139][0m 2037760 total steps have happened
[32m[20221208 15:01:52 @agent_ppo2.py:115][0m #------------------------ Iteration 995 --------------------------#
[32m[20221208 15:01:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:52 @agent_ppo2.py:179][0m |           0.1344 |           8.6747 |       -1682.2959 |
[32m[20221208 15:01:52 @agent_ppo2.py:179][0m |           0.1439 |           7.2569 |       -1200.8616 |
[32m[20221208 15:01:52 @agent_ppo2.py:179][0m |           0.0573 |           6.7486 |       -1344.6520 |
[32m[20221208 15:01:52 @agent_ppo2.py:179][0m |           0.0183 |           6.4294 |       -1619.0330 |
[32m[20221208 15:01:53 @agent_ppo2.py:179][0m |          -0.0029 |           6.2283 |       -1694.1083 |
[32m[20221208 15:01:53 @agent_ppo2.py:179][0m |          -0.0113 |           5.9926 |       -1781.4729 |
[32m[20221208 15:01:53 @agent_ppo2.py:179][0m |          -0.0304 |           5.8151 |       -1875.3423 |
[32m[20221208 15:01:53 @agent_ppo2.py:179][0m |          -0.0417 |           5.6263 |       -1933.2419 |
[32m[20221208 15:01:53 @agent_ppo2.py:179][0m |          -0.0462 |           5.5628 |       -1959.9786 |
[32m[20221208 15:01:53 @agent_ppo2.py:179][0m |          -0.0587 |           5.4291 |       -2068.4888 |
[32m[20221208 15:01:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.98
[32m[20221208 15:01:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.43
[32m[20221208 15:01:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.29
[32m[20221208 15:01:53 @agent_ppo2.py:137][0m Total time:      25.32 min
[32m[20221208 15:01:53 @agent_ppo2.py:139][0m 2039808 total steps have happened
[32m[20221208 15:01:53 @agent_ppo2.py:115][0m #------------------------ Iteration 996 --------------------------#
[32m[20221208 15:01:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |           0.1331 |           8.7744 |       -1725.0958 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |           0.0610 |           7.9560 |       -1516.0551 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |           0.0028 |           7.5162 |       -1741.3258 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0138 |           7.3021 |       -1761.6071 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0263 |           7.1505 |       -1801.6169 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0464 |           7.0387 |       -1907.9880 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0525 |           6.9010 |       -1930.8541 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0531 |           6.8302 |       -1927.5954 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0630 |           6.7062 |       -1959.7233 |
[32m[20221208 15:01:54 @agent_ppo2.py:179][0m |          -0.0680 |           6.7227 |       -2006.9030 |
[32m[20221208 15:01:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:01:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.22
[32m[20221208 15:01:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 198.10
[32m[20221208 15:01:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.28
[32m[20221208 15:01:55 @agent_ppo2.py:137][0m Total time:      25.34 min
[32m[20221208 15:01:55 @agent_ppo2.py:139][0m 2041856 total steps have happened
[32m[20221208 15:01:55 @agent_ppo2.py:115][0m #------------------------ Iteration 997 --------------------------#
[32m[20221208 15:01:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:01:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:55 @agent_ppo2.py:179][0m |           0.0837 |           8.2078 |       -1776.2519 |
[32m[20221208 15:01:55 @agent_ppo2.py:179][0m |           0.0442 |           6.8419 |       -1623.1278 |
[32m[20221208 15:01:55 @agent_ppo2.py:179][0m |          -0.0107 |           6.3740 |       -1737.7981 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0209 |           6.0136 |       -1797.6483 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0372 |           5.8205 |       -1880.5193 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0511 |           5.6003 |       -1914.1052 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0613 |           5.5126 |       -1956.8079 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0671 |           5.3091 |       -1991.1708 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0720 |           5.2406 |       -2032.3731 |
[32m[20221208 15:01:56 @agent_ppo2.py:179][0m |          -0.0663 |           5.0793 |       -2030.5383 |
[32m[20221208 15:01:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:01:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 157.55
[32m[20221208 15:01:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.36
[32m[20221208 15:01:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.51
[32m[20221208 15:01:56 @agent_ppo2.py:137][0m Total time:      25.37 min
[32m[20221208 15:01:56 @agent_ppo2.py:139][0m 2043904 total steps have happened
[32m[20221208 15:01:56 @agent_ppo2.py:115][0m #------------------------ Iteration 998 --------------------------#
[32m[20221208 15:01:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |           0.1220 |           8.5600 |       -1781.8612 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |           0.1680 |           7.5111 |       -1085.2104 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |           0.0644 |           7.0710 |       -1199.0618 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |           0.0235 |           6.7397 |       -1440.5846 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |           0.0001 |           6.4999 |       -1571.2686 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |          -0.0238 |           6.3855 |       -1704.3006 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |          -0.0379 |           6.2593 |       -1763.3525 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |          -0.0499 |           6.2060 |       -1831.8562 |
[32m[20221208 15:01:57 @agent_ppo2.py:179][0m |          -0.0518 |           6.0240 |       -1856.7651 |
[32m[20221208 15:01:58 @agent_ppo2.py:179][0m |          -0.0612 |           6.0218 |       -1911.5344 |
[32m[20221208 15:01:58 @agent_ppo2.py:124][0m Policy update time: 0.88 s
[32m[20221208 15:01:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.35
[32m[20221208 15:01:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.18
[32m[20221208 15:01:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.49
[32m[20221208 15:01:58 @agent_ppo2.py:137][0m Total time:      25.40 min
[32m[20221208 15:01:58 @agent_ppo2.py:139][0m 2045952 total steps have happened
[32m[20221208 15:01:58 @agent_ppo2.py:115][0m #------------------------ Iteration 999 --------------------------#
[32m[20221208 15:01:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:01:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |           0.1716 |           7.7921 |       -1581.0372 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |           0.1090 |           6.7243 |       -1234.4435 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |           0.0391 |           6.2327 |       -1376.8266 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |           0.0036 |           5.9386 |       -1495.3201 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |          -0.0280 |           5.7081 |       -1593.9617 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |          -0.0411 |           5.5116 |       -1688.5858 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |          -0.0494 |           5.4204 |       -1766.0539 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |          -0.0549 |           5.2677 |       -1753.6146 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |          -0.0628 |           5.1734 |       -1812.4735 |
[32m[20221208 15:01:59 @agent_ppo2.py:179][0m |          -0.0683 |           5.0765 |       -1855.1899 |
[32m[20221208 15:01:59 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.09
[32m[20221208 15:02:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.51
[32m[20221208 15:02:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.92
[32m[20221208 15:02:00 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 209.75
[32m[20221208 15:02:00 @agent_ppo2.py:137][0m Total time:      25.42 min
[32m[20221208 15:02:00 @agent_ppo2.py:139][0m 2048000 total steps have happened
[32m[20221208 15:02:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221208 15:02:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:00 @agent_ppo2.py:179][0m |           0.1577 |           8.1447 |       -1281.6645 |
[32m[20221208 15:02:00 @agent_ppo2.py:179][0m |           0.1343 |           7.2724 |        -815.2842 |
[32m[20221208 15:02:00 @agent_ppo2.py:179][0m |           0.0613 |           6.8127 |       -1046.8774 |
[32m[20221208 15:02:00 @agent_ppo2.py:179][0m |           0.0332 |           6.5201 |       -1188.1830 |
[32m[20221208 15:02:00 @agent_ppo2.py:179][0m |           0.0040 |           6.3318 |       -1258.3003 |
[32m[20221208 15:02:00 @agent_ppo2.py:179][0m |          -0.0237 |           6.1584 |       -1420.4937 |
[32m[20221208 15:02:01 @agent_ppo2.py:179][0m |          -0.0373 |           5.9776 |       -1494.3381 |
[32m[20221208 15:02:01 @agent_ppo2.py:179][0m |          -0.0497 |           5.8615 |       -1540.1640 |
[32m[20221208 15:02:01 @agent_ppo2.py:179][0m |          -0.0555 |           5.6980 |       -1598.1198 |
[32m[20221208 15:02:01 @agent_ppo2.py:179][0m |          -0.0631 |           5.6246 |       -1648.5037 |
[32m[20221208 15:02:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.90
[32m[20221208 15:02:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.85
[32m[20221208 15:02:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.55
[32m[20221208 15:02:01 @agent_ppo2.py:137][0m Total time:      25.45 min
[32m[20221208 15:02:01 @agent_ppo2.py:139][0m 2050048 total steps have happened
[32m[20221208 15:02:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221208 15:02:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |           0.1057 |           7.1103 |       -1753.4580 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |           0.1049 |           6.1142 |       -1263.0413 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |           0.0305 |           5.8297 |       -1478.1728 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0029 |           5.6312 |       -1621.1406 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0157 |           5.4618 |       -1737.8141 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0312 |           5.4171 |       -1784.0866 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0456 |           5.2713 |       -1828.9281 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0555 |           5.2168 |       -1899.3811 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0630 |           5.1484 |       -1925.6939 |
[32m[20221208 15:02:02 @agent_ppo2.py:179][0m |          -0.0672 |           5.0873 |       -1964.0658 |
[32m[20221208 15:02:02 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.87
[32m[20221208 15:02:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.43
[32m[20221208 15:02:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 146.25
[32m[20221208 15:02:03 @agent_ppo2.py:137][0m Total time:      25.47 min
[32m[20221208 15:02:03 @agent_ppo2.py:139][0m 2052096 total steps have happened
[32m[20221208 15:02:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221208 15:02:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:03 @agent_ppo2.py:179][0m |           0.1164 |           8.1346 |       -1749.4293 |
[32m[20221208 15:02:03 @agent_ppo2.py:179][0m |           0.1350 |           7.1773 |       -1233.1369 |
[32m[20221208 15:02:03 @agent_ppo2.py:179][0m |           0.0679 |           6.8286 |       -1198.7363 |
[32m[20221208 15:02:03 @agent_ppo2.py:179][0m |           0.0314 |           6.5369 |       -1313.1585 |
[32m[20221208 15:02:03 @agent_ppo2.py:179][0m |          -0.0004 |           6.3419 |       -1476.6766 |
[32m[20221208 15:02:04 @agent_ppo2.py:179][0m |          -0.0169 |           6.2415 |       -1564.7231 |
[32m[20221208 15:02:04 @agent_ppo2.py:179][0m |          -0.0256 |           6.0942 |       -1620.3645 |
[32m[20221208 15:02:04 @agent_ppo2.py:179][0m |          -0.0378 |           5.9732 |       -1679.0807 |
[32m[20221208 15:02:04 @agent_ppo2.py:179][0m |          -0.0452 |           5.8692 |       -1717.7047 |
[32m[20221208 15:02:04 @agent_ppo2.py:179][0m |          -0.0488 |           5.7647 |       -1741.1794 |
[32m[20221208 15:02:04 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:02:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.21
[32m[20221208 15:02:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.77
[32m[20221208 15:02:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.59
[32m[20221208 15:02:04 @agent_ppo2.py:137][0m Total time:      25.50 min
[32m[20221208 15:02:04 @agent_ppo2.py:139][0m 2054144 total steps have happened
[32m[20221208 15:02:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221208 15:02:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |           0.0911 |           7.0027 |       -1691.5769 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |           0.1661 |           5.3917 |       -1041.0036 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |           0.0983 |           4.8687 |        -802.9534 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |           0.0313 |           4.5351 |       -1137.5964 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |          -0.0051 |           4.2533 |       -1262.9666 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |          -0.0237 |           4.0141 |       -1359.0661 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |          -0.0404 |           3.8550 |       -1440.5792 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |          -0.0530 |           3.7151 |       -1521.3846 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |          -0.0564 |           3.5896 |       -1564.3593 |
[32m[20221208 15:02:05 @agent_ppo2.py:179][0m |          -0.0626 |           3.4774 |       -1592.5047 |
[32m[20221208 15:02:05 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.72
[32m[20221208 15:02:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.75
[32m[20221208 15:02:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.52
[32m[20221208 15:02:06 @agent_ppo2.py:137][0m Total time:      25.52 min
[32m[20221208 15:02:06 @agent_ppo2.py:139][0m 2056192 total steps have happened
[32m[20221208 15:02:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221208 15:02:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:06 @agent_ppo2.py:179][0m |           0.0901 |           7.2946 |       -1557.1358 |
[32m[20221208 15:02:06 @agent_ppo2.py:179][0m |           0.0167 |           5.9525 |       -1258.9454 |
[32m[20221208 15:02:06 @agent_ppo2.py:179][0m |          -0.0214 |           5.5241 |       -1296.2158 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0520 |           5.2187 |       -1358.2530 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0641 |           4.9128 |       -1389.8067 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0703 |           4.8088 |       -1425.0513 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0781 |           4.6431 |       -1421.1029 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0771 |           4.5676 |       -1462.9507 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0863 |           4.4708 |       -1496.9548 |
[32m[20221208 15:02:07 @agent_ppo2.py:179][0m |          -0.0839 |           4.3604 |       -1497.2400 |
[32m[20221208 15:02:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.41
[32m[20221208 15:02:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.70
[32m[20221208 15:02:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.31
[32m[20221208 15:02:07 @agent_ppo2.py:137][0m Total time:      25.55 min
[32m[20221208 15:02:07 @agent_ppo2.py:139][0m 2058240 total steps have happened
[32m[20221208 15:02:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221208 15:02:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |           0.1753 |           8.3625 |       -1481.6928 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |           0.1451 |           7.5038 |       -1038.6362 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |           0.0922 |           6.9998 |        -990.7359 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |           0.0243 |           6.7124 |       -1270.7906 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |          -0.0038 |           6.4871 |       -1435.8332 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |          -0.0214 |           6.3120 |       -1527.3926 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |          -0.0349 |           6.2032 |       -1580.7647 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |          -0.0416 |           6.0786 |       -1631.1155 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |          -0.0522 |           5.9285 |       -1704.7484 |
[32m[20221208 15:02:08 @agent_ppo2.py:179][0m |          -0.0627 |           5.8322 |       -1773.2601 |
[32m[20221208 15:02:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.71
[32m[20221208 15:02:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.66
[32m[20221208 15:02:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.17
[32m[20221208 15:02:09 @agent_ppo2.py:137][0m Total time:      25.58 min
[32m[20221208 15:02:09 @agent_ppo2.py:139][0m 2060288 total steps have happened
[32m[20221208 15:02:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221208 15:02:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:09 @agent_ppo2.py:179][0m |           0.0680 |           6.4674 |       -1733.0262 |
[32m[20221208 15:02:09 @agent_ppo2.py:179][0m |           0.0342 |           5.2237 |       -1583.6349 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0146 |           4.7266 |       -1634.7074 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0439 |           4.4137 |       -1746.4818 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0523 |           4.2497 |       -1773.4150 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0618 |           4.0243 |       -1813.8371 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0702 |           3.8897 |       -1841.7657 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0752 |           3.8016 |       -1862.5814 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0806 |           3.6746 |       -1894.5715 |
[32m[20221208 15:02:10 @agent_ppo2.py:179][0m |          -0.0845 |           3.5805 |       -1917.8862 |
[32m[20221208 15:02:10 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.17
[32m[20221208 15:02:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.05
[32m[20221208 15:02:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.48
[32m[20221208 15:02:10 @agent_ppo2.py:137][0m Total time:      25.60 min
[32m[20221208 15:02:10 @agent_ppo2.py:139][0m 2062336 total steps have happened
[32m[20221208 15:02:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221208 15:02:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:02:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |           0.1287 |           7.0621 |       -1440.9027 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |           0.0227 |           6.2334 |       -1287.5908 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |           0.0059 |           5.8217 |       -1363.5245 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |          -0.0236 |           5.5422 |       -1437.8996 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |          -0.0362 |           5.3596 |       -1492.8999 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |          -0.0499 |           5.2182 |       -1535.0869 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |          -0.0604 |           5.0823 |       -1562.2945 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |          -0.0655 |           4.9731 |       -1613.5870 |
[32m[20221208 15:02:11 @agent_ppo2.py:179][0m |          -0.0684 |           4.8858 |       -1628.0609 |
[32m[20221208 15:02:12 @agent_ppo2.py:179][0m |          -0.0720 |           4.7909 |       -1649.3128 |
[32m[20221208 15:02:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.91
[32m[20221208 15:02:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.79
[32m[20221208 15:02:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.37
[32m[20221208 15:02:12 @agent_ppo2.py:137][0m Total time:      25.63 min
[32m[20221208 15:02:12 @agent_ppo2.py:139][0m 2064384 total steps have happened
[32m[20221208 15:02:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221208 15:02:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:12 @agent_ppo2.py:179][0m |           0.1946 |           7.4007 |       -1566.2680 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |           0.1208 |           6.0136 |       -1054.3242 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |           0.0355 |           5.5031 |       -1476.6945 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0034 |           5.2341 |       -1660.8779 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0196 |           5.0086 |       -1668.8680 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0442 |           4.8646 |       -1733.3464 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0569 |           4.7194 |       -1800.5724 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0657 |           4.5409 |       -1878.6069 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0732 |           4.4714 |       -1889.6594 |
[32m[20221208 15:02:13 @agent_ppo2.py:179][0m |          -0.0768 |           4.3553 |       -1923.1605 |
[32m[20221208 15:02:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.72
[32m[20221208 15:02:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.26
[32m[20221208 15:02:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.66
[32m[20221208 15:02:13 @agent_ppo2.py:137][0m Total time:      25.65 min
[32m[20221208 15:02:13 @agent_ppo2.py:139][0m 2066432 total steps have happened
[32m[20221208 15:02:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221208 15:02:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |           0.1361 |           6.4262 |       -1831.5104 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |           0.1327 |           5.4747 |       -1057.0964 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |           0.0570 |           5.0379 |       -1043.0924 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |           0.0029 |           4.7308 |       -1266.2552 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |          -0.0271 |           4.4770 |       -1404.2760 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |          -0.0406 |           4.2902 |       -1479.0445 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |          -0.0534 |           4.1518 |       -1520.5104 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |          -0.0624 |           4.0107 |       -1591.1381 |
[32m[20221208 15:02:14 @agent_ppo2.py:179][0m |          -0.0718 |           3.9116 |       -1637.2056 |
[32m[20221208 15:02:15 @agent_ppo2.py:179][0m |          -0.0779 |           3.7918 |       -1671.8564 |
[32m[20221208 15:02:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 142.15
[32m[20221208 15:02:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.73
[32m[20221208 15:02:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 126.09
[32m[20221208 15:02:15 @agent_ppo2.py:137][0m Total time:      25.68 min
[32m[20221208 15:02:15 @agent_ppo2.py:139][0m 2068480 total steps have happened
[32m[20221208 15:02:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221208 15:02:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |           0.1239 |           8.7942 |       -1703.0838 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |           0.0947 |           8.0256 |       -1397.9810 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |           0.0605 |           7.7618 |       -1327.9843 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |           0.0119 |           7.5242 |       -1650.4686 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |          -0.0130 |           7.3241 |       -1723.7611 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |          -0.0250 |           7.2265 |       -1829.4899 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |          -0.0366 |           7.0192 |       -1897.1588 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |          -0.0451 |           6.9823 |       -1969.8324 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |          -0.0562 |           6.9264 |       -2033.1036 |
[32m[20221208 15:02:16 @agent_ppo2.py:179][0m |          -0.0615 |           6.7720 |       -2075.2501 |
[32m[20221208 15:02:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.56
[32m[20221208 15:02:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 171.62
[32m[20221208 15:02:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.01
[32m[20221208 15:02:16 @agent_ppo2.py:137][0m Total time:      25.70 min
[32m[20221208 15:02:16 @agent_ppo2.py:139][0m 2070528 total steps have happened
[32m[20221208 15:02:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221208 15:02:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |           0.0728 |           6.6491 |       -1754.0985 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |           0.0386 |           5.5121 |       -1560.4804 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |           0.0093 |           5.0810 |       -1665.2296 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |          -0.0206 |           4.8685 |       -1638.4561 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |          -0.0464 |           4.6320 |       -1793.2742 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |          -0.0601 |           4.4489 |       -1828.2727 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |          -0.0681 |           4.3058 |       -1877.9800 |
[32m[20221208 15:02:17 @agent_ppo2.py:179][0m |          -0.0738 |           4.2727 |       -1926.8269 |
[32m[20221208 15:02:18 @agent_ppo2.py:179][0m |          -0.0733 |           4.1379 |       -1926.2010 |
[32m[20221208 15:02:18 @agent_ppo2.py:179][0m |          -0.0790 |           4.0386 |       -1953.6047 |
[32m[20221208 15:02:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.70
[32m[20221208 15:02:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.13
[32m[20221208 15:02:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.66
[32m[20221208 15:02:18 @agent_ppo2.py:137][0m Total time:      25.73 min
[32m[20221208 15:02:18 @agent_ppo2.py:139][0m 2072576 total steps have happened
[32m[20221208 15:02:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221208 15:02:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |           0.0924 |           7.1707 |       -1512.9190 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |           0.1256 |           6.3846 |       -1091.8979 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |           0.0011 |           6.0386 |       -1339.4789 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0245 |           5.7602 |       -1504.1281 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0484 |           5.5659 |       -1592.3825 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0577 |           5.4589 |       -1639.5228 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0676 |           5.3292 |       -1719.3792 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0753 |           5.2487 |       -1728.8610 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0777 |           5.2188 |       -1768.9227 |
[32m[20221208 15:02:19 @agent_ppo2.py:179][0m |          -0.0855 |           5.0787 |       -1813.8825 |
[32m[20221208 15:02:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:02:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.77
[32m[20221208 15:02:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.57
[32m[20221208 15:02:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 137.83
[32m[20221208 15:02:19 @agent_ppo2.py:137][0m Total time:      25.75 min
[32m[20221208 15:02:19 @agent_ppo2.py:139][0m 2074624 total steps have happened
[32m[20221208 15:02:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221208 15:02:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |           0.0928 |           8.5388 |       -1963.7630 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |           0.0589 |           6.9200 |       -1712.5230 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |          -0.0082 |           6.3542 |       -1920.9345 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |          -0.0358 |           6.0274 |       -2049.8389 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |          -0.0513 |           5.7482 |       -2116.6320 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |          -0.0580 |           5.5712 |       -2155.9875 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |          -0.0664 |           5.3619 |       -2183.3716 |
[32m[20221208 15:02:20 @agent_ppo2.py:179][0m |          -0.0734 |           5.2559 |       -2258.7282 |
[32m[20221208 15:02:21 @agent_ppo2.py:179][0m |          -0.0767 |           5.1277 |       -2299.2990 |
[32m[20221208 15:02:21 @agent_ppo2.py:179][0m |          -0.0711 |           4.9987 |       -2279.0559 |
[32m[20221208 15:02:21 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.07
[32m[20221208 15:02:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.98
[32m[20221208 15:02:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 123.93
[32m[20221208 15:02:21 @agent_ppo2.py:137][0m Total time:      25.78 min
[32m[20221208 15:02:21 @agent_ppo2.py:139][0m 2076672 total steps have happened
[32m[20221208 15:02:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221208 15:02:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |           0.1067 |           6.0911 |       -1661.9117 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |           0.0538 |           5.2346 |       -1261.5593 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |           0.0114 |           4.9722 |       -1498.5727 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0164 |           4.7713 |       -1585.9542 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0393 |           4.6397 |       -1716.0931 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0393 |           4.5325 |       -1776.5775 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0518 |           4.4577 |       -1801.9083 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0595 |           4.3470 |       -1808.3503 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0640 |           4.2766 |       -1873.0359 |
[32m[20221208 15:02:22 @agent_ppo2.py:179][0m |          -0.0644 |           4.2191 |       -1887.8657 |
[32m[20221208 15:02:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:02:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.06
[32m[20221208 15:02:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.00
[32m[20221208 15:02:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.98
[32m[20221208 15:02:22 @agent_ppo2.py:137][0m Total time:      25.80 min
[32m[20221208 15:02:22 @agent_ppo2.py:139][0m 2078720 total steps have happened
[32m[20221208 15:02:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221208 15:02:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |           0.0982 |           7.9971 |       -1855.7035 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |           0.1120 |           7.1347 |       -1324.7710 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |           0.0506 |           6.8431 |       -1261.1205 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |           0.0099 |           6.6142 |       -1594.2888 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |          -0.0169 |           6.4741 |       -1760.2336 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |          -0.0301 |           6.3409 |       -1835.5168 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |          -0.0355 |           6.2239 |       -1894.6414 |
[32m[20221208 15:02:23 @agent_ppo2.py:179][0m |          -0.0422 |           6.1461 |       -1952.9366 |
[32m[20221208 15:02:24 @agent_ppo2.py:179][0m |          -0.0517 |           6.0752 |       -2004.8978 |
[32m[20221208 15:02:24 @agent_ppo2.py:179][0m |          -0.0597 |           6.0350 |       -2085.7284 |
[32m[20221208 15:02:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.19
[32m[20221208 15:02:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.64
[32m[20221208 15:02:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.07
[32m[20221208 15:02:24 @agent_ppo2.py:137][0m Total time:      25.83 min
[32m[20221208 15:02:24 @agent_ppo2.py:139][0m 2080768 total steps have happened
[32m[20221208 15:02:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221208 15:02:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |           0.0740 |           7.5462 |       -1960.6713 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |           0.0356 |           6.8996 |       -1455.6098 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0019 |           6.6479 |       -1601.0224 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0302 |           6.4454 |       -1743.9588 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0456 |           6.3532 |       -1826.2208 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0551 |           6.2187 |       -1881.4317 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0666 |           6.1560 |       -1941.8591 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0734 |           6.0713 |       -2009.6127 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0733 |           6.0353 |       -2010.8400 |
[32m[20221208 15:02:25 @agent_ppo2.py:179][0m |          -0.0771 |           6.0067 |       -2072.0963 |
[32m[20221208 15:02:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.91
[32m[20221208 15:02:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.76
[32m[20221208 15:02:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.90
[32m[20221208 15:02:26 @agent_ppo2.py:137][0m Total time:      25.85 min
[32m[20221208 15:02:26 @agent_ppo2.py:139][0m 2082816 total steps have happened
[32m[20221208 15:02:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221208 15:02:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |           0.0919 |           5.5207 |       -2110.5645 |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |           0.0911 |           4.6811 |       -1498.2588 |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |           0.0392 |           4.4206 |       -1681.0738 |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |           0.0072 |           4.2646 |       -1741.2872 |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |          -0.0251 |           4.1438 |       -1912.7345 |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |          -0.0444 |           4.0414 |       -2011.0759 |
[32m[20221208 15:02:26 @agent_ppo2.py:179][0m |          -0.0586 |           3.9351 |       -2061.2396 |
[32m[20221208 15:02:27 @agent_ppo2.py:179][0m |          -0.0662 |           3.8598 |       -2093.6250 |
[32m[20221208 15:02:27 @agent_ppo2.py:179][0m |          -0.0680 |           3.7757 |       -2139.6650 |
[32m[20221208 15:02:27 @agent_ppo2.py:179][0m |          -0.0703 |           3.7146 |       -2175.4161 |
[32m[20221208 15:02:27 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.85
[32m[20221208 15:02:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.53
[32m[20221208 15:02:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.29
[32m[20221208 15:02:27 @agent_ppo2.py:137][0m Total time:      25.88 min
[32m[20221208 15:02:27 @agent_ppo2.py:139][0m 2084864 total steps have happened
[32m[20221208 15:02:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221208 15:02:28 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:02:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |           0.1077 |           5.0012 |       -1757.6163 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |           0.0833 |           4.1492 |        -978.8605 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |           0.0364 |           3.8266 |       -1161.1566 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |           0.0008 |           3.6591 |       -1321.8689 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |          -0.0191 |           3.5100 |       -1402.8521 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |          -0.0353 |           3.3872 |       -1496.3092 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |          -0.0420 |           3.3248 |       -1532.8827 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |          -0.0512 |           3.2653 |       -1620.0674 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |          -0.0574 |           3.1587 |       -1653.0019 |
[32m[20221208 15:02:28 @agent_ppo2.py:179][0m |          -0.0620 |           3.1194 |       -1723.1242 |
[32m[20221208 15:02:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.64
[32m[20221208 15:02:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.81
[32m[20221208 15:02:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.47
[32m[20221208 15:02:29 @agent_ppo2.py:137][0m Total time:      25.91 min
[32m[20221208 15:02:29 @agent_ppo2.py:139][0m 2086912 total steps have happened
[32m[20221208 15:02:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221208 15:02:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:29 @agent_ppo2.py:179][0m |           0.0854 |           6.1989 |       -1461.0622 |
[32m[20221208 15:02:29 @agent_ppo2.py:179][0m |           0.0736 |           5.5039 |       -1106.9935 |
[32m[20221208 15:02:29 @agent_ppo2.py:179][0m |           0.0171 |           5.3070 |       -1200.9654 |
[32m[20221208 15:02:29 @agent_ppo2.py:179][0m |           0.0074 |           5.2061 |       -1226.3163 |
[32m[20221208 15:02:29 @agent_ppo2.py:179][0m |          -0.0202 |           5.0876 |       -1349.8577 |
[32m[20221208 15:02:29 @agent_ppo2.py:179][0m |          -0.0301 |           5.0059 |       -1422.3224 |
[32m[20221208 15:02:30 @agent_ppo2.py:179][0m |          -0.0389 |           4.9563 |       -1467.4252 |
[32m[20221208 15:02:30 @agent_ppo2.py:179][0m |          -0.0458 |           4.9145 |       -1508.9658 |
[32m[20221208 15:02:30 @agent_ppo2.py:179][0m |          -0.0417 |           4.8918 |       -1472.3427 |
[32m[20221208 15:02:30 @agent_ppo2.py:179][0m |          -0.0473 |           4.8170 |       -1556.0434 |
[32m[20221208 15:02:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:02:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.37
[32m[20221208 15:02:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.61
[32m[20221208 15:02:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 140.29
[32m[20221208 15:02:30 @agent_ppo2.py:137][0m Total time:      25.93 min
[32m[20221208 15:02:30 @agent_ppo2.py:139][0m 2088960 total steps have happened
[32m[20221208 15:02:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221208 15:02:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |           0.1408 |           7.2614 |       -2224.2230 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |           0.1564 |           6.2747 |       -1398.9047 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |           0.0798 |           5.8696 |       -1315.8182 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |           0.0235 |           5.6416 |       -1592.6027 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |          -0.0045 |           5.3338 |       -1853.0323 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |          -0.0261 |           5.1731 |       -1975.5197 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |          -0.0252 |           5.0780 |       -2010.1153 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |          -0.0461 |           4.9295 |       -2103.8751 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |          -0.0556 |           4.7812 |       -2231.8667 |
[32m[20221208 15:02:31 @agent_ppo2.py:179][0m |          -0.0623 |           4.7289 |       -2267.1494 |
[32m[20221208 15:02:31 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:02:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.26
[32m[20221208 15:02:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.21
[32m[20221208 15:02:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.80
[32m[20221208 15:02:32 @agent_ppo2.py:137][0m Total time:      25.96 min
[32m[20221208 15:02:32 @agent_ppo2.py:139][0m 2091008 total steps have happened
[32m[20221208 15:02:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221208 15:02:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:32 @agent_ppo2.py:179][0m |           0.1010 |           9.8132 |       -2258.5291 |
[32m[20221208 15:02:32 @agent_ppo2.py:179][0m |           0.0750 |           8.5326 |       -1830.6823 |
[32m[20221208 15:02:32 @agent_ppo2.py:179][0m |           0.0065 |           7.9128 |       -2108.4054 |
[32m[20221208 15:02:32 @agent_ppo2.py:179][0m |          -0.0103 |           7.5659 |       -2206.8673 |
[32m[20221208 15:02:32 @agent_ppo2.py:179][0m |          -0.0246 |           7.2507 |       -2256.8302 |
[32m[20221208 15:02:32 @agent_ppo2.py:179][0m |          -0.0350 |           7.0542 |       -2380.0629 |
[32m[20221208 15:02:33 @agent_ppo2.py:179][0m |          -0.0481 |           6.8833 |       -2415.8057 |
[32m[20221208 15:02:33 @agent_ppo2.py:179][0m |          -0.0621 |           6.7754 |       -2480.6639 |
[32m[20221208 15:02:33 @agent_ppo2.py:179][0m |          -0.0655 |           6.6746 |       -2543.9004 |
[32m[20221208 15:02:33 @agent_ppo2.py:179][0m |          -0.0656 |           6.5693 |       -2543.4054 |
[32m[20221208 15:02:33 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.57
[32m[20221208 15:02:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 177.42
[32m[20221208 15:02:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.06
[32m[20221208 15:02:33 @agent_ppo2.py:137][0m Total time:      25.98 min
[32m[20221208 15:02:33 @agent_ppo2.py:139][0m 2093056 total steps have happened
[32m[20221208 15:02:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221208 15:02:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |           0.1388 |           9.1133 |       -2096.2623 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |           0.1222 |           7.8029 |       -1421.9764 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |           0.0321 |           7.2822 |       -1939.1494 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0040 |           6.9955 |       -2097.3380 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0166 |           6.8135 |       -2211.3489 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0379 |           6.6126 |       -2315.6121 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0438 |           6.4367 |       -2340.9357 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0521 |           6.3108 |       -2432.1734 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0630 |           6.1890 |       -2462.9786 |
[32m[20221208 15:02:34 @agent_ppo2.py:179][0m |          -0.0694 |           6.0281 |       -2545.5590 |
[32m[20221208 15:02:34 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:02:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 145.36
[32m[20221208 15:02:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.62
[32m[20221208 15:02:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.72
[32m[20221208 15:02:35 @agent_ppo2.py:137][0m Total time:      26.01 min
[32m[20221208 15:02:35 @agent_ppo2.py:139][0m 2095104 total steps have happened
[32m[20221208 15:02:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221208 15:02:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:35 @agent_ppo2.py:179][0m |           0.1240 |           8.8957 |       -2195.8338 |
[32m[20221208 15:02:35 @agent_ppo2.py:179][0m |           0.0849 |           7.4799 |       -1782.0111 |
[32m[20221208 15:02:35 @agent_ppo2.py:179][0m |           0.0459 |           7.0217 |       -2003.5279 |
[32m[20221208 15:02:35 @agent_ppo2.py:179][0m |           0.0015 |           6.6244 |       -2262.0681 |
[32m[20221208 15:02:35 @agent_ppo2.py:179][0m |          -0.0161 |           6.3663 |       -2256.1563 |
[32m[20221208 15:02:36 @agent_ppo2.py:179][0m |          -0.0381 |           6.1387 |       -2406.4841 |
[32m[20221208 15:02:36 @agent_ppo2.py:179][0m |          -0.0496 |           5.9772 |       -2514.1361 |
[32m[20221208 15:02:36 @agent_ppo2.py:179][0m |          -0.0559 |           5.8259 |       -2534.3835 |
[32m[20221208 15:02:36 @agent_ppo2.py:179][0m |          -0.0657 |           5.6831 |       -2610.7584 |
[32m[20221208 15:02:36 @agent_ppo2.py:179][0m |          -0.0692 |           5.5674 |       -2657.9035 |
[32m[20221208 15:02:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.40
[32m[20221208 15:02:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.06
[32m[20221208 15:02:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.81
[32m[20221208 15:02:36 @agent_ppo2.py:137][0m Total time:      26.03 min
[32m[20221208 15:02:36 @agent_ppo2.py:139][0m 2097152 total steps have happened
[32m[20221208 15:02:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221208 15:02:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |           0.1068 |           8.8786 |       -2272.1139 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |           0.1004 |           7.9007 |       -1743.3247 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |           0.0410 |           7.4571 |       -1941.7302 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |           0.0440 |           7.1459 |       -2001.1784 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |           0.0013 |           6.8915 |       -2187.3458 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |          -0.0180 |           6.7176 |       -2275.3197 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |          -0.0322 |           6.5048 |       -2366.3477 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |          -0.0463 |           6.4060 |       -2459.8794 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |          -0.0472 |           6.2874 |       -2495.2874 |
[32m[20221208 15:02:37 @agent_ppo2.py:179][0m |          -0.0534 |           6.2347 |       -2569.9004 |
[32m[20221208 15:02:37 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:02:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.16
[32m[20221208 15:02:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.67
[32m[20221208 15:02:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.19
[32m[20221208 15:02:38 @agent_ppo2.py:137][0m Total time:      26.06 min
[32m[20221208 15:02:38 @agent_ppo2.py:139][0m 2099200 total steps have happened
[32m[20221208 15:02:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221208 15:02:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:38 @agent_ppo2.py:179][0m |           0.1228 |           8.1515 |       -2212.6484 |
[32m[20221208 15:02:38 @agent_ppo2.py:179][0m |           0.0742 |           7.6848 |       -1677.5334 |
[32m[20221208 15:02:38 @agent_ppo2.py:179][0m |           0.0313 |           7.4983 |       -1885.5624 |
[32m[20221208 15:02:38 @agent_ppo2.py:179][0m |           0.0099 |           7.3350 |       -2020.9938 |
[32m[20221208 15:02:39 @agent_ppo2.py:179][0m |          -0.0168 |           7.2282 |       -2169.9603 |
[32m[20221208 15:02:39 @agent_ppo2.py:179][0m |          -0.0317 |           7.1048 |       -2288.8377 |
[32m[20221208 15:02:39 @agent_ppo2.py:179][0m |          -0.0324 |           7.0104 |       -2367.4234 |
[32m[20221208 15:02:39 @agent_ppo2.py:179][0m |          -0.0304 |           6.9591 |       -2338.3811 |
[32m[20221208 15:02:39 @agent_ppo2.py:179][0m |          -0.0505 |           6.8792 |       -2445.6160 |
[32m[20221208 15:02:39 @agent_ppo2.py:179][0m |          -0.0589 |           6.8331 |       -2472.4702 |
[32m[20221208 15:02:39 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.65
[32m[20221208 15:02:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.22
[32m[20221208 15:02:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.89
[32m[20221208 15:02:39 @agent_ppo2.py:137][0m Total time:      26.08 min
[32m[20221208 15:02:39 @agent_ppo2.py:139][0m 2101248 total steps have happened
[32m[20221208 15:02:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221208 15:02:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |           0.1270 |           6.9130 |       -2070.1770 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |           0.1051 |           6.4853 |       -1315.1351 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |           0.0753 |           6.2511 |       -1245.6435 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |           0.0151 |           6.1165 |       -1609.2217 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |          -0.0123 |           6.0151 |       -1709.4995 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |          -0.0310 |           5.8898 |       -1833.2022 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |          -0.0440 |           5.8093 |       -1911.0041 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |          -0.0524 |           5.7513 |       -1929.1050 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |          -0.0586 |           5.6831 |       -1991.9449 |
[32m[20221208 15:02:40 @agent_ppo2.py:179][0m |          -0.0672 |           5.6485 |       -2035.2520 |
[32m[20221208 15:02:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.47
[32m[20221208 15:02:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.73
[32m[20221208 15:02:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.68
[32m[20221208 15:02:41 @agent_ppo2.py:137][0m Total time:      26.11 min
[32m[20221208 15:02:41 @agent_ppo2.py:139][0m 2103296 total steps have happened
[32m[20221208 15:02:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221208 15:02:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:41 @agent_ppo2.py:179][0m |           0.1264 |           7.5228 |       -2203.1722 |
[32m[20221208 15:02:41 @agent_ppo2.py:179][0m |           0.0920 |           6.7240 |       -1635.3049 |
[32m[20221208 15:02:41 @agent_ppo2.py:179][0m |           0.0208 |           6.4285 |       -1976.8346 |
[32m[20221208 15:02:41 @agent_ppo2.py:179][0m |          -0.0069 |           6.2317 |       -2129.3951 |
[32m[20221208 15:02:42 @agent_ppo2.py:179][0m |          -0.0189 |           6.1287 |       -2212.4229 |
[32m[20221208 15:02:42 @agent_ppo2.py:179][0m |          -0.0370 |           6.0014 |       -2272.7683 |
[32m[20221208 15:02:42 @agent_ppo2.py:179][0m |          -0.0504 |           5.8999 |       -2365.9257 |
[32m[20221208 15:02:42 @agent_ppo2.py:179][0m |          -0.0585 |           5.7963 |       -2452.3970 |
[32m[20221208 15:02:42 @agent_ppo2.py:179][0m |          -0.0531 |           5.7655 |       -2351.8282 |
[32m[20221208 15:02:42 @agent_ppo2.py:179][0m |          -0.0643 |           5.7018 |       -2427.2904 |
[32m[20221208 15:02:42 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:02:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.29
[32m[20221208 15:02:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.85
[32m[20221208 15:02:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.65
[32m[20221208 15:02:42 @agent_ppo2.py:137][0m Total time:      26.13 min
[32m[20221208 15:02:42 @agent_ppo2.py:139][0m 2105344 total steps have happened
[32m[20221208 15:02:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221208 15:02:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |           0.0955 |           9.8484 |       -2040.5433 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |           0.0974 |           9.0165 |       -1510.6970 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |           0.0191 |           8.6778 |       -1628.0130 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |           0.0004 |           8.4882 |       -1708.2203 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |          -0.0167 |           8.4207 |       -1820.5980 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |          -0.0336 |           8.2991 |       -1973.3684 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |          -0.0311 |           8.2010 |       -1973.1065 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |          -0.0445 |           8.1343 |       -2045.7851 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |          -0.0520 |           8.0688 |       -2132.4269 |
[32m[20221208 15:02:43 @agent_ppo2.py:179][0m |          -0.0580 |           8.0518 |       -2229.9134 |
[32m[20221208 15:02:43 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.75
[32m[20221208 15:02:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.33
[32m[20221208 15:02:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.25
[32m[20221208 15:02:44 @agent_ppo2.py:137][0m Total time:      26.16 min
[32m[20221208 15:02:44 @agent_ppo2.py:139][0m 2107392 total steps have happened
[32m[20221208 15:02:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221208 15:02:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:44 @agent_ppo2.py:179][0m |           0.1158 |           6.3008 |       -2208.0107 |
[32m[20221208 15:02:44 @agent_ppo2.py:179][0m |           0.1008 |           5.6271 |       -1419.9778 |
[32m[20221208 15:02:44 @agent_ppo2.py:179][0m |           0.0422 |           5.1994 |       -1624.2840 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |           0.0029 |           4.9939 |       -1742.5157 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |          -0.0234 |           4.8003 |       -1860.2763 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |          -0.0361 |           4.6595 |       -1947.3608 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |          -0.0514 |           4.5202 |       -2088.6983 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |          -0.0602 |           4.4034 |       -2143.1782 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |          -0.0653 |           4.3528 |       -2176.7956 |
[32m[20221208 15:02:45 @agent_ppo2.py:179][0m |          -0.0710 |           4.2551 |       -2228.0596 |
[32m[20221208 15:02:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:02:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.72
[32m[20221208 15:02:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.20
[32m[20221208 15:02:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 132.23
[32m[20221208 15:02:45 @agent_ppo2.py:137][0m Total time:      26.18 min
[32m[20221208 15:02:45 @agent_ppo2.py:139][0m 2109440 total steps have happened
[32m[20221208 15:02:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221208 15:02:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |           0.0818 |           6.9718 |       -2047.7817 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |           0.0472 |           5.7580 |       -1786.7652 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |           0.0129 |           5.3102 |       -1998.1489 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0160 |           5.0283 |       -2071.5333 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0347 |           4.7873 |       -2233.8082 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0441 |           4.5895 |       -2265.4983 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0518 |           4.4678 |       -2336.5113 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0552 |           4.3456 |       -2357.0784 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0554 |           4.2329 |       -2382.3263 |
[32m[20221208 15:02:46 @agent_ppo2.py:179][0m |          -0.0605 |           4.2064 |       -2429.3056 |
[32m[20221208 15:02:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:02:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.42
[32m[20221208 15:02:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.02
[32m[20221208 15:02:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.97
[32m[20221208 15:02:47 @agent_ppo2.py:137][0m Total time:      26.21 min
[32m[20221208 15:02:47 @agent_ppo2.py:139][0m 2111488 total steps have happened
[32m[20221208 15:02:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221208 15:02:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:47 @agent_ppo2.py:179][0m |           0.0832 |           6.7450 |       -1428.5150 |
[32m[20221208 15:02:47 @agent_ppo2.py:179][0m |           0.0476 |           5.7352 |       -1067.2530 |
[32m[20221208 15:02:47 @agent_ppo2.py:179][0m |           0.0152 |           5.4008 |       -1208.7001 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0186 |           5.1761 |       -1373.3666 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0307 |           5.0067 |       -1439.1687 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0331 |           4.8940 |       -1466.6530 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0443 |           4.8612 |       -1564.4773 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0498 |           4.7946 |       -1599.0961 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0581 |           4.6969 |       -1691.1319 |
[32m[20221208 15:02:48 @agent_ppo2.py:179][0m |          -0.0642 |           4.6602 |       -1729.3367 |
[32m[20221208 15:02:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:02:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.17
[32m[20221208 15:02:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.68
[32m[20221208 15:02:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 117.89
[32m[20221208 15:02:48 @agent_ppo2.py:137][0m Total time:      26.23 min
[32m[20221208 15:02:48 @agent_ppo2.py:139][0m 2113536 total steps have happened
[32m[20221208 15:02:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221208 15:02:49 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:02:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |           0.1564 |           7.2705 |       -2031.7849 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |           0.1359 |           6.4435 |       -1138.2904 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |           0.0699 |           6.0915 |       -1421.6060 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |           0.0191 |           5.9285 |       -1808.0211 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |          -0.0087 |           5.5942 |       -2028.5496 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |          -0.0244 |           5.5048 |       -2130.3184 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |          -0.0332 |           5.3890 |       -2176.5385 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |          -0.0408 |           5.2328 |       -2230.2644 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |          -0.0537 |           5.1667 |       -2344.0228 |
[32m[20221208 15:02:49 @agent_ppo2.py:179][0m |          -0.0600 |           5.0438 |       -2366.7882 |
[32m[20221208 15:02:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:02:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.61
[32m[20221208 15:02:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.85
[32m[20221208 15:02:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.77
[32m[20221208 15:02:50 @agent_ppo2.py:137][0m Total time:      26.26 min
[32m[20221208 15:02:50 @agent_ppo2.py:139][0m 2115584 total steps have happened
[32m[20221208 15:02:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221208 15:02:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:50 @agent_ppo2.py:179][0m |           0.0629 |           8.2335 |       -2454.5509 |
[32m[20221208 15:02:50 @agent_ppo2.py:179][0m |           0.0405 |           7.1753 |       -2202.6843 |
[32m[20221208 15:02:50 @agent_ppo2.py:179][0m |           0.0019 |           6.7489 |       -2409.2498 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0216 |           6.4268 |       -2420.2037 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0301 |           6.1970 |       -2521.2503 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0337 |           6.0260 |       -2497.1285 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0532 |           5.8321 |       -2558.8969 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0595 |           5.7217 |       -2635.1054 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0639 |           5.6509 |       -2607.3281 |
[32m[20221208 15:02:51 @agent_ppo2.py:179][0m |          -0.0668 |           5.5279 |       -2651.1584 |
[32m[20221208 15:02:51 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:02:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.98
[32m[20221208 15:02:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.25
[32m[20221208 15:02:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.87
[32m[20221208 15:02:51 @agent_ppo2.py:137][0m Total time:      26.28 min
[32m[20221208 15:02:51 @agent_ppo2.py:139][0m 2117632 total steps have happened
[32m[20221208 15:02:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221208 15:02:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |           0.0709 |           6.9217 |       -2340.0018 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |           0.0524 |           5.6607 |       -2127.5695 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |           0.0146 |           5.0645 |       -2309.5268 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0040 |           4.7354 |       -2413.7824 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0296 |           4.4711 |       -2542.9258 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0441 |           4.3073 |       -2645.5427 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0518 |           4.1561 |       -2662.4843 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0584 |           4.0423 |       -2687.1453 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0607 |           3.9514 |       -2660.9824 |
[32m[20221208 15:02:52 @agent_ppo2.py:179][0m |          -0.0608 |           3.8589 |       -2702.4629 |
[32m[20221208 15:02:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:02:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.01
[32m[20221208 15:02:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.94
[32m[20221208 15:02:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.47
[32m[20221208 15:02:53 @agent_ppo2.py:137][0m Total time:      26.31 min
[32m[20221208 15:02:53 @agent_ppo2.py:139][0m 2119680 total steps have happened
[32m[20221208 15:02:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221208 15:02:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:53 @agent_ppo2.py:179][0m |           0.1415 |           5.0486 |       -2184.6402 |
[32m[20221208 15:02:53 @agent_ppo2.py:179][0m |           0.0936 |           4.3231 |       -1698.9576 |
[32m[20221208 15:02:53 @agent_ppo2.py:179][0m |           0.0566 |           4.1161 |       -1856.3322 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |           0.0410 |           3.9343 |       -1905.7834 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |           0.0028 |           3.8251 |       -2097.8579 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |          -0.0177 |           3.7834 |       -2291.0854 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |          -0.0229 |           3.6908 |       -2361.1395 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |          -0.0322 |           3.6236 |       -2382.9710 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |          -0.0405 |           3.5749 |       -2500.2400 |
[32m[20221208 15:02:54 @agent_ppo2.py:179][0m |          -0.0371 |           3.5053 |       -2496.0496 |
[32m[20221208 15:02:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:02:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.23
[32m[20221208 15:02:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.92
[32m[20221208 15:02:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 141.55
[32m[20221208 15:02:54 @agent_ppo2.py:137][0m Total time:      26.33 min
[32m[20221208 15:02:54 @agent_ppo2.py:139][0m 2121728 total steps have happened
[32m[20221208 15:02:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221208 15:02:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:02:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |           0.1465 |           6.5908 |       -2102.5617 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |           0.1448 |           5.7259 |        -731.5758 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |           0.0790 |           5.2859 |        -942.1906 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |           0.0330 |           5.0414 |       -1384.4846 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |           0.0100 |           4.8423 |       -1618.3548 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |          -0.0105 |           4.6777 |       -1745.1115 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |          -0.0231 |           4.5658 |       -1813.4195 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |          -0.0307 |           4.4564 |       -1894.6935 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |          -0.0394 |           4.3839 |       -1975.7939 |
[32m[20221208 15:02:55 @agent_ppo2.py:179][0m |          -0.0434 |           4.2454 |       -1956.5681 |
[32m[20221208 15:02:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:02:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.91
[32m[20221208 15:02:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.37
[32m[20221208 15:02:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.17
[32m[20221208 15:02:56 @agent_ppo2.py:137][0m Total time:      26.36 min
[32m[20221208 15:02:56 @agent_ppo2.py:139][0m 2123776 total steps have happened
[32m[20221208 15:02:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221208 15:02:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:56 @agent_ppo2.py:179][0m |           0.1171 |           6.9070 |       -2096.2085 |
[32m[20221208 15:02:56 @agent_ppo2.py:179][0m |           0.0685 |           5.8624 |       -1711.1861 |
[32m[20221208 15:02:56 @agent_ppo2.py:179][0m |          -0.0014 |           5.3281 |       -2034.9045 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0238 |           5.0587 |       -2223.1154 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0385 |           4.8729 |       -2355.9308 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0386 |           4.7052 |       -2407.2209 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0445 |           4.5565 |       -2312.8196 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0505 |           4.4863 |       -2409.7197 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0475 |           4.4058 |       -2245.6769 |
[32m[20221208 15:02:57 @agent_ppo2.py:179][0m |          -0.0613 |           4.2936 |       -2495.6347 |
[32m[20221208 15:02:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:02:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.36
[32m[20221208 15:02:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.80
[32m[20221208 15:02:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.06
[32m[20221208 15:02:57 @agent_ppo2.py:137][0m Total time:      26.38 min
[32m[20221208 15:02:57 @agent_ppo2.py:139][0m 2125824 total steps have happened
[32m[20221208 15:02:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221208 15:02:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |           0.0981 |           6.6615 |       -2249.1866 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |           0.2233 |           5.7773 |       -1119.8954 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |           0.1024 |           5.4044 |        -839.6522 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |           0.0608 |           5.1710 |       -1106.4610 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |           0.0342 |           5.0346 |       -1304.0001 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |           0.0302 |           4.9047 |       -1336.0500 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |          -0.0022 |           4.7585 |       -1511.3688 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |          -0.0206 |           4.6772 |       -1640.0686 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |          -0.0304 |           4.5945 |       -1765.1942 |
[32m[20221208 15:02:58 @agent_ppo2.py:179][0m |          -0.0384 |           4.4938 |       -1828.6437 |
[32m[20221208 15:02:58 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:02:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.58
[32m[20221208 15:02:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.49
[32m[20221208 15:02:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.99
[32m[20221208 15:02:59 @agent_ppo2.py:137][0m Total time:      26.41 min
[32m[20221208 15:02:59 @agent_ppo2.py:139][0m 2127872 total steps have happened
[32m[20221208 15:02:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221208 15:02:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:02:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:02:59 @agent_ppo2.py:179][0m |           0.1321 |           5.4905 |       -2044.9899 |
[32m[20221208 15:02:59 @agent_ppo2.py:179][0m |           0.0924 |           4.7162 |       -1234.0571 |
[32m[20221208 15:02:59 @agent_ppo2.py:179][0m |           0.0157 |           4.3396 |       -1427.0420 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0194 |           4.0818 |       -1548.2267 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0331 |           3.9064 |       -1661.2192 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0460 |           3.7530 |       -1740.9182 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0556 |           3.6308 |       -1811.1073 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0628 |           3.5441 |       -1854.7404 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0696 |           3.4642 |       -1922.4268 |
[32m[20221208 15:03:00 @agent_ppo2.py:179][0m |          -0.0694 |           3.3713 |       -1928.3901 |
[32m[20221208 15:03:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:03:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.33
[32m[20221208 15:03:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.02
[32m[20221208 15:03:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.44
[32m[20221208 15:03:00 @agent_ppo2.py:137][0m Total time:      26.43 min
[32m[20221208 15:03:00 @agent_ppo2.py:139][0m 2129920 total steps have happened
[32m[20221208 15:03:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221208 15:03:01 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:03:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |           0.0898 |           4.4047 |       -1910.5592 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |           0.0769 |           3.9178 |       -1354.3818 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |           0.0470 |           3.6412 |       -1169.3667 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0106 |           3.4565 |       -1398.8950 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0334 |           3.3677 |       -1426.6982 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0555 |           3.2344 |       -1573.4497 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0618 |           3.1627 |       -1587.0904 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0655 |           3.0987 |       -1645.3833 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0746 |           3.0234 |       -1707.2130 |
[32m[20221208 15:03:01 @agent_ppo2.py:179][0m |          -0.0754 |           2.9659 |       -1716.6339 |
[32m[20221208 15:03:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.21
[32m[20221208 15:03:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 188.68
[32m[20221208 15:03:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.02
[32m[20221208 15:03:02 @agent_ppo2.py:137][0m Total time:      26.46 min
[32m[20221208 15:03:02 @agent_ppo2.py:139][0m 2131968 total steps have happened
[32m[20221208 15:03:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221208 15:03:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:02 @agent_ppo2.py:179][0m |           0.1213 |           9.6513 |       -2133.7054 |
[32m[20221208 15:03:02 @agent_ppo2.py:179][0m |           0.1216 |           8.6374 |       -1251.4952 |
[32m[20221208 15:03:02 @agent_ppo2.py:179][0m |           0.0664 |           8.2738 |       -1431.2271 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |           0.0296 |           8.0979 |       -1672.4233 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |           0.0035 |           7.8892 |       -1845.4791 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |          -0.0051 |           7.7472 |       -1984.3037 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |          -0.0196 |           7.6528 |       -2099.4993 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |          -0.0271 |           7.5466 |       -2183.9488 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |          -0.0388 |           7.4402 |       -2298.9348 |
[32m[20221208 15:03:03 @agent_ppo2.py:179][0m |          -0.0429 |           7.3566 |       -2372.6627 |
[32m[20221208 15:03:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.24
[32m[20221208 15:03:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.79
[32m[20221208 15:03:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.28
[32m[20221208 15:03:03 @agent_ppo2.py:137][0m Total time:      26.48 min
[32m[20221208 15:03:03 @agent_ppo2.py:139][0m 2134016 total steps have happened
[32m[20221208 15:03:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221208 15:03:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |           0.0880 |           6.0660 |       -2215.1882 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |           0.1306 |           5.3742 |        -678.4019 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |           0.0683 |           5.1135 |        -744.4328 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |           0.0326 |           4.9004 |       -1355.5604 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |           0.0022 |           4.7880 |       -1973.4076 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |          -0.0181 |           4.6436 |       -2279.8445 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |          -0.0203 |           4.5817 |       -2287.8415 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |          -0.0271 |           4.4867 |       -2297.0359 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |          -0.0329 |           4.4233 |       -2379.4905 |
[32m[20221208 15:03:04 @agent_ppo2.py:179][0m |          -0.0246 |           4.3507 |       -2093.3204 |
[32m[20221208 15:03:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:03:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.69
[32m[20221208 15:03:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.33
[32m[20221208 15:03:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.79
[32m[20221208 15:03:05 @agent_ppo2.py:137][0m Total time:      26.51 min
[32m[20221208 15:03:05 @agent_ppo2.py:139][0m 2136064 total steps have happened
[32m[20221208 15:03:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221208 15:03:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:05 @agent_ppo2.py:179][0m |           0.1192 |           5.2730 |       -1974.0805 |
[32m[20221208 15:03:05 @agent_ppo2.py:179][0m |           0.0397 |           4.4016 |       -1189.8347 |
[32m[20221208 15:03:05 @agent_ppo2.py:179][0m |          -0.0058 |           4.1319 |       -1226.3223 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0364 |           3.9519 |       -1432.8253 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0506 |           3.8812 |       -1478.9794 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0537 |           3.7821 |       -1528.9002 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0592 |           3.7034 |       -1603.4996 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0672 |           3.6485 |       -1680.5493 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0698 |           3.6077 |       -1697.1305 |
[32m[20221208 15:03:06 @agent_ppo2.py:179][0m |          -0.0731 |           3.5644 |       -1705.7409 |
[32m[20221208 15:03:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.22
[32m[20221208 15:03:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.17
[32m[20221208 15:03:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.83
[32m[20221208 15:03:06 @agent_ppo2.py:137][0m Total time:      26.53 min
[32m[20221208 15:03:06 @agent_ppo2.py:139][0m 2138112 total steps have happened
[32m[20221208 15:03:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221208 15:03:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |           0.1208 |           3.9461 |       -2061.6939 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |           0.0905 |           3.4012 |       -1442.2325 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |           0.0517 |           3.2240 |       -1454.4722 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |           0.0096 |           3.1145 |       -1554.9073 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |          -0.0121 |           3.0640 |       -1727.0564 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |          -0.0209 |           3.0169 |       -1712.5976 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |          -0.0304 |           2.9759 |       -1807.6133 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |          -0.0355 |           2.9269 |       -1875.4114 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |          -0.0395 |           2.9021 |       -1913.2743 |
[32m[20221208 15:03:07 @agent_ppo2.py:179][0m |          -0.0482 |           2.8681 |       -1979.6729 |
[32m[20221208 15:03:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.08
[32m[20221208 15:03:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.78
[32m[20221208 15:03:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.37
[32m[20221208 15:03:08 @agent_ppo2.py:137][0m Total time:      26.56 min
[32m[20221208 15:03:08 @agent_ppo2.py:139][0m 2140160 total steps have happened
[32m[20221208 15:03:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221208 15:03:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:08 @agent_ppo2.py:179][0m |           0.0799 |           4.6524 |       -2356.9567 |
[32m[20221208 15:03:08 @agent_ppo2.py:179][0m |           0.1167 |           4.1705 |       -1062.4394 |
[32m[20221208 15:03:08 @agent_ppo2.py:179][0m |           0.0591 |           3.9691 |        -983.8640 |
[32m[20221208 15:03:08 @agent_ppo2.py:179][0m |           0.0262 |           3.8496 |       -1215.9187 |
[32m[20221208 15:03:09 @agent_ppo2.py:179][0m |           0.0127 |           3.7594 |       -1379.9558 |
[32m[20221208 15:03:09 @agent_ppo2.py:179][0m |           0.0085 |           3.6413 |       -1161.4987 |
[32m[20221208 15:03:09 @agent_ppo2.py:179][0m |          -0.0053 |           3.6130 |       -1360.5723 |
[32m[20221208 15:03:09 @agent_ppo2.py:179][0m |          -0.0161 |           3.5385 |       -1445.4357 |
[32m[20221208 15:03:09 @agent_ppo2.py:179][0m |          -0.0193 |           3.4773 |       -1535.7779 |
[32m[20221208 15:03:09 @agent_ppo2.py:179][0m |          -0.0262 |           3.4256 |       -1605.1315 |
[32m[20221208 15:03:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:03:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.99
[32m[20221208 15:03:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.48
[32m[20221208 15:03:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.41
[32m[20221208 15:03:09 @agent_ppo2.py:137][0m Total time:      26.58 min
[32m[20221208 15:03:09 @agent_ppo2.py:139][0m 2142208 total steps have happened
[32m[20221208 15:03:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221208 15:03:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |           0.1042 |           4.2469 |       -2038.6293 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |           0.1276 |           3.5929 |        -908.8924 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |           0.0839 |           3.3737 |        -859.6283 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |           0.0294 |           3.2016 |       -1174.4819 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |           0.0010 |           3.0864 |       -1336.0682 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |          -0.0073 |           2.9871 |       -1395.9769 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |          -0.0234 |           2.9068 |       -1460.1203 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |          -0.0294 |           2.8500 |       -1510.7100 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |          -0.0302 |           2.7992 |       -1535.1440 |
[32m[20221208 15:03:10 @agent_ppo2.py:179][0m |          -0.0384 |           2.7736 |       -1624.7315 |
[32m[20221208 15:03:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:03:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.21
[32m[20221208 15:03:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.33
[32m[20221208 15:03:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.44
[32m[20221208 15:03:11 @agent_ppo2.py:137][0m Total time:      26.61 min
[32m[20221208 15:03:11 @agent_ppo2.py:139][0m 2144256 total steps have happened
[32m[20221208 15:03:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221208 15:03:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:11 @agent_ppo2.py:179][0m |           0.0718 |           5.9835 |       -2272.5999 |
[32m[20221208 15:03:11 @agent_ppo2.py:179][0m |           0.0393 |           4.2978 |       -2041.3169 |
[32m[20221208 15:03:11 @agent_ppo2.py:179][0m |           0.0151 |           3.8341 |       -2222.9418 |
[32m[20221208 15:03:11 @agent_ppo2.py:179][0m |          -0.0227 |           3.4195 |       -2351.4199 |
[32m[20221208 15:03:12 @agent_ppo2.py:179][0m |          -0.0398 |           3.2772 |       -2505.4145 |
[32m[20221208 15:03:12 @agent_ppo2.py:179][0m |          -0.0458 |           3.0719 |       -2565.4884 |
[32m[20221208 15:03:12 @agent_ppo2.py:179][0m |          -0.0556 |           2.9466 |       -2672.9043 |
[32m[20221208 15:03:12 @agent_ppo2.py:179][0m |          -0.0585 |           2.7983 |       -2688.0853 |
[32m[20221208 15:03:12 @agent_ppo2.py:179][0m |          -0.0569 |           2.6952 |       -2691.9701 |
[32m[20221208 15:03:12 @agent_ppo2.py:179][0m |          -0.0484 |           2.6307 |       -2590.8970 |
[32m[20221208 15:03:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:03:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.84
[32m[20221208 15:03:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.22
[32m[20221208 15:03:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.67
[32m[20221208 15:03:12 @agent_ppo2.py:137][0m Total time:      26.63 min
[32m[20221208 15:03:12 @agent_ppo2.py:139][0m 2146304 total steps have happened
[32m[20221208 15:03:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221208 15:03:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |           0.1492 |           6.9645 |       -2398.0458 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |           0.1435 |           4.6806 |       -1408.0453 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |           0.0738 |           4.2020 |       -1248.3506 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |           0.0383 |           3.8756 |       -1472.6524 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |           0.0011 |           3.6588 |       -1639.3943 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |          -0.0023 |           3.4919 |       -1703.3270 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |           0.0009 |           3.4296 |       -1715.6114 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |          -0.0081 |           3.3227 |       -1766.9189 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |          -0.0164 |           3.2514 |       -1845.3860 |
[32m[20221208 15:03:13 @agent_ppo2.py:179][0m |          -0.0310 |           3.1633 |       -1919.7536 |
[32m[20221208 15:03:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:03:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.58
[32m[20221208 15:03:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.29
[32m[20221208 15:03:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.85
[32m[20221208 15:03:14 @agent_ppo2.py:137][0m Total time:      26.66 min
[32m[20221208 15:03:14 @agent_ppo2.py:139][0m 2148352 total steps have happened
[32m[20221208 15:03:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221208 15:03:14 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:03:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:14 @agent_ppo2.py:179][0m |           0.1172 |           5.8343 |       -2135.9812 |
[32m[20221208 15:03:14 @agent_ppo2.py:179][0m |           0.0787 |           4.6949 |       -1191.5403 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |           0.0300 |           4.2744 |       -1409.6395 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0086 |           4.0488 |       -1667.2642 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0298 |           3.8945 |       -1805.8544 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0410 |           3.7957 |       -1905.0050 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0514 |           3.7029 |       -1999.8068 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0575 |           3.5924 |       -2059.1310 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0634 |           3.5510 |       -2081.1203 |
[32m[20221208 15:03:15 @agent_ppo2.py:179][0m |          -0.0675 |           3.4939 |       -2114.2359 |
[32m[20221208 15:03:15 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:03:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.90
[32m[20221208 15:03:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.43
[32m[20221208 15:03:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.36
[32m[20221208 15:03:15 @agent_ppo2.py:137][0m Total time:      26.69 min
[32m[20221208 15:03:15 @agent_ppo2.py:139][0m 2150400 total steps have happened
[32m[20221208 15:03:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221208 15:03:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |           0.0846 |           6.9586 |       -2499.4162 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |           0.0506 |           6.1052 |       -2224.1018 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |           0.0217 |           5.8029 |       -2296.9087 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |          -0.0004 |           5.6402 |       -2357.8192 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |          -0.0272 |           5.4963 |       -2442.6937 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |          -0.0423 |           5.3707 |       -2510.6736 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |          -0.0515 |           5.2964 |       -2649.9566 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |          -0.0487 |           5.1967 |       -2669.3949 |
[32m[20221208 15:03:16 @agent_ppo2.py:179][0m |          -0.0602 |           5.1620 |       -2711.1599 |
[32m[20221208 15:03:17 @agent_ppo2.py:179][0m |          -0.0539 |           5.0455 |       -2684.1776 |
[32m[20221208 15:03:17 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:03:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.65
[32m[20221208 15:03:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.28
[32m[20221208 15:03:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.36
[32m[20221208 15:03:17 @agent_ppo2.py:137][0m Total time:      26.71 min
[32m[20221208 15:03:17 @agent_ppo2.py:139][0m 2152448 total steps have happened
[32m[20221208 15:03:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221208 15:03:17 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:03:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |           0.0949 |           7.2781 |       -2145.0994 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |           0.0801 |           6.4830 |       -1313.5336 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |           0.0378 |           6.0985 |       -1487.4489 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |           0.0018 |           5.9272 |       -1665.4931 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |          -0.0141 |           5.7019 |       -1824.7532 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |          -0.0306 |           5.6012 |       -1903.7181 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |          -0.0437 |           5.4657 |       -1951.2120 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |          -0.0428 |           5.3919 |       -1962.0871 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |          -0.0480 |           5.2992 |       -2021.6048 |
[32m[20221208 15:03:18 @agent_ppo2.py:179][0m |          -0.0552 |           5.2706 |       -2070.0886 |
[32m[20221208 15:03:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:03:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.30
[32m[20221208 15:03:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.16
[32m[20221208 15:03:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.30
[32m[20221208 15:03:19 @agent_ppo2.py:137][0m Total time:      26.74 min
[32m[20221208 15:03:19 @agent_ppo2.py:139][0m 2154496 total steps have happened
[32m[20221208 15:03:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221208 15:03:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |           0.1249 |           6.1306 |       -2407.5892 |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |           0.0229 |           5.3948 |       -2125.0611 |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |          -0.0118 |           5.1151 |       -2377.4193 |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |          -0.0312 |           4.9971 |       -2515.1163 |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |          -0.0396 |           4.8079 |       -2364.5681 |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |          -0.0495 |           4.7664 |       -2506.1722 |
[32m[20221208 15:03:19 @agent_ppo2.py:179][0m |          -0.0482 |           4.6682 |       -2404.3348 |
[32m[20221208 15:03:20 @agent_ppo2.py:179][0m |          -0.0446 |           4.5777 |       -2493.7521 |
[32m[20221208 15:03:20 @agent_ppo2.py:179][0m |          -0.0567 |           4.5482 |       -2512.7276 |
[32m[20221208 15:03:20 @agent_ppo2.py:179][0m |          -0.0618 |           4.5037 |       -2692.6615 |
[32m[20221208 15:03:20 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:03:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.81
[32m[20221208 15:03:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.56
[32m[20221208 15:03:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.76
[32m[20221208 15:03:20 @agent_ppo2.py:137][0m Total time:      26.76 min
[32m[20221208 15:03:20 @agent_ppo2.py:139][0m 2156544 total steps have happened
[32m[20221208 15:03:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221208 15:03:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0947 |           3.3101 |       -1793.1393 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.1364 |           2.9433 |        -864.2078 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0942 |           2.8782 |        -822.5701 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0545 |           2.8164 |       -1287.9894 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0244 |           2.7914 |       -1663.2311 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0516 |           2.7758 |       -1365.1083 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0169 |           2.7302 |       -1760.3735 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0410 |           2.7213 |       -1559.0004 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0651 |           2.7235 |       -1251.1252 |
[32m[20221208 15:03:21 @agent_ppo2.py:179][0m |           0.0324 |           2.7029 |       -1470.4529 |
[32m[20221208 15:03:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.58
[32m[20221208 15:03:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.83
[32m[20221208 15:03:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.29
[32m[20221208 15:03:22 @agent_ppo2.py:137][0m Total time:      26.79 min
[32m[20221208 15:03:22 @agent_ppo2.py:139][0m 2158592 total steps have happened
[32m[20221208 15:03:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221208 15:03:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |           0.0584 |           3.5238 |       -1803.3908 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |           0.0313 |           2.6817 |       -1422.0330 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |           0.0021 |           2.3803 |       -1629.2299 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |          -0.0224 |           2.2491 |       -1787.8733 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |          -0.0301 |           2.1461 |       -1812.4124 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |          -0.0185 |           2.0917 |       -1792.0374 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |          -0.0300 |           2.0303 |       -1847.0663 |
[32m[20221208 15:03:22 @agent_ppo2.py:179][0m |          -0.0226 |           2.0287 |       -1814.7196 |
[32m[20221208 15:03:23 @agent_ppo2.py:179][0m |          -0.0339 |           1.9545 |       -1888.1337 |
[32m[20221208 15:03:23 @agent_ppo2.py:179][0m |          -0.0363 |           1.9020 |       -1919.5482 |
[32m[20221208 15:03:23 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:03:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.60
[32m[20221208 15:03:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.48
[32m[20221208 15:03:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 152.89
[32m[20221208 15:03:23 @agent_ppo2.py:137][0m Total time:      26.81 min
[32m[20221208 15:03:23 @agent_ppo2.py:139][0m 2160640 total steps have happened
[32m[20221208 15:03:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221208 15:03:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |           0.0991 |           4.8913 |       -1509.4532 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |           0.0614 |           4.3912 |       -1151.7022 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |           0.0253 |           4.1826 |       -1399.7188 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |           0.0005 |           4.1064 |       -1596.8667 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |          -0.0069 |           4.0249 |       -1636.4626 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |          -0.0124 |           3.9306 |       -1718.8959 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |          -0.0229 |           3.8729 |       -1919.4675 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |          -0.0257 |           3.8105 |       -1968.6012 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |          -0.0268 |           3.7921 |       -2107.1726 |
[32m[20221208 15:03:24 @agent_ppo2.py:179][0m |          -0.0326 |           3.7655 |       -2147.8657 |
[32m[20221208 15:03:24 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.76
[32m[20221208 15:03:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 156.55
[32m[20221208 15:03:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.81
[32m[20221208 15:03:24 @agent_ppo2.py:137][0m Total time:      26.84 min
[32m[20221208 15:03:25 @agent_ppo2.py:139][0m 2162688 total steps have happened
[32m[20221208 15:03:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221208 15:03:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |           0.0563 |           4.3263 |       -2118.5363 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |           0.0627 |           3.7752 |       -1132.7664 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |           0.0275 |           3.5522 |       -1268.4930 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |           0.0052 |           3.4637 |       -1437.3959 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |          -0.0196 |           3.3754 |       -1712.8480 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |          -0.0332 |           3.3334 |       -1798.5601 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |          -0.0412 |           3.2388 |       -1851.5305 |
[32m[20221208 15:03:25 @agent_ppo2.py:179][0m |          -0.0449 |           3.1707 |       -1891.7333 |
[32m[20221208 15:03:26 @agent_ppo2.py:179][0m |          -0.0431 |           3.1380 |       -1838.8241 |
[32m[20221208 15:03:26 @agent_ppo2.py:179][0m |          -0.0489 |           3.1667 |       -1912.8947 |
[32m[20221208 15:03:26 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.71
[32m[20221208 15:03:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.32
[32m[20221208 15:03:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.85
[32m[20221208 15:03:26 @agent_ppo2.py:137][0m Total time:      26.86 min
[32m[20221208 15:03:26 @agent_ppo2.py:139][0m 2164736 total steps have happened
[32m[20221208 15:03:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221208 15:03:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |           0.1175 |           7.3399 |       -1799.5985 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |           0.1188 |           6.0294 |        -852.4930 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |           0.0315 |           5.7185 |       -1112.9992 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0011 |           5.5490 |       -1352.7724 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0270 |           5.4328 |       -1520.5655 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0432 |           5.3556 |       -1609.7372 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0479 |           5.3001 |       -1648.1769 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0493 |           5.2631 |       -1682.8141 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0562 |           5.1876 |       -1723.4790 |
[32m[20221208 15:03:27 @agent_ppo2.py:179][0m |          -0.0652 |           5.1531 |       -1811.0082 |
[32m[20221208 15:03:27 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.13
[32m[20221208 15:03:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.70
[32m[20221208 15:03:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 125.32
[32m[20221208 15:03:27 @agent_ppo2.py:137][0m Total time:      26.89 min
[32m[20221208 15:03:27 @agent_ppo2.py:139][0m 2166784 total steps have happened
[32m[20221208 15:03:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221208 15:03:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.1262 |           4.9045 |       -2097.9252 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.1375 |           4.3769 |       -1374.0925 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.1251 |           4.0928 |        -631.1209 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.0990 |           3.9262 |        -731.7357 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.0723 |           3.7533 |        -893.4876 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.0467 |           3.6814 |       -1171.1590 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.0630 |           3.5544 |       -1188.3329 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.0364 |           3.4993 |       -1226.5843 |
[32m[20221208 15:03:28 @agent_ppo2.py:179][0m |           0.0215 |           3.4596 |       -1284.5835 |
[32m[20221208 15:03:29 @agent_ppo2.py:179][0m |           0.0106 |           3.4480 |       -1416.8670 |
[32m[20221208 15:03:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.69
[32m[20221208 15:03:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.69
[32m[20221208 15:03:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.22
[32m[20221208 15:03:29 @agent_ppo2.py:137][0m Total time:      26.91 min
[32m[20221208 15:03:29 @agent_ppo2.py:139][0m 2168832 total steps have happened
[32m[20221208 15:03:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221208 15:03:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:29 @agent_ppo2.py:179][0m |           0.1167 |           4.4486 |       -1732.2135 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.1117 |           3.9681 |        -722.6178 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.0588 |           3.7501 |       -1094.5547 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.0450 |           3.6077 |       -1141.3634 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.0123 |           3.5511 |       -1414.8129 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.0036 |           3.4445 |       -1570.0731 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.0078 |           3.3856 |       -1836.7896 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |           0.0055 |           3.3539 |       -1595.1103 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |          -0.0138 |           3.2938 |       -1804.1609 |
[32m[20221208 15:03:30 @agent_ppo2.py:179][0m |          -0.0071 |           3.2701 |       -1704.4125 |
[32m[20221208 15:03:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.71
[32m[20221208 15:03:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.07
[32m[20221208 15:03:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.81
[32m[20221208 15:03:30 @agent_ppo2.py:137][0m Total time:      26.94 min
[32m[20221208 15:03:30 @agent_ppo2.py:139][0m 2170880 total steps have happened
[32m[20221208 15:03:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221208 15:03:31 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |           0.1187 |           5.0871 |       -2057.3634 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |           0.0850 |           4.4562 |       -1662.4509 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |           0.0189 |           4.2308 |       -1978.9135 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |           0.0005 |           4.0815 |       -2024.9415 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |          -0.0154 |           3.9952 |       -2038.8783 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |          -0.0295 |           3.9187 |       -2113.9859 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |          -0.0469 |           3.8399 |       -2170.6990 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |          -0.0566 |           3.8219 |       -2214.3962 |
[32m[20221208 15:03:31 @agent_ppo2.py:179][0m |          -0.0641 |           3.7762 |       -2214.4810 |
[32m[20221208 15:03:32 @agent_ppo2.py:179][0m |          -0.0683 |           3.7489 |       -2287.4284 |
[32m[20221208 15:03:32 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.78
[32m[20221208 15:03:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.93
[32m[20221208 15:03:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.58
[32m[20221208 15:03:32 @agent_ppo2.py:137][0m Total time:      26.96 min
[32m[20221208 15:03:32 @agent_ppo2.py:139][0m 2172928 total steps have happened
[32m[20221208 15:03:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221208 15:03:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:32 @agent_ppo2.py:179][0m |           0.0663 |           5.1218 |       -1755.6758 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |           0.0232 |           4.4965 |       -1211.7282 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0147 |           4.2721 |       -1357.9727 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0279 |           4.1310 |       -1445.7626 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0285 |           4.0528 |       -1427.4849 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0457 |           3.9444 |       -1535.4587 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0531 |           3.9006 |       -1650.9532 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0566 |           3.8366 |       -1650.9142 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0614 |           3.7851 |       -1679.8401 |
[32m[20221208 15:03:33 @agent_ppo2.py:179][0m |          -0.0671 |           3.7356 |       -1763.2592 |
[32m[20221208 15:03:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.60
[32m[20221208 15:03:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.44
[32m[20221208 15:03:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.83
[32m[20221208 15:03:33 @agent_ppo2.py:137][0m Total time:      26.99 min
[32m[20221208 15:03:33 @agent_ppo2.py:139][0m 2174976 total steps have happened
[32m[20221208 15:03:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221208 15:03:34 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:03:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1473 |           1.7434 |       -1100.3111 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1657 |           1.6161 |         -88.9051 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1461 |           1.5869 |         -81.5860 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1481 |           1.5711 |         -55.0594 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1456 |           1.5647 |        -152.3408 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1365 |           1.5615 |        -185.9873 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1518 |           1.5780 |         -29.8479 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1451 |           1.5644 |         -39.3442 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1078 |           1.5624 |        -362.6519 |
[32m[20221208 15:03:34 @agent_ppo2.py:179][0m |           0.1159 |           1.5416 |        -779.7713 |
[32m[20221208 15:03:34 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:03:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.11
[32m[20221208 15:03:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 57.94
[32m[20221208 15:03:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.68
[32m[20221208 15:03:35 @agent_ppo2.py:137][0m Total time:      27.01 min
[32m[20221208 15:03:35 @agent_ppo2.py:139][0m 2177024 total steps have happened
[32m[20221208 15:03:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221208 15:03:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:35 @agent_ppo2.py:179][0m |           0.1762 |           3.8203 |       -1671.9981 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |           0.1929 |           3.3707 |        -666.5062 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |           0.1147 |           3.1602 |        -594.5490 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |           0.0725 |           3.0687 |        -807.4658 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |           0.0399 |           2.9944 |       -1179.8842 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |           0.0144 |           2.9334 |       -1471.6351 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |           0.0014 |           2.8974 |       -1567.7418 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |          -0.0090 |           2.8614 |       -1696.3851 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |          -0.0065 |           2.8274 |       -1579.7202 |
[32m[20221208 15:03:36 @agent_ppo2.py:179][0m |          -0.0244 |           2.8032 |       -1790.9698 |
[32m[20221208 15:03:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.57
[32m[20221208 15:03:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.26
[32m[20221208 15:03:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.78
[32m[20221208 15:03:36 @agent_ppo2.py:137][0m Total time:      27.04 min
[32m[20221208 15:03:36 @agent_ppo2.py:139][0m 2179072 total steps have happened
[32m[20221208 15:03:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221208 15:03:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.1226 |           3.6199 |       -1794.8741 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.1151 |           3.3602 |        -429.8482 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.0585 |           3.2332 |        -568.7874 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.0370 |           3.1743 |        -688.9633 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.0197 |           3.1152 |        -778.1679 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.0086 |           3.0862 |        -843.3581 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.0063 |           3.0577 |        -896.3507 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |           0.0006 |           3.0232 |        -935.8658 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |          -0.0064 |           3.0088 |       -1067.7681 |
[32m[20221208 15:03:37 @agent_ppo2.py:179][0m |          -0.0108 |           2.9893 |       -1159.2705 |
[32m[20221208 15:03:37 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.05
[32m[20221208 15:03:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.80
[32m[20221208 15:03:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.15
[32m[20221208 15:03:38 @agent_ppo2.py:137][0m Total time:      27.06 min
[32m[20221208 15:03:38 @agent_ppo2.py:139][0m 2181120 total steps have happened
[32m[20221208 15:03:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221208 15:03:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:38 @agent_ppo2.py:179][0m |           0.1342 |           2.5619 |       -1419.7358 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |           0.0967 |           2.0875 |        -747.4823 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |           0.0501 |           1.9196 |        -788.0898 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |           0.0240 |           1.8154 |        -931.2626 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |           0.0070 |           1.7509 |        -975.4977 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |          -0.0065 |           1.7022 |       -1074.7964 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |          -0.0112 |           1.6558 |       -1134.3464 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |          -0.0145 |           1.6201 |       -1136.7380 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |          -0.0208 |           1.6053 |       -1130.7255 |
[32m[20221208 15:03:39 @agent_ppo2.py:179][0m |          -0.0199 |           1.5849 |       -1219.4017 |
[32m[20221208 15:03:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.36
[32m[20221208 15:03:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.11
[32m[20221208 15:03:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.86
[32m[20221208 15:03:39 @agent_ppo2.py:137][0m Total time:      27.09 min
[32m[20221208 15:03:39 @agent_ppo2.py:139][0m 2183168 total steps have happened
[32m[20221208 15:03:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221208 15:03:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |           0.0906 |           3.6665 |       -1661.5487 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |           0.0754 |           3.3006 |       -1139.1391 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |           0.0430 |           3.1593 |       -1429.1085 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |           0.0112 |           3.0805 |       -1182.4117 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |          -0.0179 |           3.0229 |       -1593.0646 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |          -0.0293 |           2.9956 |       -1623.4559 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |          -0.0346 |           2.9291 |       -1704.9892 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |          -0.0245 |           2.9000 |       -1652.7197 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |          -0.0430 |           2.8640 |       -1884.8400 |
[32m[20221208 15:03:40 @agent_ppo2.py:179][0m |          -0.0464 |           2.8258 |       -1762.3057 |
[32m[20221208 15:03:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.96
[32m[20221208 15:03:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.93
[32m[20221208 15:03:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.99
[32m[20221208 15:03:41 @agent_ppo2.py:137][0m Total time:      27.11 min
[32m[20221208 15:03:41 @agent_ppo2.py:139][0m 2185216 total steps have happened
[32m[20221208 15:03:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221208 15:03:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:41 @agent_ppo2.py:179][0m |           0.1084 |           4.6029 |       -1734.9813 |
[32m[20221208 15:03:41 @agent_ppo2.py:179][0m |           0.0792 |           3.8982 |       -1099.9726 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |           0.0513 |           3.7468 |       -1095.1448 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |           0.0094 |           3.6010 |       -1330.3125 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |          -0.0079 |           3.4953 |       -1467.7411 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |          -0.0281 |           3.4840 |       -1711.9262 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |          -0.0352 |           3.3850 |       -1761.4908 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |          -0.0381 |           3.3267 |       -1710.7894 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |          -0.0438 |           3.2838 |       -1810.0135 |
[32m[20221208 15:03:42 @agent_ppo2.py:179][0m |          -0.0574 |           3.2839 |       -1933.6447 |
[32m[20221208 15:03:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:03:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.40
[32m[20221208 15:03:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.12
[32m[20221208 15:03:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.83
[32m[20221208 15:03:42 @agent_ppo2.py:137][0m Total time:      27.14 min
[32m[20221208 15:03:42 @agent_ppo2.py:139][0m 2187264 total steps have happened
[32m[20221208 15:03:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221208 15:03:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.1086 |           2.2941 |       -1188.5519 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0788 |           1.9425 |       -1074.9340 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0573 |           1.8398 |       -1253.3106 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0797 |           1.7666 |        -866.5481 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0269 |           1.7354 |       -1334.3800 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0101 |           1.6971 |       -1446.7938 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0273 |           1.6720 |       -1302.6790 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0409 |           1.6616 |       -1162.2858 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0301 |           1.6290 |       -1292.4610 |
[32m[20221208 15:03:43 @agent_ppo2.py:179][0m |           0.0661 |           1.6156 |        -989.1452 |
[32m[20221208 15:03:43 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.08
[32m[20221208 15:03:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.86
[32m[20221208 15:03:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.03
[32m[20221208 15:03:44 @agent_ppo2.py:137][0m Total time:      27.16 min
[32m[20221208 15:03:44 @agent_ppo2.py:139][0m 2189312 total steps have happened
[32m[20221208 15:03:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221208 15:03:44 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:03:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:44 @agent_ppo2.py:179][0m |           0.1357 |           1.8233 |        -705.9419 |
[32m[20221208 15:03:44 @agent_ppo2.py:179][0m |           0.1301 |           1.5679 |        -272.9544 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.1027 |           1.4953 |        -402.6840 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.1322 |           1.4566 |        -320.5316 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.1186 |           1.4159 |        -196.9883 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.1026 |           1.3973 |        -279.2500 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.0863 |           1.3623 |        -457.5023 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.0825 |           1.3524 |        -449.3244 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.0776 |           1.3291 |        -630.1065 |
[32m[20221208 15:03:45 @agent_ppo2.py:179][0m |           0.0849 |           1.3260 |        -542.3740 |
[32m[20221208 15:03:45 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:03:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.90
[32m[20221208 15:03:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.27
[32m[20221208 15:03:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.42
[32m[20221208 15:03:45 @agent_ppo2.py:137][0m Total time:      27.18 min
[32m[20221208 15:03:45 @agent_ppo2.py:139][0m 2191360 total steps have happened
[32m[20221208 15:03:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221208 15:03:46 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:03:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1297 |           1.3383 |        -715.3452 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1466 |           1.1309 |         -59.0157 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1375 |           1.0999 |         -69.1561 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1227 |           1.0713 |         -91.5659 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1226 |           1.0599 |        -143.1072 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1095 |           1.0536 |        -230.3095 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1140 |           1.0345 |        -217.9723 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1298 |           1.0311 |        -119.2417 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1176 |           1.0185 |        -118.9102 |
[32m[20221208 15:03:46 @agent_ppo2.py:179][0m |           0.1104 |           1.0180 |        -164.8135 |
[32m[20221208 15:03:46 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:03:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.36
[32m[20221208 15:03:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.34
[32m[20221208 15:03:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.26
[32m[20221208 15:03:47 @agent_ppo2.py:137][0m Total time:      27.21 min
[32m[20221208 15:03:47 @agent_ppo2.py:139][0m 2193408 total steps have happened
[32m[20221208 15:03:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221208 15:03:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:47 @agent_ppo2.py:179][0m |           0.1831 |           1.9854 |       -1281.2972 |
[32m[20221208 15:03:47 @agent_ppo2.py:179][0m |           0.1529 |           1.7111 |        -650.4305 |
[32m[20221208 15:03:47 @agent_ppo2.py:179][0m |           0.0478 |           1.6594 |        -856.0634 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |           0.0349 |           1.6188 |        -920.7748 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |           0.0390 |           1.5999 |        -959.1287 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |           0.0663 |           1.5747 |        -831.3996 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |           0.0192 |           1.5816 |       -1028.3338 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |          -0.0025 |           1.5564 |       -1139.9410 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |           0.0084 |           1.5596 |       -1105.1120 |
[32m[20221208 15:03:48 @agent_ppo2.py:179][0m |          -0.0066 |           1.5522 |       -1180.8128 |
[32m[20221208 15:03:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.67
[32m[20221208 15:03:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.50
[32m[20221208 15:03:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.26
[32m[20221208 15:03:48 @agent_ppo2.py:137][0m Total time:      27.23 min
[32m[20221208 15:03:48 @agent_ppo2.py:139][0m 2195456 total steps have happened
[32m[20221208 15:03:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221208 15:03:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |           0.0714 |           3.5031 |       -1184.4197 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |           0.0661 |           2.6271 |        -686.2109 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0008 |           2.4197 |        -620.9369 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0256 |           2.3270 |        -664.2820 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0401 |           2.2213 |        -716.9359 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0503 |           2.1854 |        -733.4191 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0553 |           2.1145 |        -747.4705 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0598 |           2.0695 |        -778.4697 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0590 |           2.0486 |        -776.8297 |
[32m[20221208 15:03:49 @agent_ppo2.py:179][0m |          -0.0609 |           2.0120 |        -789.0234 |
[32m[20221208 15:03:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.38
[32m[20221208 15:03:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.32
[32m[20221208 15:03:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.77
[32m[20221208 15:03:50 @agent_ppo2.py:137][0m Total time:      27.26 min
[32m[20221208 15:03:50 @agent_ppo2.py:139][0m 2197504 total steps have happened
[32m[20221208 15:03:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221208 15:03:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:50 @agent_ppo2.py:179][0m |           0.2678 |           3.2865 |       -1160.5469 |
[32m[20221208 15:03:50 @agent_ppo2.py:179][0m |           0.0560 |           2.6190 |        -778.0066 |
[32m[20221208 15:03:50 @agent_ppo2.py:179][0m |           0.0225 |           2.4305 |        -614.6277 |
[32m[20221208 15:03:50 @agent_ppo2.py:179][0m |          -0.0121 |           2.3190 |        -771.3693 |
[32m[20221208 15:03:51 @agent_ppo2.py:179][0m |          -0.0308 |           2.2464 |        -831.2490 |
[32m[20221208 15:03:51 @agent_ppo2.py:179][0m |          -0.0369 |           2.2187 |        -883.3576 |
[32m[20221208 15:03:51 @agent_ppo2.py:179][0m |          -0.0447 |           2.1509 |        -922.8648 |
[32m[20221208 15:03:51 @agent_ppo2.py:179][0m |          -0.0506 |           2.1362 |        -941.9746 |
[32m[20221208 15:03:51 @agent_ppo2.py:179][0m |          -0.0559 |           2.0929 |        -945.0985 |
[32m[20221208 15:03:51 @agent_ppo2.py:179][0m |          -0.0579 |           2.0761 |        -977.9393 |
[32m[20221208 15:03:51 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:03:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.77
[32m[20221208 15:03:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.84
[32m[20221208 15:03:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.21
[32m[20221208 15:03:51 @agent_ppo2.py:137][0m Total time:      27.28 min
[32m[20221208 15:03:51 @agent_ppo2.py:139][0m 2199552 total steps have happened
[32m[20221208 15:03:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221208 15:03:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |           0.0588 |           3.2736 |       -1000.4748 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |           0.0287 |           2.7025 |        -558.2475 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |           0.0008 |           2.5007 |        -631.6631 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0137 |           2.4359 |        -700.5879 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0233 |           2.3566 |        -716.8733 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0336 |           2.2978 |        -748.7174 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0366 |           2.2700 |        -758.7223 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0375 |           2.2281 |        -776.2407 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0413 |           2.1616 |        -800.5039 |
[32m[20221208 15:03:52 @agent_ppo2.py:179][0m |          -0.0423 |           2.1561 |        -826.6258 |
[32m[20221208 15:03:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:03:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.32
[32m[20221208 15:03:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.39
[32m[20221208 15:03:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.22
[32m[20221208 15:03:53 @agent_ppo2.py:137][0m Total time:      27.31 min
[32m[20221208 15:03:53 @agent_ppo2.py:139][0m 2201600 total steps have happened
[32m[20221208 15:03:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221208 15:03:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:53 @agent_ppo2.py:179][0m |           0.0553 |           2.1222 |       -1182.3323 |
[32m[20221208 15:03:53 @agent_ppo2.py:179][0m |           0.0495 |           1.6997 |        -801.7774 |
[32m[20221208 15:03:53 @agent_ppo2.py:179][0m |           0.0193 |           1.5801 |        -883.0980 |
[32m[20221208 15:03:53 @agent_ppo2.py:179][0m |           0.0072 |           1.5096 |        -813.3157 |
[32m[20221208 15:03:54 @agent_ppo2.py:179][0m |           0.0163 |           1.4534 |        -750.7404 |
[32m[20221208 15:03:54 @agent_ppo2.py:179][0m |           0.0199 |           1.4221 |        -578.3633 |
[32m[20221208 15:03:54 @agent_ppo2.py:179][0m |          -0.0201 |           1.3928 |        -942.0739 |
[32m[20221208 15:03:54 @agent_ppo2.py:179][0m |          -0.0123 |           1.3950 |        -896.9272 |
[32m[20221208 15:03:54 @agent_ppo2.py:179][0m |          -0.0118 |           1.3756 |        -926.0085 |
[32m[20221208 15:03:54 @agent_ppo2.py:179][0m |          -0.0034 |           1.3601 |        -824.4816 |
[32m[20221208 15:03:54 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:03:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.38
[32m[20221208 15:03:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.73
[32m[20221208 15:03:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.24
[32m[20221208 15:03:54 @agent_ppo2.py:137][0m Total time:      27.33 min
[32m[20221208 15:03:54 @agent_ppo2.py:139][0m 2203648 total steps have happened
[32m[20221208 15:03:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221208 15:03:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:03:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |           0.7519 |           1.9077 |        -810.9007 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |           0.1225 |           1.4317 |        -296.9970 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |           0.0677 |           1.2530 |        -367.8067 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |           0.0258 |           1.1555 |        -431.8078 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |          -0.0040 |           1.0963 |        -483.4836 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |          -0.0125 |           1.0526 |        -499.9644 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |          -0.0222 |           1.0190 |        -529.2991 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |          -0.0314 |           0.9920 |        -547.0658 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |          -0.0353 |           0.9679 |        -571.3008 |
[32m[20221208 15:03:55 @agent_ppo2.py:179][0m |          -0.0435 |           0.9465 |        -575.3022 |
[32m[20221208 15:03:55 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:03:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.86
[32m[20221208 15:03:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.29
[32m[20221208 15:03:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.62
[32m[20221208 15:03:56 @agent_ppo2.py:137][0m Total time:      27.36 min
[32m[20221208 15:03:56 @agent_ppo2.py:139][0m 2205696 total steps have happened
[32m[20221208 15:03:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221208 15:03:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:56 @agent_ppo2.py:179][0m |           0.0843 |           1.8533 |       -1323.3279 |
[32m[20221208 15:03:56 @agent_ppo2.py:179][0m |           0.0812 |           1.5104 |        -956.6253 |
[32m[20221208 15:03:56 @agent_ppo2.py:179][0m |           0.0625 |           1.3938 |        -824.9317 |
[32m[20221208 15:03:56 @agent_ppo2.py:179][0m |           0.0197 |           1.3375 |        -910.4609 |
[32m[20221208 15:03:56 @agent_ppo2.py:179][0m |           0.0163 |           1.2927 |        -989.8189 |
[32m[20221208 15:03:57 @agent_ppo2.py:179][0m |           0.0077 |           1.2626 |        -970.4151 |
[32m[20221208 15:03:57 @agent_ppo2.py:179][0m |          -0.0161 |           1.2250 |       -1111.0231 |
[32m[20221208 15:03:57 @agent_ppo2.py:179][0m |          -0.0001 |           1.2048 |       -1048.4367 |
[32m[20221208 15:03:57 @agent_ppo2.py:179][0m |          -0.0280 |           1.1896 |       -1150.7851 |
[32m[20221208 15:03:57 @agent_ppo2.py:179][0m |          -0.0111 |           1.1743 |       -1159.9944 |
[32m[20221208 15:03:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.65
[32m[20221208 15:03:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.82
[32m[20221208 15:03:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.53
[32m[20221208 15:03:57 @agent_ppo2.py:137][0m Total time:      27.38 min
[32m[20221208 15:03:57 @agent_ppo2.py:139][0m 2207744 total steps have happened
[32m[20221208 15:03:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221208 15:03:58 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |           0.0744 |           2.5161 |       -1217.5813 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |           0.0683 |           2.0901 |        -993.0511 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |           0.0538 |           1.9444 |        -857.6240 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |           0.0012 |           1.8634 |        -992.2092 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |          -0.0037 |           1.8240 |       -1120.1478 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |          -0.0350 |           1.7758 |       -1200.4998 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |          -0.0363 |           1.7419 |       -1169.7977 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |          -0.0490 |           1.7096 |       -1197.7257 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |          -0.0518 |           1.6857 |       -1229.6006 |
[32m[20221208 15:03:58 @agent_ppo2.py:179][0m |          -0.0471 |           1.6876 |       -1230.0821 |
[32m[20221208 15:03:58 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:03:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.54
[32m[20221208 15:03:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.79
[32m[20221208 15:03:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.69
[32m[20221208 15:03:59 @agent_ppo2.py:137][0m Total time:      27.41 min
[32m[20221208 15:03:59 @agent_ppo2.py:139][0m 2209792 total steps have happened
[32m[20221208 15:03:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221208 15:03:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:03:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:03:59 @agent_ppo2.py:179][0m |           0.2093 |           2.6448 |        -804.5691 |
[32m[20221208 15:03:59 @agent_ppo2.py:179][0m |           0.3196 |           2.3165 |        -258.3219 |
[32m[20221208 15:03:59 @agent_ppo2.py:179][0m |           0.0993 |           2.2134 |        -261.7653 |
[32m[20221208 15:03:59 @agent_ppo2.py:179][0m |           0.0708 |           2.1413 |        -326.8925 |
[32m[20221208 15:03:59 @agent_ppo2.py:179][0m |           0.0430 |           2.1015 |        -461.6244 |
[32m[20221208 15:04:00 @agent_ppo2.py:179][0m |           0.0447 |           2.0481 |        -517.5782 |
[32m[20221208 15:04:00 @agent_ppo2.py:179][0m |           0.0153 |           2.0485 |        -628.2658 |
[32m[20221208 15:04:00 @agent_ppo2.py:179][0m |           0.0416 |           2.0105 |        -599.6508 |
[32m[20221208 15:04:00 @agent_ppo2.py:179][0m |           0.0257 |           1.9966 |        -540.1199 |
[32m[20221208 15:04:00 @agent_ppo2.py:179][0m |           0.0250 |           1.9812 |        -510.6529 |
[32m[20221208 15:04:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.88
[32m[20221208 15:04:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.38
[32m[20221208 15:04:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.40
[32m[20221208 15:04:00 @agent_ppo2.py:137][0m Total time:      27.43 min
[32m[20221208 15:04:00 @agent_ppo2.py:139][0m 2211840 total steps have happened
[32m[20221208 15:04:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221208 15:04:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |           0.0832 |           2.9171 |       -1041.3329 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |           0.0238 |           2.6023 |       -1077.5915 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |           0.0076 |           2.4869 |        -786.4325 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0142 |           2.4310 |        -704.6265 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0314 |           2.3638 |        -724.6392 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0405 |           2.3213 |        -734.4025 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0366 |           2.2889 |        -743.7901 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0481 |           2.2614 |        -777.8414 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0548 |           2.2211 |        -914.0620 |
[32m[20221208 15:04:01 @agent_ppo2.py:179][0m |          -0.0696 |           2.2204 |       -1141.2934 |
[32m[20221208 15:04:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.60
[32m[20221208 15:04:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.43
[32m[20221208 15:04:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.50
[32m[20221208 15:04:02 @agent_ppo2.py:137][0m Total time:      27.46 min
[32m[20221208 15:04:02 @agent_ppo2.py:139][0m 2213888 total steps have happened
[32m[20221208 15:04:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221208 15:04:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:02 @agent_ppo2.py:179][0m |           0.0931 |           2.3289 |       -1071.2125 |
[32m[20221208 15:04:02 @agent_ppo2.py:179][0m |           0.0520 |           1.7683 |        -946.6336 |
[32m[20221208 15:04:02 @agent_ppo2.py:179][0m |           0.0779 |           1.5865 |        -626.4176 |
[32m[20221208 15:04:02 @agent_ppo2.py:179][0m |           0.0094 |           1.5026 |        -690.4169 |
[32m[20221208 15:04:02 @agent_ppo2.py:179][0m |          -0.0081 |           1.4171 |        -711.3916 |
[32m[20221208 15:04:02 @agent_ppo2.py:179][0m |          -0.0237 |           1.3636 |        -761.1950 |
[32m[20221208 15:04:03 @agent_ppo2.py:179][0m |          -0.0359 |           1.3388 |        -813.9851 |
[32m[20221208 15:04:03 @agent_ppo2.py:179][0m |          -0.0482 |           1.3066 |        -928.9141 |
[32m[20221208 15:04:03 @agent_ppo2.py:179][0m |          -0.0559 |           1.2632 |       -1061.2908 |
[32m[20221208 15:04:03 @agent_ppo2.py:179][0m |          -0.0561 |           1.2473 |       -1089.6241 |
[32m[20221208 15:04:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.44
[32m[20221208 15:04:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.37
[32m[20221208 15:04:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.18
[32m[20221208 15:04:03 @agent_ppo2.py:137][0m Total time:      27.48 min
[32m[20221208 15:04:03 @agent_ppo2.py:139][0m 2215936 total steps have happened
[32m[20221208 15:04:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221208 15:04:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |           0.1619 |           2.2546 |        -894.3423 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |           0.1517 |           1.6274 |        -585.5070 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |           0.1070 |           1.4799 |        -557.5736 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |           0.0282 |           1.3766 |        -813.8294 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |           0.0039 |           1.3128 |        -916.6554 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |          -0.0265 |           1.2876 |       -1008.0662 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |          -0.0295 |           1.2412 |       -1041.9507 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |          -0.0320 |           1.2150 |       -1010.4628 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |          -0.0523 |           1.1962 |       -1079.2112 |
[32m[20221208 15:04:04 @agent_ppo2.py:179][0m |          -0.0539 |           1.1907 |       -1091.5731 |
[32m[20221208 15:04:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.76
[32m[20221208 15:04:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.61
[32m[20221208 15:04:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.13
[32m[20221208 15:04:05 @agent_ppo2.py:137][0m Total time:      27.51 min
[32m[20221208 15:04:05 @agent_ppo2.py:139][0m 2217984 total steps have happened
[32m[20221208 15:04:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221208 15:04:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:05 @agent_ppo2.py:179][0m |           0.0663 |           2.3830 |       -1085.8540 |
[32m[20221208 15:04:05 @agent_ppo2.py:179][0m |           0.0344 |           1.6895 |       -1000.7460 |
[32m[20221208 15:04:05 @agent_ppo2.py:179][0m |           0.0125 |           1.4831 |        -971.0066 |
[32m[20221208 15:04:05 @agent_ppo2.py:179][0m |          -0.0041 |           1.3726 |       -1076.0392 |
[32m[20221208 15:04:05 @agent_ppo2.py:179][0m |          -0.0052 |           1.3155 |       -1019.8316 |
[32m[20221208 15:04:05 @agent_ppo2.py:179][0m |          -0.0164 |           1.2594 |       -1057.2242 |
[32m[20221208 15:04:06 @agent_ppo2.py:179][0m |          -0.0297 |           1.2368 |       -1111.5348 |
[32m[20221208 15:04:06 @agent_ppo2.py:179][0m |          -0.0182 |           1.1834 |       -1068.6365 |
[32m[20221208 15:04:06 @agent_ppo2.py:179][0m |          -0.0201 |           1.1481 |       -1071.1707 |
[32m[20221208 15:04:06 @agent_ppo2.py:179][0m |          -0.0165 |           1.1380 |       -1023.3444 |
[32m[20221208 15:04:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.81
[32m[20221208 15:04:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.57
[32m[20221208 15:04:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.91
[32m[20221208 15:04:06 @agent_ppo2.py:137][0m Total time:      27.53 min
[32m[20221208 15:04:06 @agent_ppo2.py:139][0m 2220032 total steps have happened
[32m[20221208 15:04:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221208 15:04:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |           0.1014 |           4.6636 |        -974.2862 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |           0.0587 |           3.7369 |        -834.1512 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |           0.0009 |           3.4230 |        -926.9540 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0245 |           3.3092 |        -980.7512 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0421 |           3.1801 |        -981.2272 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0606 |           3.0790 |       -1019.2686 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0687 |           3.0338 |       -1039.8937 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0731 |           3.0080 |       -1038.3125 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0795 |           2.9201 |       -1049.8757 |
[32m[20221208 15:04:07 @agent_ppo2.py:179][0m |          -0.0870 |           2.8905 |       -1069.4329 |
[32m[20221208 15:04:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.85
[32m[20221208 15:04:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.53
[32m[20221208 15:04:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.02
[32m[20221208 15:04:08 @agent_ppo2.py:137][0m Total time:      27.56 min
[32m[20221208 15:04:08 @agent_ppo2.py:139][0m 2222080 total steps have happened
[32m[20221208 15:04:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221208 15:04:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |           0.0689 |           2.1748 |       -1015.5255 |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |           0.0257 |           1.6646 |        -999.7685 |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |           0.0168 |           1.4820 |        -849.9531 |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |          -0.0062 |           1.3957 |        -969.5727 |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |          -0.0184 |           1.3340 |        -986.1658 |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |          -0.0296 |           1.2842 |       -1027.9081 |
[32m[20221208 15:04:08 @agent_ppo2.py:179][0m |          -0.0420 |           1.2349 |       -1084.9367 |
[32m[20221208 15:04:09 @agent_ppo2.py:179][0m |          -0.0296 |           1.2193 |       -1030.2231 |
[32m[20221208 15:04:09 @agent_ppo2.py:179][0m |          -0.0217 |           1.1965 |       -1038.7280 |
[32m[20221208 15:04:09 @agent_ppo2.py:179][0m |          -0.0248 |           1.1712 |        -992.4404 |
[32m[20221208 15:04:09 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.02
[32m[20221208 15:04:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.24
[32m[20221208 15:04:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.04
[32m[20221208 15:04:09 @agent_ppo2.py:137][0m Total time:      27.58 min
[32m[20221208 15:04:09 @agent_ppo2.py:139][0m 2224128 total steps have happened
[32m[20221208 15:04:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221208 15:04:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |           0.0878 |           1.8774 |        -813.7203 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |           0.0283 |           1.6517 |        -681.9199 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |           0.0173 |           1.5777 |        -688.8098 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0208 |           1.5270 |        -759.0343 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0045 |           1.5115 |        -764.0183 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0260 |           1.4530 |        -791.3959 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0393 |           1.4400 |        -840.2521 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0301 |           1.4187 |        -805.4915 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0429 |           1.3974 |        -819.6429 |
[32m[20221208 15:04:10 @agent_ppo2.py:179][0m |          -0.0406 |           1.3849 |        -845.8573 |
[32m[20221208 15:04:10 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.45
[32m[20221208 15:04:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 64.55
[32m[20221208 15:04:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.56
[32m[20221208 15:04:11 @agent_ppo2.py:137][0m Total time:      27.61 min
[32m[20221208 15:04:11 @agent_ppo2.py:139][0m 2226176 total steps have happened
[32m[20221208 15:04:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221208 15:04:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |           0.0716 |           1.9629 |        -698.4281 |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |          -0.0092 |           1.7588 |        -654.8298 |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |          -0.0401 |           1.7222 |        -714.2662 |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |          -0.0538 |           1.6740 |        -721.3508 |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |          -0.0589 |           1.6469 |        -758.4901 |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |          -0.0586 |           1.6439 |        -753.5984 |
[32m[20221208 15:04:11 @agent_ppo2.py:179][0m |          -0.0404 |           1.6140 |        -671.9376 |
[32m[20221208 15:04:12 @agent_ppo2.py:179][0m |          -0.0647 |           1.5821 |        -727.3497 |
[32m[20221208 15:04:12 @agent_ppo2.py:179][0m |          -0.0682 |           1.5973 |        -743.1889 |
[32m[20221208 15:04:12 @agent_ppo2.py:179][0m |          -0.0772 |           1.5573 |        -779.7210 |
[32m[20221208 15:04:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.89
[32m[20221208 15:04:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.46
[32m[20221208 15:04:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.61
[32m[20221208 15:04:12 @agent_ppo2.py:137][0m Total time:      27.63 min
[32m[20221208 15:04:12 @agent_ppo2.py:139][0m 2228224 total steps have happened
[32m[20221208 15:04:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221208 15:04:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |           0.1060 |           1.3833 |        -737.1674 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |           0.0260 |           1.2031 |        -563.5937 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |           0.0186 |           1.1754 |        -620.5856 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |           0.0044 |           1.1640 |        -643.0542 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |          -0.0138 |           1.1455 |        -647.5159 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |          -0.0196 |           1.1427 |        -689.0193 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |          -0.0244 |           1.1276 |        -705.4360 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |          -0.0307 |           1.1380 |        -723.6653 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |          -0.0251 |           1.1294 |        -730.6360 |
[32m[20221208 15:04:13 @agent_ppo2.py:179][0m |           0.0473 |           1.1270 |        -569.2079 |
[32m[20221208 15:04:13 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.35
[32m[20221208 15:04:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.78
[32m[20221208 15:04:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.24
[32m[20221208 15:04:14 @agent_ppo2.py:137][0m Total time:      27.65 min
[32m[20221208 15:04:14 @agent_ppo2.py:139][0m 2230272 total steps have happened
[32m[20221208 15:04:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221208 15:04:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |           0.0784 |           2.5370 |       -1050.3723 |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |           0.1156 |           2.0021 |        -807.1266 |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |           0.0650 |           1.8592 |        -801.9766 |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |          -0.0020 |           1.7249 |        -907.2447 |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |          -0.0306 |           1.6397 |        -945.6584 |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |          -0.0472 |           1.5906 |        -962.4870 |
[32m[20221208 15:04:14 @agent_ppo2.py:179][0m |          -0.0596 |           1.5363 |        -976.6494 |
[32m[20221208 15:04:15 @agent_ppo2.py:179][0m |          -0.0553 |           1.4971 |        -974.5271 |
[32m[20221208 15:04:15 @agent_ppo2.py:179][0m |          -0.0653 |           1.4693 |        -981.1361 |
[32m[20221208 15:04:15 @agent_ppo2.py:179][0m |          -0.0738 |           1.4576 |       -1013.8292 |
[32m[20221208 15:04:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.20
[32m[20221208 15:04:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.88
[32m[20221208 15:04:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.97
[32m[20221208 15:04:15 @agent_ppo2.py:137][0m Total time:      27.68 min
[32m[20221208 15:04:15 @agent_ppo2.py:139][0m 2232320 total steps have happened
[32m[20221208 15:04:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221208 15:04:15 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:04:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |           0.0716 |           2.3992 |       -1071.5890 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |           0.0619 |           2.1437 |        -781.9904 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0052 |           2.0353 |        -765.7934 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0270 |           1.9460 |        -825.8407 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0455 |           1.9056 |        -846.0052 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0631 |           1.8806 |        -902.3315 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0635 |           1.8377 |        -904.4266 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0540 |           1.8342 |        -867.5883 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0748 |           1.7939 |        -946.1319 |
[32m[20221208 15:04:16 @agent_ppo2.py:179][0m |          -0.0771 |           1.7788 |        -945.6305 |
[32m[20221208 15:04:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.39
[32m[20221208 15:04:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.67
[32m[20221208 15:04:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.17
[32m[20221208 15:04:17 @agent_ppo2.py:137][0m Total time:      27.70 min
[32m[20221208 15:04:17 @agent_ppo2.py:139][0m 2234368 total steps have happened
[32m[20221208 15:04:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221208 15:04:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |           0.0991 |           2.4828 |        -871.3564 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |           0.0617 |           2.1816 |        -533.5018 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |           0.0242 |           2.1006 |        -668.1208 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |          -0.0101 |           2.0591 |        -655.1698 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |          -0.0265 |           2.0126 |        -720.7731 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |          -0.0244 |           1.9748 |        -676.0968 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |          -0.0260 |           1.9544 |        -680.5305 |
[32m[20221208 15:04:17 @agent_ppo2.py:179][0m |          -0.0508 |           1.9367 |        -773.2375 |
[32m[20221208 15:04:18 @agent_ppo2.py:179][0m |          -0.0669 |           1.9153 |        -799.7706 |
[32m[20221208 15:04:18 @agent_ppo2.py:179][0m |          -0.0618 |           1.9174 |        -791.4731 |
[32m[20221208 15:04:18 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.80
[32m[20221208 15:04:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.42
[32m[20221208 15:04:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.97
[32m[20221208 15:04:18 @agent_ppo2.py:137][0m Total time:      27.73 min
[32m[20221208 15:04:18 @agent_ppo2.py:139][0m 2236416 total steps have happened
[32m[20221208 15:04:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221208 15:04:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:04:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |           0.0881 |           2.0416 |        -889.6585 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |           0.0917 |           1.8302 |        -465.9667 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |           0.0352 |           1.7668 |        -818.0136 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |           0.0090 |           1.7590 |        -866.5026 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |          -0.0133 |           1.7443 |        -956.9825 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |          -0.0155 |           1.6939 |        -928.5530 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |          -0.0299 |           1.6712 |       -1010.7175 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |          -0.0334 |           1.6537 |       -1005.5759 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |          -0.0325 |           1.6617 |        -980.9578 |
[32m[20221208 15:04:19 @agent_ppo2.py:179][0m |          -0.0285 |           1.6281 |       -1009.7334 |
[32m[20221208 15:04:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.72
[32m[20221208 15:04:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.52
[32m[20221208 15:04:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.16
[32m[20221208 15:04:20 @agent_ppo2.py:137][0m Total time:      27.75 min
[32m[20221208 15:04:20 @agent_ppo2.py:139][0m 2238464 total steps have happened
[32m[20221208 15:04:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221208 15:04:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |           0.0985 |           2.3619 |        -914.4612 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |           0.1381 |           2.0964 |        -674.2951 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |           0.0159 |           1.9902 |        -541.1420 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |          -0.0158 |           1.9248 |        -555.1266 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |          -0.0398 |           1.9076 |        -580.6642 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |          -0.0487 |           1.8645 |        -601.4892 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |          -0.0573 |           1.8272 |        -634.8504 |
[32m[20221208 15:04:20 @agent_ppo2.py:179][0m |          -0.0636 |           1.8079 |        -643.2811 |
[32m[20221208 15:04:21 @agent_ppo2.py:179][0m |          -0.0676 |           1.7956 |        -670.7501 |
[32m[20221208 15:04:21 @agent_ppo2.py:179][0m |          -0.0670 |           1.7732 |        -673.1545 |
[32m[20221208 15:04:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.05
[32m[20221208 15:04:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.53
[32m[20221208 15:04:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.76
[32m[20221208 15:04:21 @agent_ppo2.py:137][0m Total time:      27.78 min
[32m[20221208 15:04:21 @agent_ppo2.py:139][0m 2240512 total steps have happened
[32m[20221208 15:04:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221208 15:04:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |           0.2542 |           3.1887 |        -937.3923 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |           0.0695 |           2.7836 |        -783.2288 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |           0.0046 |           2.6548 |        -804.6063 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0353 |           2.5732 |        -891.5734 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0519 |           2.5216 |        -920.7886 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0662 |           2.4745 |        -951.2902 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0726 |           2.4351 |        -972.4873 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0731 |           2.4119 |        -974.2406 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0768 |           2.3633 |        -974.8221 |
[32m[20221208 15:04:22 @agent_ppo2.py:179][0m |          -0.0801 |           2.3343 |        -985.7076 |
[32m[20221208 15:04:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.34
[32m[20221208 15:04:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.03
[32m[20221208 15:04:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.58
[32m[20221208 15:04:22 @agent_ppo2.py:137][0m Total time:      27.80 min
[32m[20221208 15:04:22 @agent_ppo2.py:139][0m 2242560 total steps have happened
[32m[20221208 15:04:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221208 15:04:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |           0.0605 |           2.0721 |        -969.5511 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |           0.0419 |           1.8302 |        -783.0280 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |           0.0101 |           1.7281 |        -799.0834 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |          -0.0030 |           1.6588 |        -845.7422 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |          -0.0269 |           1.6210 |        -876.4560 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |          -0.0307 |           1.5776 |        -829.6827 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |          -0.0416 |           1.5444 |        -814.0127 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |          -0.0414 |           1.5030 |        -818.6147 |
[32m[20221208 15:04:23 @agent_ppo2.py:179][0m |          -0.0559 |           1.4819 |        -877.9543 |
[32m[20221208 15:04:24 @agent_ppo2.py:179][0m |          -0.0542 |           1.4414 |        -935.7406 |
[32m[20221208 15:04:24 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.13
[32m[20221208 15:04:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.61
[32m[20221208 15:04:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.20
[32m[20221208 15:04:24 @agent_ppo2.py:137][0m Total time:      27.83 min
[32m[20221208 15:04:24 @agent_ppo2.py:139][0m 2244608 total steps have happened
[32m[20221208 15:04:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221208 15:04:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.1549 |           1.0520 |        -658.9835 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.1478 |           0.9791 |        -198.1507 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.1030 |           0.9595 |        -203.7048 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0858 |           0.9319 |        -245.0742 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0665 |           0.9211 |        -299.6992 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0496 |           0.9229 |        -473.2641 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0891 |           0.9256 |        -287.0426 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0707 |           0.9236 |        -309.1817 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0172 |           0.9040 |        -509.9141 |
[32m[20221208 15:04:25 @agent_ppo2.py:179][0m |           0.0048 |           0.9007 |        -587.2342 |
[32m[20221208 15:04:25 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.28
[32m[20221208 15:04:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.18
[32m[20221208 15:04:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.24
[32m[20221208 15:04:25 @agent_ppo2.py:137][0m Total time:      27.85 min
[32m[20221208 15:04:25 @agent_ppo2.py:139][0m 2246656 total steps have happened
[32m[20221208 15:04:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221208 15:04:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |           0.1379 |           1.8956 |        -855.6519 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |           0.0812 |           1.4199 |        -779.0152 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |           0.0210 |           1.3201 |        -829.8600 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |          -0.0054 |           1.2744 |        -854.0782 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |          -0.0096 |           1.2512 |        -859.6210 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |          -0.0123 |           1.2166 |        -737.1294 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |          -0.0012 |           1.2068 |        -499.4857 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |          -0.0209 |           1.1898 |        -616.6092 |
[32m[20221208 15:04:26 @agent_ppo2.py:179][0m |          -0.0217 |           1.1925 |        -549.4952 |
[32m[20221208 15:04:27 @agent_ppo2.py:179][0m |          -0.0270 |           1.1647 |        -565.5474 |
[32m[20221208 15:04:27 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.23
[32m[20221208 15:04:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.15
[32m[20221208 15:04:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.43
[32m[20221208 15:04:27 @agent_ppo2.py:137][0m Total time:      27.88 min
[32m[20221208 15:04:27 @agent_ppo2.py:139][0m 2248704 total steps have happened
[32m[20221208 15:04:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221208 15:04:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:27 @agent_ppo2.py:179][0m |           0.1658 |           2.8646 |        -844.1969 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |           0.0983 |           2.3315 |        -538.1819 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |           0.0151 |           2.1715 |        -686.3335 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0200 |           2.0551 |        -739.9637 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0382 |           1.9827 |        -767.9798 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0544 |           1.9273 |        -799.7834 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0598 |           1.8909 |        -806.2257 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0722 |           1.8458 |        -828.2199 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0763 |           1.7988 |        -833.9166 |
[32m[20221208 15:04:28 @agent_ppo2.py:179][0m |          -0.0762 |           1.7644 |        -827.2488 |
[32m[20221208 15:04:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.85
[32m[20221208 15:04:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.25
[32m[20221208 15:04:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.68
[32m[20221208 15:04:28 @agent_ppo2.py:137][0m Total time:      27.90 min
[32m[20221208 15:04:28 @agent_ppo2.py:139][0m 2250752 total steps have happened
[32m[20221208 15:04:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221208 15:04:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |           0.1163 |           2.0099 |        -664.0688 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |           0.0638 |           1.5817 |        -391.6737 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |           0.0382 |           1.5137 |        -419.6620 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |           0.0097 |           1.4673 |        -437.4671 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |          -0.0073 |           1.4211 |        -522.3730 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |          -0.0112 |           1.3919 |        -560.0003 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |          -0.0143 |           1.4001 |        -547.6509 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |          -0.0279 |           1.3840 |        -621.6434 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |          -0.0284 |           1.3762 |        -615.8151 |
[32m[20221208 15:04:29 @agent_ppo2.py:179][0m |          -0.0313 |           1.3566 |        -692.1854 |
[32m[20221208 15:04:29 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.22
[32m[20221208 15:04:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.32
[32m[20221208 15:04:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.19
[32m[20221208 15:04:30 @agent_ppo2.py:137][0m Total time:      27.93 min
[32m[20221208 15:04:30 @agent_ppo2.py:139][0m 2252800 total steps have happened
[32m[20221208 15:04:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221208 15:04:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:30 @agent_ppo2.py:179][0m |           0.1092 |           3.3210 |        -930.2776 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |           0.0592 |           2.8154 |        -674.6625 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |           0.0180 |           2.5826 |        -779.2739 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0109 |           2.3850 |        -949.3587 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0303 |           2.2592 |        -991.2776 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0353 |           2.1751 |        -999.9730 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0444 |           2.1115 |       -1030.7164 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0437 |           2.0493 |       -1019.4360 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0465 |           1.9681 |       -1007.4681 |
[32m[20221208 15:04:31 @agent_ppo2.py:179][0m |          -0.0544 |           1.9423 |       -1045.3640 |
[32m[20221208 15:04:31 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.84
[32m[20221208 15:04:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.72
[32m[20221208 15:04:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.71
[32m[20221208 15:04:31 @agent_ppo2.py:137][0m Total time:      27.95 min
[32m[20221208 15:04:31 @agent_ppo2.py:139][0m 2254848 total steps have happened
[32m[20221208 15:04:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221208 15:04:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1813 |           1.0015 |        -345.9079 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1724 |           0.9325 |         -35.3502 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1490 |           0.9050 |         -45.8317 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1369 |           0.8798 |         -46.6021 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1310 |           0.8646 |         -58.5495 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1280 |           0.8510 |         -62.9046 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1254 |           0.8441 |         -71.2563 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1214 |           0.8370 |         -75.8947 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1199 |           0.8268 |         -80.9048 |
[32m[20221208 15:04:32 @agent_ppo2.py:179][0m |           0.1180 |           0.8239 |         -83.7475 |
[32m[20221208 15:04:32 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:04:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.16
[32m[20221208 15:04:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.78
[32m[20221208 15:04:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.93
[32m[20221208 15:04:33 @agent_ppo2.py:137][0m Total time:      27.98 min
[32m[20221208 15:04:33 @agent_ppo2.py:139][0m 2256896 total steps have happened
[32m[20221208 15:04:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221208 15:04:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:33 @agent_ppo2.py:179][0m |           0.0854 |           1.5064 |        -514.4300 |
[32m[20221208 15:04:33 @agent_ppo2.py:179][0m |           0.0343 |           1.2212 |        -235.7262 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0172 |           1.1465 |        -291.4591 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0318 |           1.1037 |        -308.9506 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0489 |           1.0784 |        -336.9904 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0527 |           1.0522 |        -341.6429 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0582 |           1.0375 |        -354.0839 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0654 |           1.0171 |        -361.0235 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0712 |           1.0041 |        -366.4054 |
[32m[20221208 15:04:34 @agent_ppo2.py:179][0m |          -0.0702 |           1.0083 |        -362.6433 |
[32m[20221208 15:04:34 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.81
[32m[20221208 15:04:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.14
[32m[20221208 15:04:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.77
[32m[20221208 15:04:34 @agent_ppo2.py:137][0m Total time:      28.00 min
[32m[20221208 15:04:34 @agent_ppo2.py:139][0m 2258944 total steps have happened
[32m[20221208 15:04:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221208 15:04:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1307 |           1.5274 |        -739.5956 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1977 |           1.2562 |        -299.1237 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1352 |           1.2120 |        -166.1478 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.0786 |           1.1702 |        -301.6112 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1137 |           1.1460 |        -247.3466 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1145 |           1.1278 |        -260.0467 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1407 |           1.1362 |        -194.7668 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.1075 |           1.0948 |        -205.9659 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.0985 |           1.0890 |        -218.2964 |
[32m[20221208 15:04:35 @agent_ppo2.py:179][0m |           0.0905 |           1.0768 |        -238.6185 |
[32m[20221208 15:04:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:04:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.88
[32m[20221208 15:04:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.74
[32m[20221208 15:04:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.56
[32m[20221208 15:04:36 @agent_ppo2.py:137][0m Total time:      28.03 min
[32m[20221208 15:04:36 @agent_ppo2.py:139][0m 2260992 total steps have happened
[32m[20221208 15:04:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221208 15:04:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:36 @agent_ppo2.py:179][0m |           0.0719 |           1.3480 |        -600.1949 |
[32m[20221208 15:04:36 @agent_ppo2.py:179][0m |           0.0101 |           1.1430 |        -408.0420 |
[32m[20221208 15:04:36 @agent_ppo2.py:179][0m |          -0.0265 |           1.1034 |        -461.4788 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0146 |           1.1054 |        -440.6584 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0420 |           1.0967 |        -486.2743 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0426 |           1.0777 |        -486.8449 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0489 |           1.0932 |        -487.1289 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0574 |           1.0675 |        -515.5537 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0523 |           1.0626 |        -520.7977 |
[32m[20221208 15:04:37 @agent_ppo2.py:179][0m |          -0.0551 |           1.0824 |        -518.7331 |
[32m[20221208 15:04:37 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:04:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.51
[32m[20221208 15:04:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.20
[32m[20221208 15:04:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.29
[32m[20221208 15:04:37 @agent_ppo2.py:137][0m Total time:      28.05 min
[32m[20221208 15:04:37 @agent_ppo2.py:139][0m 2263040 total steps have happened
[32m[20221208 15:04:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221208 15:04:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |           0.0632 |           1.1946 |        -676.8378 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |           0.0091 |           1.0537 |        -456.5942 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |           0.0004 |           1.0143 |        -449.1981 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0311 |           1.0042 |        -517.9301 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0375 |           0.9896 |        -532.1022 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0278 |           0.9925 |        -482.8535 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0220 |           0.9793 |        -503.2004 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0356 |           0.9779 |        -543.4653 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0432 |           0.9787 |        -561.4525 |
[32m[20221208 15:04:38 @agent_ppo2.py:179][0m |          -0.0419 |           0.9727 |        -551.9921 |
[32m[20221208 15:04:38 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.29
[32m[20221208 15:04:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.62
[32m[20221208 15:04:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.59
[32m[20221208 15:04:39 @agent_ppo2.py:137][0m Total time:      28.07 min
[32m[20221208 15:04:39 @agent_ppo2.py:139][0m 2265088 total steps have happened
[32m[20221208 15:04:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221208 15:04:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:39 @agent_ppo2.py:179][0m |           0.0629 |           1.1382 |        -750.6576 |
[32m[20221208 15:04:39 @agent_ppo2.py:179][0m |           0.0108 |           0.9750 |        -421.6758 |
[32m[20221208 15:04:39 @agent_ppo2.py:179][0m |          -0.0177 |           0.9594 |        -472.4959 |
[32m[20221208 15:04:39 @agent_ppo2.py:179][0m |          -0.0257 |           0.9416 |        -493.1674 |
[32m[20221208 15:04:39 @agent_ppo2.py:179][0m |          -0.0130 |           0.9275 |        -501.1023 |
[32m[20221208 15:04:40 @agent_ppo2.py:179][0m |          -0.0205 |           0.9223 |        -499.0256 |
[32m[20221208 15:04:40 @agent_ppo2.py:179][0m |          -0.0315 |           0.9393 |        -536.0355 |
[32m[20221208 15:04:40 @agent_ppo2.py:179][0m |          -0.0292 |           0.9287 |        -527.0080 |
[32m[20221208 15:04:40 @agent_ppo2.py:179][0m |          -0.0395 |           0.9202 |        -547.9253 |
[32m[20221208 15:04:40 @agent_ppo2.py:179][0m |          -0.0289 |           0.9159 |        -531.2628 |
[32m[20221208 15:04:40 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.84
[32m[20221208 15:04:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.40
[32m[20221208 15:04:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.43
[32m[20221208 15:04:40 @agent_ppo2.py:137][0m Total time:      28.10 min
[32m[20221208 15:04:40 @agent_ppo2.py:139][0m 2267136 total steps have happened
[32m[20221208 15:04:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221208 15:04:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |           0.1052 |           1.1204 |        -635.5496 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |           0.0298 |           1.0234 |        -351.2138 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0147 |           1.0100 |        -410.8925 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0090 |           0.9963 |        -418.5480 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0271 |           0.9875 |        -444.4354 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0382 |           0.9848 |        -471.0163 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0203 |           0.9734 |        -470.7779 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0234 |           0.9756 |        -451.7865 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0424 |           0.9556 |        -489.9109 |
[32m[20221208 15:04:41 @agent_ppo2.py:179][0m |          -0.0508 |           0.9605 |        -494.3471 |
[32m[20221208 15:04:41 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.59
[32m[20221208 15:04:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.32
[32m[20221208 15:04:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.08
[32m[20221208 15:04:42 @agent_ppo2.py:137][0m Total time:      28.12 min
[32m[20221208 15:04:42 @agent_ppo2.py:139][0m 2269184 total steps have happened
[32m[20221208 15:04:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221208 15:04:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:42 @agent_ppo2.py:179][0m |           0.1046 |           0.9218 |        -622.8873 |
[32m[20221208 15:04:42 @agent_ppo2.py:179][0m |           0.0602 |           0.8911 |        -320.8309 |
[32m[20221208 15:04:42 @agent_ppo2.py:179][0m |           0.0780 |           0.8880 |        -302.4328 |
[32m[20221208 15:04:42 @agent_ppo2.py:179][0m |           0.0549 |           0.8852 |        -319.3854 |
[32m[20221208 15:04:42 @agent_ppo2.py:179][0m |           0.0533 |           0.8828 |        -324.0001 |
[32m[20221208 15:04:42 @agent_ppo2.py:179][0m |           0.0624 |           0.8909 |        -319.9756 |
[32m[20221208 15:04:43 @agent_ppo2.py:179][0m |           0.0340 |           0.8721 |        -344.2026 |
[32m[20221208 15:04:43 @agent_ppo2.py:179][0m |           0.0547 |           0.8745 |        -325.6609 |
[32m[20221208 15:04:43 @agent_ppo2.py:179][0m |           0.0465 |           0.8864 |        -343.7088 |
[32m[20221208 15:04:43 @agent_ppo2.py:179][0m |           0.0416 |           0.8662 |        -340.8824 |
[32m[20221208 15:04:43 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:04:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.77
[32m[20221208 15:04:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.21
[32m[20221208 15:04:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.93
[32m[20221208 15:04:43 @agent_ppo2.py:137][0m Total time:      28.15 min
[32m[20221208 15:04:43 @agent_ppo2.py:139][0m 2271232 total steps have happened
[32m[20221208 15:04:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221208 15:04:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |           0.1534 |           1.1032 |        -541.2482 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |           0.1204 |           0.9404 |        -153.8120 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |           0.0759 |           0.8971 |        -147.8251 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |           0.0129 |           0.8853 |        -300.0420 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |          -0.0213 |           0.8774 |        -375.5923 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |          -0.0290 |           0.8711 |        -392.8972 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |          -0.0036 |           0.8534 |        -316.2952 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |           0.0000 |           0.8448 |        -286.3185 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |          -0.0372 |           0.8349 |        -388.7442 |
[32m[20221208 15:04:44 @agent_ppo2.py:179][0m |          -0.0475 |           0.8388 |        -402.8768 |
[32m[20221208 15:04:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.34
[32m[20221208 15:04:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.05
[32m[20221208 15:04:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.80
[32m[20221208 15:04:45 @agent_ppo2.py:137][0m Total time:      28.17 min
[32m[20221208 15:04:45 @agent_ppo2.py:139][0m 2273280 total steps have happened
[32m[20221208 15:04:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221208 15:04:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |           0.1498 |           1.3537 |        -601.9978 |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |           0.0167 |           1.0774 |        -328.6528 |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |          -0.0234 |           1.0346 |        -409.2956 |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |          -0.0373 |           0.9791 |        -442.4068 |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |          -0.0457 |           0.9506 |        -438.8600 |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |          -0.0425 |           0.9254 |        -392.1408 |
[32m[20221208 15:04:45 @agent_ppo2.py:179][0m |          -0.0552 |           0.9162 |        -442.4188 |
[32m[20221208 15:04:46 @agent_ppo2.py:179][0m |          -0.0650 |           0.8982 |        -470.7514 |
[32m[20221208 15:04:46 @agent_ppo2.py:179][0m |          -0.0723 |           0.8879 |        -478.2006 |
[32m[20221208 15:04:46 @agent_ppo2.py:179][0m |          -0.0783 |           0.8854 |        -491.0915 |
[32m[20221208 15:04:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.40
[32m[20221208 15:04:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.72
[32m[20221208 15:04:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.07
[32m[20221208 15:04:46 @agent_ppo2.py:137][0m Total time:      28.20 min
[32m[20221208 15:04:46 @agent_ppo2.py:139][0m 2275328 total steps have happened
[32m[20221208 15:04:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221208 15:04:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:04:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |           0.1084 |           2.2462 |        -769.3906 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |           0.1028 |           1.9549 |        -419.5915 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |           0.0757 |           1.8015 |        -336.8783 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |           0.0139 |           1.7212 |        -317.3866 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |          -0.0148 |           1.6748 |        -380.8958 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |          -0.0275 |           1.6039 |        -399.5639 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |          -0.0347 |           1.5352 |        -437.3748 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |          -0.0429 |           1.4882 |        -457.2592 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |          -0.0442 |           1.4601 |        -467.2610 |
[32m[20221208 15:04:47 @agent_ppo2.py:179][0m |          -0.0428 |           1.4062 |        -467.7664 |
[32m[20221208 15:04:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.49
[32m[20221208 15:04:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.53
[32m[20221208 15:04:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.17
[32m[20221208 15:04:48 @agent_ppo2.py:137][0m Total time:      28.22 min
[32m[20221208 15:04:48 @agent_ppo2.py:139][0m 2277376 total steps have happened
[32m[20221208 15:04:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221208 15:04:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |           0.1115 |           2.0189 |        -933.4387 |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |           0.2443 |           1.4455 |        -715.2810 |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |           0.0204 |           1.3288 |        -818.8993 |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |          -0.0136 |           1.2417 |        -894.1002 |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |          -0.0364 |           1.1874 |        -947.7718 |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |          -0.0484 |           1.1449 |        -981.3368 |
[32m[20221208 15:04:48 @agent_ppo2.py:179][0m |          -0.0534 |           1.1109 |        -973.5601 |
[32m[20221208 15:04:49 @agent_ppo2.py:179][0m |          -0.0582 |           1.0891 |       -1005.7216 |
[32m[20221208 15:04:49 @agent_ppo2.py:179][0m |          -0.0651 |           1.0748 |       -1024.6245 |
[32m[20221208 15:04:49 @agent_ppo2.py:179][0m |          -0.0694 |           1.0665 |       -1025.5247 |
[32m[20221208 15:04:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:04:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.75
[32m[20221208 15:04:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.48
[32m[20221208 15:04:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.18
[32m[20221208 15:04:49 @agent_ppo2.py:137][0m Total time:      28.25 min
[32m[20221208 15:04:49 @agent_ppo2.py:139][0m 2279424 total steps have happened
[32m[20221208 15:04:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221208 15:04:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |           0.1245 |           3.2805 |        -983.6979 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |           0.1141 |           2.8315 |        -735.9064 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |           0.0340 |           2.6627 |        -763.7461 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0091 |           2.5331 |        -842.6623 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0236 |           2.4096 |        -883.1320 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0409 |           2.3141 |        -942.0234 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0505 |           2.2942 |        -980.0308 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0537 |           2.2268 |        -995.6198 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0580 |           2.1839 |        -984.1675 |
[32m[20221208 15:04:50 @agent_ppo2.py:179][0m |          -0.0651 |           2.1317 |       -1027.8393 |
[32m[20221208 15:04:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.16
[32m[20221208 15:04:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.32
[32m[20221208 15:04:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.31
[32m[20221208 15:04:51 @agent_ppo2.py:137][0m Total time:      28.27 min
[32m[20221208 15:04:51 @agent_ppo2.py:139][0m 2281472 total steps have happened
[32m[20221208 15:04:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221208 15:04:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.1074 |           1.3896 |        -883.5271 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.4500 |           1.1874 |        -460.6063 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.0682 |           1.0973 |        -451.6143 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.0409 |           1.0497 |        -527.5964 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.0169 |           1.0208 |        -530.6139 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.0057 |           0.9966 |        -555.4035 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |          -0.0135 |           0.9769 |        -621.8779 |
[32m[20221208 15:04:51 @agent_ppo2.py:179][0m |           0.0124 |           0.9777 |        -532.6611 |
[32m[20221208 15:04:52 @agent_ppo2.py:179][0m |           0.0078 |           0.9447 |        -534.7071 |
[32m[20221208 15:04:52 @agent_ppo2.py:179][0m |          -0.0194 |           0.9324 |        -631.6330 |
[32m[20221208 15:04:52 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.33
[32m[20221208 15:04:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.73
[32m[20221208 15:04:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.63
[32m[20221208 15:04:52 @agent_ppo2.py:137][0m Total time:      28.30 min
[32m[20221208 15:04:52 @agent_ppo2.py:139][0m 2283520 total steps have happened
[32m[20221208 15:04:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221208 15:04:52 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:04:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |           0.0617 |           1.4640 |        -745.2599 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |           0.0156 |           1.2372 |        -668.2124 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0269 |           1.1289 |        -702.0394 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0399 |           1.0573 |        -724.0060 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0473 |           1.0269 |        -727.0907 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0604 |           1.0123 |        -750.5760 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0631 |           0.9786 |        -766.1538 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0628 |           0.9869 |        -775.4472 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0697 |           0.9443 |        -786.6641 |
[32m[20221208 15:04:53 @agent_ppo2.py:179][0m |          -0.0713 |           0.9267 |        -790.8175 |
[32m[20221208 15:04:53 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:04:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.88
[32m[20221208 15:04:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.52
[32m[20221208 15:04:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.86
[32m[20221208 15:04:54 @agent_ppo2.py:137][0m Total time:      28.32 min
[32m[20221208 15:04:54 @agent_ppo2.py:139][0m 2285568 total steps have happened
[32m[20221208 15:04:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221208 15:04:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |           0.0860 |           1.8205 |       -1019.9695 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |           0.0475 |           1.4610 |        -869.9656 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |          -0.0019 |           1.4020 |        -864.6877 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |          -0.0071 |           1.3339 |        -843.4334 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |          -0.0405 |           1.2733 |        -865.6248 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |          -0.0510 |           1.2523 |        -896.3804 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |          -0.0601 |           1.2189 |        -909.6263 |
[32m[20221208 15:04:54 @agent_ppo2.py:179][0m |          -0.0657 |           1.1995 |        -927.3174 |
[32m[20221208 15:04:55 @agent_ppo2.py:179][0m |          -0.0674 |           1.1874 |        -934.6482 |
[32m[20221208 15:04:55 @agent_ppo2.py:179][0m |          -0.0721 |           1.1650 |        -950.3613 |
[32m[20221208 15:04:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:04:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.37
[32m[20221208 15:04:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.66
[32m[20221208 15:04:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.97
[32m[20221208 15:04:55 @agent_ppo2.py:137][0m Total time:      28.35 min
[32m[20221208 15:04:55 @agent_ppo2.py:139][0m 2287616 total steps have happened
[32m[20221208 15:04:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221208 15:04:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |           0.1141 |           0.8299 |        -944.3007 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |           0.1347 |           0.6640 |        -549.5715 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |           0.0593 |           0.6271 |        -732.3843 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |           0.0170 |           0.6171 |        -862.3897 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |           0.0131 |           0.6058 |        -898.0118 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |          -0.0073 |           0.5930 |        -946.7866 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |          -0.0155 |           0.5859 |        -988.0066 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |          -0.0245 |           0.5852 |       -1018.6888 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |          -0.0298 |           0.5757 |       -1017.0024 |
[32m[20221208 15:04:56 @agent_ppo2.py:179][0m |          -0.0258 |           0.5776 |       -1025.3126 |
[32m[20221208 15:04:56 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.08
[32m[20221208 15:04:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 39.29
[32m[20221208 15:04:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.69
[32m[20221208 15:04:56 @agent_ppo2.py:137][0m Total time:      28.37 min
[32m[20221208 15:04:56 @agent_ppo2.py:139][0m 2289664 total steps have happened
[32m[20221208 15:04:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221208 15:04:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |           0.0745 |           0.7708 |        -973.3475 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |           0.1249 |           0.6357 |        -548.1474 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |           0.0595 |           0.6103 |        -425.2739 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |           0.0181 |           0.5832 |        -470.7548 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |           0.0053 |           0.5760 |        -517.5075 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |          -0.0020 |           0.5690 |        -547.4206 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |          -0.0003 |           0.5609 |        -568.5814 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |          -0.0074 |           0.5551 |        -589.4892 |
[32m[20221208 15:04:57 @agent_ppo2.py:179][0m |          -0.0144 |           0.5536 |        -606.1169 |
[32m[20221208 15:04:58 @agent_ppo2.py:179][0m |          -0.0211 |           0.5499 |        -609.1812 |
[32m[20221208 15:04:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:04:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.86
[32m[20221208 15:04:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.56
[32m[20221208 15:04:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.82
[32m[20221208 15:04:58 @agent_ppo2.py:137][0m Total time:      28.39 min
[32m[20221208 15:04:58 @agent_ppo2.py:139][0m 2291712 total steps have happened
[32m[20221208 15:04:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221208 15:04:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:04:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |           0.0671 |           2.5409 |        -941.3054 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |           0.0339 |           2.1030 |        -899.6836 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |           0.0082 |           1.9572 |        -924.1265 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0095 |           1.8643 |        -841.6384 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0347 |           1.7894 |        -680.2175 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0532 |           1.7475 |        -672.2344 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0621 |           1.6854 |        -671.0340 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0665 |           1.6778 |        -669.6631 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0686 |           1.6333 |        -675.9680 |
[32m[20221208 15:04:59 @agent_ppo2.py:179][0m |          -0.0730 |           1.6059 |        -668.2858 |
[32m[20221208 15:04:59 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:04:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.11
[32m[20221208 15:04:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.25
[32m[20221208 15:04:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.30
[32m[20221208 15:04:59 @agent_ppo2.py:137][0m Total time:      28.42 min
[32m[20221208 15:04:59 @agent_ppo2.py:139][0m 2293760 total steps have happened
[32m[20221208 15:04:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1120 --------------------------#
[32m[20221208 15:05:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |           0.0594 |           2.2602 |        -911.3433 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |           0.0618 |           2.0021 |        -726.8018 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |           0.0154 |           1.9094 |        -865.7077 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0080 |           1.8285 |        -888.5542 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0184 |           1.7950 |        -921.6273 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0349 |           1.7921 |        -975.2522 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0345 |           1.7683 |        -994.1377 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0407 |           1.7720 |        -979.7566 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0376 |           1.7337 |        -986.4619 |
[32m[20221208 15:05:00 @agent_ppo2.py:179][0m |          -0.0445 |           1.6875 |        -994.1153 |
[32m[20221208 15:05:00 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.14
[32m[20221208 15:05:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.14
[32m[20221208 15:05:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.05
[32m[20221208 15:05:01 @agent_ppo2.py:137][0m Total time:      28.44 min
[32m[20221208 15:05:01 @agent_ppo2.py:139][0m 2295808 total steps have happened
[32m[20221208 15:05:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1121 --------------------------#
[32m[20221208 15:05:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:01 @agent_ppo2.py:179][0m |           0.0750 |           2.8272 |        -971.3614 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |           0.0472 |           2.2849 |        -800.0492 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0019 |           2.0932 |        -839.9551 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0321 |           1.9789 |        -874.4697 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0516 |           1.8845 |        -903.0959 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0654 |           1.7976 |        -910.8102 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0689 |           1.7351 |        -938.6995 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0769 |           1.6967 |        -957.0754 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0798 |           1.6217 |        -959.7154 |
[32m[20221208 15:05:02 @agent_ppo2.py:179][0m |          -0.0807 |           1.5849 |        -958.3406 |
[32m[20221208 15:05:02 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:05:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.91
[32m[20221208 15:05:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.03
[32m[20221208 15:05:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.24
[32m[20221208 15:05:02 @agent_ppo2.py:137][0m Total time:      28.47 min
[32m[20221208 15:05:02 @agent_ppo2.py:139][0m 2297856 total steps have happened
[32m[20221208 15:05:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1122 --------------------------#
[32m[20221208 15:05:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |           0.0755 |           1.6757 |        -967.2702 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |           0.0544 |           1.2306 |        -770.9149 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |           0.0072 |           1.1230 |        -812.4748 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0221 |           1.0572 |        -854.1326 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0352 |           1.0159 |        -890.4246 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0321 |           0.9632 |        -885.3927 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0482 |           0.9432 |        -905.4450 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0497 |           0.9368 |        -919.9828 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0547 |           0.8999 |        -912.9893 |
[32m[20221208 15:05:03 @agent_ppo2.py:179][0m |          -0.0489 |           0.8878 |        -913.2477 |
[32m[20221208 15:05:03 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.07
[32m[20221208 15:05:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.11
[32m[20221208 15:05:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.19
[32m[20221208 15:05:04 @agent_ppo2.py:137][0m Total time:      28.49 min
[32m[20221208 15:05:04 @agent_ppo2.py:139][0m 2299904 total steps have happened
[32m[20221208 15:05:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1123 --------------------------#
[32m[20221208 15:05:04 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:05:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:04 @agent_ppo2.py:179][0m |           0.0977 |           1.2100 |        -574.6522 |
[32m[20221208 15:05:04 @agent_ppo2.py:179][0m |           0.0587 |           0.9851 |        -214.1634 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |           0.0100 |           0.9354 |        -287.1384 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0123 |           0.8934 |        -339.2544 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0276 |           0.8664 |        -374.5227 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0373 |           0.8431 |        -398.5301 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0458 |           0.8323 |        -410.0251 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0512 |           0.8253 |        -419.1849 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0543 |           0.8081 |        -440.2254 |
[32m[20221208 15:05:05 @agent_ppo2.py:179][0m |          -0.0582 |           0.7950 |        -476.2230 |
[32m[20221208 15:05:05 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.73
[32m[20221208 15:05:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.50
[32m[20221208 15:05:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.40
[32m[20221208 15:05:05 @agent_ppo2.py:137][0m Total time:      28.52 min
[32m[20221208 15:05:05 @agent_ppo2.py:139][0m 2301952 total steps have happened
[32m[20221208 15:05:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1124 --------------------------#
[32m[20221208 15:05:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.1871 |           1.1491 |        -705.2368 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.1116 |           0.8528 |        -437.2590 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.0556 |           0.8044 |        -466.5430 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.0224 |           0.7845 |        -533.8424 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.0183 |           0.7770 |        -561.6481 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.0032 |           0.7639 |        -589.0624 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.0000 |           0.7586 |        -620.2775 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |           0.0058 |           0.7545 |        -616.2555 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |          -0.0135 |           0.7489 |        -661.9452 |
[32m[20221208 15:05:06 @agent_ppo2.py:179][0m |          -0.0121 |           0.7478 |        -685.8809 |
[32m[20221208 15:05:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.53
[32m[20221208 15:05:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.92
[32m[20221208 15:05:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.95
[32m[20221208 15:05:07 @agent_ppo2.py:137][0m Total time:      28.54 min
[32m[20221208 15:05:07 @agent_ppo2.py:139][0m 2304000 total steps have happened
[32m[20221208 15:05:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1125 --------------------------#
[32m[20221208 15:05:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:07 @agent_ppo2.py:179][0m |           0.0678 |           0.8135 |        -903.2072 |
[32m[20221208 15:05:07 @agent_ppo2.py:179][0m |           0.1127 |           0.6520 |        -338.0497 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0996 |           0.6313 |         -60.2288 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0820 |           0.6154 |         -77.3423 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0774 |           0.6118 |         -96.8619 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0756 |           0.6045 |         -97.8624 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0681 |           0.6001 |        -123.2382 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0716 |           0.6004 |        -418.0099 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0832 |           0.5975 |         -89.9237 |
[32m[20221208 15:05:08 @agent_ppo2.py:179][0m |           0.0740 |           0.5960 |         -85.6239 |
[32m[20221208 15:05:08 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.39
[32m[20221208 15:05:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 56.71
[32m[20221208 15:05:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.63
[32m[20221208 15:05:08 @agent_ppo2.py:137][0m Total time:      28.57 min
[32m[20221208 15:05:08 @agent_ppo2.py:139][0m 2306048 total steps have happened
[32m[20221208 15:05:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1126 --------------------------#
[32m[20221208 15:05:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0791 |           0.9131 |        -743.7863 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0981 |           0.7944 |        -304.5106 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0987 |           0.7602 |        -471.4278 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.1956 |           0.7481 |        -243.7690 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0371 |           0.7367 |        -612.0212 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0251 |           0.7298 |        -667.9145 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0061 |           0.7062 |        -670.2475 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0742 |           0.6997 |        -337.2559 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |          -0.0018 |           0.6891 |        -747.0332 |
[32m[20221208 15:05:09 @agent_ppo2.py:179][0m |           0.0269 |           0.6856 |        -585.4817 |
[32m[20221208 15:05:09 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.86
[32m[20221208 15:05:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 56.87
[32m[20221208 15:05:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.84
[32m[20221208 15:05:10 @agent_ppo2.py:137][0m Total time:      28.59 min
[32m[20221208 15:05:10 @agent_ppo2.py:139][0m 2308096 total steps have happened
[32m[20221208 15:05:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1127 --------------------------#
[32m[20221208 15:05:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:10 @agent_ppo2.py:179][0m |           0.0833 |           1.2821 |        -741.7723 |
[32m[20221208 15:05:10 @agent_ppo2.py:179][0m |           0.0722 |           1.0312 |        -635.7142 |
[32m[20221208 15:05:10 @agent_ppo2.py:179][0m |           0.0396 |           0.9526 |        -636.0945 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |           0.0237 |           0.9205 |        -598.4554 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |           0.0165 |           0.8870 |        -613.2435 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |          -0.0070 |           0.8625 |        -692.1645 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |           0.0007 |           0.8506 |        -722.7484 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |          -0.0080 |           0.8304 |        -745.7807 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |          -0.0002 |           0.8290 |        -737.8175 |
[32m[20221208 15:05:11 @agent_ppo2.py:179][0m |          -0.0042 |           0.8127 |        -757.6696 |
[32m[20221208 15:05:11 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.15
[32m[20221208 15:05:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.93
[32m[20221208 15:05:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.91
[32m[20221208 15:05:11 @agent_ppo2.py:137][0m Total time:      28.62 min
[32m[20221208 15:05:11 @agent_ppo2.py:139][0m 2310144 total steps have happened
[32m[20221208 15:05:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1128 --------------------------#
[32m[20221208 15:05:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |           0.0856 |           1.6434 |        -664.0723 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |           0.0434 |           1.2619 |        -553.0722 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |           0.0009 |           1.1719 |        -488.0892 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0289 |           1.1327 |        -460.8042 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0494 |           1.0869 |        -427.8004 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0622 |           1.0742 |        -359.9913 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0667 |           1.0441 |        -360.2732 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0702 |           1.0222 |        -369.4821 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0728 |           1.0186 |        -365.3764 |
[32m[20221208 15:05:12 @agent_ppo2.py:179][0m |          -0.0726 |           1.0037 |        -346.4531 |
[32m[20221208 15:05:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.07
[32m[20221208 15:05:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.02
[32m[20221208 15:05:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.11
[32m[20221208 15:05:13 @agent_ppo2.py:137][0m Total time:      28.64 min
[32m[20221208 15:05:13 @agent_ppo2.py:139][0m 2312192 total steps have happened
[32m[20221208 15:05:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1129 --------------------------#
[32m[20221208 15:05:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:13 @agent_ppo2.py:179][0m |           0.0516 |           1.2092 |        -398.3137 |
[32m[20221208 15:05:13 @agent_ppo2.py:179][0m |           0.0031 |           0.9617 |        -220.0588 |
[32m[20221208 15:05:13 @agent_ppo2.py:179][0m |          -0.0193 |           0.8817 |        -229.2329 |
[32m[20221208 15:05:13 @agent_ppo2.py:179][0m |          -0.0335 |           0.8497 |        -239.0314 |
[32m[20221208 15:05:14 @agent_ppo2.py:179][0m |          -0.0393 |           0.8245 |        -253.8972 |
[32m[20221208 15:05:14 @agent_ppo2.py:179][0m |          -0.0393 |           0.8102 |        -277.1831 |
[32m[20221208 15:05:14 @agent_ppo2.py:179][0m |          -0.0449 |           0.8005 |        -263.5631 |
[32m[20221208 15:05:14 @agent_ppo2.py:179][0m |          -0.0472 |           0.7956 |        -274.8377 |
[32m[20221208 15:05:14 @agent_ppo2.py:179][0m |          -0.0487 |           0.7889 |        -279.1194 |
[32m[20221208 15:05:14 @agent_ppo2.py:179][0m |          -0.0477 |           0.7860 |        -275.1309 |
[32m[20221208 15:05:14 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:05:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.05
[32m[20221208 15:05:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.34
[32m[20221208 15:05:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.66
[32m[20221208 15:05:14 @agent_ppo2.py:137][0m Total time:      28.67 min
[32m[20221208 15:05:14 @agent_ppo2.py:139][0m 2314240 total steps have happened
[32m[20221208 15:05:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1130 --------------------------#
[32m[20221208 15:05:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.1003 |           0.9085 |        -389.8751 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.1035 |           0.7990 |        -207.5049 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0595 |           0.7593 |        -261.6135 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0423 |           0.7481 |        -364.5213 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0459 |           0.7360 |        -229.6640 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0426 |           0.7274 |        -232.8725 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0374 |           0.7168 |        -249.8702 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0367 |           0.7202 |        -250.8222 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0348 |           0.7140 |        -254.7416 |
[32m[20221208 15:05:15 @agent_ppo2.py:179][0m |           0.0309 |           0.7081 |        -260.1984 |
[32m[20221208 15:05:15 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:05:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.67
[32m[20221208 15:05:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.66
[32m[20221208 15:05:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.77
[32m[20221208 15:05:16 @agent_ppo2.py:137][0m Total time:      28.69 min
[32m[20221208 15:05:16 @agent_ppo2.py:139][0m 2316288 total steps have happened
[32m[20221208 15:05:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1131 --------------------------#
[32m[20221208 15:05:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:16 @agent_ppo2.py:179][0m |           0.1416 |           1.9540 |        -603.4981 |
[32m[20221208 15:05:16 @agent_ppo2.py:179][0m |           0.1513 |           1.5107 |        -340.3800 |
[32m[20221208 15:05:16 @agent_ppo2.py:179][0m |           0.0215 |           1.4251 |        -382.7352 |
[32m[20221208 15:05:16 @agent_ppo2.py:179][0m |          -0.0248 |           1.3580 |        -461.1569 |
[32m[20221208 15:05:16 @agent_ppo2.py:179][0m |          -0.0505 |           1.3116 |        -501.2774 |
[32m[20221208 15:05:17 @agent_ppo2.py:179][0m |          -0.0579 |           1.2933 |        -514.3122 |
[32m[20221208 15:05:17 @agent_ppo2.py:179][0m |          -0.0676 |           1.2621 |        -543.7763 |
[32m[20221208 15:05:17 @agent_ppo2.py:179][0m |          -0.0746 |           1.2357 |        -567.6315 |
[32m[20221208 15:05:17 @agent_ppo2.py:179][0m |          -0.0824 |           1.2345 |        -574.6509 |
[32m[20221208 15:05:17 @agent_ppo2.py:179][0m |          -0.0840 |           1.2471 |        -583.4977 |
[32m[20221208 15:05:17 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:05:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.53
[32m[20221208 15:05:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.12
[32m[20221208 15:05:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.44
[32m[20221208 15:05:17 @agent_ppo2.py:137][0m Total time:      28.72 min
[32m[20221208 15:05:17 @agent_ppo2.py:139][0m 2318336 total steps have happened
[32m[20221208 15:05:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1132 --------------------------#
[32m[20221208 15:05:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |           0.0984 |           2.2653 |        -722.6020 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |           0.0344 |           1.8498 |        -648.1144 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0080 |           1.7446 |        -697.9365 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0124 |           1.7054 |        -699.5523 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0450 |           1.6644 |        -759.7968 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0549 |           1.6380 |        -796.4634 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0568 |           1.5918 |        -830.3450 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0585 |           1.5828 |        -810.1668 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0704 |           1.5769 |        -851.1967 |
[32m[20221208 15:05:18 @agent_ppo2.py:179][0m |          -0.0709 |           1.5447 |        -853.2183 |
[32m[20221208 15:05:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:05:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.91
[32m[20221208 15:05:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.30
[32m[20221208 15:05:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.51
[32m[20221208 15:05:19 @agent_ppo2.py:137][0m Total time:      28.74 min
[32m[20221208 15:05:19 @agent_ppo2.py:139][0m 2320384 total steps have happened
[32m[20221208 15:05:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1133 --------------------------#
[32m[20221208 15:05:19 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:05:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:19 @agent_ppo2.py:179][0m |           0.0868 |           1.2133 |        -624.2301 |
[32m[20221208 15:05:19 @agent_ppo2.py:179][0m |           0.0123 |           0.9676 |        -465.7205 |
[32m[20221208 15:05:19 @agent_ppo2.py:179][0m |          -0.0247 |           0.9025 |        -485.6461 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0425 |           0.8634 |        -508.1443 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0496 |           0.8364 |        -507.5940 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0636 |           0.8184 |        -533.0120 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0598 |           0.8161 |        -535.8539 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0655 |           0.8022 |        -534.7270 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0723 |           0.7880 |        -534.1280 |
[32m[20221208 15:05:20 @agent_ppo2.py:179][0m |          -0.0766 |           0.7909 |        -547.5899 |
[32m[20221208 15:05:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:05:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.13
[32m[20221208 15:05:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.14
[32m[20221208 15:05:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.57
[32m[20221208 15:05:20 @agent_ppo2.py:137][0m Total time:      28.77 min
[32m[20221208 15:05:20 @agent_ppo2.py:139][0m 2322432 total steps have happened
[32m[20221208 15:05:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1134 --------------------------#
[32m[20221208 15:05:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |           0.1619 |           2.0311 |        -522.5425 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |           0.0798 |           1.5124 |        -300.2052 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |           0.0160 |           1.3766 |        -418.5741 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0241 |           1.2957 |        -490.6612 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0378 |           1.2616 |        -516.2807 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0526 |           1.2099 |        -527.4342 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0653 |           1.1794 |        -566.8264 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0736 |           1.1578 |        -590.1167 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0753 |           1.1400 |        -605.3990 |
[32m[20221208 15:05:21 @agent_ppo2.py:179][0m |          -0.0816 |           1.1227 |        -612.2879 |
[32m[20221208 15:05:21 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:05:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.69
[32m[20221208 15:05:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.71
[32m[20221208 15:05:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.89
[32m[20221208 15:05:22 @agent_ppo2.py:137][0m Total time:      28.79 min
[32m[20221208 15:05:22 @agent_ppo2.py:139][0m 2324480 total steps have happened
[32m[20221208 15:05:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1135 --------------------------#
[32m[20221208 15:05:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:22 @agent_ppo2.py:179][0m |           0.0814 |           1.3357 |        -526.5633 |
[32m[20221208 15:05:22 @agent_ppo2.py:179][0m |           0.0464 |           1.0522 |        -228.7280 |
[32m[20221208 15:05:22 @agent_ppo2.py:179][0m |          -0.0088 |           0.9735 |        -277.6660 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0294 |           0.9182 |        -295.4206 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0416 |           0.8827 |        -323.0257 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0507 |           0.8495 |        -322.8200 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0609 |           0.8291 |        -336.9952 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0622 |           0.8089 |        -357.0328 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0666 |           0.7956 |        -363.5074 |
[32m[20221208 15:05:23 @agent_ppo2.py:179][0m |          -0.0674 |           0.7875 |        -365.9082 |
[32m[20221208 15:05:23 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.00
[32m[20221208 15:05:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.98
[32m[20221208 15:05:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.52
[32m[20221208 15:05:23 @agent_ppo2.py:137][0m Total time:      28.82 min
[32m[20221208 15:05:23 @agent_ppo2.py:139][0m 2326528 total steps have happened
[32m[20221208 15:05:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1136 --------------------------#
[32m[20221208 15:05:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0753 |           1.6901 |        -702.1611 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0934 |           1.4528 |        -363.3420 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.1040 |           1.4411 |        -144.3438 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0656 |           1.4215 |        -481.7544 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0980 |           1.3854 |        -182.7225 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0919 |           1.3789 |        -170.4157 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0823 |           1.3608 |        -224.7898 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0937 |           1.3894 |        -175.7680 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0849 |           1.3771 |        -192.7353 |
[32m[20221208 15:05:24 @agent_ppo2.py:179][0m |           0.0761 |           1.3663 |        -221.3391 |
[32m[20221208 15:05:24 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:05:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.22
[32m[20221208 15:05:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.70
[32m[20221208 15:05:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.38
[32m[20221208 15:05:25 @agent_ppo2.py:137][0m Total time:      28.84 min
[32m[20221208 15:05:25 @agent_ppo2.py:139][0m 2328576 total steps have happened
[32m[20221208 15:05:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1137 --------------------------#
[32m[20221208 15:05:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:25 @agent_ppo2.py:179][0m |           0.1069 |           2.3737 |        -736.4743 |
[32m[20221208 15:05:25 @agent_ppo2.py:179][0m |           0.0767 |           1.6701 |        -562.3777 |
[32m[20221208 15:05:25 @agent_ppo2.py:179][0m |           0.0345 |           1.5261 |        -683.3694 |
[32m[20221208 15:05:25 @agent_ppo2.py:179][0m |          -0.0073 |           1.4380 |        -722.2169 |
[32m[20221208 15:05:26 @agent_ppo2.py:179][0m |          -0.0282 |           1.3863 |        -791.5167 |
[32m[20221208 15:05:26 @agent_ppo2.py:179][0m |          -0.0298 |           1.3493 |        -775.0335 |
[32m[20221208 15:05:26 @agent_ppo2.py:179][0m |          -0.0209 |           1.3010 |        -748.6254 |
[32m[20221208 15:05:26 @agent_ppo2.py:179][0m |          -0.0459 |           1.2814 |        -825.1992 |
[32m[20221208 15:05:26 @agent_ppo2.py:179][0m |          -0.0397 |           1.2449 |        -794.7801 |
[32m[20221208 15:05:26 @agent_ppo2.py:179][0m |          -0.0485 |           1.2399 |        -835.4823 |
[32m[20221208 15:05:26 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.22
[32m[20221208 15:05:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.97
[32m[20221208 15:05:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.89
[32m[20221208 15:05:26 @agent_ppo2.py:137][0m Total time:      28.87 min
[32m[20221208 15:05:26 @agent_ppo2.py:139][0m 2330624 total steps have happened
[32m[20221208 15:05:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1138 --------------------------#
[32m[20221208 15:05:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |           0.1124 |           2.3141 |        -745.1282 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |           0.0699 |           1.9593 |        -305.9564 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |           0.0309 |           1.8730 |        -345.6980 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |           0.0102 |           1.8201 |        -386.8497 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |          -0.0025 |           1.7914 |        -425.3800 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |          -0.0096 |           1.7673 |        -427.7878 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |          -0.0157 |           1.7318 |        -457.0753 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |          -0.0230 |           1.7141 |        -473.8286 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |          -0.0286 |           1.6832 |        -494.0619 |
[32m[20221208 15:05:27 @agent_ppo2.py:179][0m |          -0.0319 |           1.6876 |        -501.4302 |
[32m[20221208 15:05:27 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.16
[32m[20221208 15:05:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.97
[32m[20221208 15:05:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.29
[32m[20221208 15:05:28 @agent_ppo2.py:137][0m Total time:      28.89 min
[32m[20221208 15:05:28 @agent_ppo2.py:139][0m 2332672 total steps have happened
[32m[20221208 15:05:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1139 --------------------------#
[32m[20221208 15:05:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:28 @agent_ppo2.py:179][0m |           0.1637 |           3.4606 |        -667.6918 |
[32m[20221208 15:05:28 @agent_ppo2.py:179][0m |           0.0642 |           2.9817 |        -466.1301 |
[32m[20221208 15:05:28 @agent_ppo2.py:179][0m |          -0.0122 |           2.8121 |        -589.0590 |
[32m[20221208 15:05:28 @agent_ppo2.py:179][0m |          -0.0400 |           2.6808 |        -662.6771 |
[32m[20221208 15:05:28 @agent_ppo2.py:179][0m |          -0.0580 |           2.6096 |        -686.3763 |
[32m[20221208 15:05:29 @agent_ppo2.py:179][0m |          -0.0743 |           2.5297 |        -725.0067 |
[32m[20221208 15:05:29 @agent_ppo2.py:179][0m |          -0.0741 |           2.4756 |        -739.4044 |
[32m[20221208 15:05:29 @agent_ppo2.py:179][0m |          -0.0797 |           2.4372 |        -756.7819 |
[32m[20221208 15:05:29 @agent_ppo2.py:179][0m |          -0.0890 |           2.3841 |        -770.7038 |
[32m[20221208 15:05:29 @agent_ppo2.py:179][0m |          -0.0955 |           2.3437 |        -794.0054 |
[32m[20221208 15:05:29 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:05:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.92
[32m[20221208 15:05:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.52
[32m[20221208 15:05:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.04
[32m[20221208 15:05:29 @agent_ppo2.py:137][0m Total time:      28.91 min
[32m[20221208 15:05:29 @agent_ppo2.py:139][0m 2334720 total steps have happened
[32m[20221208 15:05:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1140 --------------------------#
[32m[20221208 15:05:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |           0.1220 |           2.4861 |        -892.8343 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |           0.1044 |           2.0594 |        -767.3007 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |           0.0553 |           1.8287 |        -789.1319 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |           0.0239 |           1.7163 |        -816.8429 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |          -0.0044 |           1.6988 |        -870.9458 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |          -0.0081 |           1.6053 |        -837.4935 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |          -0.0320 |           1.5500 |        -869.4862 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |          -0.0339 |           1.5328 |        -879.1712 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |          -0.0382 |           1.4699 |        -879.7305 |
[32m[20221208 15:05:30 @agent_ppo2.py:179][0m |          -0.0525 |           1.4593 |        -891.7222 |
[32m[20221208 15:05:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:05:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.32
[32m[20221208 15:05:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.33
[32m[20221208 15:05:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.58
[32m[20221208 15:05:31 @agent_ppo2.py:137][0m Total time:      28.94 min
[32m[20221208 15:05:31 @agent_ppo2.py:139][0m 2336768 total steps have happened
[32m[20221208 15:05:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1141 --------------------------#
[32m[20221208 15:05:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:31 @agent_ppo2.py:179][0m |           0.0763 |           1.1843 |        -533.8831 |
[32m[20221208 15:05:31 @agent_ppo2.py:179][0m |           0.0643 |           1.0947 |        -384.6612 |
[32m[20221208 15:05:31 @agent_ppo2.py:179][0m |           0.0538 |           1.0227 |        -376.3133 |
[32m[20221208 15:05:31 @agent_ppo2.py:179][0m |           0.0408 |           0.9899 |        -450.6451 |
[32m[20221208 15:05:31 @agent_ppo2.py:179][0m |           0.0372 |           0.9631 |        -546.3294 |
[32m[20221208 15:05:31 @agent_ppo2.py:179][0m |           0.0503 |           0.9438 |        -312.8073 |
[32m[20221208 15:05:32 @agent_ppo2.py:179][0m |           0.0227 |           0.9185 |        -436.0237 |
[32m[20221208 15:05:32 @agent_ppo2.py:179][0m |           0.0042 |           0.9045 |        -566.8244 |
[32m[20221208 15:05:32 @agent_ppo2.py:179][0m |           0.0124 |           0.8907 |        -767.3118 |
[32m[20221208 15:05:32 @agent_ppo2.py:179][0m |           0.0480 |           0.8760 |        -506.5264 |
[32m[20221208 15:05:32 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.83
[32m[20221208 15:05:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.89
[32m[20221208 15:05:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.50
[32m[20221208 15:05:32 @agent_ppo2.py:137][0m Total time:      28.96 min
[32m[20221208 15:05:32 @agent_ppo2.py:139][0m 2338816 total steps have happened
[32m[20221208 15:05:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1142 --------------------------#
[32m[20221208 15:05:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |           0.0785 |           3.1586 |        -551.7482 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |           0.0309 |           2.9232 |        -365.3770 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |           0.0147 |           2.8543 |        -383.4850 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0219 |           2.7790 |        -425.8016 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0369 |           2.7871 |        -453.9747 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0385 |           2.7483 |        -474.5926 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0493 |           2.6911 |        -469.6486 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0569 |           2.6639 |        -494.1223 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0574 |           2.6603 |        -506.2062 |
[32m[20221208 15:05:33 @agent_ppo2.py:179][0m |          -0.0628 |           2.6539 |        -507.4057 |
[32m[20221208 15:05:33 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.33
[32m[20221208 15:05:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.87
[32m[20221208 15:05:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.64
[32m[20221208 15:05:34 @agent_ppo2.py:137][0m Total time:      28.99 min
[32m[20221208 15:05:34 @agent_ppo2.py:139][0m 2340864 total steps have happened
[32m[20221208 15:05:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1143 --------------------------#
[32m[20221208 15:05:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |           0.1128 |           1.7735 |        -655.4926 |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |           0.0566 |           1.3704 |        -506.2894 |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |           0.0033 |           1.2631 |        -573.7493 |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |           0.0269 |           1.2097 |        -536.2889 |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |          -0.0180 |           1.1850 |        -598.0726 |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |          -0.0266 |           1.1555 |        -625.1594 |
[32m[20221208 15:05:34 @agent_ppo2.py:179][0m |          -0.0002 |           1.1333 |        -594.2971 |
[32m[20221208 15:05:35 @agent_ppo2.py:179][0m |           0.0616 |           1.1262 |        -437.7982 |
[32m[20221208 15:05:35 @agent_ppo2.py:179][0m |          -0.0158 |           1.1169 |        -595.6748 |
[32m[20221208 15:05:35 @agent_ppo2.py:179][0m |          -0.0077 |           1.1071 |        -632.5938 |
[32m[20221208 15:05:35 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.35
[32m[20221208 15:05:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.90
[32m[20221208 15:05:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.71
[32m[20221208 15:05:35 @agent_ppo2.py:137][0m Total time:      29.01 min
[32m[20221208 15:05:35 @agent_ppo2.py:139][0m 2342912 total steps have happened
[32m[20221208 15:05:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1144 --------------------------#
[32m[20221208 15:05:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |           0.0877 |           1.0561 |        -590.2185 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |           0.0738 |           0.9190 |        -196.9298 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |           0.0467 |           0.8791 |        -207.3250 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |           0.0326 |           0.8558 |        -233.2825 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |           0.0240 |           0.8434 |        -249.7303 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |           0.0181 |           0.8335 |        -267.4192 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |          -0.0072 |           0.8265 |        -348.2453 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |          -0.0457 |           0.8224 |        -519.6732 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |          -0.0239 |           0.8210 |        -451.7922 |
[32m[20221208 15:05:36 @agent_ppo2.py:179][0m |          -0.0305 |           0.8201 |        -450.5984 |
[32m[20221208 15:05:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.02
[32m[20221208 15:05:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.43
[32m[20221208 15:05:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.66
[32m[20221208 15:05:37 @agent_ppo2.py:137][0m Total time:      29.04 min
[32m[20221208 15:05:37 @agent_ppo2.py:139][0m 2344960 total steps have happened
[32m[20221208 15:05:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1145 --------------------------#
[32m[20221208 15:05:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |           0.0582 |           1.3267 |        -838.3897 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |           0.0593 |           1.1599 |        -633.7267 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |           0.0449 |           1.1349 |        -537.6246 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |          -0.0023 |           1.0960 |        -594.8728 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |           0.0148 |           1.0735 |        -562.8490 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |           0.0216 |           1.0551 |        -543.4145 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |          -0.0073 |           1.0448 |        -585.5458 |
[32m[20221208 15:05:37 @agent_ppo2.py:179][0m |           0.0022 |           1.0463 |        -564.0370 |
[32m[20221208 15:05:38 @agent_ppo2.py:179][0m |           0.0086 |           1.0538 |        -582.7175 |
[32m[20221208 15:05:38 @agent_ppo2.py:179][0m |          -0.0024 |           1.0217 |        -586.6742 |
[32m[20221208 15:05:38 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:05:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.34
[32m[20221208 15:05:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.32
[32m[20221208 15:05:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.63
[32m[20221208 15:05:38 @agent_ppo2.py:137][0m Total time:      29.06 min
[32m[20221208 15:05:38 @agent_ppo2.py:139][0m 2347008 total steps have happened
[32m[20221208 15:05:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1146 --------------------------#
[32m[20221208 15:05:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |           0.0537 |           1.7726 |        -813.0584 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |           0.0396 |           1.3804 |        -479.1187 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |           0.0011 |           1.2714 |        -673.9216 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0247 |           1.2255 |        -791.1119 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0320 |           1.1758 |        -817.4596 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0379 |           1.1654 |        -850.9461 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0483 |           1.1458 |        -867.0513 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0408 |           1.1197 |        -851.1406 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0482 |           1.0972 |        -871.1678 |
[32m[20221208 15:05:39 @agent_ppo2.py:179][0m |          -0.0556 |           1.0907 |        -872.8318 |
[32m[20221208 15:05:39 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.25
[32m[20221208 15:05:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.11
[32m[20221208 15:05:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.10
[32m[20221208 15:05:39 @agent_ppo2.py:137][0m Total time:      29.09 min
[32m[20221208 15:05:39 @agent_ppo2.py:139][0m 2349056 total steps have happened
[32m[20221208 15:05:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1147 --------------------------#
[32m[20221208 15:05:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |           0.0804 |           2.1297 |        -762.6508 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |           0.0547 |           1.6333 |        -446.0903 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |           0.0036 |           1.4480 |        -466.8386 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |          -0.0197 |           1.3485 |        -488.8423 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |          -0.0368 |           1.2702 |        -514.8781 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |          -0.0456 |           1.2147 |        -536.9936 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |          -0.0503 |           1.1776 |        -538.8981 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |          -0.0533 |           1.1419 |        -552.6256 |
[32m[20221208 15:05:40 @agent_ppo2.py:179][0m |          -0.0509 |           1.1286 |        -550.7105 |
[32m[20221208 15:05:41 @agent_ppo2.py:179][0m |          -0.0553 |           1.1138 |        -572.6561 |
[32m[20221208 15:05:41 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.68
[32m[20221208 15:05:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.98
[32m[20221208 15:05:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.46
[32m[20221208 15:05:41 @agent_ppo2.py:137][0m Total time:      29.11 min
[32m[20221208 15:05:41 @agent_ppo2.py:139][0m 2351104 total steps have happened
[32m[20221208 15:05:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1148 --------------------------#
[32m[20221208 15:05:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:41 @agent_ppo2.py:179][0m |           0.0709 |           1.7549 |        -656.2164 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |           0.0421 |           1.4936 |        -334.6642 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |           0.0060 |           1.3614 |        -320.7960 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0215 |           1.3042 |        -337.6695 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0437 |           1.2630 |        -368.0841 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0452 |           1.2459 |        -370.6124 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0536 |           1.2097 |        -391.8706 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0587 |           1.1964 |        -386.3806 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0648 |           1.1767 |        -399.3763 |
[32m[20221208 15:05:42 @agent_ppo2.py:179][0m |          -0.0658 |           1.1666 |        -401.8142 |
[32m[20221208 15:05:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.72
[32m[20221208 15:05:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.08
[32m[20221208 15:05:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.56
[32m[20221208 15:05:42 @agent_ppo2.py:137][0m Total time:      29.14 min
[32m[20221208 15:05:42 @agent_ppo2.py:139][0m 2353152 total steps have happened
[32m[20221208 15:05:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1149 --------------------------#
[32m[20221208 15:05:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |           0.1131 |           2.4742 |        -573.7766 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |           0.0680 |           2.2208 |        -332.2638 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |           0.0025 |           2.1395 |        -428.5591 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0239 |           2.0632 |        -476.6635 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0344 |           1.9669 |        -496.5289 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0496 |           1.9547 |        -502.8608 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0579 |           1.9266 |        -539.6690 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0632 |           1.8762 |        -553.0819 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0686 |           1.8583 |        -562.9186 |
[32m[20221208 15:05:43 @agent_ppo2.py:179][0m |          -0.0708 |           1.8299 |        -577.2890 |
[32m[20221208 15:05:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:05:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.65
[32m[20221208 15:05:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.74
[32m[20221208 15:05:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.59
[32m[20221208 15:05:44 @agent_ppo2.py:137][0m Total time:      29.16 min
[32m[20221208 15:05:44 @agent_ppo2.py:139][0m 2355200 total steps have happened
[32m[20221208 15:05:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1150 --------------------------#
[32m[20221208 15:05:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:44 @agent_ppo2.py:179][0m |           0.0772 |           2.3587 |        -617.4867 |
[32m[20221208 15:05:44 @agent_ppo2.py:179][0m |           0.0620 |           2.1036 |        -391.2234 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |           0.0068 |           2.0232 |        -406.4774 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0160 |           1.9651 |        -439.1560 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0245 |           1.9039 |        -455.5455 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0349 |           1.8609 |        -496.6463 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0406 |           1.8352 |        -505.4651 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0438 |           1.8083 |        -515.1299 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0470 |           1.7924 |        -524.1506 |
[32m[20221208 15:05:45 @agent_ppo2.py:179][0m |          -0.0423 |           1.7663 |        -511.4395 |
[32m[20221208 15:05:45 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.67
[32m[20221208 15:05:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.59
[32m[20221208 15:05:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.45
[32m[20221208 15:05:45 @agent_ppo2.py:137][0m Total time:      29.18 min
[32m[20221208 15:05:45 @agent_ppo2.py:139][0m 2357248 total steps have happened
[32m[20221208 15:05:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1151 --------------------------#
[32m[20221208 15:05:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |           0.1672 |           3.4583 |        -636.7168 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |           0.1137 |           2.9792 |        -374.5912 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |           0.0285 |           2.8363 |        -529.5150 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0129 |           2.7534 |        -618.5762 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0349 |           2.6541 |        -659.8288 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0485 |           2.6254 |        -687.4091 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0573 |           2.5526 |        -718.8026 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0649 |           2.4992 |        -748.2487 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0712 |           2.4725 |        -777.9486 |
[32m[20221208 15:05:46 @agent_ppo2.py:179][0m |          -0.0752 |           2.4289 |        -784.6955 |
[32m[20221208 15:05:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:05:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.57
[32m[20221208 15:05:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.91
[32m[20221208 15:05:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.01
[32m[20221208 15:05:47 @agent_ppo2.py:137][0m Total time:      29.21 min
[32m[20221208 15:05:47 @agent_ppo2.py:139][0m 2359296 total steps have happened
[32m[20221208 15:05:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1152 --------------------------#
[32m[20221208 15:05:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:47 @agent_ppo2.py:179][0m |           0.1159 |           1.2624 |        -764.8676 |
[32m[20221208 15:05:47 @agent_ppo2.py:179][0m |          36.6327 |           1.0693 |        -403.1735 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |           0.0065 |           0.9799 |        -465.6773 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0306 |           0.9274 |        -521.7258 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0489 |           0.8968 |        -554.4941 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0604 |           0.8594 |        -584.9228 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0719 |           0.8328 |        -606.2427 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0766 |           0.8286 |        -619.0213 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0822 |           0.8042 |        -631.8595 |
[32m[20221208 15:05:48 @agent_ppo2.py:179][0m |          -0.0853 |           0.7889 |        -651.3245 |
[32m[20221208 15:05:48 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:05:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.83
[32m[20221208 15:05:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.84
[32m[20221208 15:05:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.84
[32m[20221208 15:05:48 @agent_ppo2.py:137][0m Total time:      29.23 min
[32m[20221208 15:05:48 @agent_ppo2.py:139][0m 2361344 total steps have happened
[32m[20221208 15:05:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1153 --------------------------#
[32m[20221208 15:05:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |           0.0745 |           2.3873 |       -1001.2143 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |           0.0692 |           1.9189 |        -893.3130 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |           0.0198 |           1.7946 |        -864.4261 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0096 |           1.7759 |        -806.5347 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0253 |           1.6875 |        -774.4167 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0390 |           1.6686 |        -850.6397 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0469 |           1.6337 |        -883.7204 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0435 |           1.6093 |        -864.9950 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0327 |           1.5866 |        -761.4437 |
[32m[20221208 15:05:49 @agent_ppo2.py:179][0m |          -0.0385 |           1.5590 |        -599.0026 |
[32m[20221208 15:05:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.71
[32m[20221208 15:05:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.61
[32m[20221208 15:05:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.90
[32m[20221208 15:05:50 @agent_ppo2.py:137][0m Total time:      29.26 min
[32m[20221208 15:05:50 @agent_ppo2.py:139][0m 2363392 total steps have happened
[32m[20221208 15:05:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1154 --------------------------#
[32m[20221208 15:05:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:50 @agent_ppo2.py:179][0m |           0.1395 |           1.5355 |        -758.1983 |
[32m[20221208 15:05:50 @agent_ppo2.py:179][0m |           0.1099 |           1.1415 |        -444.9060 |
[32m[20221208 15:05:50 @agent_ppo2.py:179][0m |           0.0085 |           1.0080 |        -480.5421 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0311 |           0.9463 |        -567.7229 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0543 |           0.8938 |        -623.2459 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0681 |           0.8691 |        -639.0451 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0778 |           0.8380 |        -676.6525 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0828 |           0.8096 |        -694.9738 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0898 |           0.7967 |        -713.4994 |
[32m[20221208 15:05:51 @agent_ppo2.py:179][0m |          -0.0908 |           0.7758 |        -722.0592 |
[32m[20221208 15:05:51 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.80
[32m[20221208 15:05:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.64
[32m[20221208 15:05:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.07
[32m[20221208 15:05:51 @agent_ppo2.py:137][0m Total time:      29.28 min
[32m[20221208 15:05:51 @agent_ppo2.py:139][0m 2365440 total steps have happened
[32m[20221208 15:05:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1155 --------------------------#
[32m[20221208 15:05:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:05:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |           0.0862 |           1.9968 |       -1036.0556 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |           0.0897 |           1.6686 |        -427.7759 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |           0.0242 |           1.5281 |        -518.9130 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0003 |           1.4437 |        -731.1862 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0178 |           1.3832 |        -995.8559 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0234 |           1.3445 |        -913.1739 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0388 |           1.3082 |        -974.0486 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0361 |           1.2906 |        -952.6764 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0406 |           1.2499 |        -949.7118 |
[32m[20221208 15:05:52 @agent_ppo2.py:179][0m |          -0.0479 |           1.2246 |       -1197.8117 |
[32m[20221208 15:05:52 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.80
[32m[20221208 15:05:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.49
[32m[20221208 15:05:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.38
[32m[20221208 15:05:53 @agent_ppo2.py:137][0m Total time:      29.31 min
[32m[20221208 15:05:53 @agent_ppo2.py:139][0m 2367488 total steps have happened
[32m[20221208 15:05:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1156 --------------------------#
[32m[20221208 15:05:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:53 @agent_ppo2.py:179][0m |           0.1079 |           3.4832 |       -1033.6194 |
[32m[20221208 15:05:53 @agent_ppo2.py:179][0m |           0.0507 |           2.8404 |        -977.7961 |
[32m[20221208 15:05:53 @agent_ppo2.py:179][0m |           0.0130 |           2.6558 |       -1032.1430 |
[32m[20221208 15:05:53 @agent_ppo2.py:179][0m |          -0.0189 |           2.5607 |        -985.1620 |
[32m[20221208 15:05:54 @agent_ppo2.py:179][0m |          -0.0371 |           2.5007 |        -921.0445 |
[32m[20221208 15:05:54 @agent_ppo2.py:179][0m |          -0.0460 |           2.4323 |        -913.4663 |
[32m[20221208 15:05:54 @agent_ppo2.py:179][0m |          -0.0493 |           2.3794 |        -908.4905 |
[32m[20221208 15:05:54 @agent_ppo2.py:179][0m |          -0.0618 |           2.3530 |        -939.0544 |
[32m[20221208 15:05:54 @agent_ppo2.py:179][0m |          -0.0669 |           2.2833 |        -976.0455 |
[32m[20221208 15:05:54 @agent_ppo2.py:179][0m |          -0.0690 |           2.2435 |        -984.0451 |
[32m[20221208 15:05:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.23
[32m[20221208 15:05:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.74
[32m[20221208 15:05:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.27
[32m[20221208 15:05:54 @agent_ppo2.py:137][0m Total time:      29.33 min
[32m[20221208 15:05:54 @agent_ppo2.py:139][0m 2369536 total steps have happened
[32m[20221208 15:05:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1157 --------------------------#
[32m[20221208 15:05:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |           0.1117 |           1.3037 |       -1047.6719 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |           0.0494 |           1.0037 |        -556.6159 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0079 |           0.8884 |        -517.6802 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0359 |           0.8213 |        -577.8628 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0473 |           0.7908 |        -581.4618 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0555 |           0.7623 |        -622.7668 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0670 |           0.7294 |        -627.8453 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0689 |           0.7062 |        -652.4625 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0777 |           0.6973 |        -662.8266 |
[32m[20221208 15:05:55 @agent_ppo2.py:179][0m |          -0.0734 |           0.6855 |        -671.6933 |
[32m[20221208 15:05:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.22
[32m[20221208 15:05:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 93.15
[32m[20221208 15:05:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.47
[32m[20221208 15:05:56 @agent_ppo2.py:137][0m Total time:      29.36 min
[32m[20221208 15:05:56 @agent_ppo2.py:139][0m 2371584 total steps have happened
[32m[20221208 15:05:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1158 --------------------------#
[32m[20221208 15:05:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:56 @agent_ppo2.py:179][0m |           0.0815 |           2.0394 |        -927.0154 |
[32m[20221208 15:05:56 @agent_ppo2.py:179][0m |           0.0701 |           1.7043 |        -596.6203 |
[32m[20221208 15:05:56 @agent_ppo2.py:179][0m |           0.0162 |           1.5230 |        -466.6680 |
[32m[20221208 15:05:56 @agent_ppo2.py:179][0m |          -0.0050 |           1.4305 |        -545.3962 |
[32m[20221208 15:05:56 @agent_ppo2.py:179][0m |          -0.0207 |           1.3316 |        -618.6597 |
[32m[20221208 15:05:57 @agent_ppo2.py:179][0m |          -0.0237 |           1.2749 |        -596.8978 |
[32m[20221208 15:05:57 @agent_ppo2.py:179][0m |          -0.0261 |           1.2022 |        -580.6124 |
[32m[20221208 15:05:57 @agent_ppo2.py:179][0m |          -0.0324 |           1.1553 |        -637.1221 |
[32m[20221208 15:05:57 @agent_ppo2.py:179][0m |          -0.0372 |           1.1396 |        -633.2728 |
[32m[20221208 15:05:57 @agent_ppo2.py:179][0m |          -0.0394 |           1.0862 |        -683.6197 |
[32m[20221208 15:05:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:05:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.65
[32m[20221208 15:05:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.56
[32m[20221208 15:05:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.17
[32m[20221208 15:05:57 @agent_ppo2.py:137][0m Total time:      29.38 min
[32m[20221208 15:05:57 @agent_ppo2.py:139][0m 2373632 total steps have happened
[32m[20221208 15:05:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1159 --------------------------#
[32m[20221208 15:05:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |           0.0501 |           0.5075 |       -1176.4750 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |           0.0326 |           0.3954 |        -997.7015 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |           0.0004 |           0.3767 |       -1124.9545 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |           0.0331 |           0.3676 |        -988.5870 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |          -0.0102 |           0.3615 |       -1166.4835 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |          -0.0077 |           0.3575 |       -1165.2825 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |          -0.0079 |           0.3525 |       -1124.8650 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |           0.0076 |           0.3481 |       -1106.6948 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |          -0.0002 |           0.3465 |       -1155.3588 |
[32m[20221208 15:05:58 @agent_ppo2.py:179][0m |           0.1113 |           0.3445 |        -636.1030 |
[32m[20221208 15:05:58 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:05:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.27
[32m[20221208 15:05:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 25.79
[32m[20221208 15:05:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.50
[32m[20221208 15:05:59 @agent_ppo2.py:137][0m Total time:      29.41 min
[32m[20221208 15:05:59 @agent_ppo2.py:139][0m 2375680 total steps have happened
[32m[20221208 15:05:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1160 --------------------------#
[32m[20221208 15:05:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:05:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:05:59 @agent_ppo2.py:179][0m |           0.1596 |           0.4306 |        -686.0356 |
[32m[20221208 15:05:59 @agent_ppo2.py:179][0m |           0.1795 |           0.3678 |         -36.9336 |
[32m[20221208 15:05:59 @agent_ppo2.py:179][0m |           0.1246 |           0.3535 |         -51.4899 |
[32m[20221208 15:05:59 @agent_ppo2.py:179][0m |           0.1156 |           0.3469 |         -60.3608 |
[32m[20221208 15:05:59 @agent_ppo2.py:179][0m |           0.1034 |           0.3426 |        -102.2954 |
[32m[20221208 15:05:59 @agent_ppo2.py:179][0m |           0.0992 |           0.3389 |        -133.5335 |
[32m[20221208 15:06:00 @agent_ppo2.py:179][0m |           0.1013 |           0.3350 |        -101.6776 |
[32m[20221208 15:06:00 @agent_ppo2.py:179][0m |           0.0959 |           0.3324 |        -135.4223 |
[32m[20221208 15:06:00 @agent_ppo2.py:179][0m |           0.0949 |           0.3299 |        -128.6239 |
[32m[20221208 15:06:00 @agent_ppo2.py:179][0m |           0.0915 |           0.3282 |        -159.0507 |
[32m[20221208 15:06:00 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:06:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 19.12
[32m[20221208 15:06:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 19.37
[32m[20221208 15:06:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.36
[32m[20221208 15:06:00 @agent_ppo2.py:137][0m Total time:      29.43 min
[32m[20221208 15:06:00 @agent_ppo2.py:139][0m 2377728 total steps have happened
[32m[20221208 15:06:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1161 --------------------------#
[32m[20221208 15:06:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |           0.0797 |           0.9494 |       -1007.1367 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |           0.0864 |           0.7915 |        -682.6476 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |           0.0433 |           0.7360 |        -611.9943 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |           0.0186 |           0.7006 |        -381.9695 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |          -0.0208 |           0.6796 |        -586.1827 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |          -0.0183 |           0.6633 |        -557.1446 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |          -0.0190 |           0.6717 |        -364.3000 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |          -0.0319 |           0.6480 |        -397.2858 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |          -0.0379 |           0.6422 |        -436.1157 |
[32m[20221208 15:06:01 @agent_ppo2.py:179][0m |          -0.0432 |           0.6291 |        -431.0223 |
[32m[20221208 15:06:01 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.03
[32m[20221208 15:06:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.17
[32m[20221208 15:06:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.75
[32m[20221208 15:06:02 @agent_ppo2.py:137][0m Total time:      29.45 min
[32m[20221208 15:06:02 @agent_ppo2.py:139][0m 2379776 total steps have happened
[32m[20221208 15:06:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1162 --------------------------#
[32m[20221208 15:06:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |           0.0924 |           1.6916 |        -951.0937 |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |           0.0746 |           1.3276 |        -756.6732 |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |           0.0189 |           1.2390 |        -767.9271 |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |          -0.0092 |           1.1892 |        -822.1038 |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |          -0.0216 |           1.1376 |        -844.1222 |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |          -0.0474 |           1.1146 |        -833.7871 |
[32m[20221208 15:06:02 @agent_ppo2.py:179][0m |          -0.0611 |           1.0949 |        -889.3540 |
[32m[20221208 15:06:03 @agent_ppo2.py:179][0m |          -0.0678 |           1.0733 |        -911.6716 |
[32m[20221208 15:06:03 @agent_ppo2.py:179][0m |          -0.0773 |           1.0595 |        -961.9621 |
[32m[20221208 15:06:03 @agent_ppo2.py:179][0m |          -0.0788 |           1.0418 |        -968.2915 |
[32m[20221208 15:06:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.29
[32m[20221208 15:06:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.11
[32m[20221208 15:06:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.29
[32m[20221208 15:06:03 @agent_ppo2.py:137][0m Total time:      29.48 min
[32m[20221208 15:06:03 @agent_ppo2.py:139][0m 2381824 total steps have happened
[32m[20221208 15:06:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1163 --------------------------#
[32m[20221208 15:06:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |           0.0850 |           1.7089 |        -944.1190 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |           0.0458 |           1.4527 |        -950.7673 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |           0.0194 |           1.3505 |        -973.7249 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0074 |           1.3019 |       -1020.4688 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0184 |           1.2650 |       -1069.5567 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0254 |           1.2518 |       -1086.2400 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0353 |           1.2093 |       -1111.9879 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0426 |           1.2011 |       -1152.2548 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0440 |           1.1686 |       -1174.1052 |
[32m[20221208 15:06:04 @agent_ppo2.py:179][0m |          -0.0489 |           1.1705 |       -1195.8916 |
[32m[20221208 15:06:04 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.60
[32m[20221208 15:06:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.17
[32m[20221208 15:06:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.12
[32m[20221208 15:06:05 @agent_ppo2.py:137][0m Total time:      29.50 min
[32m[20221208 15:06:05 @agent_ppo2.py:139][0m 2383872 total steps have happened
[32m[20221208 15:06:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1164 --------------------------#
[32m[20221208 15:06:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |           0.0738 |           1.0179 |        -993.9430 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |           0.0344 |           0.6851 |        -951.7232 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |           0.0204 |           0.5896 |        -953.2902 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |          -0.0178 |           0.5479 |        -924.2376 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |          -0.0304 |           0.5112 |        -917.3089 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |          -0.0449 |           0.4942 |       -1009.6356 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |          -0.0519 |           0.4776 |       -1089.6993 |
[32m[20221208 15:06:05 @agent_ppo2.py:179][0m |          -0.0508 |           0.4673 |       -1031.0864 |
[32m[20221208 15:06:06 @agent_ppo2.py:179][0m |          -0.0555 |           0.4591 |       -1037.0527 |
[32m[20221208 15:06:06 @agent_ppo2.py:179][0m |          -0.0588 |           0.4540 |       -1075.9695 |
[32m[20221208 15:06:06 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.30
[32m[20221208 15:06:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.32
[32m[20221208 15:06:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.36
[32m[20221208 15:06:06 @agent_ppo2.py:137][0m Total time:      29.53 min
[32m[20221208 15:06:06 @agent_ppo2.py:139][0m 2385920 total steps have happened
[32m[20221208 15:06:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1165 --------------------------#
[32m[20221208 15:06:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |           0.1108 |           3.6187 |       -1050.6564 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |           0.0739 |           3.1757 |        -797.9647 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |           0.0251 |           3.0567 |        -902.4573 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |           0.0070 |           2.9764 |        -929.6160 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |          -0.0210 |           2.8858 |        -991.7701 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |          -0.0365 |           2.8429 |       -1044.5379 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |          -0.0358 |           2.7927 |       -1036.6307 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |          -0.0508 |           2.7667 |       -1098.9145 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |          -0.0539 |           2.7341 |       -1125.1357 |
[32m[20221208 15:06:07 @agent_ppo2.py:179][0m |          -0.0589 |           2.6902 |       -1151.7973 |
[32m[20221208 15:06:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.60
[32m[20221208 15:06:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.91
[32m[20221208 15:06:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.14
[32m[20221208 15:06:07 @agent_ppo2.py:137][0m Total time:      29.55 min
[32m[20221208 15:06:07 @agent_ppo2.py:139][0m 2387968 total steps have happened
[32m[20221208 15:06:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1166 --------------------------#
[32m[20221208 15:06:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:06:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |           0.0865 |           3.2239 |       -1133.7262 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |           0.0336 |           2.2676 |       -1039.2403 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |           0.0181 |           2.0464 |        -971.1583 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |          -0.0177 |           1.8523 |        -975.7669 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |          -0.0448 |           1.7844 |       -1036.4655 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |          -0.0573 |           1.6800 |       -1065.0650 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |          -0.0646 |           1.6125 |       -1088.1231 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |          -0.0675 |           1.5510 |       -1077.0425 |
[32m[20221208 15:06:08 @agent_ppo2.py:179][0m |          -0.0738 |           1.5264 |       -1073.3545 |
[32m[20221208 15:06:09 @agent_ppo2.py:179][0m |          -0.0799 |           1.4695 |       -1095.0827 |
[32m[20221208 15:06:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:06:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.52
[32m[20221208 15:06:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.42
[32m[20221208 15:06:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.68
[32m[20221208 15:06:09 @agent_ppo2.py:137][0m Total time:      29.58 min
[32m[20221208 15:06:09 @agent_ppo2.py:139][0m 2390016 total steps have happened
[32m[20221208 15:06:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1167 --------------------------#
[32m[20221208 15:06:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |           0.0939 |           1.2016 |       -1213.5082 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |           0.0985 |           0.9361 |        -631.9054 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0065 |           0.8325 |        -352.2169 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0370 |           0.7825 |        -395.0766 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0479 |           0.7445 |        -422.2127 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0581 |           0.7134 |        -427.7866 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0587 |           0.6893 |        -444.2950 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0653 |           0.6750 |        -463.9244 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0758 |           0.6539 |        -468.2614 |
[32m[20221208 15:06:10 @agent_ppo2.py:179][0m |          -0.0737 |           0.6450 |        -470.6305 |
[32m[20221208 15:06:10 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.53
[32m[20221208 15:06:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.66
[32m[20221208 15:06:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.10
[32m[20221208 15:06:10 @agent_ppo2.py:137][0m Total time:      29.60 min
[32m[20221208 15:06:10 @agent_ppo2.py:139][0m 2392064 total steps have happened
[32m[20221208 15:06:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1168 --------------------------#
[32m[20221208 15:06:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |           0.0548 |           1.1872 |       -1126.0006 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |           0.0276 |           0.9202 |       -1026.8614 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |           0.0073 |           0.8149 |       -1075.5087 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0192 |           0.7736 |       -1110.3353 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0247 |           0.7231 |       -1032.5717 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0098 |           0.6876 |        -717.5872 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0220 |           0.6593 |        -681.2100 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0436 |           0.6357 |       -1144.4705 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0423 |           0.6146 |       -1081.1509 |
[32m[20221208 15:06:11 @agent_ppo2.py:179][0m |          -0.0491 |           0.5971 |       -1168.4215 |
[32m[20221208 15:06:11 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.20
[32m[20221208 15:06:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.02
[32m[20221208 15:06:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.15
[32m[20221208 15:06:12 @agent_ppo2.py:137][0m Total time:      29.63 min
[32m[20221208 15:06:12 @agent_ppo2.py:139][0m 2394112 total steps have happened
[32m[20221208 15:06:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1169 --------------------------#
[32m[20221208 15:06:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:12 @agent_ppo2.py:179][0m |           0.0650 |           2.3827 |       -1123.5562 |
[32m[20221208 15:06:12 @agent_ppo2.py:179][0m |           0.0227 |           2.0465 |       -1027.0737 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |           0.0168 |           1.9328 |        -942.4174 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0210 |           1.8684 |       -1029.9129 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0377 |           1.8237 |        -935.8274 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0519 |           1.7955 |       -1046.3932 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0588 |           1.7479 |       -1091.5706 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0640 |           1.7165 |       -1107.7758 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0650 |           1.6817 |       -1121.5189 |
[32m[20221208 15:06:13 @agent_ppo2.py:179][0m |          -0.0690 |           1.6553 |       -1096.0879 |
[32m[20221208 15:06:13 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.41
[32m[20221208 15:06:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.75
[32m[20221208 15:06:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.36
[32m[20221208 15:06:13 @agent_ppo2.py:137][0m Total time:      29.65 min
[32m[20221208 15:06:13 @agent_ppo2.py:139][0m 2396160 total steps have happened
[32m[20221208 15:06:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1170 --------------------------#
[32m[20221208 15:06:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:06:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |           0.0765 |           3.1394 |       -1104.8973 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |           0.0472 |           2.6060 |        -862.5635 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0010 |           2.4676 |        -914.1828 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0194 |           2.3399 |        -975.1568 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0267 |           2.2608 |        -969.7874 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0346 |           2.1576 |       -1013.2577 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0434 |           2.1297 |       -1019.2588 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0450 |           2.0837 |       -1036.1138 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0478 |           2.0176 |       -1046.6100 |
[32m[20221208 15:06:14 @agent_ppo2.py:179][0m |          -0.0539 |           1.9683 |       -1057.4289 |
[32m[20221208 15:06:14 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.37
[32m[20221208 15:06:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.82
[32m[20221208 15:06:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.19
[32m[20221208 15:06:15 @agent_ppo2.py:137][0m Total time:      29.68 min
[32m[20221208 15:06:15 @agent_ppo2.py:139][0m 2398208 total steps have happened
[32m[20221208 15:06:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1171 --------------------------#
[32m[20221208 15:06:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:15 @agent_ppo2.py:179][0m |           0.1141 |           3.1555 |        -894.1700 |
[32m[20221208 15:06:15 @agent_ppo2.py:179][0m |           0.0554 |           2.5958 |        -754.9868 |
[32m[20221208 15:06:15 @agent_ppo2.py:179][0m |           0.0152 |           2.4259 |        -786.7957 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0130 |           2.3042 |        -789.4283 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0364 |           2.2539 |        -816.9538 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0482 |           2.1714 |        -819.4249 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0600 |           2.1039 |        -852.3484 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0662 |           2.0668 |        -864.2032 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0724 |           2.0331 |        -867.5414 |
[32m[20221208 15:06:16 @agent_ppo2.py:179][0m |          -0.0742 |           1.9895 |        -881.2408 |
[32m[20221208 15:06:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.58
[32m[20221208 15:06:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.15
[32m[20221208 15:06:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.70
[32m[20221208 15:06:16 @agent_ppo2.py:137][0m Total time:      29.70 min
[32m[20221208 15:06:16 @agent_ppo2.py:139][0m 2400256 total steps have happened
[32m[20221208 15:06:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1172 --------------------------#
[32m[20221208 15:06:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |           0.1063 |           1.2592 |       -1073.9067 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |           0.3240 |           1.1981 |        -848.3238 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0170 |           1.1504 |        -900.4155 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0248 |           1.1417 |        -932.7212 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0328 |           1.1336 |        -938.9714 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0361 |           1.1273 |        -955.3420 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0430 |           1.1109 |        -980.0636 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0417 |           1.1229 |        -975.7017 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0465 |           1.1086 |        -985.9052 |
[32m[20221208 15:06:17 @agent_ppo2.py:179][0m |          -0.0440 |           1.1035 |        -975.1222 |
[32m[20221208 15:06:17 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:06:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.91
[32m[20221208 15:06:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 93.28
[32m[20221208 15:06:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.06
[32m[20221208 15:06:18 @agent_ppo2.py:137][0m Total time:      29.73 min
[32m[20221208 15:06:18 @agent_ppo2.py:139][0m 2402304 total steps have happened
[32m[20221208 15:06:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1173 --------------------------#
[32m[20221208 15:06:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:06:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:18 @agent_ppo2.py:179][0m |           0.0987 |           2.3403 |       -1038.9817 |
[32m[20221208 15:06:18 @agent_ppo2.py:179][0m |           0.0594 |           2.0429 |        -929.1263 |
[32m[20221208 15:06:18 @agent_ppo2.py:179][0m |           0.0216 |           1.9263 |       -1013.9547 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0075 |           1.8379 |       -1080.3667 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0182 |           1.8061 |       -1113.5722 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0282 |           1.7479 |       -1118.7243 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0327 |           1.7204 |       -1136.7834 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0369 |           1.6999 |       -1169.7553 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0436 |           1.6753 |       -1184.5289 |
[32m[20221208 15:06:19 @agent_ppo2.py:179][0m |          -0.0460 |           1.6474 |       -1190.8249 |
[32m[20221208 15:06:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.94
[32m[20221208 15:06:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.67
[32m[20221208 15:06:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.36
[32m[20221208 15:06:19 @agent_ppo2.py:137][0m Total time:      29.75 min
[32m[20221208 15:06:19 @agent_ppo2.py:139][0m 2404352 total steps have happened
[32m[20221208 15:06:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1174 --------------------------#
[32m[20221208 15:06:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |           0.0463 |           1.1431 |       -1092.2932 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0035 |           1.0086 |        -944.5550 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0226 |           0.9811 |        -968.8039 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0317 |           0.9491 |        -986.9189 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0412 |           0.9373 |       -1006.2855 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0434 |           0.9249 |       -1027.6054 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0458 |           0.9487 |       -1039.9632 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0481 |           0.9024 |       -1050.1359 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0491 |           0.9285 |       -1061.0559 |
[32m[20221208 15:06:20 @agent_ppo2.py:179][0m |          -0.0479 |           0.8939 |       -1049.6217 |
[32m[20221208 15:06:20 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:06:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.48
[32m[20221208 15:06:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.52
[32m[20221208 15:06:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.72
[32m[20221208 15:06:21 @agent_ppo2.py:137][0m Total time:      29.77 min
[32m[20221208 15:06:21 @agent_ppo2.py:139][0m 2406400 total steps have happened
[32m[20221208 15:06:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1175 --------------------------#
[32m[20221208 15:06:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:21 @agent_ppo2.py:179][0m |           0.0818 |           0.8117 |       -1084.0880 |
[32m[20221208 15:06:21 @agent_ppo2.py:179][0m |           0.0356 |           0.5100 |        -945.8343 |
[32m[20221208 15:06:21 @agent_ppo2.py:179][0m |          -0.0040 |           0.4387 |       -1027.2988 |
[32m[20221208 15:06:21 @agent_ppo2.py:179][0m |          -0.0097 |           0.3991 |       -1035.5248 |
[32m[20221208 15:06:21 @agent_ppo2.py:179][0m |          -0.0111 |           0.3818 |       -1045.5517 |
[32m[20221208 15:06:22 @agent_ppo2.py:179][0m |          -0.0148 |           0.3622 |       -1051.7888 |
[32m[20221208 15:06:22 @agent_ppo2.py:179][0m |          -0.0259 |           0.3478 |       -1079.3004 |
[32m[20221208 15:06:22 @agent_ppo2.py:179][0m |          -0.0220 |           0.3444 |       -1074.8827 |
[32m[20221208 15:06:22 @agent_ppo2.py:179][0m |          -0.0250 |           0.3383 |       -1095.3575 |
[32m[20221208 15:06:22 @agent_ppo2.py:179][0m |          -0.0273 |           0.3297 |       -1107.8170 |
[32m[20221208 15:06:22 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.66
[32m[20221208 15:06:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.20
[32m[20221208 15:06:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.55
[32m[20221208 15:06:22 @agent_ppo2.py:137][0m Total time:      29.80 min
[32m[20221208 15:06:22 @agent_ppo2.py:139][0m 2408448 total steps have happened
[32m[20221208 15:06:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1176 --------------------------#
[32m[20221208 15:06:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |           0.1187 |           4.8305 |        -882.3889 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |           0.0637 |           3.7257 |        -746.1255 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |           0.0172 |           3.3285 |        -779.9102 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0125 |           3.0755 |        -819.1382 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0269 |           2.9284 |        -826.3639 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0435 |           2.7481 |        -877.1679 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0460 |           2.6809 |        -902.6570 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0527 |           2.5893 |        -897.6748 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0579 |           2.5262 |        -934.6429 |
[32m[20221208 15:06:23 @agent_ppo2.py:179][0m |          -0.0621 |           2.4789 |        -943.6227 |
[32m[20221208 15:06:23 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.72
[32m[20221208 15:06:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.68
[32m[20221208 15:06:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.73
[32m[20221208 15:06:24 @agent_ppo2.py:137][0m Total time:      29.82 min
[32m[20221208 15:06:24 @agent_ppo2.py:139][0m 2410496 total steps have happened
[32m[20221208 15:06:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1177 --------------------------#
[32m[20221208 15:06:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:24 @agent_ppo2.py:179][0m |           0.1212 |           2.6224 |       -1003.5603 |
[32m[20221208 15:06:24 @agent_ppo2.py:179][0m |           0.0971 |           2.1182 |        -837.4668 |
[32m[20221208 15:06:24 @agent_ppo2.py:179][0m |           0.0278 |           1.9678 |        -897.0518 |
[32m[20221208 15:06:24 @agent_ppo2.py:179][0m |          -0.0105 |           1.9107 |       -1005.1101 |
[32m[20221208 15:06:24 @agent_ppo2.py:179][0m |          -0.0210 |           1.8614 |       -1062.4593 |
[32m[20221208 15:06:25 @agent_ppo2.py:179][0m |          -0.0318 |           1.8058 |       -1097.4253 |
[32m[20221208 15:06:25 @agent_ppo2.py:179][0m |          -0.0428 |           1.7586 |       -1131.1078 |
[32m[20221208 15:06:25 @agent_ppo2.py:179][0m |          -0.0523 |           1.7399 |       -1172.0799 |
[32m[20221208 15:06:25 @agent_ppo2.py:179][0m |          -0.0544 |           1.7295 |       -1190.2957 |
[32m[20221208 15:06:25 @agent_ppo2.py:179][0m |          -0.0600 |           1.6995 |       -1197.3416 |
[32m[20221208 15:06:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:06:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.48
[32m[20221208 15:06:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.68
[32m[20221208 15:06:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.75
[32m[20221208 15:06:25 @agent_ppo2.py:137][0m Total time:      29.85 min
[32m[20221208 15:06:25 @agent_ppo2.py:139][0m 2412544 total steps have happened
[32m[20221208 15:06:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1178 --------------------------#
[32m[20221208 15:06:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |           0.0559 |           2.1531 |       -1125.3993 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |           0.0792 |           1.9122 |        -956.6867 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |           0.0119 |           1.8258 |        -986.4538 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |           0.0048 |           1.7593 |        -681.8672 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |          -0.0222 |           1.7260 |        -620.6868 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |          -0.0474 |           1.6925 |        -681.0475 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |          -0.0563 |           1.6611 |        -709.0443 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |          -0.0579 |           1.6465 |        -729.8728 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |          -0.0592 |           1.6248 |        -730.7992 |
[32m[20221208 15:06:26 @agent_ppo2.py:179][0m |          -0.0615 |           1.6034 |        -731.1308 |
[32m[20221208 15:06:26 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.52
[32m[20221208 15:06:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.16
[32m[20221208 15:06:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.57
[32m[20221208 15:06:27 @agent_ppo2.py:137][0m Total time:      29.87 min
[32m[20221208 15:06:27 @agent_ppo2.py:139][0m 2414592 total steps have happened
[32m[20221208 15:06:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1179 --------------------------#
[32m[20221208 15:06:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |           0.1206 |           4.1601 |       -1056.8524 |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |           0.0937 |           3.5359 |        -514.8173 |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |           0.0501 |           3.3155 |        -519.3311 |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |           0.0045 |           3.2122 |        -645.3529 |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |          -0.0149 |           3.1352 |        -725.2941 |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |          -0.0313 |           3.1246 |        -758.2136 |
[32m[20221208 15:06:27 @agent_ppo2.py:179][0m |          -0.0388 |           3.0471 |        -778.8342 |
[32m[20221208 15:06:28 @agent_ppo2.py:179][0m |          -0.0464 |           2.9904 |        -818.3471 |
[32m[20221208 15:06:28 @agent_ppo2.py:179][0m |          -0.0496 |           2.9663 |        -819.7145 |
[32m[20221208 15:06:28 @agent_ppo2.py:179][0m |          -0.0544 |           2.9523 |        -844.3628 |
[32m[20221208 15:06:28 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.98
[32m[20221208 15:06:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.21
[32m[20221208 15:06:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.49
[32m[20221208 15:06:28 @agent_ppo2.py:137][0m Total time:      29.90 min
[32m[20221208 15:06:28 @agent_ppo2.py:139][0m 2416640 total steps have happened
[32m[20221208 15:06:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1180 --------------------------#
[32m[20221208 15:06:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |           0.0999 |           3.6831 |        -911.2054 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |           0.1468 |           3.3269 |        -571.8285 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |           0.0202 |           3.2183 |        -622.5911 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0040 |           3.1471 |        -724.5853 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0235 |           3.0705 |        -770.9480 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0317 |           3.0538 |        -794.8397 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0434 |           3.0062 |        -845.0612 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0545 |           2.9992 |        -891.8873 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0635 |           2.9906 |        -927.0664 |
[32m[20221208 15:06:29 @agent_ppo2.py:179][0m |          -0.0678 |           2.9135 |        -944.8222 |
[32m[20221208 15:06:29 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.02
[32m[20221208 15:06:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.41
[32m[20221208 15:06:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.44
[32m[20221208 15:06:30 @agent_ppo2.py:137][0m Total time:      29.92 min
[32m[20221208 15:06:30 @agent_ppo2.py:139][0m 2418688 total steps have happened
[32m[20221208 15:06:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1181 --------------------------#
[32m[20221208 15:06:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:06:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |           0.1774 |           3.7321 |       -1157.3197 |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |           0.1092 |           3.3895 |        -547.6040 |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |           0.0488 |           3.2031 |        -565.0317 |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |           0.0140 |           3.1345 |        -688.7385 |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |          -0.0074 |           3.0290 |        -759.2742 |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |          -0.0217 |           2.9776 |        -791.2781 |
[32m[20221208 15:06:30 @agent_ppo2.py:179][0m |          -0.0307 |           2.8998 |        -827.7823 |
[32m[20221208 15:06:31 @agent_ppo2.py:179][0m |          -0.0416 |           2.8667 |        -865.1025 |
[32m[20221208 15:06:31 @agent_ppo2.py:179][0m |          -0.0440 |           2.8174 |        -877.1512 |
[32m[20221208 15:06:31 @agent_ppo2.py:179][0m |          -0.0468 |           2.7765 |        -901.3709 |
[32m[20221208 15:06:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.26
[32m[20221208 15:06:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.30
[32m[20221208 15:06:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 20.63
[32m[20221208 15:06:31 @agent_ppo2.py:137][0m Total time:      29.95 min
[32m[20221208 15:06:31 @agent_ppo2.py:139][0m 2420736 total steps have happened
[32m[20221208 15:06:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1182 --------------------------#
[32m[20221208 15:06:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |           0.1330 |           2.1714 |        -969.5146 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |           0.0655 |           1.6433 |        -614.5817 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |           0.0101 |           1.4046 |        -714.9178 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0128 |           1.2824 |        -780.1103 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0271 |           1.1917 |        -819.4344 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0361 |           1.1307 |        -864.9432 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0395 |           1.0847 |        -888.9547 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0426 |           1.0321 |        -882.7599 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0491 |           0.9955 |        -915.9115 |
[32m[20221208 15:06:32 @agent_ppo2.py:179][0m |          -0.0496 |           0.9675 |        -920.8708 |
[32m[20221208 15:06:32 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.66
[32m[20221208 15:06:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.81
[32m[20221208 15:06:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 20.80
[32m[20221208 15:06:33 @agent_ppo2.py:137][0m Total time:      29.97 min
[32m[20221208 15:06:33 @agent_ppo2.py:139][0m 2422784 total steps have happened
[32m[20221208 15:06:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1183 --------------------------#
[32m[20221208 15:06:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |           0.1456 |           3.3545 |       -1003.4094 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |           0.1125 |           2.8690 |        -667.8269 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |           0.0437 |           2.7502 |        -523.7342 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |           0.0198 |           2.6498 |        -539.1459 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |          -0.0037 |           2.6729 |        -614.4373 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |          -0.0174 |           2.6023 |        -667.9735 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |          -0.0230 |           2.5508 |        -690.7514 |
[32m[20221208 15:06:33 @agent_ppo2.py:179][0m |          -0.0329 |           2.5079 |        -743.6837 |
[32m[20221208 15:06:34 @agent_ppo2.py:179][0m |          -0.0358 |           2.5125 |        -734.6377 |
[32m[20221208 15:06:34 @agent_ppo2.py:179][0m |          -0.0429 |           2.4626 |        -766.5984 |
[32m[20221208 15:06:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.08
[32m[20221208 15:06:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.54
[32m[20221208 15:06:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.81
[32m[20221208 15:06:34 @agent_ppo2.py:137][0m Total time:      30.00 min
[32m[20221208 15:06:34 @agent_ppo2.py:139][0m 2424832 total steps have happened
[32m[20221208 15:06:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1184 --------------------------#
[32m[20221208 15:06:34 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:06:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |           0.1122 |           1.7024 |        -946.4917 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |           0.2102 |           1.2663 |        -498.9460 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |           0.0794 |           1.1336 |        -428.1035 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |           0.0319 |           1.0600 |        -478.4159 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |           0.0064 |           1.0141 |        -544.1541 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |          -0.0199 |           0.9691 |        -594.4954 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |          -0.0304 |           0.9398 |        -634.8225 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |          -0.0388 |           0.9227 |        -663.9132 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |          -0.0468 |           0.8971 |        -692.7771 |
[32m[20221208 15:06:35 @agent_ppo2.py:179][0m |          -0.0533 |           0.8747 |        -688.6420 |
[32m[20221208 15:06:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.03
[32m[20221208 15:06:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.67
[32m[20221208 15:06:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.28
[32m[20221208 15:06:36 @agent_ppo2.py:137][0m Total time:      30.02 min
[32m[20221208 15:06:36 @agent_ppo2.py:139][0m 2426880 total steps have happened
[32m[20221208 15:06:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1185 --------------------------#
[32m[20221208 15:06:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |           0.1028 |           0.8124 |        -879.9517 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |           0.0295 |           0.6749 |        -231.0954 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |          -0.0108 |           0.6327 |        -270.0079 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |          -0.0237 |           0.6014 |        -286.3605 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |          -0.0328 |           0.5830 |        -311.6730 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |          -0.0392 |           0.5622 |        -320.6601 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |          -0.0427 |           0.5448 |        -328.0424 |
[32m[20221208 15:06:36 @agent_ppo2.py:179][0m |          -0.0424 |           0.5262 |        -328.1548 |
[32m[20221208 15:06:37 @agent_ppo2.py:179][0m |          -0.0445 |           0.5153 |        -338.8477 |
[32m[20221208 15:06:37 @agent_ppo2.py:179][0m |          -0.0451 |           0.5008 |        -354.9658 |
[32m[20221208 15:06:37 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:06:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 33.86
[32m[20221208 15:06:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 64.23
[32m[20221208 15:06:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.06
[32m[20221208 15:06:37 @agent_ppo2.py:137][0m Total time:      30.05 min
[32m[20221208 15:06:37 @agent_ppo2.py:139][0m 2428928 total steps have happened
[32m[20221208 15:06:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1186 --------------------------#
[32m[20221208 15:06:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |           0.0862 |           3.7193 |       -1191.6500 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |           0.0582 |           2.9428 |       -1047.3347 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0004 |           2.7612 |       -1154.9929 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0044 |           2.6026 |       -1145.3103 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0307 |           2.5384 |       -1185.6747 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0399 |           2.4003 |       -1211.6075 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0468 |           2.2748 |       -1204.7854 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0556 |           2.2353 |       -1215.8995 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0553 |           2.1581 |       -1249.6484 |
[32m[20221208 15:06:38 @agent_ppo2.py:179][0m |          -0.0512 |           2.1124 |       -1250.5142 |
[32m[20221208 15:06:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.85
[32m[20221208 15:06:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.66
[32m[20221208 15:06:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 121.89
[32m[20221208 15:06:38 @agent_ppo2.py:137][0m Total time:      30.07 min
[32m[20221208 15:06:38 @agent_ppo2.py:139][0m 2430976 total steps have happened
[32m[20221208 15:06:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1187 --------------------------#
[32m[20221208 15:06:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |           0.0606 |           1.9780 |       -1192.1676 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |           0.0329 |           1.6877 |        -981.1265 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |           0.0101 |           1.5603 |       -1179.4614 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |          -0.0091 |           1.4897 |       -1155.7447 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |          -0.0177 |           1.4238 |       -1131.5938 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |          -0.0322 |           1.4004 |       -1181.4449 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |          -0.0347 |           1.3542 |       -1227.2412 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |          -0.0430 |           1.3170 |       -1191.6211 |
[32m[20221208 15:06:39 @agent_ppo2.py:179][0m |          -0.0407 |           1.2930 |       -1164.2012 |
[32m[20221208 15:06:40 @agent_ppo2.py:179][0m |          -0.0101 |           1.2550 |        -662.2333 |
[32m[20221208 15:06:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.26
[32m[20221208 15:06:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.10
[32m[20221208 15:06:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.10
[32m[20221208 15:06:40 @agent_ppo2.py:137][0m Total time:      30.09 min
[32m[20221208 15:06:40 @agent_ppo2.py:139][0m 2433024 total steps have happened
[32m[20221208 15:06:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1188 --------------------------#
[32m[20221208 15:06:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:06:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:40 @agent_ppo2.py:179][0m |           0.0461 |           1.4649 |        -961.9867 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0556 |           1.2691 |        -331.8439 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0250 |           1.2799 |        -306.3723 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0178 |           1.2527 |        -373.1933 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0056 |           1.2350 |        -422.4512 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |          -0.0036 |           1.2298 |        -688.8019 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |          -0.0192 |           1.2313 |        -990.3695 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0028 |           1.2236 |        -787.8954 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0105 |           1.2239 |        -370.5703 |
[32m[20221208 15:06:41 @agent_ppo2.py:179][0m |           0.0020 |           1.2261 |        -425.8171 |
[32m[20221208 15:06:41 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.17
[32m[20221208 15:06:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.79
[32m[20221208 15:06:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.35
[32m[20221208 15:06:41 @agent_ppo2.py:137][0m Total time:      30.12 min
[32m[20221208 15:06:41 @agent_ppo2.py:139][0m 2435072 total steps have happened
[32m[20221208 15:06:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1189 --------------------------#
[32m[20221208 15:06:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.1045 |           2.1639 |       -1163.7571 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.1597 |           1.9520 |       -1073.9503 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.0944 |           1.8295 |       -1099.6503 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.0698 |           1.8106 |       -1070.7244 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.0391 |           1.7178 |       -1117.3234 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.0156 |           1.7113 |       -1198.0550 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |           0.0259 |           1.6382 |       -1210.1307 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |          -0.0102 |           1.6339 |       -1238.8038 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |          -0.0124 |           1.6137 |       -1228.6550 |
[32m[20221208 15:06:42 @agent_ppo2.py:179][0m |          -0.0059 |           1.6253 |       -1202.9955 |
[32m[20221208 15:06:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:06:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.70
[32m[20221208 15:06:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.08
[32m[20221208 15:06:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.41
[32m[20221208 15:06:43 @agent_ppo2.py:137][0m Total time:      30.14 min
[32m[20221208 15:06:43 @agent_ppo2.py:139][0m 2437120 total steps have happened
[32m[20221208 15:06:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1190 --------------------------#
[32m[20221208 15:06:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:43 @agent_ppo2.py:179][0m |           0.0900 |           0.6958 |       -1012.3256 |
[32m[20221208 15:06:43 @agent_ppo2.py:179][0m |           0.0703 |           0.5965 |        -558.0095 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0041 |           0.5656 |        -380.6124 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0392 |           0.5423 |        -402.1840 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0572 |           0.5283 |        -442.0426 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0667 |           0.5147 |        -458.5134 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0752 |           0.5102 |        -464.7289 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0812 |           0.5007 |        -485.9317 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0837 |           0.4955 |        -494.3906 |
[32m[20221208 15:06:44 @agent_ppo2.py:179][0m |          -0.0857 |           0.4821 |        -498.0391 |
[32m[20221208 15:06:44 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.73
[32m[20221208 15:06:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.25
[32m[20221208 15:06:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.12
[32m[20221208 15:06:44 @agent_ppo2.py:137][0m Total time:      30.17 min
[32m[20221208 15:06:44 @agent_ppo2.py:139][0m 2439168 total steps have happened
[32m[20221208 15:06:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1191 --------------------------#
[32m[20221208 15:06:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |           0.2327 |           1.2953 |       -1221.6999 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |           0.0551 |           0.8374 |        -950.2080 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |           0.0267 |           0.6945 |       -1103.7003 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0034 |           0.6185 |       -1322.3545 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0197 |           0.5777 |       -1330.9871 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0278 |           0.5452 |       -1349.8937 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0340 |           0.5211 |       -1364.3124 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0420 |           0.5032 |       -1375.4196 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0442 |           0.4869 |       -1413.5810 |
[32m[20221208 15:06:45 @agent_ppo2.py:179][0m |          -0.0459 |           0.4778 |       -1393.0779 |
[32m[20221208 15:06:45 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.59
[32m[20221208 15:06:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.00
[32m[20221208 15:06:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.03
[32m[20221208 15:06:46 @agent_ppo2.py:137][0m Total time:      30.19 min
[32m[20221208 15:06:46 @agent_ppo2.py:139][0m 2441216 total steps have happened
[32m[20221208 15:06:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1192 --------------------------#
[32m[20221208 15:06:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:06:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:46 @agent_ppo2.py:179][0m |           0.1032 |           1.2880 |       -1173.0056 |
[32m[20221208 15:06:46 @agent_ppo2.py:179][0m |           0.0550 |           0.9970 |        -993.1790 |
[32m[20221208 15:06:46 @agent_ppo2.py:179][0m |           0.0146 |           0.9278 |        -962.4792 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0111 |           0.8684 |       -1117.5108 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0197 |           0.8528 |       -1137.6955 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0309 |           0.8291 |       -1193.5423 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0325 |           0.8042 |       -1197.6863 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0365 |           0.8280 |       -1175.1804 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0440 |           0.7825 |       -1211.7029 |
[32m[20221208 15:06:47 @agent_ppo2.py:179][0m |          -0.0457 |           0.7555 |       -1256.5200 |
[32m[20221208 15:06:47 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.16
[32m[20221208 15:06:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.16
[32m[20221208 15:06:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.63
[32m[20221208 15:06:47 @agent_ppo2.py:137][0m Total time:      30.22 min
[32m[20221208 15:06:47 @agent_ppo2.py:139][0m 2443264 total steps have happened
[32m[20221208 15:06:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1193 --------------------------#
[32m[20221208 15:06:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |           0.1265 |           3.7430 |       -1039.4642 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |           0.1471 |           3.1525 |        -525.4410 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |           0.0892 |           2.9439 |        -537.0727 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |           0.0439 |           2.8236 |        -738.2958 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |           0.0301 |           2.7327 |        -821.3999 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |           0.0122 |           2.7082 |        -897.6387 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |          -0.0076 |           2.6519 |        -983.6288 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |          -0.0179 |           2.5884 |       -1020.9330 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |          -0.0310 |           2.5655 |       -1066.5833 |
[32m[20221208 15:06:48 @agent_ppo2.py:179][0m |          -0.0362 |           2.5535 |       -1106.2265 |
[32m[20221208 15:06:48 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.32
[32m[20221208 15:06:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.38
[32m[20221208 15:06:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.94
[32m[20221208 15:06:49 @agent_ppo2.py:137][0m Total time:      30.24 min
[32m[20221208 15:06:49 @agent_ppo2.py:139][0m 2445312 total steps have happened
[32m[20221208 15:06:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1194 --------------------------#
[32m[20221208 15:06:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:49 @agent_ppo2.py:179][0m |           0.1037 |           0.8764 |       -1154.9661 |
[32m[20221208 15:06:49 @agent_ppo2.py:179][0m |           0.0092 |           0.5069 |       -1080.4775 |
[32m[20221208 15:06:49 @agent_ppo2.py:179][0m |          -0.0126 |           0.4426 |       -1113.3604 |
[32m[20221208 15:06:49 @agent_ppo2.py:179][0m |          -0.0238 |           0.4124 |       -1149.5946 |
[32m[20221208 15:06:50 @agent_ppo2.py:179][0m |          -0.0351 |           0.3958 |       -1190.1708 |
[32m[20221208 15:06:50 @agent_ppo2.py:179][0m |          -0.0370 |           0.3810 |       -1187.9257 |
[32m[20221208 15:06:50 @agent_ppo2.py:179][0m |          -0.0393 |           0.3714 |       -1209.1747 |
[32m[20221208 15:06:50 @agent_ppo2.py:179][0m |          -0.0409 |           0.3643 |       -1230.3579 |
[32m[20221208 15:06:50 @agent_ppo2.py:179][0m |          -0.0412 |           0.3590 |       -1241.3308 |
[32m[20221208 15:06:50 @agent_ppo2.py:179][0m |          -0.0428 |           0.3529 |       -1245.2675 |
[32m[20221208 15:06:50 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:06:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 27.73
[32m[20221208 15:06:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 34.03
[32m[20221208 15:06:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.14
[32m[20221208 15:06:50 @agent_ppo2.py:137][0m Total time:      30.27 min
[32m[20221208 15:06:50 @agent_ppo2.py:139][0m 2447360 total steps have happened
[32m[20221208 15:06:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1195 --------------------------#
[32m[20221208 15:06:51 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 15:06:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |           0.1417 |           1.7515 |       -1161.3289 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |           0.0965 |           1.2215 |        -898.1230 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |           0.0248 |           1.0893 |        -983.8455 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0221 |           1.0253 |       -1078.9461 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0396 |           0.9812 |       -1145.3449 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0575 |           0.9448 |       -1172.3852 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0632 |           0.9147 |       -1184.1285 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0745 |           0.9032 |       -1214.6195 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0712 |           0.8733 |       -1231.8219 |
[32m[20221208 15:06:51 @agent_ppo2.py:179][0m |          -0.0817 |           0.8574 |       -1233.7003 |
[32m[20221208 15:06:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.15
[32m[20221208 15:06:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.39
[32m[20221208 15:06:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.14
[32m[20221208 15:06:52 @agent_ppo2.py:137][0m Total time:      30.29 min
[32m[20221208 15:06:52 @agent_ppo2.py:139][0m 2449408 total steps have happened
[32m[20221208 15:06:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1196 --------------------------#
[32m[20221208 15:06:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:06:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:52 @agent_ppo2.py:179][0m |           0.0868 |           3.1158 |       -1160.8077 |
[32m[20221208 15:06:52 @agent_ppo2.py:179][0m |           0.0419 |           2.7349 |       -1174.1842 |
[32m[20221208 15:06:52 @agent_ppo2.py:179][0m |          -0.0025 |           2.5338 |       -1207.9649 |
[32m[20221208 15:06:52 @agent_ppo2.py:179][0m |          -0.0153 |           2.4284 |       -1239.5873 |
[32m[20221208 15:06:53 @agent_ppo2.py:179][0m |          -0.0278 |           2.3402 |       -1240.2115 |
[32m[20221208 15:06:53 @agent_ppo2.py:179][0m |          -0.0461 |           2.2519 |       -1281.9311 |
[32m[20221208 15:06:53 @agent_ppo2.py:179][0m |          -0.0503 |           2.1995 |       -1295.1693 |
[32m[20221208 15:06:53 @agent_ppo2.py:179][0m |          -0.0558 |           2.1672 |       -1293.0523 |
[32m[20221208 15:06:53 @agent_ppo2.py:179][0m |          -0.0591 |           2.1181 |       -1327.4135 |
[32m[20221208 15:06:53 @agent_ppo2.py:179][0m |          -0.0628 |           2.0642 |       -1347.2709 |
[32m[20221208 15:06:53 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:06:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.94
[32m[20221208 15:06:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.33
[32m[20221208 15:06:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.30
[32m[20221208 15:06:53 @agent_ppo2.py:137][0m Total time:      30.32 min
[32m[20221208 15:06:53 @agent_ppo2.py:139][0m 2451456 total steps have happened
[32m[20221208 15:06:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1197 --------------------------#
[32m[20221208 15:06:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |           0.0753 |           2.1190 |       -1215.5325 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |           0.0767 |           1.7927 |       -1103.7636 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |           0.0350 |           1.6315 |       -1066.7035 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0099 |           1.5495 |       -1170.0262 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0278 |           1.4537 |       -1245.0395 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0394 |           1.3980 |       -1290.6661 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0516 |           1.3540 |       -1328.5013 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0529 |           1.3105 |       -1280.4571 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0584 |           1.2583 |       -1331.6761 |
[32m[20221208 15:06:54 @agent_ppo2.py:179][0m |          -0.0559 |           1.2393 |       -1340.2087 |
[32m[20221208 15:06:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.51
[32m[20221208 15:06:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.49
[32m[20221208 15:06:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.06
[32m[20221208 15:06:55 @agent_ppo2.py:137][0m Total time:      30.34 min
[32m[20221208 15:06:55 @agent_ppo2.py:139][0m 2453504 total steps have happened
[32m[20221208 15:06:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1198 --------------------------#
[32m[20221208 15:06:55 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 15:06:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:55 @agent_ppo2.py:179][0m |           0.3068 |           0.4489 |       -1214.4735 |
[32m[20221208 15:06:55 @agent_ppo2.py:179][0m |           0.0529 |           0.3437 |       -1027.2651 |
[32m[20221208 15:06:55 @agent_ppo2.py:179][0m |           0.0150 |           0.3243 |       -1173.2266 |
[32m[20221208 15:06:55 @agent_ppo2.py:179][0m |           0.0095 |           0.3153 |       -1149.5595 |
[32m[20221208 15:06:56 @agent_ppo2.py:179][0m |          -0.0006 |           0.3106 |       -1173.3912 |
[32m[20221208 15:06:56 @agent_ppo2.py:179][0m |           0.0186 |           0.3065 |       -1110.0353 |
[32m[20221208 15:06:56 @agent_ppo2.py:179][0m |          -0.0040 |           0.3057 |       -1152.5932 |
[32m[20221208 15:06:56 @agent_ppo2.py:179][0m |          -0.0111 |           0.3024 |       -1213.4639 |
[32m[20221208 15:06:56 @agent_ppo2.py:179][0m |          -0.0085 |           0.3020 |       -1154.4419 |
[32m[20221208 15:06:56 @agent_ppo2.py:179][0m |          -0.0121 |           0.3011 |       -1193.6876 |
[32m[20221208 15:06:56 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:06:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.33
[32m[20221208 15:06:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 28.86
[32m[20221208 15:06:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.89
[32m[20221208 15:06:56 @agent_ppo2.py:137][0m Total time:      30.37 min
[32m[20221208 15:06:56 @agent_ppo2.py:139][0m 2455552 total steps have happened
[32m[20221208 15:06:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1199 --------------------------#
[32m[20221208 15:06:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:06:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |           0.0932 |           2.1088 |       -1106.5360 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |           0.0446 |           1.8887 |        -693.3110 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |           0.0120 |           1.6729 |        -714.9149 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0211 |           1.5376 |        -803.4792 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0361 |           1.4760 |        -842.7570 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0479 |           1.4363 |        -874.0374 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0360 |           1.3846 |        -832.6094 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0486 |           1.3467 |        -853.7720 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0586 |           1.3216 |        -894.9006 |
[32m[20221208 15:06:57 @agent_ppo2.py:179][0m |          -0.0622 |           1.2899 |        -914.0647 |
[32m[20221208 15:06:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:06:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.41
[32m[20221208 15:06:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.69
[32m[20221208 15:06:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.91
[32m[20221208 15:06:58 @agent_ppo2.py:137][0m Total time:      30.39 min
[32m[20221208 15:06:58 @agent_ppo2.py:139][0m 2457600 total steps have happened
[32m[20221208 15:06:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1200 --------------------------#
[32m[20221208 15:06:58 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:06:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:06:58 @agent_ppo2.py:179][0m |           0.0661 |           1.1034 |       -1105.1059 |
[32m[20221208 15:06:58 @agent_ppo2.py:179][0m |           0.0294 |           0.7353 |       -1017.1697 |
[32m[20221208 15:06:58 @agent_ppo2.py:179][0m |          -0.0043 |           0.6256 |       -1043.8620 |
[32m[20221208 15:06:58 @agent_ppo2.py:179][0m |          -0.0206 |           0.5773 |       -1038.2255 |
[32m[20221208 15:06:59 @agent_ppo2.py:179][0m |          -0.0194 |           0.5296 |       -1031.5396 |
[32m[20221208 15:06:59 @agent_ppo2.py:179][0m |          -0.0349 |           0.5115 |       -1072.2039 |
[32m[20221208 15:06:59 @agent_ppo2.py:179][0m |          -0.0328 |           0.4854 |       -1056.4841 |
[32m[20221208 15:06:59 @agent_ppo2.py:179][0m |          -0.0339 |           0.4674 |       -1074.8856 |
[32m[20221208 15:06:59 @agent_ppo2.py:179][0m |          -0.0419 |           0.4572 |       -1054.9724 |
[32m[20221208 15:06:59 @agent_ppo2.py:179][0m |          -0.0354 |           0.4401 |       -1072.6053 |
[32m[20221208 15:06:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:06:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 37.30
[32m[20221208 15:06:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.31
[32m[20221208 15:06:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.03
[32m[20221208 15:06:59 @agent_ppo2.py:137][0m Total time:      30.42 min
[32m[20221208 15:06:59 @agent_ppo2.py:139][0m 2459648 total steps have happened
[32m[20221208 15:06:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1201 --------------------------#
[32m[20221208 15:07:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |           0.1010 |           1.8766 |       -1023.4365 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |           0.0750 |           1.6208 |        -887.6797 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |           0.0129 |           1.4799 |        -989.2354 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0205 |           1.3974 |       -1059.7290 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0327 |           1.3125 |       -1092.8831 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0437 |           1.2647 |       -1117.0496 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0521 |           1.2392 |       -1137.2784 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0530 |           1.1939 |       -1156.2662 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0586 |           1.1725 |       -1160.6878 |
[32m[20221208 15:07:00 @agent_ppo2.py:179][0m |          -0.0588 |           1.1302 |       -1179.3031 |
[32m[20221208 15:07:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:07:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.93
[32m[20221208 15:07:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.15
[32m[20221208 15:07:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.66
[32m[20221208 15:07:01 @agent_ppo2.py:137][0m Total time:      30.44 min
[32m[20221208 15:07:01 @agent_ppo2.py:139][0m 2461696 total steps have happened
[32m[20221208 15:07:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1202 --------------------------#
[32m[20221208 15:07:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:01 @agent_ppo2.py:179][0m |           0.0939 |           0.6712 |       -1151.5242 |
[32m[20221208 15:07:01 @agent_ppo2.py:179][0m |           0.0722 |           0.4701 |        -943.6630 |
[32m[20221208 15:07:01 @agent_ppo2.py:179][0m |           0.0102 |           0.4339 |        -956.3176 |
[32m[20221208 15:07:01 @agent_ppo2.py:179][0m |          -0.0124 |           0.4125 |       -1096.0262 |
[32m[20221208 15:07:01 @agent_ppo2.py:179][0m |          -0.0398 |           0.4020 |       -1114.6861 |
[32m[20221208 15:07:02 @agent_ppo2.py:179][0m |          -0.0552 |           0.3892 |       -1179.7811 |
[32m[20221208 15:07:02 @agent_ppo2.py:179][0m |          -0.0609 |           0.3839 |       -1194.0875 |
[32m[20221208 15:07:02 @agent_ppo2.py:179][0m |          -0.0678 |           0.3755 |       -1218.7889 |
[32m[20221208 15:07:02 @agent_ppo2.py:179][0m |          -0.0626 |           0.3711 |       -1220.2327 |
[32m[20221208 15:07:02 @agent_ppo2.py:179][0m |          -0.0603 |           0.3681 |       -1187.7718 |
[32m[20221208 15:07:02 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:07:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.24
[32m[20221208 15:07:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.90
[32m[20221208 15:07:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.66
[32m[20221208 15:07:02 @agent_ppo2.py:137][0m Total time:      30.46 min
[32m[20221208 15:07:02 @agent_ppo2.py:139][0m 2463744 total steps have happened
[32m[20221208 15:07:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1203 --------------------------#
[32m[20221208 15:07:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |           0.0880 |           0.4720 |        -957.8028 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |           0.0213 |           0.3790 |        -727.6185 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0017 |           0.3646 |        -779.3102 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0192 |           0.3550 |        -821.5927 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0189 |           0.3486 |        -820.1621 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0281 |           0.3458 |        -826.7970 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0109 |           0.3384 |        -811.5031 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0110 |           0.3341 |        -830.1486 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0386 |           0.3329 |        -881.7146 |
[32m[20221208 15:07:03 @agent_ppo2.py:179][0m |          -0.0295 |           0.3372 |        -874.7192 |
[32m[20221208 15:07:03 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:07:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.85
[32m[20221208 15:07:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 34.42
[32m[20221208 15:07:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.13
[32m[20221208 15:07:04 @agent_ppo2.py:137][0m Total time:      30.49 min
[32m[20221208 15:07:04 @agent_ppo2.py:139][0m 2465792 total steps have happened
[32m[20221208 15:07:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1204 --------------------------#
[32m[20221208 15:07:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:04 @agent_ppo2.py:179][0m |           0.1033 |           1.5748 |       -1079.6009 |
[32m[20221208 15:07:04 @agent_ppo2.py:179][0m |           0.1463 |           1.0316 |        -956.2914 |
[32m[20221208 15:07:04 @agent_ppo2.py:179][0m |           0.0477 |           0.8355 |        -960.8920 |
[32m[20221208 15:07:04 @agent_ppo2.py:179][0m |           0.0199 |           0.7366 |       -1086.3566 |
[32m[20221208 15:07:04 @agent_ppo2.py:179][0m |           0.0001 |           0.6711 |       -1113.0926 |
[32m[20221208 15:07:04 @agent_ppo2.py:179][0m |          -0.0110 |           0.6209 |       -1148.8296 |
[32m[20221208 15:07:05 @agent_ppo2.py:179][0m |          -0.0184 |           0.5846 |       -1179.7660 |
[32m[20221208 15:07:05 @agent_ppo2.py:179][0m |          -0.0205 |           0.5588 |       -1186.1250 |
[32m[20221208 15:07:05 @agent_ppo2.py:179][0m |          -0.0202 |           0.5260 |       -1177.9664 |
[32m[20221208 15:07:05 @agent_ppo2.py:179][0m |          -0.0322 |           0.5074 |       -1225.3581 |
[32m[20221208 15:07:05 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:07:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.79
[32m[20221208 15:07:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.10
[32m[20221208 15:07:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.30
[32m[20221208 15:07:05 @agent_ppo2.py:137][0m Total time:      30.51 min
[32m[20221208 15:07:05 @agent_ppo2.py:139][0m 2467840 total steps have happened
[32m[20221208 15:07:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1205 --------------------------#
[32m[20221208 15:07:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |           0.0817 |           0.5105 |        -812.4854 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |           0.0365 |           0.3546 |        -435.2106 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0078 |           0.3350 |        -422.1664 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0293 |           0.3212 |        -458.8987 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0353 |           0.3208 |        -468.4563 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0392 |           0.3090 |        -475.3162 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0417 |           0.3028 |        -479.3502 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0429 |           0.3000 |        -499.1248 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0437 |           0.2963 |        -500.7514 |
[32m[20221208 15:07:06 @agent_ppo2.py:179][0m |          -0.0497 |           0.2929 |        -511.4557 |
[32m[20221208 15:07:06 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:07:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.82
[32m[20221208 15:07:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.65
[32m[20221208 15:07:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.46
[32m[20221208 15:07:07 @agent_ppo2.py:137][0m Total time:      30.54 min
[32m[20221208 15:07:07 @agent_ppo2.py:139][0m 2469888 total steps have happened
[32m[20221208 15:07:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1206 --------------------------#
[32m[20221208 15:07:07 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:07:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |           0.1481 |           0.6736 |       -1252.3660 |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |           0.0797 |           0.5103 |       -1052.4983 |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |           0.0082 |           0.4674 |       -1138.6196 |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |          -0.0077 |           0.4471 |       -1142.5426 |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |          -0.0292 |           0.4287 |       -1193.8948 |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |          -0.0385 |           0.4176 |       -1228.0054 |
[32m[20221208 15:07:07 @agent_ppo2.py:179][0m |          -0.0511 |           0.4046 |       -1252.1467 |
[32m[20221208 15:07:08 @agent_ppo2.py:179][0m |          -0.0545 |           0.3964 |       -1276.2553 |
[32m[20221208 15:07:08 @agent_ppo2.py:179][0m |          -0.0574 |           0.3846 |       -1276.1021 |
[32m[20221208 15:07:08 @agent_ppo2.py:179][0m |          -0.0631 |           0.3828 |       -1308.3080 |
[32m[20221208 15:07:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:07:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 36.12
[32m[20221208 15:07:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.68
[32m[20221208 15:07:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.24
[32m[20221208 15:07:08 @agent_ppo2.py:137][0m Total time:      30.56 min
[32m[20221208 15:07:08 @agent_ppo2.py:139][0m 2471936 total steps have happened
[32m[20221208 15:07:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1207 --------------------------#
[32m[20221208 15:07:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |           0.0953 |           1.5133 |       -1154.3002 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |           0.0513 |           1.2948 |        -978.4608 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |           0.0163 |           1.2234 |       -1074.1679 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0264 |           1.1718 |       -1178.8650 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0439 |           1.1260 |       -1174.6936 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0511 |           1.1171 |       -1225.1478 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0591 |           1.0812 |       -1235.2971 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0630 |           1.0632 |       -1254.4516 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0614 |           1.0562 |       -1244.3855 |
[32m[20221208 15:07:09 @agent_ppo2.py:179][0m |          -0.0705 |           1.0356 |       -1255.5259 |
[32m[20221208 15:07:09 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.23
[32m[20221208 15:07:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.69
[32m[20221208 15:07:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.10
[32m[20221208 15:07:10 @agent_ppo2.py:137][0m Total time:      30.59 min
[32m[20221208 15:07:10 @agent_ppo2.py:139][0m 2473984 total steps have happened
[32m[20221208 15:07:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1208 --------------------------#
[32m[20221208 15:07:10 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |           0.0791 |           2.3722 |       -1194.9880 |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |           0.0165 |           1.8923 |       -1143.1575 |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |          -0.0189 |           1.7342 |       -1177.5459 |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |          -0.0361 |           1.5944 |       -1249.5091 |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |          -0.0517 |           1.5281 |       -1286.0906 |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |          -0.0569 |           1.4892 |       -1309.1462 |
[32m[20221208 15:07:10 @agent_ppo2.py:179][0m |          -0.0703 |           1.4304 |       -1350.7669 |
[32m[20221208 15:07:11 @agent_ppo2.py:179][0m |          -0.0722 |           1.4051 |       -1351.0453 |
[32m[20221208 15:07:11 @agent_ppo2.py:179][0m |          -0.0779 |           1.3829 |       -1373.9421 |
[32m[20221208 15:07:11 @agent_ppo2.py:179][0m |          -0.0832 |           1.3741 |       -1393.0328 |
[32m[20221208 15:07:11 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.19
[32m[20221208 15:07:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.32
[32m[20221208 15:07:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.79
[32m[20221208 15:07:11 @agent_ppo2.py:137][0m Total time:      30.61 min
[32m[20221208 15:07:11 @agent_ppo2.py:139][0m 2476032 total steps have happened
[32m[20221208 15:07:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1209 --------------------------#
[32m[20221208 15:07:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |           0.0700 |           1.4147 |       -1227.3023 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |           0.0341 |           1.2755 |       -1216.6121 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |           0.0023 |           1.1931 |       -1211.6282 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0058 |           1.1556 |       -1262.6216 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0228 |           1.1389 |       -1291.6495 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0263 |           1.1271 |       -1265.5195 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0475 |           1.0780 |       -1338.5041 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0464 |           1.0592 |       -1345.7803 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0503 |           1.0755 |       -1373.7543 |
[32m[20221208 15:07:12 @agent_ppo2.py:179][0m |          -0.0396 |           1.0257 |       -1315.8672 |
[32m[20221208 15:07:12 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:07:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.57
[32m[20221208 15:07:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.72
[32m[20221208 15:07:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.07
[32m[20221208 15:07:12 @agent_ppo2.py:137][0m Total time:      30.64 min
[32m[20221208 15:07:12 @agent_ppo2.py:139][0m 2478080 total steps have happened
[32m[20221208 15:07:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1210 --------------------------#
[32m[20221208 15:07:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |           0.1188 |           1.0856 |       -1196.7508 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |           0.0500 |           0.8316 |       -1123.8031 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |           0.0047 |           0.7286 |       -1289.1635 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |          -0.0146 |           0.6748 |       -1346.1344 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |          -0.0281 |           0.6354 |       -1386.4760 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |          -0.0278 |           0.6134 |       -1288.1762 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |          -0.0380 |           0.5971 |       -1355.2683 |
[32m[20221208 15:07:13 @agent_ppo2.py:179][0m |          -0.0465 |           0.5812 |       -1403.1441 |
[32m[20221208 15:07:14 @agent_ppo2.py:179][0m |          -0.0450 |           0.5717 |       -1402.2281 |
[32m[20221208 15:07:14 @agent_ppo2.py:179][0m |          -0.0541 |           0.5752 |       -1462.0627 |
[32m[20221208 15:07:14 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:07:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.59
[32m[20221208 15:07:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.45
[32m[20221208 15:07:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.09
[32m[20221208 15:07:14 @agent_ppo2.py:137][0m Total time:      30.66 min
[32m[20221208 15:07:14 @agent_ppo2.py:139][0m 2480128 total steps have happened
[32m[20221208 15:07:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1211 --------------------------#
[32m[20221208 15:07:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |           0.1040 |           0.3871 |       -1055.4149 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |           0.0246 |           0.3630 |        -946.8536 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0166 |           0.3538 |       -1024.5067 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0166 |           0.3462 |       -1033.6211 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0355 |           0.3413 |       -1066.2420 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0068 |           0.3389 |       -1005.2339 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0278 |           0.3448 |       -1071.0089 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0334 |           0.3437 |       -1085.2185 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0328 |           0.3350 |       -1079.4812 |
[32m[20221208 15:07:15 @agent_ppo2.py:179][0m |          -0.0402 |           0.3330 |       -1100.8989 |
[32m[20221208 15:07:15 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:07:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 18.61
[32m[20221208 15:07:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 20.28
[32m[20221208 15:07:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.27
[32m[20221208 15:07:15 @agent_ppo2.py:137][0m Total time:      30.69 min
[32m[20221208 15:07:15 @agent_ppo2.py:139][0m 2482176 total steps have happened
[32m[20221208 15:07:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1212 --------------------------#
[32m[20221208 15:07:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |           0.0735 |           0.5509 |       -1398.1115 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |           0.0388 |           0.4364 |       -1212.8313 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |           0.0049 |           0.4068 |       -1395.0867 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0245 |           0.3938 |       -1464.9142 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0282 |           0.3812 |       -1444.5981 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0273 |           0.3743 |       -1398.0369 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0364 |           0.3675 |       -1334.0408 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0500 |           0.3637 |       -1517.5161 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0479 |           0.3578 |       -1532.5868 |
[32m[20221208 15:07:16 @agent_ppo2.py:179][0m |          -0.0595 |           0.3530 |       -1590.3819 |
[32m[20221208 15:07:16 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:07:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.52
[32m[20221208 15:07:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.74
[32m[20221208 15:07:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.27
[32m[20221208 15:07:17 @agent_ppo2.py:137][0m Total time:      30.71 min
[32m[20221208 15:07:17 @agent_ppo2.py:139][0m 2484224 total steps have happened
[32m[20221208 15:07:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1213 --------------------------#
[32m[20221208 15:07:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:17 @agent_ppo2.py:179][0m |           0.1026 |           3.0400 |       -1313.4700 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |           0.1484 |           2.4355 |        -951.0246 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |           0.0389 |           2.2388 |       -1063.8415 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |           0.0072 |           2.1212 |       -1144.0137 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |          -0.0118 |           2.0213 |       -1204.6160 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |          -0.0238 |           1.9603 |       -1238.5291 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |          -0.0383 |           1.8761 |       -1300.8851 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |          -0.0407 |           1.8639 |       -1306.6347 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |          -0.0499 |           1.7958 |       -1309.6054 |
[32m[20221208 15:07:18 @agent_ppo2.py:179][0m |          -0.0567 |           1.7708 |       -1350.5445 |
[32m[20221208 15:07:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:07:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.07
[32m[20221208 15:07:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.91
[32m[20221208 15:07:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.88
[32m[20221208 15:07:18 @agent_ppo2.py:137][0m Total time:      30.74 min
[32m[20221208 15:07:18 @agent_ppo2.py:139][0m 2486272 total steps have happened
[32m[20221208 15:07:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1214 --------------------------#
[32m[20221208 15:07:19 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:07:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |           0.1278 |           4.3219 |       -1377.8329 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |           0.2039 |           3.9095 |       -1009.4333 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |           0.0333 |           3.7669 |       -1090.2642 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |           0.0009 |           3.7190 |       -1218.8103 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |          -0.0197 |           3.6133 |       -1316.0099 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |          -0.0275 |           3.5449 |       -1330.9826 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |          -0.0395 |           3.5189 |       -1397.8733 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |          -0.0500 |           3.4713 |       -1432.6887 |
[32m[20221208 15:07:19 @agent_ppo2.py:179][0m |          -0.0591 |           3.4185 |       -1501.5636 |
[32m[20221208 15:07:20 @agent_ppo2.py:179][0m |          -0.0647 |           3.3869 |       -1520.9560 |
[32m[20221208 15:07:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:07:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.86
[32m[20221208 15:07:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.85
[32m[20221208 15:07:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.19
[32m[20221208 15:07:20 @agent_ppo2.py:137][0m Total time:      30.76 min
[32m[20221208 15:07:20 @agent_ppo2.py:139][0m 2488320 total steps have happened
[32m[20221208 15:07:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1215 --------------------------#
[32m[20221208 15:07:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |           0.0487 |           2.9596 |       -1564.7187 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |           0.0164 |           2.4217 |       -1419.1252 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0278 |           2.2613 |       -1545.6772 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0404 |           2.1316 |       -1581.1703 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0496 |           2.1039 |       -1610.4758 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0612 |           2.0177 |       -1641.4691 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0669 |           1.9833 |       -1669.3249 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0706 |           1.9503 |       -1700.7833 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0721 |           1.9367 |       -1716.1314 |
[32m[20221208 15:07:21 @agent_ppo2.py:179][0m |          -0.0771 |           1.8980 |       -1736.5209 |
[32m[20221208 15:07:21 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 15:07:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.00
[32m[20221208 15:07:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.39
[32m[20221208 15:07:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.05
[32m[20221208 15:07:22 @agent_ppo2.py:137][0m Total time:      30.79 min
[32m[20221208 15:07:22 @agent_ppo2.py:139][0m 2490368 total steps have happened
[32m[20221208 15:07:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1216 --------------------------#
[32m[20221208 15:07:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |           0.0987 |           1.9315 |       -1421.1085 |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |           0.0597 |           1.3318 |       -1196.9850 |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |           0.0039 |           1.1153 |       -1335.5250 |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |          -0.0222 |           1.0113 |       -1362.6714 |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |          -0.0383 |           0.9382 |       -1435.8246 |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |          -0.0477 |           0.8975 |       -1484.4250 |
[32m[20221208 15:07:22 @agent_ppo2.py:179][0m |          -0.0582 |           0.8575 |       -1505.6011 |
[32m[20221208 15:07:23 @agent_ppo2.py:179][0m |          -0.0688 |           0.8390 |       -1536.7735 |
[32m[20221208 15:07:23 @agent_ppo2.py:179][0m |          -0.0733 |           0.8146 |       -1572.0561 |
[32m[20221208 15:07:23 @agent_ppo2.py:179][0m |          -0.0743 |           0.7885 |       -1585.5308 |
[32m[20221208 15:07:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:07:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.00
[32m[20221208 15:07:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.62
[32m[20221208 15:07:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.36
[32m[20221208 15:07:23 @agent_ppo2.py:137][0m Total time:      30.81 min
[32m[20221208 15:07:23 @agent_ppo2.py:139][0m 2492416 total steps have happened
[32m[20221208 15:07:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1217 --------------------------#
[32m[20221208 15:07:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |           0.1637 |           4.6476 |       -1269.0905 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |           0.1300 |           3.9933 |        -822.7102 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |           0.0504 |           3.7569 |       -1006.6494 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |           0.0204 |           3.6165 |       -1182.4968 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |           0.0008 |           3.4352 |       -1251.2601 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |          -0.0208 |           3.3623 |       -1363.9773 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |          -0.0345 |           3.2703 |       -1434.2376 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |          -0.0426 |           3.2184 |       -1507.0190 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |          -0.0491 |           3.2083 |       -1494.0691 |
[32m[20221208 15:07:24 @agent_ppo2.py:179][0m |          -0.0574 |           3.1438 |       -1517.6127 |
[32m[20221208 15:07:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:07:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.11
[32m[20221208 15:07:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.93
[32m[20221208 15:07:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.90
[32m[20221208 15:07:25 @agent_ppo2.py:137][0m Total time:      30.84 min
[32m[20221208 15:07:25 @agent_ppo2.py:139][0m 2494464 total steps have happened
[32m[20221208 15:07:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1218 --------------------------#
[32m[20221208 15:07:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |           0.0914 |           2.6858 |       -1531.1928 |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |           0.0635 |           2.1039 |        -997.5021 |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |           0.0109 |           1.9474 |       -1329.6903 |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |          -0.0139 |           1.7949 |       -1429.0922 |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |          -0.0201 |           1.7072 |       -1447.9464 |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |          -0.0339 |           1.6300 |       -1461.2166 |
[32m[20221208 15:07:25 @agent_ppo2.py:179][0m |          -0.0452 |           1.5810 |       -1503.6404 |
[32m[20221208 15:07:26 @agent_ppo2.py:179][0m |          -0.0475 |           1.5232 |       -1528.2547 |
[32m[20221208 15:07:26 @agent_ppo2.py:179][0m |          -0.0554 |           1.4752 |       -1557.5297 |
[32m[20221208 15:07:26 @agent_ppo2.py:179][0m |          -0.0610 |           1.4544 |       -1560.0922 |
[32m[20221208 15:07:26 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.24
[32m[20221208 15:07:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.66
[32m[20221208 15:07:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.55
[32m[20221208 15:07:26 @agent_ppo2.py:137][0m Total time:      30.86 min
[32m[20221208 15:07:26 @agent_ppo2.py:139][0m 2496512 total steps have happened
[32m[20221208 15:07:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1219 --------------------------#
[32m[20221208 15:07:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |           0.0731 |           1.4428 |       -1448.1882 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |           0.0305 |           0.8513 |       -1414.5317 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0056 |           0.7292 |       -1437.3751 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0233 |           0.6663 |       -1471.0684 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0430 |           0.6252 |       -1502.2051 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0426 |           0.5971 |       -1491.3778 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0523 |           0.5758 |       -1547.2710 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0594 |           0.5570 |       -1572.5549 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0524 |           0.5339 |       -1563.6134 |
[32m[20221208 15:07:27 @agent_ppo2.py:179][0m |          -0.0598 |           0.5244 |       -1568.1318 |
[32m[20221208 15:07:27 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.67
[32m[20221208 15:07:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.62
[32m[20221208 15:07:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.63
[32m[20221208 15:07:28 @agent_ppo2.py:137][0m Total time:      30.89 min
[32m[20221208 15:07:28 @agent_ppo2.py:139][0m 2498560 total steps have happened
[32m[20221208 15:07:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1220 --------------------------#
[32m[20221208 15:07:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |           1.5939 |           2.0737 |       -1349.1385 |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |           0.0364 |           1.7887 |       -1185.3001 |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |          -0.0105 |           1.7192 |       -1313.8114 |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |          -0.0400 |           1.6264 |       -1363.1788 |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |          -0.0577 |           1.5612 |       -1406.4095 |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |          -0.0592 |           1.5199 |       -1403.7722 |
[32m[20221208 15:07:28 @agent_ppo2.py:179][0m |          -0.0657 |           1.5258 |       -1413.8217 |
[32m[20221208 15:07:29 @agent_ppo2.py:179][0m |          -0.0727 |           1.4438 |       -1447.1161 |
[32m[20221208 15:07:29 @agent_ppo2.py:179][0m |          -0.0798 |           1.4305 |       -1476.9210 |
[32m[20221208 15:07:29 @agent_ppo2.py:179][0m |          -0.0829 |           1.4290 |       -1502.6400 |
[32m[20221208 15:07:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.14
[32m[20221208 15:07:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.08
[32m[20221208 15:07:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.40
[32m[20221208 15:07:29 @agent_ppo2.py:137][0m Total time:      30.91 min
[32m[20221208 15:07:29 @agent_ppo2.py:139][0m 2500608 total steps have happened
[32m[20221208 15:07:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1221 --------------------------#
[32m[20221208 15:07:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |           0.1098 |           0.4104 |       -1512.1992 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |           0.0436 |           0.3446 |       -1389.1947 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |           0.0090 |           0.3251 |       -1500.4472 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0113 |           0.3159 |       -1489.8086 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0204 |           0.3093 |       -1528.2304 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0107 |           0.3033 |       -1504.4342 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0191 |           0.3006 |       -1550.8930 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0236 |           0.2966 |       -1561.8563 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0270 |           0.2944 |       -1551.4793 |
[32m[20221208 15:07:30 @agent_ppo2.py:179][0m |          -0.0234 |           0.2913 |       -1512.3485 |
[32m[20221208 15:07:30 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:07:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.66
[32m[20221208 15:07:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 32.22
[32m[20221208 15:07:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.90
[32m[20221208 15:07:31 @agent_ppo2.py:137][0m Total time:      30.94 min
[32m[20221208 15:07:31 @agent_ppo2.py:139][0m 2502656 total steps have happened
[32m[20221208 15:07:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1222 --------------------------#
[32m[20221208 15:07:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |           0.1019 |           2.1655 |       -1306.5104 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |           0.0527 |           1.8859 |        -974.1373 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |           0.0038 |           1.8026 |       -1054.5086 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |          -0.0270 |           1.7247 |       -1037.1338 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |          -0.0473 |           1.6833 |       -1021.8856 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |          -0.0588 |           1.6243 |       -1016.4143 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |          -0.0642 |           1.6161 |       -1042.9599 |
[32m[20221208 15:07:31 @agent_ppo2.py:179][0m |          -0.0687 |           1.5843 |       -1089.9116 |
[32m[20221208 15:07:32 @agent_ppo2.py:179][0m |          -0.0678 |           1.5500 |       -1057.7051 |
[32m[20221208 15:07:32 @agent_ppo2.py:179][0m |          -0.0758 |           1.5387 |       -1060.4722 |
[32m[20221208 15:07:32 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.10
[32m[20221208 15:07:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.16
[32m[20221208 15:07:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.76
[32m[20221208 15:07:32 @agent_ppo2.py:137][0m Total time:      30.96 min
[32m[20221208 15:07:32 @agent_ppo2.py:139][0m 2504704 total steps have happened
[32m[20221208 15:07:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1223 --------------------------#
[32m[20221208 15:07:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |           0.1088 |           2.0728 |       -1472.1453 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |           0.0658 |           1.5784 |       -1305.0030 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |           0.0235 |           1.4052 |       -1383.0486 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0070 |           1.3147 |       -1445.7784 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0185 |           1.2064 |       -1507.4325 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0388 |           1.1483 |       -1542.8503 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0403 |           1.1033 |       -1540.1205 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0462 |           1.0718 |       -1541.0244 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0496 |           1.0345 |       -1571.8415 |
[32m[20221208 15:07:33 @agent_ppo2.py:179][0m |          -0.0521 |           1.0127 |       -1604.1733 |
[32m[20221208 15:07:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.08
[32m[20221208 15:07:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.88
[32m[20221208 15:07:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.97
[32m[20221208 15:07:33 @agent_ppo2.py:137][0m Total time:      30.99 min
[32m[20221208 15:07:33 @agent_ppo2.py:139][0m 2506752 total steps have happened
[32m[20221208 15:07:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1224 --------------------------#
[32m[20221208 15:07:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |           0.0826 |           1.9463 |       -1270.8281 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |           0.0509 |           1.6566 |       -1013.7604 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0076 |           1.5424 |       -1149.1661 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0258 |           1.4878 |       -1227.7632 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0415 |           1.4366 |       -1285.7931 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0495 |           1.3681 |       -1294.7799 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0513 |           1.3413 |       -1309.9638 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0600 |           1.3143 |       -1357.6556 |
[32m[20221208 15:07:34 @agent_ppo2.py:179][0m |          -0.0619 |           1.3381 |       -1364.9572 |
[32m[20221208 15:07:35 @agent_ppo2.py:179][0m |          -0.0647 |           1.2834 |       -1385.6118 |
[32m[20221208 15:07:35 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.90
[32m[20221208 15:07:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.50
[32m[20221208 15:07:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.04
[32m[20221208 15:07:35 @agent_ppo2.py:137][0m Total time:      31.01 min
[32m[20221208 15:07:35 @agent_ppo2.py:139][0m 2508800 total steps have happened
[32m[20221208 15:07:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1225 --------------------------#
[32m[20221208 15:07:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |           0.0974 |           2.3727 |       -1404.8134 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |           0.0774 |           1.9550 |       -1206.3014 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |           0.0224 |           1.7916 |       -1362.4145 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0023 |           1.7125 |       -1459.1013 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0124 |           1.6368 |       -1458.5420 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0353 |           1.5663 |       -1495.2018 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0497 |           1.5343 |       -1554.7052 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0467 |           1.5094 |       -1526.9535 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0520 |           1.4881 |       -1589.2490 |
[32m[20221208 15:07:36 @agent_ppo2.py:179][0m |          -0.0600 |           1.4629 |       -1623.1017 |
[32m[20221208 15:07:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.68
[32m[20221208 15:07:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.02
[32m[20221208 15:07:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.67
[32m[20221208 15:07:36 @agent_ppo2.py:137][0m Total time:      31.04 min
[32m[20221208 15:07:36 @agent_ppo2.py:139][0m 2510848 total steps have happened
[32m[20221208 15:07:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1226 --------------------------#
[32m[20221208 15:07:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1491 |           0.5120 |        -658.5294 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.2046 |           0.4801 |         -43.8641 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1537 |           0.4737 |         -50.0564 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1492 |           0.4654 |         -53.8484 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1464 |           0.4630 |         -61.0837 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1437 |           0.4639 |         -80.9737 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1395 |           0.4596 |         -86.5303 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1370 |           0.4576 |         -92.2314 |
[32m[20221208 15:07:37 @agent_ppo2.py:179][0m |           0.1344 |           0.4561 |        -103.9342 |
[32m[20221208 15:07:38 @agent_ppo2.py:179][0m |           0.1323 |           0.4561 |        -120.3750 |
[32m[20221208 15:07:38 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.27
[32m[20221208 15:07:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.69
[32m[20221208 15:07:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.75
[32m[20221208 15:07:38 @agent_ppo2.py:137][0m Total time:      31.06 min
[32m[20221208 15:07:38 @agent_ppo2.py:139][0m 2512896 total steps have happened
[32m[20221208 15:07:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1227 --------------------------#
[32m[20221208 15:07:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:38 @agent_ppo2.py:179][0m |           0.0749 |           0.3960 |       -1294.7881 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0957 |           0.3579 |        -652.4938 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0550 |           0.3523 |        -798.8917 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0485 |           0.3514 |        -958.4542 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0997 |           0.3476 |        -325.7320 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.1065 |           0.3470 |        -252.7516 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0838 |           0.3477 |        -292.8102 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0408 |           0.3465 |        -514.9655 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0400 |           0.3449 |        -528.9413 |
[32m[20221208 15:07:39 @agent_ppo2.py:179][0m |           0.0441 |           0.3438 |        -525.2543 |
[32m[20221208 15:07:39 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:07:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 30.72
[32m[20221208 15:07:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.62
[32m[20221208 15:07:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.73
[32m[20221208 15:07:39 @agent_ppo2.py:137][0m Total time:      31.09 min
[32m[20221208 15:07:39 @agent_ppo2.py:139][0m 2514944 total steps have happened
[32m[20221208 15:07:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1228 --------------------------#
[32m[20221208 15:07:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |           0.1044 |           2.8916 |       -1215.0829 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |           0.1311 |           2.5696 |        -921.7284 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |           0.0665 |           2.4226 |        -860.8996 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |           0.0158 |           2.3748 |       -1056.8742 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |          -0.0093 |           2.2951 |       -1142.5283 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |          -0.0225 |           2.2139 |       -1172.2318 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |          -0.0381 |           2.1964 |       -1267.8933 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |          -0.0489 |           2.1314 |       -1270.6439 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |          -0.0582 |           2.1314 |       -1327.3200 |
[32m[20221208 15:07:40 @agent_ppo2.py:179][0m |          -0.0618 |           2.0752 |       -1326.4027 |
[32m[20221208 15:07:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:07:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.66
[32m[20221208 15:07:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.68
[32m[20221208 15:07:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.57
[32m[20221208 15:07:41 @agent_ppo2.py:137][0m Total time:      31.11 min
[32m[20221208 15:07:41 @agent_ppo2.py:139][0m 2516992 total steps have happened
[32m[20221208 15:07:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1229 --------------------------#
[32m[20221208 15:07:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:41 @agent_ppo2.py:179][0m |           0.1847 |           2.7348 |       -1201.6013 |
[32m[20221208 15:07:41 @agent_ppo2.py:179][0m |           0.1118 |           2.5112 |        -846.0981 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |           0.0619 |           2.4148 |        -920.6964 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |           0.0390 |           2.3275 |        -991.2797 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |           0.0106 |           2.2860 |       -1137.9487 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |          -0.0092 |           2.2213 |       -1231.4247 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |          -0.0224 |           2.1894 |       -1295.8195 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |          -0.0314 |           2.1689 |       -1316.4764 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |          -0.0380 |           2.1060 |       -1347.1141 |
[32m[20221208 15:07:42 @agent_ppo2.py:179][0m |          -0.0454 |           2.0824 |       -1382.2169 |
[32m[20221208 15:07:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:07:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.18
[32m[20221208 15:07:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.58
[32m[20221208 15:07:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.04
[32m[20221208 15:07:42 @agent_ppo2.py:137][0m Total time:      31.14 min
[32m[20221208 15:07:42 @agent_ppo2.py:139][0m 2519040 total steps have happened
[32m[20221208 15:07:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1230 --------------------------#
[32m[20221208 15:07:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |           0.1366 |           2.1649 |       -1145.9342 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |           0.0441 |           1.8704 |        -970.4783 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0047 |           1.7431 |       -1155.1196 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0256 |           1.6275 |       -1203.9894 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0402 |           1.5575 |       -1254.7672 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0468 |           1.4953 |       -1272.5770 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0572 |           1.4461 |       -1309.4131 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0614 |           1.4088 |       -1297.5664 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0649 |           1.3723 |       -1310.9893 |
[32m[20221208 15:07:43 @agent_ppo2.py:179][0m |          -0.0697 |           1.3357 |       -1328.4487 |
[32m[20221208 15:07:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:07:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.50
[32m[20221208 15:07:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.70
[32m[20221208 15:07:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.08
[32m[20221208 15:07:44 @agent_ppo2.py:137][0m Total time:      31.16 min
[32m[20221208 15:07:44 @agent_ppo2.py:139][0m 2521088 total steps have happened
[32m[20221208 15:07:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1231 --------------------------#
[32m[20221208 15:07:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:44 @agent_ppo2.py:179][0m |           0.1118 |           1.2621 |       -1169.3536 |
[32m[20221208 15:07:44 @agent_ppo2.py:179][0m |           0.1192 |           0.9471 |        -483.4533 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |           0.0646 |           0.8518 |        -521.6231 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |           0.0168 |           0.8032 |        -845.3088 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |           0.0248 |           0.7637 |        -793.5801 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |           0.0191 |           0.7440 |        -711.7342 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |          -0.0085 |           0.7196 |        -875.4373 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |          -0.0343 |           0.7039 |       -1112.4826 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |          -0.0114 |           0.6887 |       -1084.6878 |
[32m[20221208 15:07:45 @agent_ppo2.py:179][0m |          -0.0154 |           0.6779 |        -973.0283 |
[32m[20221208 15:07:45 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.15
[32m[20221208 15:07:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.34
[32m[20221208 15:07:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.25
[32m[20221208 15:07:45 @agent_ppo2.py:137][0m Total time:      31.18 min
[32m[20221208 15:07:45 @agent_ppo2.py:139][0m 2523136 total steps have happened
[32m[20221208 15:07:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1232 --------------------------#
[32m[20221208 15:07:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:07:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |           0.0469 |           1.1770 |       -1081.4104 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |           0.0011 |           1.0518 |       -1104.2054 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0189 |           1.0037 |       -1121.5735 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0170 |           0.9731 |       -1087.6057 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0300 |           0.9465 |       -1136.1078 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0412 |           0.9556 |       -1182.4448 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0424 |           0.9415 |       -1188.9750 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0422 |           0.9055 |       -1184.9070 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0452 |           0.9013 |       -1194.6811 |
[32m[20221208 15:07:46 @agent_ppo2.py:179][0m |          -0.0522 |           0.8859 |       -1223.0518 |
[32m[20221208 15:07:46 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.20
[32m[20221208 15:07:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.44
[32m[20221208 15:07:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.35
[32m[20221208 15:07:47 @agent_ppo2.py:137][0m Total time:      31.21 min
[32m[20221208 15:07:47 @agent_ppo2.py:139][0m 2525184 total steps have happened
[32m[20221208 15:07:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1233 --------------------------#
[32m[20221208 15:07:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:47 @agent_ppo2.py:179][0m |           0.0587 |           2.8126 |       -1177.3691 |
[32m[20221208 15:07:47 @agent_ppo2.py:179][0m |           0.0257 |           2.4230 |       -1141.0694 |
[32m[20221208 15:07:47 @agent_ppo2.py:179][0m |          -0.0144 |           2.2373 |       -1170.1142 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0276 |           2.1333 |       -1207.3170 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0328 |           2.0201 |       -1218.7016 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0449 |           1.9367 |       -1258.9612 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0540 |           1.8976 |       -1286.4862 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0562 |           1.8344 |       -1291.4439 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0541 |           1.7714 |       -1290.1074 |
[32m[20221208 15:07:48 @agent_ppo2.py:179][0m |          -0.0631 |           1.7101 |       -1327.3583 |
[32m[20221208 15:07:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.86
[32m[20221208 15:07:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.60
[32m[20221208 15:07:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.09
[32m[20221208 15:07:48 @agent_ppo2.py:137][0m Total time:      31.23 min
[32m[20221208 15:07:48 @agent_ppo2.py:139][0m 2527232 total steps have happened
[32m[20221208 15:07:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1234 --------------------------#
[32m[20221208 15:07:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |           0.0925 |           2.3517 |       -1108.0700 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |           0.2206 |           2.0034 |        -955.7870 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |           0.0260 |           1.8580 |        -954.8280 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0154 |           1.7594 |       -1103.4129 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0326 |           1.6935 |       -1154.4897 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0428 |           1.6777 |       -1194.1452 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0547 |           1.6162 |       -1227.5297 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0614 |           1.5669 |       -1266.6928 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0668 |           1.5458 |       -1281.2041 |
[32m[20221208 15:07:49 @agent_ppo2.py:179][0m |          -0.0716 |           1.5167 |       -1297.0083 |
[32m[20221208 15:07:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.76
[32m[20221208 15:07:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.69
[32m[20221208 15:07:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.16
[32m[20221208 15:07:50 @agent_ppo2.py:137][0m Total time:      31.26 min
[32m[20221208 15:07:50 @agent_ppo2.py:139][0m 2529280 total steps have happened
[32m[20221208 15:07:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1235 --------------------------#
[32m[20221208 15:07:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:50 @agent_ppo2.py:179][0m |           0.1156 |           1.9994 |       -1097.7123 |
[32m[20221208 15:07:50 @agent_ppo2.py:179][0m |           0.1590 |           1.6502 |        -840.8036 |
[32m[20221208 15:07:50 @agent_ppo2.py:179][0m |           0.0348 |           1.5278 |        -946.8786 |
[32m[20221208 15:07:50 @agent_ppo2.py:179][0m |          -0.0054 |           1.4534 |        -995.9231 |
[32m[20221208 15:07:51 @agent_ppo2.py:179][0m |          -0.0281 |           1.4015 |       -1097.6818 |
[32m[20221208 15:07:51 @agent_ppo2.py:179][0m |          -0.0397 |           1.3628 |       -1132.8501 |
[32m[20221208 15:07:51 @agent_ppo2.py:179][0m |          -0.0484 |           1.3231 |       -1162.3160 |
[32m[20221208 15:07:51 @agent_ppo2.py:179][0m |          -0.0558 |           1.3051 |       -1192.1286 |
[32m[20221208 15:07:51 @agent_ppo2.py:179][0m |          -0.0610 |           1.2770 |       -1214.5670 |
[32m[20221208 15:07:51 @agent_ppo2.py:179][0m |          -0.0633 |           1.2513 |       -1227.8673 |
[32m[20221208 15:07:51 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.50
[32m[20221208 15:07:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.84
[32m[20221208 15:07:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.90
[32m[20221208 15:07:51 @agent_ppo2.py:137][0m Total time:      31.28 min
[32m[20221208 15:07:51 @agent_ppo2.py:139][0m 2531328 total steps have happened
[32m[20221208 15:07:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1236 --------------------------#
[32m[20221208 15:07:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |           0.1037 |           1.5613 |       -1193.8217 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |           0.0421 |           1.2087 |       -1028.3718 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |           0.0026 |           1.0910 |       -1156.0489 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0246 |           1.0334 |       -1232.9968 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0368 |           0.9847 |       -1246.9727 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0451 |           0.9446 |       -1277.5557 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0527 |           0.9202 |       -1309.1178 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0508 |           0.8928 |       -1289.1435 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0573 |           0.8632 |       -1323.1123 |
[32m[20221208 15:07:52 @agent_ppo2.py:179][0m |          -0.0611 |           0.8490 |       -1359.0023 |
[32m[20221208 15:07:52 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:07:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.69
[32m[20221208 15:07:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.95
[32m[20221208 15:07:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.57
[32m[20221208 15:07:53 @agent_ppo2.py:137][0m Total time:      31.31 min
[32m[20221208 15:07:53 @agent_ppo2.py:139][0m 2533376 total steps have happened
[32m[20221208 15:07:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1237 --------------------------#
[32m[20221208 15:07:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:53 @agent_ppo2.py:179][0m |           0.0472 |           1.8881 |       -1152.8736 |
[32m[20221208 15:07:53 @agent_ppo2.py:179][0m |           0.0400 |           1.5794 |        -858.9385 |
[32m[20221208 15:07:53 @agent_ppo2.py:179][0m |          -0.0017 |           1.4980 |       -1133.1656 |
[32m[20221208 15:07:53 @agent_ppo2.py:179][0m |          -0.0150 |           1.4085 |       -1180.5677 |
[32m[20221208 15:07:54 @agent_ppo2.py:179][0m |          -0.0189 |           1.3716 |       -1128.1726 |
[32m[20221208 15:07:54 @agent_ppo2.py:179][0m |          -0.0298 |           1.3380 |       -1144.4215 |
[32m[20221208 15:07:54 @agent_ppo2.py:179][0m |          -0.0284 |           1.3112 |       -1060.1858 |
[32m[20221208 15:07:54 @agent_ppo2.py:179][0m |          -0.0452 |           1.2465 |       -1274.1909 |
[32m[20221208 15:07:54 @agent_ppo2.py:179][0m |          -0.0452 |           1.2275 |       -1269.7387 |
[32m[20221208 15:07:54 @agent_ppo2.py:179][0m |          -0.0478 |           1.1976 |       -1314.5366 |
[32m[20221208 15:07:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.12
[32m[20221208 15:07:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.62
[32m[20221208 15:07:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.83
[32m[20221208 15:07:54 @agent_ppo2.py:137][0m Total time:      31.33 min
[32m[20221208 15:07:54 @agent_ppo2.py:139][0m 2535424 total steps have happened
[32m[20221208 15:07:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1238 --------------------------#
[32m[20221208 15:07:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |           0.1072 |           2.6088 |        -892.7538 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |           0.0678 |           2.2989 |        -660.4627 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |           0.0213 |           2.2410 |        -787.6932 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0178 |           2.1783 |        -894.4956 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0226 |           2.1495 |        -934.7962 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0355 |           2.0987 |        -953.4885 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0539 |           2.0661 |        -996.5676 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0559 |           2.0369 |       -1026.2271 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0627 |           2.0021 |       -1048.5231 |
[32m[20221208 15:07:55 @agent_ppo2.py:179][0m |          -0.0636 |           2.0047 |       -1066.4045 |
[32m[20221208 15:07:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:07:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.71
[32m[20221208 15:07:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.84
[32m[20221208 15:07:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.77
[32m[20221208 15:07:56 @agent_ppo2.py:137][0m Total time:      31.36 min
[32m[20221208 15:07:56 @agent_ppo2.py:139][0m 2537472 total steps have happened
[32m[20221208 15:07:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1239 --------------------------#
[32m[20221208 15:07:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:56 @agent_ppo2.py:179][0m |           0.1094 |           1.7695 |        -931.7153 |
[32m[20221208 15:07:56 @agent_ppo2.py:179][0m |           0.0522 |           1.5088 |        -626.3054 |
[32m[20221208 15:07:56 @agent_ppo2.py:179][0m |           0.0475 |           1.4278 |        -613.4135 |
[32m[20221208 15:07:56 @agent_ppo2.py:179][0m |           0.0107 |           1.3841 |        -451.6810 |
[32m[20221208 15:07:56 @agent_ppo2.py:179][0m |          -0.0137 |           1.3568 |        -750.3690 |
[32m[20221208 15:07:57 @agent_ppo2.py:179][0m |          -0.0215 |           1.3411 |        -926.3815 |
[32m[20221208 15:07:57 @agent_ppo2.py:179][0m |           0.0061 |           1.3226 |        -629.8939 |
[32m[20221208 15:07:57 @agent_ppo2.py:179][0m |          -0.0149 |           1.3006 |        -401.9525 |
[32m[20221208 15:07:57 @agent_ppo2.py:179][0m |          -0.0262 |           1.2900 |        -527.9909 |
[32m[20221208 15:07:57 @agent_ppo2.py:179][0m |          -0.0356 |           1.2747 |        -687.7905 |
[32m[20221208 15:07:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:07:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.26
[32m[20221208 15:07:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.87
[32m[20221208 15:07:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.34
[32m[20221208 15:07:57 @agent_ppo2.py:137][0m Total time:      31.38 min
[32m[20221208 15:07:57 @agent_ppo2.py:139][0m 2539520 total steps have happened
[32m[20221208 15:07:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1240 --------------------------#
[32m[20221208 15:07:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0880 |           0.8525 |       -1125.6813 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0291 |           0.6883 |       -1035.3608 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0498 |           0.6363 |        -952.0667 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0696 |           0.5999 |        -807.1903 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0652 |           0.5783 |        -839.7993 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0054 |           0.5623 |       -1090.1847 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0045 |           0.5501 |       -1115.7698 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0110 |           0.5392 |       -1049.7955 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0292 |           0.5299 |        -995.6339 |
[32m[20221208 15:07:58 @agent_ppo2.py:179][0m |           0.0017 |           0.5248 |       -1095.5733 |
[32m[20221208 15:07:58 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:07:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.86
[32m[20221208 15:07:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.49
[32m[20221208 15:07:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.66
[32m[20221208 15:07:59 @agent_ppo2.py:137][0m Total time:      31.41 min
[32m[20221208 15:07:59 @agent_ppo2.py:139][0m 2541568 total steps have happened
[32m[20221208 15:07:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1241 --------------------------#
[32m[20221208 15:07:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:07:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:07:59 @agent_ppo2.py:179][0m |           0.1633 |           3.0123 |       -1067.4226 |
[32m[20221208 15:07:59 @agent_ppo2.py:179][0m |           0.0429 |           2.6150 |        -981.5256 |
[32m[20221208 15:07:59 @agent_ppo2.py:179][0m |           0.0054 |           2.5476 |        -955.7161 |
[32m[20221208 15:07:59 @agent_ppo2.py:179][0m |          -0.0273 |           2.4435 |       -1042.1646 |
[32m[20221208 15:07:59 @agent_ppo2.py:179][0m |          -0.0436 |           2.4126 |       -1087.2284 |
[32m[20221208 15:08:00 @agent_ppo2.py:179][0m |          -0.0512 |           2.3684 |       -1121.0122 |
[32m[20221208 15:08:00 @agent_ppo2.py:179][0m |          -0.0511 |           2.3466 |       -1130.0071 |
[32m[20221208 15:08:00 @agent_ppo2.py:179][0m |          -0.0613 |           2.3046 |       -1173.2652 |
[32m[20221208 15:08:00 @agent_ppo2.py:179][0m |          -0.0654 |           2.2739 |       -1173.9871 |
[32m[20221208 15:08:00 @agent_ppo2.py:179][0m |          -0.0629 |           2.2444 |       -1189.9899 |
[32m[20221208 15:08:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.03
[32m[20221208 15:08:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.45
[32m[20221208 15:08:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.52
[32m[20221208 15:08:00 @agent_ppo2.py:137][0m Total time:      31.43 min
[32m[20221208 15:08:00 @agent_ppo2.py:139][0m 2543616 total steps have happened
[32m[20221208 15:08:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1242 --------------------------#
[32m[20221208 15:08:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |           0.0676 |           2.4111 |       -1081.3022 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |           0.0423 |           1.9865 |        -858.4155 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0006 |           1.8232 |        -993.4507 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0263 |           1.7344 |       -1110.3071 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0376 |           1.6588 |       -1146.6239 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0464 |           1.6135 |       -1180.0962 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0516 |           1.5586 |       -1186.7054 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0521 |           1.5261 |       -1189.8137 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0550 |           1.4898 |       -1224.8853 |
[32m[20221208 15:08:01 @agent_ppo2.py:179][0m |          -0.0583 |           1.4649 |       -1229.9170 |
[32m[20221208 15:08:01 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.64
[32m[20221208 15:08:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.53
[32m[20221208 15:08:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.14
[32m[20221208 15:08:02 @agent_ppo2.py:137][0m Total time:      31.46 min
[32m[20221208 15:08:02 @agent_ppo2.py:139][0m 2545664 total steps have happened
[32m[20221208 15:08:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1243 --------------------------#
[32m[20221208 15:08:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:02 @agent_ppo2.py:179][0m |           0.2960 |           3.0952 |       -1027.0114 |
[32m[20221208 15:08:02 @agent_ppo2.py:179][0m |           0.0871 |           2.7111 |        -739.0018 |
[32m[20221208 15:08:02 @agent_ppo2.py:179][0m |           0.0329 |           2.6001 |        -863.5432 |
[32m[20221208 15:08:02 @agent_ppo2.py:179][0m |           0.0001 |           2.5064 |        -909.6223 |
[32m[20221208 15:08:02 @agent_ppo2.py:179][0m |          -0.0150 |           2.4695 |        -959.9545 |
[32m[20221208 15:08:02 @agent_ppo2.py:179][0m |          -0.0309 |           2.4241 |       -1002.9761 |
[32m[20221208 15:08:03 @agent_ppo2.py:179][0m |          -0.0288 |           2.3690 |        -999.2329 |
[32m[20221208 15:08:03 @agent_ppo2.py:179][0m |          -0.0403 |           2.3409 |       -1033.3537 |
[32m[20221208 15:08:03 @agent_ppo2.py:179][0m |          -0.0488 |           2.3017 |       -1072.0215 |
[32m[20221208 15:08:03 @agent_ppo2.py:179][0m |          -0.0506 |           2.2640 |       -1090.2585 |
[32m[20221208 15:08:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.97
[32m[20221208 15:08:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.11
[32m[20221208 15:08:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.27
[32m[20221208 15:08:03 @agent_ppo2.py:137][0m Total time:      31.48 min
[32m[20221208 15:08:03 @agent_ppo2.py:139][0m 2547712 total steps have happened
[32m[20221208 15:08:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1244 --------------------------#
[32m[20221208 15:08:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |           0.0944 |           1.8632 |        -945.3376 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |           0.0501 |           1.4896 |        -859.6842 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |           0.0245 |           1.3361 |        -937.3044 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |           0.0076 |           1.2399 |       -1008.3932 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |          -0.0064 |           1.1604 |       -1035.2368 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |          -0.0221 |           1.1174 |       -1125.7811 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |          -0.0184 |           1.0691 |       -1109.7448 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |          -0.0303 |           1.0396 |       -1153.5516 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |          -0.0358 |           0.9993 |       -1154.5980 |
[32m[20221208 15:08:04 @agent_ppo2.py:179][0m |          -0.0394 |           0.9762 |       -1174.0061 |
[32m[20221208 15:08:04 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.35
[32m[20221208 15:08:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.85
[32m[20221208 15:08:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.20
[32m[20221208 15:08:05 @agent_ppo2.py:137][0m Total time:      31.51 min
[32m[20221208 15:08:05 @agent_ppo2.py:139][0m 2549760 total steps have happened
[32m[20221208 15:08:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1245 --------------------------#
[32m[20221208 15:08:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |           0.0945 |           2.1950 |       -1027.7278 |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |           0.0548 |           1.8723 |        -983.1738 |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |           0.0141 |           1.7572 |       -1058.8290 |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |          -0.0034 |           1.6865 |       -1060.6038 |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |          -0.0221 |           1.6352 |       -1093.4325 |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |          -0.0377 |           1.5836 |       -1154.6939 |
[32m[20221208 15:08:05 @agent_ppo2.py:179][0m |          -0.0363 |           1.5434 |       -1146.6046 |
[32m[20221208 15:08:06 @agent_ppo2.py:179][0m |          -0.0376 |           1.5211 |       -1119.3657 |
[32m[20221208 15:08:06 @agent_ppo2.py:179][0m |          -0.0333 |           1.5248 |       -1117.7379 |
[32m[20221208 15:08:06 @agent_ppo2.py:179][0m |          -0.0386 |           1.4708 |       -1120.6089 |
[32m[20221208 15:08:06 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.59
[32m[20221208 15:08:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.69
[32m[20221208 15:08:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.39
[32m[20221208 15:08:06 @agent_ppo2.py:137][0m Total time:      31.53 min
[32m[20221208 15:08:06 @agent_ppo2.py:139][0m 2551808 total steps have happened
[32m[20221208 15:08:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1246 --------------------------#
[32m[20221208 15:08:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |           0.0399 |           2.4581 |       -1057.1711 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |           0.0234 |           2.0235 |       -1046.6061 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |           0.0007 |           1.8982 |       -1048.3879 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0189 |           1.7498 |       -1085.1833 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0248 |           1.6751 |       -1119.1451 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0176 |           1.6096 |       -1096.3930 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0353 |           1.5329 |       -1129.7085 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0391 |           1.4631 |       -1162.2073 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0433 |           1.4205 |       -1149.0215 |
[32m[20221208 15:08:07 @agent_ppo2.py:179][0m |          -0.0465 |           1.3859 |       -1187.1680 |
[32m[20221208 15:08:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.00
[32m[20221208 15:08:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.34
[32m[20221208 15:08:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.36
[32m[20221208 15:08:08 @agent_ppo2.py:137][0m Total time:      31.55 min
[32m[20221208 15:08:08 @agent_ppo2.py:139][0m 2553856 total steps have happened
[32m[20221208 15:08:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1247 --------------------------#
[32m[20221208 15:08:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |           0.1125 |           1.0695 |        -647.6313 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |           0.1665 |           0.8928 |        -224.8981 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |           0.0616 |           0.8135 |        -273.2420 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |           0.0320 |           0.7695 |        -383.1446 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |           0.0136 |           0.7422 |        -517.8541 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |           0.0010 |           0.7200 |        -569.5380 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |          -0.0100 |           0.7026 |        -730.8869 |
[32m[20221208 15:08:08 @agent_ppo2.py:179][0m |          -0.0148 |           0.6868 |        -854.9965 |
[32m[20221208 15:08:09 @agent_ppo2.py:179][0m |          -0.0185 |           0.6691 |        -778.6671 |
[32m[20221208 15:08:09 @agent_ppo2.py:179][0m |          -0.0193 |           0.6605 |        -951.8736 |
[32m[20221208 15:08:09 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.16
[32m[20221208 15:08:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.06
[32m[20221208 15:08:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.82
[32m[20221208 15:08:09 @agent_ppo2.py:137][0m Total time:      31.58 min
[32m[20221208 15:08:09 @agent_ppo2.py:139][0m 2555904 total steps have happened
[32m[20221208 15:08:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1248 --------------------------#
[32m[20221208 15:08:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |           0.0986 |           0.8343 |        -833.5759 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |           0.0706 |           0.6387 |        -763.5815 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |           0.0672 |           0.6187 |        -753.5106 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |           0.0318 |           0.6005 |        -891.0888 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |           0.0161 |           0.5952 |        -956.0854 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |           0.0457 |           0.5911 |        -810.1628 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |          -0.0013 |           0.5878 |        -986.5265 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |          -0.0023 |           0.5816 |       -1013.1293 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |          -0.0042 |           0.5818 |       -1006.1297 |
[32m[20221208 15:08:10 @agent_ppo2.py:179][0m |          -0.0038 |           0.5802 |        -998.1320 |
[32m[20221208 15:08:10 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.31
[32m[20221208 15:08:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.29
[32m[20221208 15:08:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.77
[32m[20221208 15:08:11 @agent_ppo2.py:137][0m Total time:      31.60 min
[32m[20221208 15:08:11 @agent_ppo2.py:139][0m 2557952 total steps have happened
[32m[20221208 15:08:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1249 --------------------------#
[32m[20221208 15:08:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.1245 |           1.0086 |        -599.6192 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0983 |           0.9179 |        -218.9774 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0627 |           0.8647 |        -236.8882 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0436 |           0.8476 |        -359.1330 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0237 |           0.8146 |        -531.0822 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0084 |           0.8004 |        -625.9922 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0880 |           0.7857 |        -567.5618 |
[32m[20221208 15:08:11 @agent_ppo2.py:179][0m |           0.0311 |           0.7802 |        -493.4031 |
[32m[20221208 15:08:12 @agent_ppo2.py:179][0m |          -0.0036 |           0.7654 |        -614.7096 |
[32m[20221208 15:08:12 @agent_ppo2.py:179][0m |          -0.0104 |           0.7535 |        -699.9699 |
[32m[20221208 15:08:12 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.87
[32m[20221208 15:08:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.49
[32m[20221208 15:08:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.53
[32m[20221208 15:08:12 @agent_ppo2.py:137][0m Total time:      31.63 min
[32m[20221208 15:08:12 @agent_ppo2.py:139][0m 2560000 total steps have happened
[32m[20221208 15:08:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1250 --------------------------#
[32m[20221208 15:08:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |           0.0697 |           2.0262 |        -891.1093 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |           0.2431 |           1.6331 |        -716.4425 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |           0.0544 |           1.4977 |        -493.7811 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |           0.0223 |           1.4253 |        -597.9389 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |          -0.0098 |           1.3972 |        -715.8000 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |          -0.0155 |           1.3313 |        -745.3018 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |          -0.0363 |           1.2864 |        -807.0432 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |          -0.0425 |           1.2667 |        -856.9428 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |          -0.0483 |           1.2437 |        -863.3301 |
[32m[20221208 15:08:13 @agent_ppo2.py:179][0m |          -0.0554 |           1.2255 |        -908.4567 |
[32m[20221208 15:08:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:08:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.00
[32m[20221208 15:08:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.59
[32m[20221208 15:08:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.50
[32m[20221208 15:08:13 @agent_ppo2.py:137][0m Total time:      31.65 min
[32m[20221208 15:08:13 @agent_ppo2.py:139][0m 2562048 total steps have happened
[32m[20221208 15:08:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1251 --------------------------#
[32m[20221208 15:08:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |           0.0919 |           0.7307 |        -836.3660 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |           0.1377 |           0.6500 |        -599.1186 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |           0.1113 |           0.6266 |        -540.7187 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |           0.0493 |           0.6122 |        -487.5063 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |           0.0339 |           0.6021 |        -518.2814 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |          -0.0115 |           0.5978 |        -623.2181 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |          -0.0187 |           0.5911 |        -631.2498 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |          -0.0078 |           0.5851 |        -611.5557 |
[32m[20221208 15:08:14 @agent_ppo2.py:179][0m |          -0.0132 |           0.5880 |        -632.9537 |
[32m[20221208 15:08:15 @agent_ppo2.py:179][0m |          -0.0105 |           0.5805 |        -622.6049 |
[32m[20221208 15:08:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.21
[32m[20221208 15:08:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.01
[32m[20221208 15:08:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.41
[32m[20221208 15:08:15 @agent_ppo2.py:137][0m Total time:      31.68 min
[32m[20221208 15:08:15 @agent_ppo2.py:139][0m 2564096 total steps have happened
[32m[20221208 15:08:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1252 --------------------------#
[32m[20221208 15:08:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |           0.0798 |           0.8563 |        -720.2495 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |           0.0559 |           0.6689 |        -584.3556 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |           0.0039 |           0.6464 |        -744.1307 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0024 |           0.6371 |        -825.1212 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0028 |           0.6272 |        -806.6268 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0076 |           0.6233 |        -834.5368 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0060 |           0.6154 |        -833.4479 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0144 |           0.6162 |        -848.9368 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0147 |           0.6102 |        -851.6206 |
[32m[20221208 15:08:16 @agent_ppo2.py:179][0m |          -0.0170 |           0.6106 |        -853.3196 |
[32m[20221208 15:08:16 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.89
[32m[20221208 15:08:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.89
[32m[20221208 15:08:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.73
[32m[20221208 15:08:16 @agent_ppo2.py:137][0m Total time:      31.70 min
[32m[20221208 15:08:16 @agent_ppo2.py:139][0m 2566144 total steps have happened
[32m[20221208 15:08:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1253 --------------------------#
[32m[20221208 15:08:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |           0.1439 |           2.0668 |        -782.3387 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |           0.0700 |           1.7718 |        -594.8961 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |           0.0027 |           1.6629 |        -693.3112 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |          -0.0197 |           1.6536 |        -717.9925 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |          -0.0294 |           1.5814 |        -749.5134 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |          -0.0403 |           1.5510 |        -769.2950 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |          -0.0458 |           1.5281 |        -770.4372 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |          -0.0540 |           1.5011 |        -790.7959 |
[32m[20221208 15:08:17 @agent_ppo2.py:179][0m |          -0.0582 |           1.4899 |        -793.1974 |
[32m[20221208 15:08:18 @agent_ppo2.py:179][0m |          -0.0599 |           1.4823 |        -802.6742 |
[32m[20221208 15:08:18 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.77
[32m[20221208 15:08:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.73
[32m[20221208 15:08:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.20
[32m[20221208 15:08:18 @agent_ppo2.py:137][0m Total time:      31.73 min
[32m[20221208 15:08:18 @agent_ppo2.py:139][0m 2568192 total steps have happened
[32m[20221208 15:08:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1254 --------------------------#
[32m[20221208 15:08:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:18 @agent_ppo2.py:179][0m |           0.0650 |           1.7924 |        -768.9360 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |           0.0331 |           1.5216 |        -799.2093 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |           0.0063 |           1.4010 |        -780.8403 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |           0.0129 |           1.3251 |        -529.2236 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |          -0.0295 |           1.2500 |        -553.5610 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |          -0.0431 |           1.1881 |        -591.4936 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |          -0.0516 |           1.1172 |        -616.5990 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |          -0.0554 |           1.0904 |        -629.8918 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |          -0.0575 |           1.0380 |        -622.3526 |
[32m[20221208 15:08:19 @agent_ppo2.py:179][0m |          -0.0594 |           1.0116 |        -634.0366 |
[32m[20221208 15:08:19 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.31
[32m[20221208 15:08:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.16
[32m[20221208 15:08:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.29
[32m[20221208 15:08:19 @agent_ppo2.py:137][0m Total time:      31.75 min
[32m[20221208 15:08:19 @agent_ppo2.py:139][0m 2570240 total steps have happened
[32m[20221208 15:08:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1255 --------------------------#
[32m[20221208 15:08:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.1694 |           1.0416 |        -675.3329 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.1673 |           0.8411 |        -123.1475 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0956 |           0.7739 |        -139.1292 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0812 |           0.7443 |        -158.4894 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0631 |           0.7296 |        -213.2484 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0582 |           0.7146 |        -335.0521 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0679 |           0.7099 |        -197.0934 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0478 |           0.7041 |        -232.3578 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0418 |           0.6987 |        -257.3721 |
[32m[20221208 15:08:20 @agent_ppo2.py:179][0m |           0.0307 |           0.6884 |        -320.4332 |
[32m[20221208 15:08:20 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.45
[32m[20221208 15:08:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.27
[32m[20221208 15:08:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.06
[32m[20221208 15:08:21 @agent_ppo2.py:137][0m Total time:      31.78 min
[32m[20221208 15:08:21 @agent_ppo2.py:139][0m 2572288 total steps have happened
[32m[20221208 15:08:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1256 --------------------------#
[32m[20221208 15:08:21 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:08:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:21 @agent_ppo2.py:179][0m |           0.0785 |           2.2582 |        -702.7803 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |           0.0483 |           1.7558 |        -646.8011 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0098 |           1.5772 |        -658.4444 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0273 |           1.5032 |        -696.5583 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0444 |           1.4118 |        -719.8730 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0536 |           1.3750 |        -737.5999 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0619 |           1.3219 |        -752.7520 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0582 |           1.2827 |        -741.5681 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0619 |           1.2548 |        -770.2509 |
[32m[20221208 15:08:22 @agent_ppo2.py:179][0m |          -0.0648 |           1.2455 |        -772.0108 |
[32m[20221208 15:08:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.21
[32m[20221208 15:08:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.07
[32m[20221208 15:08:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.06
[32m[20221208 15:08:22 @agent_ppo2.py:137][0m Total time:      31.80 min
[32m[20221208 15:08:22 @agent_ppo2.py:139][0m 2574336 total steps have happened
[32m[20221208 15:08:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1257 --------------------------#
[32m[20221208 15:08:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |           0.1153 |           5.3813 |        -735.2089 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |           0.1533 |           4.9087 |        -598.4895 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |           0.0245 |           4.8041 |        -642.4451 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0093 |           4.6032 |        -680.1578 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0334 |           4.5020 |        -716.4671 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0455 |           4.4298 |        -736.1206 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0519 |           4.3772 |        -762.0872 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0591 |           4.3262 |        -785.8522 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0639 |           4.2851 |        -794.2160 |
[32m[20221208 15:08:23 @agent_ppo2.py:179][0m |          -0.0682 |           4.2329 |        -815.9117 |
[32m[20221208 15:08:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.52
[32m[20221208 15:08:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.54
[32m[20221208 15:08:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.84
[32m[20221208 15:08:24 @agent_ppo2.py:137][0m Total time:      31.83 min
[32m[20221208 15:08:24 @agent_ppo2.py:139][0m 2576384 total steps have happened
[32m[20221208 15:08:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1258 --------------------------#
[32m[20221208 15:08:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:24 @agent_ppo2.py:179][0m |           0.0646 |           2.5465 |        -757.5495 |
[32m[20221208 15:08:24 @agent_ppo2.py:179][0m |           0.0665 |           2.2730 |        -551.0167 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |           0.0113 |           2.1670 |        -453.2896 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0144 |           2.0920 |        -497.9516 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0323 |           2.0517 |        -492.2175 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0463 |           1.9873 |        -519.7243 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0538 |           1.9586 |        -536.8298 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0590 |           1.9146 |        -545.3197 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0606 |           1.8862 |        -557.0198 |
[32m[20221208 15:08:25 @agent_ppo2.py:179][0m |          -0.0586 |           1.8671 |        -551.7798 |
[32m[20221208 15:08:25 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.30
[32m[20221208 15:08:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.96
[32m[20221208 15:08:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.15
[32m[20221208 15:08:25 @agent_ppo2.py:137][0m Total time:      31.85 min
[32m[20221208 15:08:25 @agent_ppo2.py:139][0m 2578432 total steps have happened
[32m[20221208 15:08:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1259 --------------------------#
[32m[20221208 15:08:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.1232 |           0.6982 |        -535.5213 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.2958 |           0.6303 |         -99.3944 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.1276 |           0.6093 |         -37.5948 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.1170 |           0.6032 |         -39.9992 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.1052 |           0.5950 |         -57.4650 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.0876 |           0.5901 |         -91.4193 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.0678 |           0.5855 |        -157.2350 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.0497 |           0.5841 |        -254.6901 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.0595 |           0.5808 |        -299.9582 |
[32m[20221208 15:08:26 @agent_ppo2.py:179][0m |           0.0607 |           0.5822 |        -303.0348 |
[32m[20221208 15:08:26 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:08:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.34
[32m[20221208 15:08:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.14
[32m[20221208 15:08:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.26
[32m[20221208 15:08:27 @agent_ppo2.py:137][0m Total time:      31.88 min
[32m[20221208 15:08:27 @agent_ppo2.py:139][0m 2580480 total steps have happened
[32m[20221208 15:08:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1260 --------------------------#
[32m[20221208 15:08:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:27 @agent_ppo2.py:179][0m |           0.1065 |           2.5225 |        -639.6707 |
[32m[20221208 15:08:27 @agent_ppo2.py:179][0m |           0.0936 |           2.2373 |        -479.6688 |
[32m[20221208 15:08:27 @agent_ppo2.py:179][0m |           0.0538 |           2.1244 |        -525.5811 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |           0.0108 |           2.0427 |        -625.0185 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |          -0.0083 |           1.9960 |        -690.4054 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |          -0.0264 |           1.9370 |        -714.5180 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |          -0.0109 |           1.9133 |        -687.3891 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |          -0.0101 |           1.8850 |        -557.4033 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |          -0.0364 |           1.8492 |        -686.1818 |
[32m[20221208 15:08:28 @agent_ppo2.py:179][0m |          -0.0450 |           1.8201 |        -719.4055 |
[32m[20221208 15:08:28 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.73
[32m[20221208 15:08:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.58
[32m[20221208 15:08:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.17
[32m[20221208 15:08:28 @agent_ppo2.py:137][0m Total time:      31.90 min
[32m[20221208 15:08:28 @agent_ppo2.py:139][0m 2582528 total steps have happened
[32m[20221208 15:08:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1261 --------------------------#
[32m[20221208 15:08:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1360 |           0.6513 |        -461.4518 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1483 |           0.6506 |        -164.4233 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1432 |           0.6429 |        -154.6813 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1789 |           0.6448 |         -11.2413 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1394 |           0.6502 |          -5.4907 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1344 |           0.6465 |         -10.5439 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1295 |           0.6381 |         -17.3738 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1213 |           0.6393 |         -30.3392 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1264 |           0.6387 |        -148.3566 |
[32m[20221208 15:08:29 @agent_ppo2.py:179][0m |           0.1361 |           0.6377 |         -20.1639 |
[32m[20221208 15:08:29 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.18
[32m[20221208 15:08:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.77
[32m[20221208 15:08:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.85
[32m[20221208 15:08:30 @agent_ppo2.py:137][0m Total time:      31.93 min
[32m[20221208 15:08:30 @agent_ppo2.py:139][0m 2584576 total steps have happened
[32m[20221208 15:08:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1262 --------------------------#
[32m[20221208 15:08:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:30 @agent_ppo2.py:179][0m |           0.1282 |           1.4003 |        -560.4374 |
[32m[20221208 15:08:30 @agent_ppo2.py:179][0m |           0.1392 |           1.2037 |        -288.0362 |
[32m[20221208 15:08:30 @agent_ppo2.py:179][0m |           0.0806 |           1.1756 |        -321.8526 |
[32m[20221208 15:08:30 @agent_ppo2.py:179][0m |           0.0527 |           1.1373 |        -403.1054 |
[32m[20221208 15:08:31 @agent_ppo2.py:179][0m |           0.0770 |           1.1056 |        -372.4830 |
[32m[20221208 15:08:31 @agent_ppo2.py:179][0m |           0.0593 |           1.0903 |        -303.2087 |
[32m[20221208 15:08:31 @agent_ppo2.py:179][0m |           0.0479 |           1.0874 |        -325.6949 |
[32m[20221208 15:08:31 @agent_ppo2.py:179][0m |           0.0351 |           1.0704 |        -335.8237 |
[32m[20221208 15:08:31 @agent_ppo2.py:179][0m |           0.0293 |           1.0382 |        -349.1999 |
[32m[20221208 15:08:31 @agent_ppo2.py:179][0m |          -0.0104 |           1.0428 |        -543.8178 |
[32m[20221208 15:08:31 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.67
[32m[20221208 15:08:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.20
[32m[20221208 15:08:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.01
[32m[20221208 15:08:31 @agent_ppo2.py:137][0m Total time:      31.95 min
[32m[20221208 15:08:31 @agent_ppo2.py:139][0m 2586624 total steps have happened
[32m[20221208 15:08:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1263 --------------------------#
[32m[20221208 15:08:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |           0.0706 |           1.9345 |        -602.1524 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |           0.0358 |           1.6869 |        -397.0826 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |           0.0304 |           1.6002 |        -374.3407 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |           0.0030 |           1.5506 |        -219.6022 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |          -0.0083 |           1.5200 |        -212.9294 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |          -0.0164 |           1.4747 |        -254.3075 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |          -0.0196 |           1.4671 |        -270.1083 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |          -0.0235 |           1.4594 |        -270.6787 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |          -0.0263 |           1.4098 |        -278.6667 |
[32m[20221208 15:08:32 @agent_ppo2.py:179][0m |          -0.0286 |           1.3863 |        -271.1192 |
[32m[20221208 15:08:32 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.38
[32m[20221208 15:08:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 111.18
[32m[20221208 15:08:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.22
[32m[20221208 15:08:33 @agent_ppo2.py:137][0m Total time:      31.97 min
[32m[20221208 15:08:33 @agent_ppo2.py:139][0m 2588672 total steps have happened
[32m[20221208 15:08:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1264 --------------------------#
[32m[20221208 15:08:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:33 @agent_ppo2.py:179][0m |           0.1100 |           2.2254 |        -576.4264 |
[32m[20221208 15:08:33 @agent_ppo2.py:179][0m |           0.0274 |           1.8219 |        -568.7571 |
[32m[20221208 15:08:33 @agent_ppo2.py:179][0m |           0.0134 |           1.6503 |        -563.0062 |
[32m[20221208 15:08:33 @agent_ppo2.py:179][0m |          -0.0163 |           1.4988 |        -606.7448 |
[32m[20221208 15:08:33 @agent_ppo2.py:179][0m |          -0.0062 |           1.4051 |        -547.8737 |
[32m[20221208 15:08:34 @agent_ppo2.py:179][0m |          -0.0245 |           1.3441 |        -571.6593 |
[32m[20221208 15:08:34 @agent_ppo2.py:179][0m |          -0.0342 |           1.3032 |        -646.0588 |
[32m[20221208 15:08:34 @agent_ppo2.py:179][0m |          -0.0412 |           1.2760 |        -643.4493 |
[32m[20221208 15:08:34 @agent_ppo2.py:179][0m |          -0.0411 |           1.2392 |        -654.3263 |
[32m[20221208 15:08:34 @agent_ppo2.py:179][0m |          -0.0304 |           1.1974 |        -642.3522 |
[32m[20221208 15:08:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.61
[32m[20221208 15:08:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.15
[32m[20221208 15:08:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.88
[32m[20221208 15:08:34 @agent_ppo2.py:137][0m Total time:      32.00 min
[32m[20221208 15:08:34 @agent_ppo2.py:139][0m 2590720 total steps have happened
[32m[20221208 15:08:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1265 --------------------------#
[32m[20221208 15:08:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:08:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |           0.0801 |           3.1947 |        -561.3100 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |           0.0845 |           2.6361 |        -468.9775 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |           0.0210 |           2.5008 |        -525.4051 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0139 |           2.4004 |        -545.7611 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0266 |           2.3046 |        -576.0046 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0320 |           2.2831 |        -599.2856 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0390 |           2.2079 |        -593.2524 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0442 |           2.1944 |        -603.2919 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0470 |           2.1437 |        -612.8379 |
[32m[20221208 15:08:35 @agent_ppo2.py:179][0m |          -0.0492 |           2.1180 |        -611.2043 |
[32m[20221208 15:08:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.19
[32m[20221208 15:08:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.68
[32m[20221208 15:08:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.87
[32m[20221208 15:08:36 @agent_ppo2.py:137][0m Total time:      32.02 min
[32m[20221208 15:08:36 @agent_ppo2.py:139][0m 2592768 total steps have happened
[32m[20221208 15:08:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1266 --------------------------#
[32m[20221208 15:08:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:36 @agent_ppo2.py:179][0m |           0.0661 |           2.4419 |        -584.7883 |
[32m[20221208 15:08:36 @agent_ppo2.py:179][0m |           0.0730 |           1.9539 |        -501.0425 |
[32m[20221208 15:08:36 @agent_ppo2.py:179][0m |           0.0274 |           1.7772 |        -495.1505 |
[32m[20221208 15:08:36 @agent_ppo2.py:179][0m |          -0.0049 |           1.6514 |        -456.4732 |
[32m[20221208 15:08:36 @agent_ppo2.py:179][0m |          -0.0403 |           1.5830 |        -552.3683 |
[32m[20221208 15:08:36 @agent_ppo2.py:179][0m |          -0.0524 |           1.4911 |        -569.5644 |
[32m[20221208 15:08:37 @agent_ppo2.py:179][0m |          -0.0545 |           1.4369 |        -588.0326 |
[32m[20221208 15:08:37 @agent_ppo2.py:179][0m |          -0.0597 |           1.3875 |        -592.4429 |
[32m[20221208 15:08:37 @agent_ppo2.py:179][0m |          -0.0585 |           1.3367 |        -484.7033 |
[32m[20221208 15:08:37 @agent_ppo2.py:179][0m |          -0.0676 |           1.3030 |        -496.1866 |
[32m[20221208 15:08:37 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.40
[32m[20221208 15:08:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.00
[32m[20221208 15:08:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.16
[32m[20221208 15:08:37 @agent_ppo2.py:137][0m Total time:      32.05 min
[32m[20221208 15:08:37 @agent_ppo2.py:139][0m 2594816 total steps have happened
[32m[20221208 15:08:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1267 --------------------------#
[32m[20221208 15:08:38 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:08:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |           0.0837 |           3.7343 |        -623.8938 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |           0.0613 |           3.2028 |        -560.1971 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |           0.0118 |           3.0137 |        -554.1535 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0277 |           2.9062 |        -598.6199 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0414 |           2.8366 |        -617.4220 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0610 |           2.7980 |        -636.3275 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0666 |           2.6988 |        -652.3573 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0716 |           2.6740 |        -660.0092 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0807 |           2.6379 |        -671.6472 |
[32m[20221208 15:08:38 @agent_ppo2.py:179][0m |          -0.0773 |           2.5823 |        -680.6312 |
[32m[20221208 15:08:38 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:08:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.32
[32m[20221208 15:08:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.33
[32m[20221208 15:08:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.99
[32m[20221208 15:08:39 @agent_ppo2.py:137][0m Total time:      32.07 min
[32m[20221208 15:08:39 @agent_ppo2.py:139][0m 2596864 total steps have happened
[32m[20221208 15:08:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1268 --------------------------#
[32m[20221208 15:08:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:39 @agent_ppo2.py:179][0m |           0.0636 |           1.4710 |        -619.2164 |
[32m[20221208 15:08:39 @agent_ppo2.py:179][0m |           0.0249 |           1.1886 |        -476.7275 |
[32m[20221208 15:08:39 @agent_ppo2.py:179][0m |          -0.0234 |           1.0587 |        -489.1942 |
[32m[20221208 15:08:39 @agent_ppo2.py:179][0m |          -0.0317 |           0.9944 |        -497.4821 |
[32m[20221208 15:08:39 @agent_ppo2.py:179][0m |          -0.0572 |           0.9311 |        -499.5966 |
[32m[20221208 15:08:39 @agent_ppo2.py:179][0m |          -0.0644 |           0.8965 |        -514.1000 |
[32m[20221208 15:08:40 @agent_ppo2.py:179][0m |          -0.0720 |           0.8632 |        -528.5890 |
[32m[20221208 15:08:40 @agent_ppo2.py:179][0m |          -0.0747 |           0.8312 |        -523.6729 |
[32m[20221208 15:08:40 @agent_ppo2.py:179][0m |          -0.0786 |           0.8132 |        -544.4029 |
[32m[20221208 15:08:40 @agent_ppo2.py:179][0m |          -0.0816 |           0.7941 |        -539.8836 |
[32m[20221208 15:08:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.61
[32m[20221208 15:08:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.64
[32m[20221208 15:08:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.06
[32m[20221208 15:08:40 @agent_ppo2.py:137][0m Total time:      32.10 min
[32m[20221208 15:08:40 @agent_ppo2.py:139][0m 2598912 total steps have happened
[32m[20221208 15:08:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1269 --------------------------#
[32m[20221208 15:08:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.1695 |           0.5307 |        -473.1951 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0864 |           0.5086 |        -193.0126 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0654 |           0.5007 |        -174.1935 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0299 |           0.4945 |        -241.1706 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0325 |           0.4870 |        -227.4908 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0330 |           0.4860 |        -208.4453 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0109 |           0.4852 |        -256.5299 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |          -0.0005 |           0.4852 |        -289.2079 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0306 |           0.4814 |        -254.6791 |
[32m[20221208 15:08:41 @agent_ppo2.py:179][0m |           0.0071 |           0.4772 |        -283.5634 |
[32m[20221208 15:08:41 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:08:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 29.05
[32m[20221208 15:08:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.18
[32m[20221208 15:08:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.18
[32m[20221208 15:08:42 @agent_ppo2.py:137][0m Total time:      32.12 min
[32m[20221208 15:08:42 @agent_ppo2.py:139][0m 2600960 total steps have happened
[32m[20221208 15:08:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1270 --------------------------#
[32m[20221208 15:08:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |           0.0736 |           2.3185 |        -607.1060 |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |           0.0547 |           1.7132 |        -451.7659 |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |           0.0268 |           1.5663 |        -351.6140 |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |           0.0051 |           1.4793 |        -378.7972 |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |          -0.0067 |           1.4144 |        -342.7624 |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |          -0.0267 |           1.3632 |        -361.1665 |
[32m[20221208 15:08:42 @agent_ppo2.py:179][0m |          -0.0339 |           1.3315 |        -414.7328 |
[32m[20221208 15:08:43 @agent_ppo2.py:179][0m |          -0.0460 |           1.3064 |        -425.5869 |
[32m[20221208 15:08:43 @agent_ppo2.py:179][0m |          -0.0532 |           1.2680 |        -453.1203 |
[32m[20221208 15:08:43 @agent_ppo2.py:179][0m |          -0.0593 |           1.2417 |        -475.2098 |
[32m[20221208 15:08:43 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.09
[32m[20221208 15:08:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.03
[32m[20221208 15:08:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.87
[32m[20221208 15:08:43 @agent_ppo2.py:137][0m Total time:      32.15 min
[32m[20221208 15:08:43 @agent_ppo2.py:139][0m 2603008 total steps have happened
[32m[20221208 15:08:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1271 --------------------------#
[32m[20221208 15:08:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |           0.1116 |           4.1318 |        -633.0935 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |           0.0971 |           3.3359 |        -489.7450 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |           0.0294 |           3.1291 |        -506.9208 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0104 |           2.9862 |        -539.2445 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0329 |           2.9376 |        -568.4769 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0434 |           2.8158 |        -587.5080 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0572 |           2.7451 |        -592.8790 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0642 |           2.7249 |        -611.7615 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0717 |           2.6661 |        -619.9765 |
[32m[20221208 15:08:44 @agent_ppo2.py:179][0m |          -0.0737 |           2.6334 |        -626.0024 |
[32m[20221208 15:08:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.53
[32m[20221208 15:08:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.44
[32m[20221208 15:08:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.60
[32m[20221208 15:08:45 @agent_ppo2.py:137][0m Total time:      32.17 min
[32m[20221208 15:08:45 @agent_ppo2.py:139][0m 2605056 total steps have happened
[32m[20221208 15:08:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1272 --------------------------#
[32m[20221208 15:08:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |           0.0601 |           1.3153 |        -627.0386 |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |           0.0498 |           1.0697 |        -567.5395 |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |          -0.0084 |           0.9871 |        -559.6535 |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |          -0.0234 |           0.9537 |        -597.8338 |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |          -0.0416 |           0.9249 |        -626.0194 |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |          -0.0440 |           0.9020 |        -629.4222 |
[32m[20221208 15:08:45 @agent_ppo2.py:179][0m |          -0.0415 |           0.8826 |        -615.4632 |
[32m[20221208 15:08:46 @agent_ppo2.py:179][0m |          -0.0500 |           0.8666 |        -637.3936 |
[32m[20221208 15:08:46 @agent_ppo2.py:179][0m |          -0.0569 |           0.8518 |        -653.8765 |
[32m[20221208 15:08:46 @agent_ppo2.py:179][0m |          -0.0554 |           0.8483 |        -653.0189 |
[32m[20221208 15:08:46 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.44
[32m[20221208 15:08:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.40
[32m[20221208 15:08:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.80
[32m[20221208 15:08:46 @agent_ppo2.py:137][0m Total time:      32.20 min
[32m[20221208 15:08:46 @agent_ppo2.py:139][0m 2607104 total steps have happened
[32m[20221208 15:08:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1273 --------------------------#
[32m[20221208 15:08:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |           0.1224 |           1.1257 |        -521.7616 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |           0.0660 |           0.8626 |        -446.3031 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |           0.0055 |           0.7764 |        -485.0348 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0201 |           0.7350 |        -523.5838 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0370 |           0.7008 |        -536.1777 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0385 |           0.6860 |        -511.2849 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0476 |           0.6651 |        -530.6732 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0529 |           0.6416 |        -559.9686 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0571 |           0.6276 |        -561.8704 |
[32m[20221208 15:08:47 @agent_ppo2.py:179][0m |          -0.0606 |           0.6180 |        -558.6984 |
[32m[20221208 15:08:47 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.84
[32m[20221208 15:08:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.39
[32m[20221208 15:08:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.18
[32m[20221208 15:08:48 @agent_ppo2.py:137][0m Total time:      32.22 min
[32m[20221208 15:08:48 @agent_ppo2.py:139][0m 2609152 total steps have happened
[32m[20221208 15:08:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1274 --------------------------#
[32m[20221208 15:08:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.0565 |           0.7498 |        -538.2076 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.1143 |           0.6485 |        -408.8755 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.1643 |           0.6350 |        -130.7841 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.1558 |           0.6297 |         -41.0864 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.1375 |           0.6242 |         -66.6854 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.1354 |           0.6190 |        -109.2557 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.0840 |           0.6182 |        -185.6535 |
[32m[20221208 15:08:48 @agent_ppo2.py:179][0m |           0.0514 |           0.6143 |        -295.5135 |
[32m[20221208 15:08:49 @agent_ppo2.py:179][0m |           0.0402 |           0.6150 |        -330.1507 |
[32m[20221208 15:08:49 @agent_ppo2.py:179][0m |           0.0111 |           0.6161 |        -379.4867 |
[32m[20221208 15:08:49 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.88
[32m[20221208 15:08:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.34
[32m[20221208 15:08:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.76
[32m[20221208 15:08:49 @agent_ppo2.py:137][0m Total time:      32.25 min
[32m[20221208 15:08:49 @agent_ppo2.py:139][0m 2611200 total steps have happened
[32m[20221208 15:08:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1275 --------------------------#
[32m[20221208 15:08:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |           0.0631 |           1.2770 |        -522.5120 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |           0.0070 |           0.8902 |        -489.8074 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0342 |           0.7893 |        -528.4330 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0464 |           0.7321 |        -542.6431 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0532 |           0.7027 |        -540.7456 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0585 |           0.6813 |        -545.5715 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0696 |           0.6560 |        -557.2753 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0694 |           0.6313 |        -566.8935 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0747 |           0.6310 |        -581.5544 |
[32m[20221208 15:08:50 @agent_ppo2.py:179][0m |          -0.0756 |           0.6098 |        -582.1570 |
[32m[20221208 15:08:50 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.79
[32m[20221208 15:08:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.70
[32m[20221208 15:08:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.35
[32m[20221208 15:08:50 @agent_ppo2.py:137][0m Total time:      32.27 min
[32m[20221208 15:08:50 @agent_ppo2.py:139][0m 2613248 total steps have happened
[32m[20221208 15:08:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1276 --------------------------#
[32m[20221208 15:08:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |           0.0723 |           0.8415 |        -487.1242 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |           0.0539 |           0.7199 |        -326.0039 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |           0.0012 |           0.6748 |        -296.0295 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |          -0.0342 |           0.6354 |        -311.4622 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |          -0.0496 |           0.6134 |        -323.2816 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |          -0.0582 |           0.6013 |        -329.8399 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |          -0.0633 |           0.5955 |        -335.9139 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |          -0.0603 |           0.5834 |        -343.0443 |
[32m[20221208 15:08:51 @agent_ppo2.py:179][0m |          -0.0594 |           0.5740 |        -313.1350 |
[32m[20221208 15:08:52 @agent_ppo2.py:179][0m |          -0.0608 |           0.5658 |        -326.0417 |
[32m[20221208 15:08:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 49.82
[32m[20221208 15:08:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.04
[32m[20221208 15:08:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 111.55
[32m[20221208 15:08:52 @agent_ppo2.py:137][0m Total time:      32.29 min
[32m[20221208 15:08:52 @agent_ppo2.py:139][0m 2615296 total steps have happened
[32m[20221208 15:08:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1277 --------------------------#
[32m[20221208 15:08:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:52 @agent_ppo2.py:179][0m |           0.0787 |           0.5146 |        -506.8902 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |           0.0822 |           0.4269 |        -437.3048 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |           0.0417 |           0.4078 |        -416.1969 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |           0.0018 |           0.4018 |        -482.2658 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |          -0.0263 |           0.3909 |        -516.0516 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |          -0.0249 |           0.3860 |        -513.8287 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |          -0.0216 |           0.3778 |        -498.2798 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |          -0.0363 |           0.3732 |        -512.1221 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |          -0.0438 |           0.3717 |        -521.6527 |
[32m[20221208 15:08:53 @agent_ppo2.py:179][0m |          -0.0476 |           0.3683 |        -529.2271 |
[32m[20221208 15:08:53 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 32.93
[32m[20221208 15:08:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 40.84
[32m[20221208 15:08:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.15
[32m[20221208 15:08:53 @agent_ppo2.py:137][0m Total time:      32.32 min
[32m[20221208 15:08:53 @agent_ppo2.py:139][0m 2617344 total steps have happened
[32m[20221208 15:08:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1278 --------------------------#
[32m[20221208 15:08:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |           0.0693 |           1.0486 |        -480.0221 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |           0.0519 |           0.8747 |        -372.2754 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |           0.0037 |           0.8131 |        -379.6988 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0156 |           0.7807 |        -400.7570 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0325 |           0.7595 |        -399.9206 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0463 |           0.7442 |        -408.3502 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0503 |           0.7349 |        -411.2446 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0540 |           0.7285 |        -416.6738 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0600 |           0.7307 |        -421.7187 |
[32m[20221208 15:08:54 @agent_ppo2.py:179][0m |          -0.0543 |           0.7229 |        -422.2959 |
[32m[20221208 15:08:54 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:08:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.21
[32m[20221208 15:08:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.08
[32m[20221208 15:08:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.45
[32m[20221208 15:08:55 @agent_ppo2.py:137][0m Total time:      32.34 min
[32m[20221208 15:08:55 @agent_ppo2.py:139][0m 2619392 total steps have happened
[32m[20221208 15:08:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1279 --------------------------#
[32m[20221208 15:08:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:55 @agent_ppo2.py:179][0m |           0.0891 |           3.7040 |        -472.6797 |
[32m[20221208 15:08:55 @agent_ppo2.py:179][0m |           0.0436 |           3.1976 |        -447.2369 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |           0.0057 |           2.9939 |        -467.1410 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0214 |           2.9466 |        -492.7787 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0360 |           2.8654 |        -490.1180 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0465 |           2.7398 |        -498.7658 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0564 |           2.7192 |        -509.2584 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0562 |           2.6496 |        -507.9772 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0628 |           2.6326 |        -522.1732 |
[32m[20221208 15:08:56 @agent_ppo2.py:179][0m |          -0.0727 |           2.5754 |        -529.2260 |
[32m[20221208 15:08:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.11
[32m[20221208 15:08:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.81
[32m[20221208 15:08:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.98
[32m[20221208 15:08:56 @agent_ppo2.py:137][0m Total time:      32.37 min
[32m[20221208 15:08:56 @agent_ppo2.py:139][0m 2621440 total steps have happened
[32m[20221208 15:08:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1280 --------------------------#
[32m[20221208 15:08:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:08:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |           0.0640 |           3.4995 |        -461.1395 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |           0.0766 |           2.9077 |        -373.6247 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0025 |           2.7054 |        -426.4804 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0316 |           2.5592 |        -454.9004 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0498 |           2.4641 |        -478.9779 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0545 |           2.4028 |        -479.7350 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0637 |           2.3293 |        -488.3911 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0703 |           2.2627 |        -509.6236 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0721 |           2.2236 |        -514.7703 |
[32m[20221208 15:08:57 @agent_ppo2.py:179][0m |          -0.0744 |           2.2049 |        -523.3315 |
[32m[20221208 15:08:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:08:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 116.70
[32m[20221208 15:08:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.25
[32m[20221208 15:08:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.23
[32m[20221208 15:08:58 @agent_ppo2.py:137][0m Total time:      32.39 min
[32m[20221208 15:08:58 @agent_ppo2.py:139][0m 2623488 total steps have happened
[32m[20221208 15:08:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1281 --------------------------#
[32m[20221208 15:08:58 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:08:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:08:58 @agent_ppo2.py:179][0m |           0.3387 |           2.6274 |        -414.6923 |
[32m[20221208 15:08:58 @agent_ppo2.py:179][0m |           0.1035 |           2.3128 |        -299.3989 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |           0.0909 |           2.1459 |        -295.9789 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |           0.0352 |           2.0785 |        -334.2425 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |          -0.0110 |           2.0061 |        -411.5090 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |          -0.0254 |           1.9470 |        -433.4544 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |          -0.0369 |           1.9191 |        -447.0959 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |          -0.0456 |           1.8831 |        -460.7049 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |          -0.0446 |           1.8405 |        -469.1049 |
[32m[20221208 15:08:59 @agent_ppo2.py:179][0m |          -0.0504 |           1.8273 |        -489.2653 |
[32m[20221208 15:08:59 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:08:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.32
[32m[20221208 15:08:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.37
[32m[20221208 15:08:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.10
[32m[20221208 15:08:59 @agent_ppo2.py:137][0m Total time:      32.42 min
[32m[20221208 15:08:59 @agent_ppo2.py:139][0m 2625536 total steps have happened
[32m[20221208 15:08:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1282 --------------------------#
[32m[20221208 15:09:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.1113 |           0.8074 |        -367.9531 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0933 |           0.6995 |        -264.9804 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0800 |           0.6851 |        -231.2333 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0480 |           0.6697 |        -239.2863 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0383 |           0.6625 |        -279.6649 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0509 |           0.6604 |        -230.6813 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0297 |           0.6492 |        -264.6520 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0230 |           0.6467 |        -279.2560 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0144 |           0.6470 |        -308.8463 |
[32m[20221208 15:09:00 @agent_ppo2.py:179][0m |           0.0668 |           0.6465 |        -227.2266 |
[32m[20221208 15:09:00 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:09:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.49
[32m[20221208 15:09:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.91
[32m[20221208 15:09:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.82
[32m[20221208 15:09:01 @agent_ppo2.py:137][0m Total time:      32.44 min
[32m[20221208 15:09:01 @agent_ppo2.py:139][0m 2627584 total steps have happened
[32m[20221208 15:09:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1283 --------------------------#
[32m[20221208 15:09:01 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:01 @agent_ppo2.py:179][0m |           0.0892 |           0.9174 |        -335.1356 |
[32m[20221208 15:09:01 @agent_ppo2.py:179][0m |           0.1079 |           0.8685 |         -71.3548 |
[32m[20221208 15:09:01 @agent_ppo2.py:179][0m |           0.0817 |           0.8633 |        -101.1827 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0702 |           0.8508 |        -121.9704 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0485 |           0.8455 |        -165.9081 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0530 |           0.8395 |        -164.9162 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0560 |           0.8381 |        -154.4416 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0385 |           0.8335 |        -190.5326 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0466 |           0.8280 |        -177.7551 |
[32m[20221208 15:09:02 @agent_ppo2.py:179][0m |           0.0317 |           0.8323 |        -270.8956 |
[32m[20221208 15:09:02 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:09:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.20
[32m[20221208 15:09:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.65
[32m[20221208 15:09:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.24
[32m[20221208 15:09:02 @agent_ppo2.py:137][0m Total time:      32.47 min
[32m[20221208 15:09:02 @agent_ppo2.py:139][0m 2629632 total steps have happened
[32m[20221208 15:09:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1284 --------------------------#
[32m[20221208 15:09:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0376 |           0.9395 |        -405.9995 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0581 |           0.8197 |        -333.0651 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0111 |           0.7996 |        -386.5678 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0220 |           0.7943 |        -353.2175 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0008 |           0.7894 |        -395.9112 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |          -0.0027 |           0.7814 |        -385.2817 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |          -0.0112 |           0.7844 |        -401.2626 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0054 |           0.7776 |        -392.6196 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |           0.0000 |           0.7730 |        -375.9319 |
[32m[20221208 15:09:03 @agent_ppo2.py:179][0m |          -0.0107 |           0.7718 |        -400.1047 |
[32m[20221208 15:09:03 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.82
[32m[20221208 15:09:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.54
[32m[20221208 15:09:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.68
[32m[20221208 15:09:04 @agent_ppo2.py:137][0m Total time:      32.49 min
[32m[20221208 15:09:04 @agent_ppo2.py:139][0m 2631680 total steps have happened
[32m[20221208 15:09:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1285 --------------------------#
[32m[20221208 15:09:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:04 @agent_ppo2.py:179][0m |           0.0718 |           1.9733 |        -330.0836 |
[32m[20221208 15:09:04 @agent_ppo2.py:179][0m |           0.0308 |           1.6971 |        -312.9231 |
[32m[20221208 15:09:04 @agent_ppo2.py:179][0m |           0.0110 |           1.6237 |        -303.8807 |
[32m[20221208 15:09:04 @agent_ppo2.py:179][0m |          -0.0179 |           1.5670 |        -316.8703 |
[32m[20221208 15:09:05 @agent_ppo2.py:179][0m |          -0.0347 |           1.5090 |        -331.9598 |
[32m[20221208 15:09:05 @agent_ppo2.py:179][0m |          -0.0432 |           1.4824 |        -352.8579 |
[32m[20221208 15:09:05 @agent_ppo2.py:179][0m |          -0.0470 |           1.4402 |        -360.0450 |
[32m[20221208 15:09:05 @agent_ppo2.py:179][0m |          -0.0509 |           1.4060 |        -355.9026 |
[32m[20221208 15:09:05 @agent_ppo2.py:179][0m |          -0.0547 |           1.3817 |        -370.3716 |
[32m[20221208 15:09:05 @agent_ppo2.py:179][0m |          -0.0546 |           1.3799 |        -368.6994 |
[32m[20221208 15:09:05 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.50
[32m[20221208 15:09:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.14
[32m[20221208 15:09:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.60
[32m[20221208 15:09:05 @agent_ppo2.py:137][0m Total time:      32.52 min
[32m[20221208 15:09:05 @agent_ppo2.py:139][0m 2633728 total steps have happened
[32m[20221208 15:09:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1286 --------------------------#
[32m[20221208 15:09:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |           0.0689 |           1.2576 |        -296.9010 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |           0.1146 |           1.0555 |        -105.5817 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0064 |           0.9899 |         -96.4092 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0331 |           0.9589 |        -112.1005 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0465 |           0.9395 |        -122.9226 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0554 |           0.9243 |        -133.9291 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0602 |           0.9118 |        -136.1457 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0611 |           0.9027 |        -137.5691 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0625 |           0.8967 |        -141.6002 |
[32m[20221208 15:09:06 @agent_ppo2.py:179][0m |          -0.0649 |           0.8823 |        -140.5355 |
[32m[20221208 15:09:06 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.60
[32m[20221208 15:09:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 113.66
[32m[20221208 15:09:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.16
[32m[20221208 15:09:07 @agent_ppo2.py:137][0m Total time:      32.54 min
[32m[20221208 15:09:07 @agent_ppo2.py:139][0m 2635776 total steps have happened
[32m[20221208 15:09:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1287 --------------------------#
[32m[20221208 15:09:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:07 @agent_ppo2.py:179][0m |           0.1480 |           3.4357 |        -355.0157 |
[32m[20221208 15:09:07 @agent_ppo2.py:179][0m |           0.0738 |           3.0230 |        -297.6810 |
[32m[20221208 15:09:07 @agent_ppo2.py:179][0m |           0.0091 |           2.6923 |        -359.3652 |
[32m[20221208 15:09:07 @agent_ppo2.py:179][0m |          -0.0221 |           2.5616 |        -371.1479 |
[32m[20221208 15:09:08 @agent_ppo2.py:179][0m |          -0.0406 |           2.4448 |        -387.2995 |
[32m[20221208 15:09:08 @agent_ppo2.py:179][0m |          -0.0512 |           2.3266 |        -396.0312 |
[32m[20221208 15:09:08 @agent_ppo2.py:179][0m |          -0.0591 |           2.2815 |        -412.4484 |
[32m[20221208 15:09:08 @agent_ppo2.py:179][0m |          -0.0612 |           2.2258 |        -409.6331 |
[32m[20221208 15:09:08 @agent_ppo2.py:179][0m |          -0.0629 |           2.1626 |        -422.6942 |
[32m[20221208 15:09:08 @agent_ppo2.py:179][0m |          -0.0636 |           2.0928 |        -419.9671 |
[32m[20221208 15:09:08 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:09:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 109.62
[32m[20221208 15:09:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.53
[32m[20221208 15:09:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.17
[32m[20221208 15:09:08 @agent_ppo2.py:137][0m Total time:      32.57 min
[32m[20221208 15:09:08 @agent_ppo2.py:139][0m 2637824 total steps have happened
[32m[20221208 15:09:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1288 --------------------------#
[32m[20221208 15:09:09 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:09:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |           0.0731 |           1.4162 |        -272.4075 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |           0.0196 |           1.1664 |        -126.9876 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0176 |           1.0652 |        -127.3476 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0354 |           1.0036 |        -151.1715 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0431 |           0.9745 |        -153.9957 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0493 |           0.9422 |        -163.2018 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0530 |           0.9204 |        -163.1373 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0545 |           0.9032 |        -164.8083 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0570 |           0.9004 |        -171.0940 |
[32m[20221208 15:09:09 @agent_ppo2.py:179][0m |          -0.0607 |           0.8832 |        -180.1924 |
[32m[20221208 15:09:09 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.04
[32m[20221208 15:09:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.77
[32m[20221208 15:09:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.96
[32m[20221208 15:09:10 @agent_ppo2.py:137][0m Total time:      32.59 min
[32m[20221208 15:09:10 @agent_ppo2.py:139][0m 2639872 total steps have happened
[32m[20221208 15:09:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1289 --------------------------#
[32m[20221208 15:09:10 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:09:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:10 @agent_ppo2.py:179][0m |           0.0871 |           1.1526 |        -229.6195 |
[32m[20221208 15:09:10 @agent_ppo2.py:179][0m |           0.1207 |           1.0301 |         -80.4866 |
[32m[20221208 15:09:10 @agent_ppo2.py:179][0m |           0.0589 |           0.9995 |         -82.7148 |
[32m[20221208 15:09:10 @agent_ppo2.py:179][0m |           0.0366 |           0.9696 |         -84.0702 |
[32m[20221208 15:09:10 @agent_ppo2.py:179][0m |           0.0257 |           0.9498 |         -98.5363 |
[32m[20221208 15:09:11 @agent_ppo2.py:179][0m |           0.0188 |           0.9359 |        -105.7769 |
[32m[20221208 15:09:11 @agent_ppo2.py:179][0m |           0.0172 |           0.9250 |        -103.4489 |
[32m[20221208 15:09:11 @agent_ppo2.py:179][0m |           0.0131 |           0.9206 |        -112.3151 |
[32m[20221208 15:09:11 @agent_ppo2.py:179][0m |           0.0078 |           0.9058 |        -115.6694 |
[32m[20221208 15:09:11 @agent_ppo2.py:179][0m |           0.0072 |           0.9022 |        -125.3971 |
[32m[20221208 15:09:11 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.48
[32m[20221208 15:09:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.41
[32m[20221208 15:09:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.42
[32m[20221208 15:09:11 @agent_ppo2.py:137][0m Total time:      32.62 min
[32m[20221208 15:09:11 @agent_ppo2.py:139][0m 2641920 total steps have happened
[32m[20221208 15:09:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1290 --------------------------#
[32m[20221208 15:09:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |           0.0737 |           2.1955 |        -358.8004 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |           0.0513 |           1.9521 |        -269.7415 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0053 |           1.8758 |        -292.0948 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0287 |           1.8355 |        -319.3511 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0360 |           1.8012 |        -322.9827 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0442 |           1.7729 |        -329.8649 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0500 |           1.7780 |        -341.3390 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0538 |           1.7441 |        -347.6707 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0558 |           1.7084 |        -346.1378 |
[32m[20221208 15:09:12 @agent_ppo2.py:179][0m |          -0.0562 |           1.6780 |        -354.1892 |
[32m[20221208 15:09:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.99
[32m[20221208 15:09:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.14
[32m[20221208 15:09:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.88
[32m[20221208 15:09:13 @agent_ppo2.py:137][0m Total time:      32.64 min
[32m[20221208 15:09:13 @agent_ppo2.py:139][0m 2643968 total steps have happened
[32m[20221208 15:09:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1291 --------------------------#
[32m[20221208 15:09:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:13 @agent_ppo2.py:179][0m |           0.0737 |           3.3345 |        -395.8721 |
[32m[20221208 15:09:13 @agent_ppo2.py:179][0m |           0.0470 |           2.7816 |        -259.5975 |
[32m[20221208 15:09:13 @agent_ppo2.py:179][0m |          -0.0026 |           2.5758 |        -282.4616 |
[32m[20221208 15:09:13 @agent_ppo2.py:179][0m |          -0.0329 |           2.4256 |        -306.3505 |
[32m[20221208 15:09:13 @agent_ppo2.py:179][0m |          -0.0478 |           2.3069 |        -324.8811 |
[32m[20221208 15:09:13 @agent_ppo2.py:179][0m |          -0.0579 |           2.2380 |        -339.6224 |
[32m[20221208 15:09:14 @agent_ppo2.py:179][0m |          -0.0618 |           2.1500 |        -337.4787 |
[32m[20221208 15:09:14 @agent_ppo2.py:179][0m |          -0.0681 |           2.0616 |        -347.2007 |
[32m[20221208 15:09:14 @agent_ppo2.py:179][0m |          -0.0735 |           2.0001 |        -353.0468 |
[32m[20221208 15:09:14 @agent_ppo2.py:179][0m |          -0.0716 |           1.9267 |        -352.3913 |
[32m[20221208 15:09:14 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.24
[32m[20221208 15:09:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.47
[32m[20221208 15:09:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.73
[32m[20221208 15:09:14 @agent_ppo2.py:137][0m Total time:      32.66 min
[32m[20221208 15:09:14 @agent_ppo2.py:139][0m 2646016 total steps have happened
[32m[20221208 15:09:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1292 --------------------------#
[32m[20221208 15:09:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |           0.0534 |           2.6677 |        -298.9442 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |           0.0269 |           1.9192 |        -261.6048 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0181 |           1.6288 |        -267.7125 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0421 |           1.4949 |        -288.3178 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0623 |           1.3930 |        -298.5378 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0720 |           1.3220 |        -312.0387 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0756 |           1.2673 |        -318.2172 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0769 |           1.2262 |        -322.3031 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0752 |           1.2017 |        -323.9794 |
[32m[20221208 15:09:15 @agent_ppo2.py:179][0m |          -0.0791 |           1.1647 |        -324.2456 |
[32m[20221208 15:09:15 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.39
[32m[20221208 15:09:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.15
[32m[20221208 15:09:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.45
[32m[20221208 15:09:16 @agent_ppo2.py:137][0m Total time:      32.69 min
[32m[20221208 15:09:16 @agent_ppo2.py:139][0m 2648064 total steps have happened
[32m[20221208 15:09:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1293 --------------------------#
[32m[20221208 15:09:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |           0.1170 |           3.8835 |        -421.1614 |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |           0.0889 |           2.7738 |        -329.2735 |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |           0.0438 |           2.4419 |        -249.8973 |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |          -0.0084 |           2.2767 |        -260.8066 |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |          -0.0249 |           2.1683 |        -269.1271 |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |          -0.0443 |           2.1058 |        -285.7369 |
[32m[20221208 15:09:16 @agent_ppo2.py:179][0m |          -0.0525 |           2.0343 |        -284.6880 |
[32m[20221208 15:09:17 @agent_ppo2.py:179][0m |          -0.0601 |           1.9291 |        -311.5483 |
[32m[20221208 15:09:17 @agent_ppo2.py:179][0m |          -0.0653 |           1.9260 |        -306.6316 |
[32m[20221208 15:09:17 @agent_ppo2.py:179][0m |          -0.0715 |           1.8484 |        -329.7483 |
[32m[20221208 15:09:17 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.94
[32m[20221208 15:09:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.50
[32m[20221208 15:09:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.41
[32m[20221208 15:09:17 @agent_ppo2.py:137][0m Total time:      32.71 min
[32m[20221208 15:09:17 @agent_ppo2.py:139][0m 2650112 total steps have happened
[32m[20221208 15:09:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1294 --------------------------#
[32m[20221208 15:09:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |           0.0772 |           2.5080 |        -444.6276 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |           0.0312 |           2.0319 |        -409.9401 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |           0.0186 |           1.8566 |        -296.3105 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0007 |           1.7684 |        -307.9070 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0186 |           1.6768 |        -330.9662 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0294 |           1.6064 |        -385.3251 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0285 |           1.5626 |        -394.3509 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0411 |           1.5130 |        -435.7035 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0359 |           1.4807 |        -444.3629 |
[32m[20221208 15:09:18 @agent_ppo2.py:179][0m |          -0.0478 |           1.4483 |        -471.2111 |
[32m[20221208 15:09:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:09:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.83
[32m[20221208 15:09:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.57
[32m[20221208 15:09:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.82
[32m[20221208 15:09:19 @agent_ppo2.py:137][0m Total time:      32.74 min
[32m[20221208 15:09:19 @agent_ppo2.py:139][0m 2652160 total steps have happened
[32m[20221208 15:09:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1295 --------------------------#
[32m[20221208 15:09:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |           0.1104 |           2.0080 |        -386.8040 |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |           0.0633 |           1.5597 |        -343.4336 |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |           0.0138 |           1.4121 |        -341.4772 |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |          -0.0370 |           1.3427 |        -361.6665 |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |          -0.0531 |           1.2705 |        -375.2780 |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |          -0.0604 |           1.2261 |        -387.1340 |
[32m[20221208 15:09:19 @agent_ppo2.py:179][0m |          -0.0547 |           1.2052 |        -388.9276 |
[32m[20221208 15:09:20 @agent_ppo2.py:179][0m |          -0.0660 |           1.1652 |        -394.6216 |
[32m[20221208 15:09:20 @agent_ppo2.py:179][0m |          -0.0697 |           1.1425 |        -396.4895 |
[32m[20221208 15:09:20 @agent_ppo2.py:179][0m |          -0.0748 |           1.1279 |        -408.9355 |
[32m[20221208 15:09:20 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.80
[32m[20221208 15:09:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.06
[32m[20221208 15:09:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.95
[32m[20221208 15:09:20 @agent_ppo2.py:137][0m Total time:      32.76 min
[32m[20221208 15:09:20 @agent_ppo2.py:139][0m 2654208 total steps have happened
[32m[20221208 15:09:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1296 --------------------------#
[32m[20221208 15:09:20 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |           0.0610 |           1.4174 |        -428.6075 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |           0.0922 |           1.0673 |        -320.0465 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |           0.0774 |           0.9403 |        -296.6208 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |           0.0126 |           0.8747 |        -348.6015 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |          -0.0025 |           0.8223 |        -397.8850 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |          -0.0184 |           0.7977 |        -406.9788 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |          -0.0128 |           0.7741 |        -402.2514 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |          -0.0080 |           0.7550 |        -394.2285 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |          -0.0226 |           0.7431 |        -418.3452 |
[32m[20221208 15:09:21 @agent_ppo2.py:179][0m |          -0.0296 |           0.7256 |        -431.8915 |
[32m[20221208 15:09:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:09:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.67
[32m[20221208 15:09:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.46
[32m[20221208 15:09:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.87
[32m[20221208 15:09:22 @agent_ppo2.py:137][0m Total time:      32.79 min
[32m[20221208 15:09:22 @agent_ppo2.py:139][0m 2656256 total steps have happened
[32m[20221208 15:09:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1297 --------------------------#
[32m[20221208 15:09:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:22 @agent_ppo2.py:179][0m |           0.2047 |           2.4174 |        -454.1100 |
[32m[20221208 15:09:22 @agent_ppo2.py:179][0m |           0.0234 |           1.8121 |        -407.1099 |
[32m[20221208 15:09:22 @agent_ppo2.py:179][0m |          -0.0092 |           1.6262 |        -424.2124 |
[32m[20221208 15:09:22 @agent_ppo2.py:179][0m |          -0.0341 |           1.5147 |        -436.3452 |
[32m[20221208 15:09:22 @agent_ppo2.py:179][0m |          -0.0479 |           1.4061 |        -445.0901 |
[32m[20221208 15:09:22 @agent_ppo2.py:179][0m |          -0.0533 |           1.3413 |        -455.7919 |
[32m[20221208 15:09:23 @agent_ppo2.py:179][0m |          -0.0591 |           1.2726 |        -456.9131 |
[32m[20221208 15:09:23 @agent_ppo2.py:179][0m |          -0.0572 |           1.2483 |        -470.0798 |
[32m[20221208 15:09:23 @agent_ppo2.py:179][0m |          -0.0617 |           1.1877 |        -473.3808 |
[32m[20221208 15:09:23 @agent_ppo2.py:179][0m |          -0.0629 |           1.1473 |        -473.2377 |
[32m[20221208 15:09:23 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 15:09:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.61
[32m[20221208 15:09:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.41
[32m[20221208 15:09:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.21
[32m[20221208 15:09:23 @agent_ppo2.py:137][0m Total time:      32.81 min
[32m[20221208 15:09:23 @agent_ppo2.py:139][0m 2658304 total steps have happened
[32m[20221208 15:09:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1298 --------------------------#
[32m[20221208 15:09:24 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:09:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |           0.0744 |           5.0644 |        -416.2524 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |           0.0450 |           4.2791 |        -395.1610 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |           0.0032 |           4.0089 |        -407.8906 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0253 |           3.8360 |        -429.2923 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0364 |           3.7212 |        -441.1977 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0503 |           3.5900 |        -448.3702 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0561 |           3.5732 |        -464.3576 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0621 |           3.4526 |        -468.8365 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0668 |           3.3753 |        -476.4082 |
[32m[20221208 15:09:24 @agent_ppo2.py:179][0m |          -0.0692 |           3.3250 |        -481.3257 |
[32m[20221208 15:09:24 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:09:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.52
[32m[20221208 15:09:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.71
[32m[20221208 15:09:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.15
[32m[20221208 15:09:25 @agent_ppo2.py:137][0m Total time:      32.84 min
[32m[20221208 15:09:25 @agent_ppo2.py:139][0m 2660352 total steps have happened
[32m[20221208 15:09:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1299 --------------------------#
[32m[20221208 15:09:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:25 @agent_ppo2.py:179][0m |           0.0430 |           0.5555 |        -407.4521 |
[32m[20221208 15:09:25 @agent_ppo2.py:179][0m |           0.0371 |           0.5075 |        -246.5844 |
[32m[20221208 15:09:25 @agent_ppo2.py:179][0m |           0.0289 |           0.5033 |        -179.2934 |
[32m[20221208 15:09:25 @agent_ppo2.py:179][0m |           0.0585 |           0.4927 |        -136.2217 |
[32m[20221208 15:09:25 @agent_ppo2.py:179][0m |           0.0183 |           0.4901 |        -185.0659 |
[32m[20221208 15:09:26 @agent_ppo2.py:179][0m |           0.0090 |           0.4881 |        -193.2440 |
[32m[20221208 15:09:26 @agent_ppo2.py:179][0m |           0.0226 |           0.4868 |        -193.3513 |
[32m[20221208 15:09:26 @agent_ppo2.py:179][0m |           0.0128 |           0.4853 |        -195.0492 |
[32m[20221208 15:09:26 @agent_ppo2.py:179][0m |           0.0284 |           0.4846 |        -183.3243 |
[32m[20221208 15:09:26 @agent_ppo2.py:179][0m |           0.0574 |           0.4857 |        -143.6077 |
[32m[20221208 15:09:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:09:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 28.96
[32m[20221208 15:09:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.58
[32m[20221208 15:09:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.33
[32m[20221208 15:09:26 @agent_ppo2.py:137][0m Total time:      32.87 min
[32m[20221208 15:09:26 @agent_ppo2.py:139][0m 2662400 total steps have happened
[32m[20221208 15:09:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1300 --------------------------#
[32m[20221208 15:09:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |           0.0658 |           1.4247 |        -424.8097 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |           0.0333 |           1.0494 |        -393.7288 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |           0.0140 |           0.9257 |        -391.8846 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0038 |           0.8679 |        -386.4937 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0069 |           0.8393 |        -381.2840 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0065 |           0.7947 |        -355.2062 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0426 |           0.7731 |        -414.8955 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0386 |           0.7519 |        -412.8359 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0299 |           0.7388 |        -407.3250 |
[32m[20221208 15:09:27 @agent_ppo2.py:179][0m |          -0.0381 |           0.7238 |        -416.1023 |
[32m[20221208 15:09:27 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.36
[32m[20221208 15:09:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.39
[32m[20221208 15:09:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.69
[32m[20221208 15:09:28 @agent_ppo2.py:137][0m Total time:      32.89 min
[32m[20221208 15:09:28 @agent_ppo2.py:139][0m 2664448 total steps have happened
[32m[20221208 15:09:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1301 --------------------------#
[32m[20221208 15:09:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:28 @agent_ppo2.py:179][0m |           0.0655 |           0.6709 |        -408.9878 |
[32m[20221208 15:09:28 @agent_ppo2.py:179][0m |           0.0130 |           0.4952 |        -388.5678 |
[32m[20221208 15:09:28 @agent_ppo2.py:179][0m |          -0.0083 |           0.4703 |        -420.1919 |
[32m[20221208 15:09:28 @agent_ppo2.py:179][0m |           0.0144 |           0.4615 |        -405.1867 |
[32m[20221208 15:09:28 @agent_ppo2.py:179][0m |          -0.0067 |           0.4557 |        -418.5901 |
[32m[20221208 15:09:28 @agent_ppo2.py:179][0m |          -0.0116 |           0.4490 |        -412.9107 |
[32m[20221208 15:09:29 @agent_ppo2.py:179][0m |           0.0061 |           0.4428 |        -404.3413 |
[32m[20221208 15:09:29 @agent_ppo2.py:179][0m |          -0.0031 |           0.4387 |        -421.8056 |
[32m[20221208 15:09:29 @agent_ppo2.py:179][0m |          -0.0234 |           0.4374 |        -438.1791 |
[32m[20221208 15:09:29 @agent_ppo2.py:179][0m |          -0.0248 |           0.4348 |        -441.7714 |
[32m[20221208 15:09:29 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 30.91
[32m[20221208 15:09:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.53
[32m[20221208 15:09:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.01
[32m[20221208 15:09:29 @agent_ppo2.py:137][0m Total time:      32.91 min
[32m[20221208 15:09:29 @agent_ppo2.py:139][0m 2666496 total steps have happened
[32m[20221208 15:09:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1302 --------------------------#
[32m[20221208 15:09:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |           0.0536 |           1.0658 |        -402.3283 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |           0.0265 |           0.8257 |        -392.0000 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0004 |           0.7489 |        -391.8211 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0190 |           0.7134 |        -396.7683 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0129 |           0.6879 |        -392.6482 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0236 |           0.6751 |        -392.0444 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0439 |           0.6614 |        -427.7674 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0423 |           0.6525 |        -417.3417 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0403 |           0.6499 |        -419.8915 |
[32m[20221208 15:09:30 @agent_ppo2.py:179][0m |          -0.0432 |           0.6394 |        -425.3702 |
[32m[20221208 15:09:30 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.42
[32m[20221208 15:09:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.73
[32m[20221208 15:09:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.24
[32m[20221208 15:09:31 @agent_ppo2.py:137][0m Total time:      32.94 min
[32m[20221208 15:09:31 @agent_ppo2.py:139][0m 2668544 total steps have happened
[32m[20221208 15:09:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1303 --------------------------#
[32m[20221208 15:09:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |           0.0488 |           0.8016 |        -400.5002 |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |           0.0381 |           0.5538 |        -330.7847 |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |           0.0003 |           0.5265 |        -366.0930 |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |           0.0016 |           0.5220 |        -401.4178 |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |           0.0198 |           0.5197 |        -375.5178 |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |          -0.0002 |           0.5177 |        -389.4095 |
[32m[20221208 15:09:31 @agent_ppo2.py:179][0m |           0.0022 |           0.5127 |        -386.0017 |
[32m[20221208 15:09:32 @agent_ppo2.py:179][0m |           0.0171 |           0.5105 |        -352.8796 |
[32m[20221208 15:09:32 @agent_ppo2.py:179][0m |           0.0096 |           0.5091 |        -352.5367 |
[32m[20221208 15:09:32 @agent_ppo2.py:179][0m |          -0.0087 |           0.5089 |        -390.5170 |
[32m[20221208 15:09:32 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:09:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.27
[32m[20221208 15:09:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.17
[32m[20221208 15:09:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 33.67
[32m[20221208 15:09:32 @agent_ppo2.py:137][0m Total time:      32.96 min
[32m[20221208 15:09:32 @agent_ppo2.py:139][0m 2670592 total steps have happened
[32m[20221208 15:09:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1304 --------------------------#
[32m[20221208 15:09:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |           0.0792 |           0.7070 |        -307.6136 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |           0.0424 |           0.5067 |        -169.8907 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |           0.0072 |           0.4722 |        -170.7225 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0016 |           0.4614 |        -151.2280 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0144 |           0.4562 |        -160.0510 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0224 |           0.4437 |        -184.3860 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0130 |           0.4422 |        -164.7776 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0260 |           0.4356 |        -177.1914 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0261 |           0.4363 |        -183.3692 |
[32m[20221208 15:09:33 @agent_ppo2.py:179][0m |          -0.0346 |           0.4332 |        -194.5154 |
[32m[20221208 15:09:33 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:09:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.06
[32m[20221208 15:09:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.01
[32m[20221208 15:09:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.18
[32m[20221208 15:09:33 @agent_ppo2.py:137][0m Total time:      32.99 min
[32m[20221208 15:09:33 @agent_ppo2.py:139][0m 2672640 total steps have happened
[32m[20221208 15:09:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1305 --------------------------#
[32m[20221208 15:09:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |           0.0702 |           1.7410 |        -358.4042 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |           0.0224 |           1.4237 |        -353.3536 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |           0.0061 |           1.2928 |        -320.4162 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |           0.0061 |           1.2498 |        -218.5169 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |          -0.0308 |           1.1748 |        -218.5468 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |          -0.0398 |           1.1431 |        -246.1925 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |          -0.0509 |           1.1215 |        -258.2748 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |          -0.0528 |           1.1012 |        -272.8185 |
[32m[20221208 15:09:34 @agent_ppo2.py:179][0m |          -0.0578 |           1.0815 |        -273.1019 |
[32m[20221208 15:09:35 @agent_ppo2.py:179][0m |          -0.0625 |           1.0684 |        -291.0143 |
[32m[20221208 15:09:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:09:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.37
[32m[20221208 15:09:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.13
[32m[20221208 15:09:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.50
[32m[20221208 15:09:35 @agent_ppo2.py:137][0m Total time:      33.01 min
[32m[20221208 15:09:35 @agent_ppo2.py:139][0m 2674688 total steps have happened
[32m[20221208 15:09:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1306 --------------------------#
[32m[20221208 15:09:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1193 |           0.5946 |        -122.6553 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1451 |           0.5934 |          -7.1546 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1282 |           0.5860 |          -5.5958 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1237 |           0.5943 |          -9.1315 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1197 |           0.5882 |         -13.2176 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1170 |           0.5880 |         -13.7546 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1198 |           0.5879 |         -12.2844 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1181 |           0.5826 |         -12.4975 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1161 |           0.5827 |         -14.4710 |
[32m[20221208 15:09:36 @agent_ppo2.py:179][0m |           0.1148 |           0.5845 |         -15.7655 |
[32m[20221208 15:09:36 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:09:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.92
[32m[20221208 15:09:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.40
[32m[20221208 15:09:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.30
[32m[20221208 15:09:36 @agent_ppo2.py:137][0m Total time:      33.04 min
[32m[20221208 15:09:36 @agent_ppo2.py:139][0m 2676736 total steps have happened
[32m[20221208 15:09:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1307 --------------------------#
[32m[20221208 15:09:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |           0.0617 |           1.0762 |        -190.6675 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |           0.0050 |           0.9356 |         -66.9978 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0156 |           0.8887 |         -76.7194 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0352 |           0.8669 |         -88.8484 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0426 |           0.8537 |         -97.1755 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0494 |           0.8318 |        -103.7996 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0487 |           0.8169 |        -104.9519 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0512 |           0.8050 |        -108.4759 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0537 |           0.7992 |        -106.0078 |
[32m[20221208 15:09:37 @agent_ppo2.py:179][0m |          -0.0534 |           0.7939 |        -105.1433 |
[32m[20221208 15:09:37 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:09:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.25
[32m[20221208 15:09:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.90
[32m[20221208 15:09:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.93
[32m[20221208 15:09:38 @agent_ppo2.py:137][0m Total time:      33.06 min
[32m[20221208 15:09:38 @agent_ppo2.py:139][0m 2678784 total steps have happened
[32m[20221208 15:09:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1308 --------------------------#
[32m[20221208 15:09:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:38 @agent_ppo2.py:179][0m |           0.0899 |           1.3584 |        -244.4260 |
[32m[20221208 15:09:38 @agent_ppo2.py:179][0m |           0.0407 |           0.9989 |        -225.5121 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |           0.0008 |           0.8922 |        -245.7105 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0173 |           0.8383 |        -250.9129 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0177 |           0.8007 |        -246.0436 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0346 |           0.7828 |        -267.5048 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0444 |           0.7591 |        -275.6061 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0417 |           0.7484 |        -267.8242 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0467 |           0.7403 |        -278.4747 |
[32m[20221208 15:09:39 @agent_ppo2.py:179][0m |          -0.0464 |           0.7308 |        -279.4339 |
[32m[20221208 15:09:39 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.22
[32m[20221208 15:09:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.75
[32m[20221208 15:09:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.56
[32m[20221208 15:09:39 @agent_ppo2.py:137][0m Total time:      33.08 min
[32m[20221208 15:09:39 @agent_ppo2.py:139][0m 2680832 total steps have happened
[32m[20221208 15:09:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1309 --------------------------#
[32m[20221208 15:09:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |           0.0638 |           1.4903 |        -318.8671 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |           0.0294 |           1.3000 |        -287.7728 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |           0.0142 |           1.2215 |        -263.1119 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0202 |           1.1886 |        -327.1971 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0326 |           1.1655 |        -357.7160 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0330 |           1.1643 |        -361.7076 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0419 |           1.1354 |        -369.0613 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0013 |           1.1202 |        -265.3618 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0091 |           1.1346 |        -261.1317 |
[32m[20221208 15:09:40 @agent_ppo2.py:179][0m |          -0.0306 |           1.1355 |        -320.8805 |
[32m[20221208 15:09:40 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.35
[32m[20221208 15:09:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.13
[32m[20221208 15:09:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.56
[32m[20221208 15:09:41 @agent_ppo2.py:137][0m Total time:      33.11 min
[32m[20221208 15:09:41 @agent_ppo2.py:139][0m 2682880 total steps have happened
[32m[20221208 15:09:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1310 --------------------------#
[32m[20221208 15:09:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:41 @agent_ppo2.py:179][0m |           0.1383 |           0.8930 |        -350.3697 |
[32m[20221208 15:09:41 @agent_ppo2.py:179][0m |           0.1555 |           0.7134 |        -319.2785 |
[32m[20221208 15:09:41 @agent_ppo2.py:179][0m |           0.0222 |           0.6878 |        -310.3574 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |           0.0019 |           0.6793 |        -313.8384 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |          -0.0086 |           0.6684 |        -330.4395 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |          -0.0056 |           0.6661 |        -328.2272 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |           0.0104 |           0.6627 |        -314.9966 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |          -0.0119 |           0.6616 |        -332.4623 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |          -0.0039 |           0.6582 |        -322.7431 |
[32m[20221208 15:09:42 @agent_ppo2.py:179][0m |          -0.0124 |           0.6592 |        -336.8719 |
[32m[20221208 15:09:42 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:09:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.66
[32m[20221208 15:09:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.80
[32m[20221208 15:09:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.61
[32m[20221208 15:09:42 @agent_ppo2.py:137][0m Total time:      33.13 min
[32m[20221208 15:09:42 @agent_ppo2.py:139][0m 2684928 total steps have happened
[32m[20221208 15:09:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1311 --------------------------#
[32m[20221208 15:09:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |           0.1342 |           1.5416 |        -299.4211 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |           0.0378 |           1.1904 |        -217.8303 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0021 |           1.1326 |        -231.1825 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0252 |           1.1125 |        -250.0055 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0444 |           1.0718 |        -263.5803 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0546 |           1.0507 |        -276.7795 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0606 |           1.0249 |        -288.6485 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0703 |           1.0161 |        -284.9421 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0681 |           0.9948 |        -288.6099 |
[32m[20221208 15:09:43 @agent_ppo2.py:179][0m |          -0.0737 |           1.0029 |        -290.2591 |
[32m[20221208 15:09:43 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.77
[32m[20221208 15:09:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.50
[32m[20221208 15:09:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.75
[32m[20221208 15:09:44 @agent_ppo2.py:137][0m Total time:      33.16 min
[32m[20221208 15:09:44 @agent_ppo2.py:139][0m 2686976 total steps have happened
[32m[20221208 15:09:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1312 --------------------------#
[32m[20221208 15:09:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:44 @agent_ppo2.py:179][0m |           0.1061 |           1.0599 |        -350.3074 |
[32m[20221208 15:09:44 @agent_ppo2.py:179][0m |           0.1110 |           0.8870 |        -222.1620 |
[32m[20221208 15:09:44 @agent_ppo2.py:179][0m |           0.1320 |           0.8142 |        -231.1395 |
[32m[20221208 15:09:44 @agent_ppo2.py:179][0m |           0.0834 |           0.7739 |        -266.2171 |
[32m[20221208 15:09:45 @agent_ppo2.py:179][0m |           0.0413 |           0.7506 |        -344.2254 |
[32m[20221208 15:09:45 @agent_ppo2.py:179][0m |           0.0008 |           0.7345 |        -395.7912 |
[32m[20221208 15:09:45 @agent_ppo2.py:179][0m |          -0.0098 |           0.7110 |        -430.5678 |
[32m[20221208 15:09:45 @agent_ppo2.py:179][0m |          -0.0281 |           0.6986 |        -446.8167 |
[32m[20221208 15:09:45 @agent_ppo2.py:179][0m |           0.0152 |           0.6904 |        -389.5798 |
[32m[20221208 15:09:45 @agent_ppo2.py:179][0m |           0.0099 |           0.6829 |        -370.8629 |
[32m[20221208 15:09:45 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:09:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.32
[32m[20221208 15:09:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.46
[32m[20221208 15:09:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.81
[32m[20221208 15:09:45 @agent_ppo2.py:137][0m Total time:      33.18 min
[32m[20221208 15:09:45 @agent_ppo2.py:139][0m 2689024 total steps have happened
[32m[20221208 15:09:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1313 --------------------------#
[32m[20221208 15:09:46 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:09:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |           0.0854 |           2.2946 |        -258.7822 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |           0.0519 |           1.8397 |        -139.3860 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |           0.0148 |           1.7600 |        -144.9896 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0078 |           1.7065 |        -162.7949 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0219 |           1.7240 |        -178.2629 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0326 |           1.6707 |        -183.2085 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0330 |           1.6538 |        -192.8869 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0418 |           1.6230 |        -198.7242 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0461 |           1.6223 |        -207.8981 |
[32m[20221208 15:09:46 @agent_ppo2.py:179][0m |          -0.0491 |           1.5944 |        -208.1617 |
[32m[20221208 15:09:46 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:09:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.14
[32m[20221208 15:09:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.12
[32m[20221208 15:09:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.93
[32m[20221208 15:09:47 @agent_ppo2.py:137][0m Total time:      33.21 min
[32m[20221208 15:09:47 @agent_ppo2.py:139][0m 2691072 total steps have happened
[32m[20221208 15:09:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1314 --------------------------#
[32m[20221208 15:09:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:47 @agent_ppo2.py:179][0m |           0.0300 |           0.7754 |        -393.6228 |
[32m[20221208 15:09:47 @agent_ppo2.py:179][0m |           0.0154 |           0.6862 |        -359.9038 |
[32m[20221208 15:09:47 @agent_ppo2.py:179][0m |           0.1153 |           0.6773 |        -271.5239 |
[32m[20221208 15:09:47 @agent_ppo2.py:179][0m |           0.0833 |           0.6598 |        -254.8906 |
[32m[20221208 15:09:47 @agent_ppo2.py:179][0m |          -0.0123 |           0.6583 |        -365.9140 |
[32m[20221208 15:09:47 @agent_ppo2.py:179][0m |          -0.0063 |           0.6480 |        -367.6526 |
[32m[20221208 15:09:48 @agent_ppo2.py:179][0m |          -0.0175 |           0.6417 |        -369.8631 |
[32m[20221208 15:09:48 @agent_ppo2.py:179][0m |          -0.0036 |           0.6418 |        -366.0662 |
[32m[20221208 15:09:48 @agent_ppo2.py:179][0m |          -0.0204 |           0.6394 |        -379.8990 |
[32m[20221208 15:09:48 @agent_ppo2.py:179][0m |          -0.0157 |           0.6421 |        -379.2285 |
[32m[20221208 15:09:48 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:09:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.79
[32m[20221208 15:09:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.43
[32m[20221208 15:09:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.65
[32m[20221208 15:09:48 @agent_ppo2.py:137][0m Total time:      33.23 min
[32m[20221208 15:09:48 @agent_ppo2.py:139][0m 2693120 total steps have happened
[32m[20221208 15:09:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1315 --------------------------#
[32m[20221208 15:09:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1979 |           0.7305 |        -388.0818 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1906 |           0.6849 |        -121.0306 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1594 |           0.6749 |         -53.7175 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1234 |           0.6728 |         -91.3928 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1503 |           0.6748 |        -152.0837 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1097 |           0.6742 |        -352.0318 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1103 |           0.6733 |        -260.5046 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1600 |           0.6685 |        -118.3522 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.1164 |           0.6675 |        -170.7373 |
[32m[20221208 15:09:49 @agent_ppo2.py:179][0m |           0.0415 |           0.6645 |        -368.3560 |
[32m[20221208 15:09:49 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:09:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.27
[32m[20221208 15:09:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.59
[32m[20221208 15:09:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.96
[32m[20221208 15:09:50 @agent_ppo2.py:137][0m Total time:      33.26 min
[32m[20221208 15:09:50 @agent_ppo2.py:139][0m 2695168 total steps have happened
[32m[20221208 15:09:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1316 --------------------------#
[32m[20221208 15:09:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |           0.0854 |           1.0903 |        -308.1441 |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |           0.0094 |           0.8648 |        -226.7434 |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |          -0.0206 |           0.8356 |        -245.3700 |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |          -0.0420 |           0.7953 |        -260.8148 |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |          -0.0546 |           0.7819 |        -267.1599 |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |          -0.0530 |           0.7641 |        -260.4092 |
[32m[20221208 15:09:50 @agent_ppo2.py:179][0m |          -0.0641 |           0.7540 |        -269.4073 |
[32m[20221208 15:09:51 @agent_ppo2.py:179][0m |          -0.0640 |           0.7484 |        -277.9201 |
[32m[20221208 15:09:51 @agent_ppo2.py:179][0m |          -0.0610 |           0.7425 |        -275.5537 |
[32m[20221208 15:09:51 @agent_ppo2.py:179][0m |          -0.0610 |           0.7186 |        -276.9095 |
[32m[20221208 15:09:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:09:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.97
[32m[20221208 15:09:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.52
[32m[20221208 15:09:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.00
[32m[20221208 15:09:51 @agent_ppo2.py:137][0m Total time:      33.28 min
[32m[20221208 15:09:51 @agent_ppo2.py:139][0m 2697216 total steps have happened
[32m[20221208 15:09:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1317 --------------------------#
[32m[20221208 15:09:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.1206 |           0.6888 |        -332.5721 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0882 |           0.6704 |        -272.3800 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.1221 |           0.6660 |        -155.4128 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0654 |           0.6651 |        -311.5869 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0809 |           0.6580 |        -353.1727 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0614 |           0.6576 |        -358.8875 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0162 |           0.6602 |        -394.7971 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0459 |           0.6572 |        -343.2902 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0200 |           0.6583 |        -379.4552 |
[32m[20221208 15:09:52 @agent_ppo2.py:179][0m |           0.0274 |           0.6581 |        -367.6495 |
[32m[20221208 15:09:52 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:09:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.04
[32m[20221208 15:09:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.12
[32m[20221208 15:09:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.39
[32m[20221208 15:09:53 @agent_ppo2.py:137][0m Total time:      33.30 min
[32m[20221208 15:09:53 @agent_ppo2.py:139][0m 2699264 total steps have happened
[32m[20221208 15:09:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1318 --------------------------#
[32m[20221208 15:09:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |           0.0444 |           0.9265 |        -394.3954 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |           0.0179 |           0.8351 |        -373.3805 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |           0.0221 |           0.7880 |        -332.8973 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |          -0.0053 |           0.7673 |        -370.3757 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |          -0.0121 |           0.7475 |        -375.9092 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |          -0.0182 |           0.7414 |        -382.5350 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |          -0.0249 |           0.7325 |        -396.8125 |
[32m[20221208 15:09:53 @agent_ppo2.py:179][0m |          -0.0195 |           0.7290 |        -383.4365 |
[32m[20221208 15:09:54 @agent_ppo2.py:179][0m |          -0.0220 |           0.7201 |        -385.7441 |
[32m[20221208 15:09:54 @agent_ppo2.py:179][0m |          -0.0241 |           0.7122 |        -373.5356 |
[32m[20221208 15:09:54 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.77
[32m[20221208 15:09:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.17
[32m[20221208 15:09:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.69
[32m[20221208 15:09:54 @agent_ppo2.py:137][0m Total time:      33.33 min
[32m[20221208 15:09:54 @agent_ppo2.py:139][0m 2701312 total steps have happened
[32m[20221208 15:09:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1319 --------------------------#
[32m[20221208 15:09:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |           0.0976 |           1.2483 |        -277.9182 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |           0.0126 |           1.1672 |        -232.3792 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0049 |           1.1598 |        -244.9485 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0259 |           1.1571 |        -258.4738 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0305 |           1.1382 |        -261.9972 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0182 |           1.1218 |        -253.4878 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |           0.0272 |           1.1265 |        -205.0880 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0185 |           1.1140 |        -245.7407 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0237 |           1.1093 |        -248.5326 |
[32m[20221208 15:09:55 @agent_ppo2.py:179][0m |          -0.0362 |           1.1143 |        -268.0135 |
[32m[20221208 15:09:55 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.48
[32m[20221208 15:09:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.89
[32m[20221208 15:09:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.74
[32m[20221208 15:09:55 @agent_ppo2.py:137][0m Total time:      33.35 min
[32m[20221208 15:09:55 @agent_ppo2.py:139][0m 2703360 total steps have happened
[32m[20221208 15:09:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1320 --------------------------#
[32m[20221208 15:09:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |           0.0741 |           1.7485 |        -267.9323 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |           0.0051 |           1.4617 |        -229.3968 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0276 |           1.4180 |        -240.3806 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0342 |           1.3901 |        -261.5392 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0497 |           1.3431 |        -263.7483 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0582 |           1.3192 |        -269.2337 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0610 |           1.3105 |        -273.2820 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0642 |           1.3044 |        -279.7990 |
[32m[20221208 15:09:56 @agent_ppo2.py:179][0m |          -0.0707 |           1.2925 |        -289.1429 |
[32m[20221208 15:09:57 @agent_ppo2.py:179][0m |          -0.0701 |           1.2655 |        -285.3789 |
[32m[20221208 15:09:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:09:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.58
[32m[20221208 15:09:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.65
[32m[20221208 15:09:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.48
[32m[20221208 15:09:57 @agent_ppo2.py:137][0m Total time:      33.38 min
[32m[20221208 15:09:57 @agent_ppo2.py:139][0m 2705408 total steps have happened
[32m[20221208 15:09:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1321 --------------------------#
[32m[20221208 15:09:57 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:09:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:57 @agent_ppo2.py:179][0m |           0.0674 |           0.9721 |        -335.4053 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |           0.0740 |           0.8525 |        -282.8036 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |           0.0240 |           0.8119 |        -304.0011 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |           0.0026 |           0.8005 |        -268.8954 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |          -0.0131 |           0.7794 |        -206.9235 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |          -0.0232 |           0.7772 |        -186.9778 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |          -0.0343 |           0.7676 |        -178.4741 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |          -0.0464 |           0.7581 |        -221.0306 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |          -0.0359 |           0.7523 |        -189.3891 |
[32m[20221208 15:09:58 @agent_ppo2.py:179][0m |          -0.0457 |           0.7492 |        -210.2859 |
[32m[20221208 15:09:58 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:09:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.07
[32m[20221208 15:09:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.32
[32m[20221208 15:09:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.56
[32m[20221208 15:09:58 @agent_ppo2.py:137][0m Total time:      33.40 min
[32m[20221208 15:09:58 @agent_ppo2.py:139][0m 2707456 total steps have happened
[32m[20221208 15:09:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1322 --------------------------#
[32m[20221208 15:09:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:09:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |           0.1194 |           1.5148 |        -318.7356 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |           0.0326 |           1.2618 |        -243.4909 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0030 |           1.2016 |        -256.7020 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0144 |           1.1651 |        -270.0337 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0218 |           1.1609 |        -278.7313 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0248 |           1.1409 |        -285.1887 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0353 |           1.1144 |        -290.1844 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0396 |           1.1003 |        -290.1652 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0320 |           1.0965 |        -295.1698 |
[32m[20221208 15:09:59 @agent_ppo2.py:179][0m |          -0.0414 |           1.0851 |        -301.1922 |
[32m[20221208 15:09:59 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.51
[32m[20221208 15:10:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.40
[32m[20221208 15:10:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.84
[32m[20221208 15:10:00 @agent_ppo2.py:137][0m Total time:      33.43 min
[32m[20221208 15:10:00 @agent_ppo2.py:139][0m 2709504 total steps have happened
[32m[20221208 15:10:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1323 --------------------------#
[32m[20221208 15:10:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:00 @agent_ppo2.py:179][0m |           0.0804 |           1.3477 |        -346.8628 |
[32m[20221208 15:10:00 @agent_ppo2.py:179][0m |           0.0241 |           1.1228 |        -305.3276 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |           0.0080 |           1.0087 |        -298.0166 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0155 |           0.9557 |        -295.2727 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0287 |           0.9197 |        -325.9207 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0232 |           0.8903 |        -308.1286 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0315 |           0.8709 |        -314.1566 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0292 |           0.8583 |        -326.6681 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0399 |           0.8254 |        -302.5419 |
[32m[20221208 15:10:01 @agent_ppo2.py:179][0m |          -0.0518 |           0.8205 |        -341.5647 |
[32m[20221208 15:10:01 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.08
[32m[20221208 15:10:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.10
[32m[20221208 15:10:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.93
[32m[20221208 15:10:01 @agent_ppo2.py:137][0m Total time:      33.45 min
[32m[20221208 15:10:01 @agent_ppo2.py:139][0m 2711552 total steps have happened
[32m[20221208 15:10:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1324 --------------------------#
[32m[20221208 15:10:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:10:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |           0.0698 |           1.1322 |        -329.0401 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |           0.0413 |           0.9453 |        -231.5720 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |           0.0234 |           0.8788 |        -195.3466 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |           0.0530 |           0.8291 |        -128.4883 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |           0.0021 |           0.8186 |         -88.7309 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |          -0.0071 |           0.8072 |         -91.6523 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |          -0.0093 |           0.7881 |         -97.7014 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |          -0.0122 |           0.7751 |         -98.9993 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |          -0.0172 |           0.7676 |        -105.4694 |
[32m[20221208 15:10:02 @agent_ppo2.py:179][0m |          -0.0172 |           0.7592 |        -108.0686 |
[32m[20221208 15:10:02 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.26
[32m[20221208 15:10:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.50
[32m[20221208 15:10:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.76
[32m[20221208 15:10:03 @agent_ppo2.py:137][0m Total time:      33.48 min
[32m[20221208 15:10:03 @agent_ppo2.py:139][0m 2713600 total steps have happened
[32m[20221208 15:10:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1325 --------------------------#
[32m[20221208 15:10:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:03 @agent_ppo2.py:179][0m |           0.0737 |           1.6282 |        -338.1105 |
[32m[20221208 15:10:03 @agent_ppo2.py:179][0m |           0.0002 |           1.3039 |        -323.2325 |
[32m[20221208 15:10:03 @agent_ppo2.py:179][0m |          -0.0335 |           1.2149 |        -338.6326 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0443 |           1.1573 |        -340.6331 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0522 |           1.0922 |        -354.8693 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0574 |           1.0567 |        -357.2954 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0635 |           1.0261 |        -358.9166 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0661 |           1.0110 |        -363.7813 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0726 |           1.0180 |        -370.3786 |
[32m[20221208 15:10:04 @agent_ppo2.py:179][0m |          -0.0745 |           1.0001 |        -370.8739 |
[32m[20221208 15:10:04 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.94
[32m[20221208 15:10:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.15
[32m[20221208 15:10:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.59
[32m[20221208 15:10:04 @agent_ppo2.py:137][0m Total time:      33.50 min
[32m[20221208 15:10:04 @agent_ppo2.py:139][0m 2715648 total steps have happened
[32m[20221208 15:10:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1326 --------------------------#
[32m[20221208 15:10:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:10:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |           0.1463 |           2.1465 |        -350.9068 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |           0.0653 |           1.7791 |        -295.8337 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |           0.0024 |           1.6821 |        -353.9570 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0208 |           1.5921 |        -365.8109 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0339 |           1.5250 |        -379.3523 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0447 |           1.4659 |        -391.3362 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0489 |           1.4436 |        -395.7858 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0510 |           1.3834 |        -403.8864 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0495 |           1.3701 |        -405.6096 |
[32m[20221208 15:10:05 @agent_ppo2.py:179][0m |          -0.0568 |           1.3318 |        -414.1780 |
[32m[20221208 15:10:05 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.52
[32m[20221208 15:10:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.85
[32m[20221208 15:10:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.60
[32m[20221208 15:10:06 @agent_ppo2.py:137][0m Total time:      33.52 min
[32m[20221208 15:10:06 @agent_ppo2.py:139][0m 2717696 total steps have happened
[32m[20221208 15:10:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1327 --------------------------#
[32m[20221208 15:10:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:06 @agent_ppo2.py:179][0m |           0.0787 |           1.5405 |        -359.3710 |
[32m[20221208 15:10:06 @agent_ppo2.py:179][0m |           0.0611 |           1.1791 |        -289.5098 |
[32m[20221208 15:10:06 @agent_ppo2.py:179][0m |           0.0268 |           1.0218 |        -276.7782 |
[32m[20221208 15:10:06 @agent_ppo2.py:179][0m |          -0.0063 |           0.9527 |        -282.6360 |
[32m[20221208 15:10:07 @agent_ppo2.py:179][0m |          -0.0256 |           0.9013 |        -290.3692 |
[32m[20221208 15:10:07 @agent_ppo2.py:179][0m |          -0.0312 |           0.8624 |        -291.3172 |
[32m[20221208 15:10:07 @agent_ppo2.py:179][0m |          -0.0340 |           0.8333 |        -288.3123 |
[32m[20221208 15:10:07 @agent_ppo2.py:179][0m |          -0.0326 |           0.7984 |        -296.1449 |
[32m[20221208 15:10:07 @agent_ppo2.py:179][0m |          -0.0390 |           0.7762 |        -294.5523 |
[32m[20221208 15:10:07 @agent_ppo2.py:179][0m |          -0.0404 |           0.7559 |        -291.3221 |
[32m[20221208 15:10:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.10
[32m[20221208 15:10:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.93
[32m[20221208 15:10:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.68
[32m[20221208 15:10:07 @agent_ppo2.py:137][0m Total time:      33.55 min
[32m[20221208 15:10:07 @agent_ppo2.py:139][0m 2719744 total steps have happened
[32m[20221208 15:10:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1328 --------------------------#
[32m[20221208 15:10:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.1397 |           0.7513 |        -353.5378 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.0399 |           0.5537 |        -321.1913 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.0858 |           0.5026 |        -239.3684 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.0504 |           0.4850 |        -284.4044 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.0558 |           0.4774 |        -298.5914 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.0048 |           0.4678 |        -376.8645 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |          -0.0173 |           0.4661 |        -392.0395 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |          -0.0128 |           0.4641 |        -392.8293 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |           0.0074 |           0.4580 |        -360.3815 |
[32m[20221208 15:10:08 @agent_ppo2.py:179][0m |          -0.0062 |           0.4595 |        -369.2973 |
[32m[20221208 15:10:08 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.93
[32m[20221208 15:10:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.30
[32m[20221208 15:10:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.13
[32m[20221208 15:10:09 @agent_ppo2.py:137][0m Total time:      33.57 min
[32m[20221208 15:10:09 @agent_ppo2.py:139][0m 2721792 total steps have happened
[32m[20221208 15:10:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1329 --------------------------#
[32m[20221208 15:10:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:10:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:09 @agent_ppo2.py:179][0m |           0.0701 |           1.3263 |        -302.7989 |
[32m[20221208 15:10:09 @agent_ppo2.py:179][0m |          -0.0096 |           1.1581 |        -263.4544 |
[32m[20221208 15:10:09 @agent_ppo2.py:179][0m |          -0.0287 |           1.1109 |        -266.9495 |
[32m[20221208 15:10:09 @agent_ppo2.py:179][0m |          -0.0342 |           1.0848 |        -275.5500 |
[32m[20221208 15:10:09 @agent_ppo2.py:179][0m |          -0.0453 |           1.0459 |        -280.5883 |
[32m[20221208 15:10:10 @agent_ppo2.py:179][0m |          -0.0448 |           1.0223 |        -279.6826 |
[32m[20221208 15:10:10 @agent_ppo2.py:179][0m |          -0.0502 |           1.0056 |        -286.8934 |
[32m[20221208 15:10:10 @agent_ppo2.py:179][0m |          -0.0544 |           0.9983 |        -291.2257 |
[32m[20221208 15:10:10 @agent_ppo2.py:179][0m |          -0.0543 |           0.9820 |        -296.9335 |
[32m[20221208 15:10:10 @agent_ppo2.py:179][0m |          -0.0557 |           0.9696 |        -298.0901 |
[32m[20221208 15:10:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.72
[32m[20221208 15:10:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.64
[32m[20221208 15:10:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.36
[32m[20221208 15:10:10 @agent_ppo2.py:137][0m Total time:      33.60 min
[32m[20221208 15:10:10 @agent_ppo2.py:139][0m 2723840 total steps have happened
[32m[20221208 15:10:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1330 --------------------------#
[32m[20221208 15:10:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |           0.0653 |           1.7314 |        -369.5649 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |           0.0187 |           1.4097 |        -341.2350 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0230 |           1.3027 |        -359.1611 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0418 |           1.2144 |        -373.8325 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0443 |           1.1890 |        -372.0193 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0566 |           1.1392 |        -381.1075 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0620 |           1.1071 |        -385.7326 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0694 |           1.0797 |        -390.3069 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0702 |           1.0660 |        -401.1226 |
[32m[20221208 15:10:11 @agent_ppo2.py:179][0m |          -0.0736 |           1.0495 |        -388.5398 |
[32m[20221208 15:10:11 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.07
[32m[20221208 15:10:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.07
[32m[20221208 15:10:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.13
[32m[20221208 15:10:12 @agent_ppo2.py:137][0m Total time:      33.62 min
[32m[20221208 15:10:12 @agent_ppo2.py:139][0m 2725888 total steps have happened
[32m[20221208 15:10:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1331 --------------------------#
[32m[20221208 15:10:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:12 @agent_ppo2.py:179][0m |           0.0807 |           1.3287 |        -387.5498 |
[32m[20221208 15:10:12 @agent_ppo2.py:179][0m |           0.0453 |           1.0372 |        -323.9618 |
[32m[20221208 15:10:12 @agent_ppo2.py:179][0m |          -0.0178 |           0.9488 |        -349.9809 |
[32m[20221208 15:10:12 @agent_ppo2.py:179][0m |          -0.0440 |           0.9159 |        -363.7709 |
[32m[20221208 15:10:12 @agent_ppo2.py:179][0m |          -0.0617 |           0.8684 |        -358.5795 |
[32m[20221208 15:10:13 @agent_ppo2.py:179][0m |          -0.0633 |           0.8489 |        -347.9693 |
[32m[20221208 15:10:13 @agent_ppo2.py:179][0m |          -0.0718 |           0.8248 |        -358.6138 |
[32m[20221208 15:10:13 @agent_ppo2.py:179][0m |          -0.0668 |           0.8162 |        -318.4421 |
[32m[20221208 15:10:13 @agent_ppo2.py:179][0m |          -0.0779 |           0.7917 |        -310.8994 |
[32m[20221208 15:10:13 @agent_ppo2.py:179][0m |          -0.0831 |           0.7823 |        -321.7636 |
[32m[20221208 15:10:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.22
[32m[20221208 15:10:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.54
[32m[20221208 15:10:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.53
[32m[20221208 15:10:13 @agent_ppo2.py:137][0m Total time:      33.65 min
[32m[20221208 15:10:13 @agent_ppo2.py:139][0m 2727936 total steps have happened
[32m[20221208 15:10:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1332 --------------------------#
[32m[20221208 15:10:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |           0.0341 |           2.1712 |        -335.9789 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0085 |           1.8307 |        -319.5087 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0389 |           1.7598 |        -327.9126 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0564 |           1.7751 |        -345.4186 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0660 |           1.6933 |        -349.5773 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0743 |           1.6511 |        -350.8471 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0715 |           1.6771 |        -342.9052 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0747 |           1.6219 |        -356.5583 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0734 |           1.5793 |        -357.6380 |
[32m[20221208 15:10:14 @agent_ppo2.py:179][0m |          -0.0722 |           1.5609 |        -346.1026 |
[32m[20221208 15:10:14 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.92
[32m[20221208 15:10:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.38
[32m[20221208 15:10:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.57
[32m[20221208 15:10:15 @agent_ppo2.py:137][0m Total time:      33.67 min
[32m[20221208 15:10:15 @agent_ppo2.py:139][0m 2729984 total steps have happened
[32m[20221208 15:10:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1333 --------------------------#
[32m[20221208 15:10:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:15 @agent_ppo2.py:179][0m |           0.0488 |           2.4954 |        -432.4386 |
[32m[20221208 15:10:15 @agent_ppo2.py:179][0m |           0.0496 |           2.2153 |        -388.2808 |
[32m[20221208 15:10:15 @agent_ppo2.py:179][0m |          -0.0174 |           2.1246 |        -402.2528 |
[32m[20221208 15:10:15 @agent_ppo2.py:179][0m |          -0.0350 |           1.9633 |        -417.8433 |
[32m[20221208 15:10:15 @agent_ppo2.py:179][0m |          -0.0466 |           1.8730 |        -433.0595 |
[32m[20221208 15:10:15 @agent_ppo2.py:179][0m |          -0.0546 |           1.8134 |        -438.0787 |
[32m[20221208 15:10:16 @agent_ppo2.py:179][0m |          -0.0621 |           1.7660 |        -424.8059 |
[32m[20221208 15:10:16 @agent_ppo2.py:179][0m |          -0.0668 |           1.7187 |        -443.2356 |
[32m[20221208 15:10:16 @agent_ppo2.py:179][0m |          -0.0708 |           1.6753 |        -448.0024 |
[32m[20221208 15:10:16 @agent_ppo2.py:179][0m |          -0.0727 |           1.6200 |        -444.1939 |
[32m[20221208 15:10:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.81
[32m[20221208 15:10:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.68
[32m[20221208 15:10:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.19
[32m[20221208 15:10:16 @agent_ppo2.py:137][0m Total time:      33.70 min
[32m[20221208 15:10:16 @agent_ppo2.py:139][0m 2732032 total steps have happened
[32m[20221208 15:10:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1334 --------------------------#
[32m[20221208 15:10:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |           0.1371 |           0.9622 |        -356.6927 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |           0.1258 |           0.7462 |        -286.6647 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |           0.0271 |           0.6700 |        -351.4344 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |           0.0019 |           0.6322 |        -376.5116 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |          -0.0024 |           0.6155 |        -386.1529 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |           0.0066 |           0.5926 |        -389.0275 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |          -0.0344 |           0.5797 |        -410.9107 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |          -0.0274 |           0.5712 |        -419.3378 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |           0.0063 |           0.5612 |        -366.1002 |
[32m[20221208 15:10:17 @agent_ppo2.py:179][0m |          -0.0008 |           0.5557 |        -382.9414 |
[32m[20221208 15:10:17 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.73
[32m[20221208 15:10:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.38
[32m[20221208 15:10:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.74
[32m[20221208 15:10:18 @agent_ppo2.py:137][0m Total time:      33.72 min
[32m[20221208 15:10:18 @agent_ppo2.py:139][0m 2734080 total steps have happened
[32m[20221208 15:10:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1335 --------------------------#
[32m[20221208 15:10:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:10:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |           0.0842 |           1.0512 |        -451.1043 |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |           0.0154 |           0.8859 |        -440.0613 |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |          -0.0204 |           0.8218 |        -459.7072 |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |          -0.0185 |           0.7797 |        -438.2762 |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |          -0.0399 |           0.7559 |        -485.6807 |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |          -0.0242 |           0.7263 |        -477.8876 |
[32m[20221208 15:10:18 @agent_ppo2.py:179][0m |          -0.0306 |           0.7043 |        -470.3402 |
[32m[20221208 15:10:19 @agent_ppo2.py:179][0m |          -0.0479 |           0.6968 |        -495.1537 |
[32m[20221208 15:10:19 @agent_ppo2.py:179][0m |          -0.0296 |           0.6818 |        -473.4736 |
[32m[20221208 15:10:19 @agent_ppo2.py:179][0m |          -0.0514 |           0.6689 |        -499.1404 |
[32m[20221208 15:10:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.97
[32m[20221208 15:10:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.57
[32m[20221208 15:10:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.68
[32m[20221208 15:10:19 @agent_ppo2.py:137][0m Total time:      33.75 min
[32m[20221208 15:10:19 @agent_ppo2.py:139][0m 2736128 total steps have happened
[32m[20221208 15:10:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1336 --------------------------#
[32m[20221208 15:10:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |           0.0906 |           0.7504 |        -434.0079 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |           0.0758 |           0.6098 |        -294.0834 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |           0.0037 |           0.5728 |        -416.2772 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0155 |           0.5492 |        -440.3020 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0284 |           0.5345 |        -448.6437 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0282 |           0.5230 |        -452.5074 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0161 |           0.5201 |        -426.1630 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0246 |           0.5085 |        -432.8914 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0452 |           0.5038 |        -463.4732 |
[32m[20221208 15:10:20 @agent_ppo2.py:179][0m |          -0.0422 |           0.5001 |        -468.6048 |
[32m[20221208 15:10:20 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.77
[32m[20221208 15:10:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.58
[32m[20221208 15:10:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.87
[32m[20221208 15:10:21 @agent_ppo2.py:137][0m Total time:      33.77 min
[32m[20221208 15:10:21 @agent_ppo2.py:139][0m 2738176 total steps have happened
[32m[20221208 15:10:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1337 --------------------------#
[32m[20221208 15:10:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |           0.1113 |           0.6693 |        -395.2426 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |           0.0437 |           0.6057 |        -370.1801 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |           0.0124 |           0.5840 |        -395.1180 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |          -0.0111 |           0.5732 |        -421.7198 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |          -0.0200 |           0.5641 |        -419.7621 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |          -0.0178 |           0.5617 |        -417.5158 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |          -0.0181 |           0.5570 |        -424.6445 |
[32m[20221208 15:10:21 @agent_ppo2.py:179][0m |          -0.0161 |           0.5531 |        -419.7342 |
[32m[20221208 15:10:22 @agent_ppo2.py:179][0m |          -0.0267 |           0.5500 |        -428.1594 |
[32m[20221208 15:10:22 @agent_ppo2.py:179][0m |          -0.0337 |           0.5469 |        -434.0416 |
[32m[20221208 15:10:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.90
[32m[20221208 15:10:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.52
[32m[20221208 15:10:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.89
[32m[20221208 15:10:22 @agent_ppo2.py:137][0m Total time:      33.80 min
[32m[20221208 15:10:22 @agent_ppo2.py:139][0m 2740224 total steps have happened
[32m[20221208 15:10:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1338 --------------------------#
[32m[20221208 15:10:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |           0.0683 |           1.1726 |        -491.4751 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |           0.0638 |           0.9658 |        -388.5177 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |           0.0090 |           0.8838 |        -428.7323 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0176 |           0.8371 |        -449.1478 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0415 |           0.8104 |        -474.5100 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0490 |           0.7830 |        -481.6837 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0577 |           0.7603 |        -487.2325 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0612 |           0.7526 |        -490.8801 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0591 |           0.7292 |        -493.7884 |
[32m[20221208 15:10:23 @agent_ppo2.py:179][0m |          -0.0617 |           0.7247 |        -497.1009 |
[32m[20221208 15:10:23 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.82
[32m[20221208 15:10:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.46
[32m[20221208 15:10:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.08
[32m[20221208 15:10:23 @agent_ppo2.py:137][0m Total time:      33.82 min
[32m[20221208 15:10:23 @agent_ppo2.py:139][0m 2742272 total steps have happened
[32m[20221208 15:10:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1339 --------------------------#
[32m[20221208 15:10:24 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:10:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |           0.0816 |           0.9205 |        -425.8348 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |           0.0567 |           0.7494 |        -338.3020 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |          -0.0090 |           0.7048 |        -353.4872 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |          -0.0206 |           0.6805 |        -362.2975 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |          -0.0529 |           0.6551 |        -381.8142 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |          -0.0650 |           0.6335 |        -397.8066 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |          -0.0573 |           0.6190 |        -388.8650 |
[32m[20221208 15:10:24 @agent_ppo2.py:179][0m |          -0.0649 |           0.6062 |        -398.2162 |
[32m[20221208 15:10:25 @agent_ppo2.py:179][0m |          -0.0559 |           0.5952 |        -393.3603 |
[32m[20221208 15:10:25 @agent_ppo2.py:179][0m |          -0.0703 |           0.5863 |        -407.4169 |
[32m[20221208 15:10:25 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.94
[32m[20221208 15:10:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.23
[32m[20221208 15:10:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.44
[32m[20221208 15:10:25 @agent_ppo2.py:137][0m Total time:      33.85 min
[32m[20221208 15:10:25 @agent_ppo2.py:139][0m 2744320 total steps have happened
[32m[20221208 15:10:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1340 --------------------------#
[32m[20221208 15:10:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |           0.0568 |           1.0317 |        -440.7853 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |           0.0155 |           0.8690 |        -394.3611 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0195 |           0.8157 |        -437.2753 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0308 |           0.7737 |        -453.6928 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0427 |           0.7625 |        -460.4382 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0455 |           0.7385 |        -457.6647 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0454 |           0.7292 |        -448.8866 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0478 |           0.7203 |        -446.5883 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0377 |           0.6961 |        -398.5816 |
[32m[20221208 15:10:26 @agent_ppo2.py:179][0m |          -0.0557 |           0.6882 |        -472.9183 |
[32m[20221208 15:10:26 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.17
[32m[20221208 15:10:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.36
[32m[20221208 15:10:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.62
[32m[20221208 15:10:26 @agent_ppo2.py:137][0m Total time:      33.87 min
[32m[20221208 15:10:26 @agent_ppo2.py:139][0m 2746368 total steps have happened
[32m[20221208 15:10:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1341 --------------------------#
[32m[20221208 15:10:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |           0.0618 |           0.5882 |        -406.2603 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |           0.1690 |           0.5212 |        -338.9533 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |           0.0343 |           0.5059 |        -328.0201 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |          -0.0006 |           0.4994 |        -339.8496 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |          -0.0163 |           0.4929 |        -354.6800 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |          -0.0182 |           0.4886 |        -353.3054 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |          -0.0350 |           0.4844 |        -366.7642 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |          -0.0265 |           0.4802 |        -361.8355 |
[32m[20221208 15:10:27 @agent_ppo2.py:179][0m |          -0.0368 |           0.4819 |        -373.1890 |
[32m[20221208 15:10:28 @agent_ppo2.py:179][0m |          -0.0418 |           0.4780 |        -387.4796 |
[32m[20221208 15:10:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.69
[32m[20221208 15:10:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 56.59
[32m[20221208 15:10:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.65
[32m[20221208 15:10:28 @agent_ppo2.py:137][0m Total time:      33.89 min
[32m[20221208 15:10:28 @agent_ppo2.py:139][0m 2748416 total steps have happened
[32m[20221208 15:10:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1342 --------------------------#
[32m[20221208 15:10:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:28 @agent_ppo2.py:179][0m |           0.1263 |           0.6332 |        -328.4001 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |           0.1642 |           0.5646 |        -177.1867 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |           0.0500 |           0.5385 |        -220.7737 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |           0.0312 |           0.5211 |        -285.0221 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |           0.0274 |           0.5128 |        -256.5539 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |          -0.0082 |           0.5090 |        -353.4825 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |           0.0074 |           0.5060 |        -333.0706 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |          -0.0075 |           0.5029 |        -375.0493 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |          -0.0098 |           0.4983 |        -384.4156 |
[32m[20221208 15:10:29 @agent_ppo2.py:179][0m |           0.0117 |           0.4900 |        -322.6845 |
[32m[20221208 15:10:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:10:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.82
[32m[20221208 15:10:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.66
[32m[20221208 15:10:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.97
[32m[20221208 15:10:29 @agent_ppo2.py:137][0m Total time:      33.92 min
[32m[20221208 15:10:29 @agent_ppo2.py:139][0m 2750464 total steps have happened
[32m[20221208 15:10:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1343 --------------------------#
[32m[20221208 15:10:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0660 |           0.9191 |        -404.7634 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0459 |           0.7979 |        -349.9394 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0026 |           0.7270 |        -359.4507 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |          -0.0039 |           0.6911 |        -380.8170 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0206 |           0.6714 |        -320.7419 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0268 |           0.6556 |        -266.2501 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0056 |           0.6469 |        -342.2116 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |           0.0106 |           0.6328 |        -313.1615 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |          -0.0113 |           0.6292 |        -361.0645 |
[32m[20221208 15:10:30 @agent_ppo2.py:179][0m |          -0.0117 |           0.6113 |        -361.2767 |
[32m[20221208 15:10:30 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.44
[32m[20221208 15:10:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.16
[32m[20221208 15:10:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 29.96
[32m[20221208 15:10:31 @agent_ppo2.py:137][0m Total time:      33.94 min
[32m[20221208 15:10:31 @agent_ppo2.py:139][0m 2752512 total steps have happened
[32m[20221208 15:10:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1344 --------------------------#
[32m[20221208 15:10:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:31 @agent_ppo2.py:179][0m |           0.0913 |           0.6934 |        -337.5097 |
[32m[20221208 15:10:31 @agent_ppo2.py:179][0m |           0.0960 |           0.6040 |        -242.7643 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0446 |           0.5840 |        -329.5123 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0692 |           0.5692 |        -298.1381 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0450 |           0.5583 |        -270.9426 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0649 |           0.5526 |        -270.5624 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.1007 |           0.5439 |        -162.6153 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0668 |           0.5388 |        -149.4246 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0560 |           0.5371 |        -165.8065 |
[32m[20221208 15:10:32 @agent_ppo2.py:179][0m |           0.0722 |           0.5310 |        -222.1898 |
[32m[20221208 15:10:32 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.67
[32m[20221208 15:10:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.96
[32m[20221208 15:10:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.65
[32m[20221208 15:10:32 @agent_ppo2.py:137][0m Total time:      33.97 min
[32m[20221208 15:10:32 @agent_ppo2.py:139][0m 2754560 total steps have happened
[32m[20221208 15:10:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1345 --------------------------#
[32m[20221208 15:10:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |           0.0713 |           0.6716 |        -344.2809 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |           0.0700 |           0.6083 |        -286.6846 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |           0.0371 |           0.5802 |        -282.5372 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0177 |           0.5694 |        -327.0530 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0262 |           0.5613 |        -325.5214 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0096 |           0.5571 |        -317.3443 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0219 |           0.5485 |        -315.9971 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0318 |           0.5450 |        -312.0879 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0470 |           0.5420 |        -330.6570 |
[32m[20221208 15:10:33 @agent_ppo2.py:179][0m |          -0.0364 |           0.5466 |        -334.4540 |
[32m[20221208 15:10:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 37.25
[32m[20221208 15:10:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.86
[32m[20221208 15:10:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.56
[32m[20221208 15:10:34 @agent_ppo2.py:137][0m Total time:      33.99 min
[32m[20221208 15:10:34 @agent_ppo2.py:139][0m 2756608 total steps have happened
[32m[20221208 15:10:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1346 --------------------------#
[32m[20221208 15:10:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:10:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:34 @agent_ppo2.py:179][0m |           0.0688 |           0.8396 |        -354.4692 |
[32m[20221208 15:10:34 @agent_ppo2.py:179][0m |           0.0089 |           0.7153 |        -332.7291 |
[32m[20221208 15:10:34 @agent_ppo2.py:179][0m |          -0.0311 |           0.6803 |        -340.4448 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0575 |           0.6542 |        -345.7634 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0670 |           0.6348 |        -353.9175 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0755 |           0.6270 |        -356.0905 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0834 |           0.6182 |        -357.9376 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0877 |           0.5989 |        -366.8228 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0914 |           0.5980 |        -366.6426 |
[32m[20221208 15:10:35 @agent_ppo2.py:179][0m |          -0.0945 |           0.5984 |        -372.4364 |
[32m[20221208 15:10:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.42
[32m[20221208 15:10:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.82
[32m[20221208 15:10:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.15
[32m[20221208 15:10:35 @agent_ppo2.py:137][0m Total time:      34.02 min
[32m[20221208 15:10:35 @agent_ppo2.py:139][0m 2758656 total steps have happened
[32m[20221208 15:10:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1347 --------------------------#
[32m[20221208 15:10:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |           0.0619 |           0.8224 |        -327.8930 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |           0.0280 |           0.6899 |        -258.5294 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |           0.0092 |           0.6525 |        -211.7187 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0323 |           0.6261 |        -231.8681 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0521 |           0.6038 |        -258.8672 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0602 |           0.5911 |        -274.7890 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0665 |           0.5797 |        -293.2255 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0719 |           0.5740 |        -296.2398 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0748 |           0.5667 |        -299.2518 |
[32m[20221208 15:10:36 @agent_ppo2.py:179][0m |          -0.0806 |           0.5563 |        -305.4593 |
[32m[20221208 15:10:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.79
[32m[20221208 15:10:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.94
[32m[20221208 15:10:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.82
[32m[20221208 15:10:37 @agent_ppo2.py:137][0m Total time:      34.04 min
[32m[20221208 15:10:37 @agent_ppo2.py:139][0m 2760704 total steps have happened
[32m[20221208 15:10:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1348 --------------------------#
[32m[20221208 15:10:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:37 @agent_ppo2.py:179][0m |           0.1229 |           2.0121 |        -336.1442 |
[32m[20221208 15:10:37 @agent_ppo2.py:179][0m |           0.3879 |           1.7174 |        -278.6371 |
[32m[20221208 15:10:37 @agent_ppo2.py:179][0m |           0.0224 |           1.6223 |        -286.3689 |
[32m[20221208 15:10:37 @agent_ppo2.py:179][0m |          -0.0112 |           1.6052 |        -299.1109 |
[32m[20221208 15:10:38 @agent_ppo2.py:179][0m |          -0.0282 |           1.5844 |        -318.0040 |
[32m[20221208 15:10:38 @agent_ppo2.py:179][0m |          -0.0404 |           1.5265 |        -327.0848 |
[32m[20221208 15:10:38 @agent_ppo2.py:179][0m |          -0.0481 |           1.4738 |        -332.9605 |
[32m[20221208 15:10:38 @agent_ppo2.py:179][0m |          -0.0549 |           1.4404 |        -331.7585 |
[32m[20221208 15:10:38 @agent_ppo2.py:179][0m |          -0.0595 |           1.3892 |        -342.9737 |
[32m[20221208 15:10:38 @agent_ppo2.py:179][0m |          -0.0620 |           1.3791 |        -349.5727 |
[32m[20221208 15:10:38 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.92
[32m[20221208 15:10:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.31
[32m[20221208 15:10:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.85
[32m[20221208 15:10:38 @agent_ppo2.py:137][0m Total time:      34.07 min
[32m[20221208 15:10:38 @agent_ppo2.py:139][0m 2762752 total steps have happened
[32m[20221208 15:10:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1349 --------------------------#
[32m[20221208 15:10:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.0883 |           0.5471 |        -329.1731 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.1146 |           0.5261 |        -221.2796 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.0509 |           0.5165 |        -250.9491 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.0494 |           0.5151 |        -245.4040 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.0206 |           0.5159 |        -273.0508 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.0288 |           0.5011 |        -273.1978 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |          -0.0025 |           0.4986 |        -294.9843 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |          -0.0055 |           0.4939 |        -290.2679 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |          -0.0098 |           0.4958 |        -299.6135 |
[32m[20221208 15:10:39 @agent_ppo2.py:179][0m |           0.0013 |           0.4919 |        -298.4211 |
[32m[20221208 15:10:39 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.82
[32m[20221208 15:10:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.10
[32m[20221208 15:10:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.78
[32m[20221208 15:10:40 @agent_ppo2.py:137][0m Total time:      34.09 min
[32m[20221208 15:10:40 @agent_ppo2.py:139][0m 2764800 total steps have happened
[32m[20221208 15:10:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1350 --------------------------#
[32m[20221208 15:10:40 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:10:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:40 @agent_ppo2.py:179][0m |           0.1551 |           0.6840 |        -258.6475 |
[32m[20221208 15:10:40 @agent_ppo2.py:179][0m |           0.0748 |           0.5983 |        -215.5616 |
[32m[20221208 15:10:40 @agent_ppo2.py:179][0m |           0.0605 |           0.5640 |        -215.4437 |
[32m[20221208 15:10:40 @agent_ppo2.py:179][0m |           0.0433 |           0.5421 |        -209.4025 |
[32m[20221208 15:10:41 @agent_ppo2.py:179][0m |          -0.0159 |           0.5271 |        -259.2598 |
[32m[20221208 15:10:41 @agent_ppo2.py:179][0m |          -0.0183 |           0.5155 |        -268.5545 |
[32m[20221208 15:10:41 @agent_ppo2.py:179][0m |          -0.0221 |           0.5075 |        -285.8507 |
[32m[20221208 15:10:41 @agent_ppo2.py:179][0m |          -0.0148 |           0.5033 |        -282.9462 |
[32m[20221208 15:10:41 @agent_ppo2.py:179][0m |          -0.0103 |           0.4968 |        -271.9274 |
[32m[20221208 15:10:41 @agent_ppo2.py:179][0m |          -0.0249 |           0.4936 |        -288.0475 |
[32m[20221208 15:10:41 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.60
[32m[20221208 15:10:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 57.09
[32m[20221208 15:10:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.45
[32m[20221208 15:10:41 @agent_ppo2.py:137][0m Total time:      34.12 min
[32m[20221208 15:10:41 @agent_ppo2.py:139][0m 2766848 total steps have happened
[32m[20221208 15:10:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1351 --------------------------#
[32m[20221208 15:10:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |           0.0706 |           1.0065 |        -278.7208 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |           0.0310 |           0.8785 |        -246.0859 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0187 |           0.8184 |        -269.4281 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0419 |           0.7881 |        -275.8846 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0493 |           0.7518 |        -283.8543 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0504 |           0.7390 |        -277.3959 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0618 |           0.7252 |        -290.9253 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0675 |           0.7022 |        -290.7166 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0654 |           0.6930 |        -286.4688 |
[32m[20221208 15:10:42 @agent_ppo2.py:179][0m |          -0.0691 |           0.6848 |        -292.2416 |
[32m[20221208 15:10:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.53
[32m[20221208 15:10:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.34
[32m[20221208 15:10:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.03
[32m[20221208 15:10:43 @agent_ppo2.py:137][0m Total time:      34.14 min
[32m[20221208 15:10:43 @agent_ppo2.py:139][0m 2768896 total steps have happened
[32m[20221208 15:10:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1352 --------------------------#
[32m[20221208 15:10:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:43 @agent_ppo2.py:179][0m |           0.0868 |           0.7205 |        -265.5963 |
[32m[20221208 15:10:43 @agent_ppo2.py:179][0m |           0.0640 |           0.6033 |        -200.8947 |
[32m[20221208 15:10:43 @agent_ppo2.py:179][0m |           0.0081 |           0.5436 |        -240.2202 |
[32m[20221208 15:10:43 @agent_ppo2.py:179][0m |          -0.0107 |           0.5247 |        -255.8542 |
[32m[20221208 15:10:43 @agent_ppo2.py:179][0m |          -0.0013 |           0.5062 |        -238.8161 |
[32m[20221208 15:10:44 @agent_ppo2.py:179][0m |          -0.0202 |           0.4957 |        -254.5557 |
[32m[20221208 15:10:44 @agent_ppo2.py:179][0m |          -0.0256 |           0.4815 |        -265.2127 |
[32m[20221208 15:10:44 @agent_ppo2.py:179][0m |          -0.0219 |           0.4775 |        -266.4418 |
[32m[20221208 15:10:44 @agent_ppo2.py:179][0m |          -0.0381 |           0.4714 |        -272.5982 |
[32m[20221208 15:10:44 @agent_ppo2.py:179][0m |          -0.0345 |           0.4673 |        -277.7874 |
[32m[20221208 15:10:44 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.21
[32m[20221208 15:10:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.68
[32m[20221208 15:10:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.11
[32m[20221208 15:10:44 @agent_ppo2.py:137][0m Total time:      34.16 min
[32m[20221208 15:10:44 @agent_ppo2.py:139][0m 2770944 total steps have happened
[32m[20221208 15:10:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1353 --------------------------#
[32m[20221208 15:10:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0607 |           0.6315 |        -297.3621 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.1215 |           0.5954 |         -59.9755 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0819 |           0.5889 |         -84.3359 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0742 |           0.5880 |        -165.7883 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0801 |           0.5848 |         -42.7262 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0907 |           0.5810 |        -157.1111 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0482 |           0.5804 |        -216.6464 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0388 |           0.5824 |        -211.7585 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.0206 |           0.5828 |        -255.2474 |
[32m[20221208 15:10:45 @agent_ppo2.py:179][0m |           0.1808 |           0.5702 |        -217.3849 |
[32m[20221208 15:10:45 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.01
[32m[20221208 15:10:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.56
[32m[20221208 15:10:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.93
[32m[20221208 15:10:46 @agent_ppo2.py:137][0m Total time:      34.19 min
[32m[20221208 15:10:46 @agent_ppo2.py:139][0m 2772992 total steps have happened
[32m[20221208 15:10:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1354 --------------------------#
[32m[20221208 15:10:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.0850 |           0.5917 |        -219.6004 |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.3963 |           0.5689 |        -100.3953 |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.1678 |           0.5654 |         -36.8733 |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.1173 |           0.5577 |         -46.2767 |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.0886 |           0.5558 |         -58.1041 |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.0759 |           0.5597 |         -76.9059 |
[32m[20221208 15:10:46 @agent_ppo2.py:179][0m |           0.0679 |           0.5582 |         -80.5224 |
[32m[20221208 15:10:47 @agent_ppo2.py:179][0m |           0.0656 |           0.5592 |         -87.8544 |
[32m[20221208 15:10:47 @agent_ppo2.py:179][0m |           0.0671 |           0.5554 |         -80.0807 |
[32m[20221208 15:10:47 @agent_ppo2.py:179][0m |           0.0636 |           0.5529 |         -85.8727 |
[32m[20221208 15:10:47 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.52
[32m[20221208 15:10:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.68
[32m[20221208 15:10:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.96
[32m[20221208 15:10:47 @agent_ppo2.py:137][0m Total time:      34.21 min
[32m[20221208 15:10:47 @agent_ppo2.py:139][0m 2775040 total steps have happened
[32m[20221208 15:10:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1355 --------------------------#
[32m[20221208 15:10:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0822 |           0.4895 |        -205.1679 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0822 |           0.4725 |        -133.8267 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0459 |           0.4750 |        -135.2440 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0497 |           0.4635 |        -143.5100 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0123 |           0.4641 |        -168.1196 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0036 |           0.4620 |        -182.2106 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |           0.0119 |           0.4604 |        -181.1016 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |          -0.0065 |           0.4578 |        -192.9930 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |          -0.0003 |           0.4560 |        -185.1689 |
[32m[20221208 15:10:48 @agent_ppo2.py:179][0m |          -0.0083 |           0.4547 |        -192.5287 |
[32m[20221208 15:10:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 35.89
[32m[20221208 15:10:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.68
[32m[20221208 15:10:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.90
[32m[20221208 15:10:49 @agent_ppo2.py:137][0m Total time:      34.24 min
[32m[20221208 15:10:49 @agent_ppo2.py:139][0m 2777088 total steps have happened
[32m[20221208 15:10:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1356 --------------------------#
[32m[20221208 15:10:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |           0.0393 |           0.7862 |        -211.7077 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |           0.0313 |           0.6798 |        -187.8294 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |           0.0345 |           0.6549 |        -114.2302 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |           0.0196 |           0.6501 |         -87.5691 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |           0.0085 |           0.6360 |        -138.4432 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |           0.0180 |           0.6251 |        -137.3559 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |          -0.0011 |           0.6225 |        -149.9685 |
[32m[20221208 15:10:49 @agent_ppo2.py:179][0m |          -0.0177 |           0.6196 |        -149.8018 |
[32m[20221208 15:10:50 @agent_ppo2.py:179][0m |          -0.0204 |           0.6152 |        -170.5752 |
[32m[20221208 15:10:50 @agent_ppo2.py:179][0m |          -0.0258 |           0.6109 |        -176.7640 |
[32m[20221208 15:10:50 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.79
[32m[20221208 15:10:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.24
[32m[20221208 15:10:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.34
[32m[20221208 15:10:50 @agent_ppo2.py:137][0m Total time:      34.26 min
[32m[20221208 15:10:50 @agent_ppo2.py:139][0m 2779136 total steps have happened
[32m[20221208 15:10:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1357 --------------------------#
[32m[20221208 15:10:50 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:10:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |           0.1234 |           0.9352 |        -149.8607 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |           0.0628 |           0.8058 |        -101.3316 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |           0.0095 |           0.7562 |        -103.7845 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0204 |           0.7197 |        -113.7803 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0332 |           0.7051 |        -127.2873 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0380 |           0.6933 |        -128.6434 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0456 |           0.6877 |        -128.1952 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0509 |           0.6799 |        -135.7979 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0577 |           0.6699 |        -140.5479 |
[32m[20221208 15:10:51 @agent_ppo2.py:179][0m |          -0.0623 |           0.6618 |        -146.0095 |
[32m[20221208 15:10:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:10:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.59
[32m[20221208 15:10:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.92
[32m[20221208 15:10:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.64
[32m[20221208 15:10:51 @agent_ppo2.py:137][0m Total time:      34.29 min
[32m[20221208 15:10:51 @agent_ppo2.py:139][0m 2781184 total steps have happened
[32m[20221208 15:10:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1358 --------------------------#
[32m[20221208 15:10:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0897 |           0.6881 |        -174.3469 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0226 |           0.5740 |        -179.9178 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0072 |           0.5401 |        -175.4896 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0432 |           0.5284 |        -168.9395 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0184 |           0.5262 |        -178.9574 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0024 |           0.5219 |        -182.4782 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0149 |           0.5169 |        -178.9513 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |           0.0117 |           0.5153 |        -181.8020 |
[32m[20221208 15:10:52 @agent_ppo2.py:179][0m |          -0.0142 |           0.5142 |        -189.8873 |
[32m[20221208 15:10:53 @agent_ppo2.py:179][0m |           0.0103 |           0.5136 |        -176.3253 |
[32m[20221208 15:10:53 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:10:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 43.78
[32m[20221208 15:10:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.05
[32m[20221208 15:10:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.03
[32m[20221208 15:10:53 @agent_ppo2.py:137][0m Total time:      34.31 min
[32m[20221208 15:10:53 @agent_ppo2.py:139][0m 2783232 total steps have happened
[32m[20221208 15:10:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1359 --------------------------#
[32m[20221208 15:10:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:53 @agent_ppo2.py:179][0m |           0.0632 |           1.2292 |        -190.8313 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0009 |           0.9831 |        -179.3875 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0332 |           0.8953 |        -187.7456 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0558 |           0.8542 |        -197.3312 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0619 |           0.8192 |        -201.6907 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0712 |           0.7950 |        -203.9619 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0730 |           0.7707 |        -206.3900 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0764 |           0.7493 |        -206.7204 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0790 |           0.7332 |        -208.1945 |
[32m[20221208 15:10:54 @agent_ppo2.py:179][0m |          -0.0791 |           0.7235 |        -209.3284 |
[32m[20221208 15:10:54 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.53
[32m[20221208 15:10:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.44
[32m[20221208 15:10:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.47
[32m[20221208 15:10:54 @agent_ppo2.py:137][0m Total time:      34.34 min
[32m[20221208 15:10:54 @agent_ppo2.py:139][0m 2785280 total steps have happened
[32m[20221208 15:10:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1360 --------------------------#
[32m[20221208 15:10:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |           0.1202 |           0.8257 |        -161.7533 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |           0.0738 |           0.7157 |         -77.0008 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |           0.0458 |           0.6998 |         -54.9975 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0081 |           0.6809 |         -62.4253 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0217 |           0.6704 |         -71.5218 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0307 |           0.6637 |        -100.2348 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0269 |           0.6563 |         -83.1504 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0400 |           0.6555 |         -89.8516 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0521 |           0.6459 |        -116.8969 |
[32m[20221208 15:10:55 @agent_ppo2.py:179][0m |          -0.0544 |           0.6460 |        -112.8021 |
[32m[20221208 15:10:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:10:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.95
[32m[20221208 15:10:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.28
[32m[20221208 15:10:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.86
[32m[20221208 15:10:56 @agent_ppo2.py:137][0m Total time:      34.36 min
[32m[20221208 15:10:56 @agent_ppo2.py:139][0m 2787328 total steps have happened
[32m[20221208 15:10:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1361 --------------------------#
[32m[20221208 15:10:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:56 @agent_ppo2.py:179][0m |           0.0613 |           0.6884 |        -156.2301 |
[32m[20221208 15:10:56 @agent_ppo2.py:179][0m |           0.0041 |           0.5571 |        -145.6164 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.0057 |           0.5459 |        -141.5822 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.0277 |           0.5411 |        -122.4885 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.1752 |           0.5355 |         -77.3298 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.0485 |           0.5349 |         -61.0370 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.0677 |           0.5397 |         -83.2118 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.2263 |           0.5315 |         -88.8987 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |           0.0035 |           0.5290 |        -131.8251 |
[32m[20221208 15:10:57 @agent_ppo2.py:179][0m |          -0.0138 |           0.5312 |        -140.2087 |
[32m[20221208 15:10:57 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.13
[32m[20221208 15:10:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.94
[32m[20221208 15:10:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.12
[32m[20221208 15:10:57 @agent_ppo2.py:137][0m Total time:      34.38 min
[32m[20221208 15:10:57 @agent_ppo2.py:139][0m 2789376 total steps have happened
[32m[20221208 15:10:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1362 --------------------------#
[32m[20221208 15:10:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0337 |           0.4634 |        -154.5878 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.1080 |           0.4481 |         -60.8883 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0417 |           0.4458 |         -73.7800 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0308 |           0.4500 |         -82.9660 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0162 |           0.4439 |        -105.8663 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0431 |           0.4487 |        -112.7025 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0416 |           0.4440 |        -121.5860 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0668 |           0.4440 |        -109.7722 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0465 |           0.4461 |        -109.4961 |
[32m[20221208 15:10:58 @agent_ppo2.py:179][0m |           0.0552 |           0.4397 |         -67.5840 |
[32m[20221208 15:10:58 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:10:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 31.68
[32m[20221208 15:10:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.46
[32m[20221208 15:10:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.27
[32m[20221208 15:10:59 @agent_ppo2.py:137][0m Total time:      34.41 min
[32m[20221208 15:10:59 @agent_ppo2.py:139][0m 2791424 total steps have happened
[32m[20221208 15:10:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1363 --------------------------#
[32m[20221208 15:10:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:10:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:10:59 @agent_ppo2.py:179][0m |           0.2097 |           0.4520 |        -108.0625 |
[32m[20221208 15:10:59 @agent_ppo2.py:179][0m |           0.0478 |           0.4425 |         -58.0355 |
[32m[20221208 15:10:59 @agent_ppo2.py:179][0m |           0.0348 |           0.4395 |         -63.4769 |
[32m[20221208 15:10:59 @agent_ppo2.py:179][0m |           0.0495 |           0.4395 |         -57.1949 |
[32m[20221208 15:11:00 @agent_ppo2.py:179][0m |           0.0255 |           0.4398 |         -63.5372 |
[32m[20221208 15:11:00 @agent_ppo2.py:179][0m |           0.0212 |           0.4342 |         -64.7001 |
[32m[20221208 15:11:00 @agent_ppo2.py:179][0m |           0.0342 |           0.4397 |         -62.3782 |
[32m[20221208 15:11:00 @agent_ppo2.py:179][0m |           0.0370 |           0.4334 |         -61.0275 |
[32m[20221208 15:11:00 @agent_ppo2.py:179][0m |           0.0481 |           0.4308 |         -55.9019 |
[32m[20221208 15:11:00 @agent_ppo2.py:179][0m |           0.0351 |           0.4309 |         -59.1607 |
[32m[20221208 15:11:00 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:11:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 31.82
[32m[20221208 15:11:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.02
[32m[20221208 15:11:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.54
[32m[20221208 15:11:00 @agent_ppo2.py:137][0m Total time:      34.43 min
[32m[20221208 15:11:00 @agent_ppo2.py:139][0m 2793472 total steps have happened
[32m[20221208 15:11:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1364 --------------------------#
[32m[20221208 15:11:01 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:11:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0703 |           0.7986 |        -127.8266 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0471 |           0.6862 |        -120.7432 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0441 |           0.6507 |        -137.7236 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0159 |           0.6308 |        -144.3802 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0112 |           0.6252 |        -151.6412 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0035 |           0.6184 |        -142.0074 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0265 |           0.6117 |        -140.4895 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0107 |           0.5987 |        -139.1726 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0214 |           0.5937 |        -133.0076 |
[32m[20221208 15:11:01 @agent_ppo2.py:179][0m |           0.0322 |           0.5924 |        -120.2674 |
[32m[20221208 15:11:01 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.99
[32m[20221208 15:11:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.45
[32m[20221208 15:11:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.59
[32m[20221208 15:11:02 @agent_ppo2.py:137][0m Total time:      34.46 min
[32m[20221208 15:11:02 @agent_ppo2.py:139][0m 2795520 total steps have happened
[32m[20221208 15:11:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1365 --------------------------#
[32m[20221208 15:11:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:02 @agent_ppo2.py:179][0m |           0.0577 |           1.8853 |        -138.1876 |
[32m[20221208 15:11:02 @agent_ppo2.py:179][0m |           0.0095 |           1.5188 |        -125.5303 |
[32m[20221208 15:11:02 @agent_ppo2.py:179][0m |          -0.0149 |           1.4213 |        -126.1323 |
[32m[20221208 15:11:02 @agent_ppo2.py:179][0m |          -0.0321 |           1.3295 |        -134.7577 |
[32m[20221208 15:11:02 @agent_ppo2.py:179][0m |          -0.0410 |           1.3008 |        -143.3125 |
[32m[20221208 15:11:03 @agent_ppo2.py:179][0m |          -0.0492 |           1.2542 |        -146.5378 |
[32m[20221208 15:11:03 @agent_ppo2.py:179][0m |          -0.0530 |           1.2190 |        -153.8970 |
[32m[20221208 15:11:03 @agent_ppo2.py:179][0m |          -0.0533 |           1.2074 |        -149.8022 |
[32m[20221208 15:11:03 @agent_ppo2.py:179][0m |          -0.0588 |           1.1563 |        -155.2752 |
[32m[20221208 15:11:03 @agent_ppo2.py:179][0m |          -0.0615 |           1.1479 |        -152.8679 |
[32m[20221208 15:11:03 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.56
[32m[20221208 15:11:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.95
[32m[20221208 15:11:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.05
[32m[20221208 15:11:03 @agent_ppo2.py:137][0m Total time:      34.48 min
[32m[20221208 15:11:03 @agent_ppo2.py:139][0m 2797568 total steps have happened
[32m[20221208 15:11:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1366 --------------------------#
[32m[20221208 15:11:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |           0.0549 |           0.9332 |        -152.4204 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |           0.0243 |           0.7607 |        -143.2737 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |           0.0311 |           0.7035 |        -125.4949 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0072 |           0.6706 |        -121.1361 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0267 |           0.6487 |        -130.9381 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0355 |           0.6365 |        -137.4431 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0438 |           0.6279 |        -139.5174 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0207 |           0.6199 |        -133.0128 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0370 |           0.6170 |        -132.6681 |
[32m[20221208 15:11:04 @agent_ppo2.py:179][0m |          -0.0326 |           0.6087 |        -136.0533 |
[32m[20221208 15:11:04 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.03
[32m[20221208 15:11:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 57.41
[32m[20221208 15:11:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.13
[32m[20221208 15:11:05 @agent_ppo2.py:137][0m Total time:      34.51 min
[32m[20221208 15:11:05 @agent_ppo2.py:139][0m 2799616 total steps have happened
[32m[20221208 15:11:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1367 --------------------------#
[32m[20221208 15:11:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:05 @agent_ppo2.py:179][0m |           0.0783 |           2.5648 |        -143.8682 |
[32m[20221208 15:11:05 @agent_ppo2.py:179][0m |           0.0288 |           2.2228 |        -140.6905 |
[32m[20221208 15:11:05 @agent_ppo2.py:179][0m |          -0.0052 |           2.0871 |        -144.9476 |
[32m[20221208 15:11:05 @agent_ppo2.py:179][0m |          -0.0320 |           2.0486 |        -154.0003 |
[32m[20221208 15:11:05 @agent_ppo2.py:179][0m |          -0.0484 |           1.9892 |        -157.4582 |
[32m[20221208 15:11:05 @agent_ppo2.py:179][0m |          -0.0565 |           1.9636 |        -159.7530 |
[32m[20221208 15:11:06 @agent_ppo2.py:179][0m |          -0.0544 |           1.9033 |        -162.9478 |
[32m[20221208 15:11:06 @agent_ppo2.py:179][0m |          -0.0596 |           1.9226 |        -163.8719 |
[32m[20221208 15:11:06 @agent_ppo2.py:179][0m |          -0.0653 |           1.8691 |        -167.9268 |
[32m[20221208 15:11:06 @agent_ppo2.py:179][0m |          -0.0671 |           1.8851 |        -165.9815 |
[32m[20221208 15:11:06 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.05
[32m[20221208 15:11:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.11
[32m[20221208 15:11:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.49
[32m[20221208 15:11:06 @agent_ppo2.py:137][0m Total time:      34.53 min
[32m[20221208 15:11:06 @agent_ppo2.py:139][0m 2801664 total steps have happened
[32m[20221208 15:11:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1368 --------------------------#
[32m[20221208 15:11:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |           0.0700 |           1.3502 |        -140.3847 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |           0.0349 |           1.0807 |        -131.0660 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |           0.0251 |           1.0459 |        -124.9579 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |           0.0065 |           0.9974 |         -88.8203 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |          -0.0197 |           0.9748 |         -86.2260 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |          -0.0282 |           0.9548 |        -101.7867 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |          -0.0364 |           0.9495 |        -101.4771 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |          -0.0395 |           0.9242 |        -113.1371 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |          -0.0443 |           0.9093 |        -115.1331 |
[32m[20221208 15:11:07 @agent_ppo2.py:179][0m |          -0.0436 |           0.8973 |        -110.4270 |
[32m[20221208 15:11:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:11:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.70
[32m[20221208 15:11:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.56
[32m[20221208 15:11:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.80
[32m[20221208 15:11:08 @agent_ppo2.py:137][0m Total time:      34.56 min
[32m[20221208 15:11:08 @agent_ppo2.py:139][0m 2803712 total steps have happened
[32m[20221208 15:11:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1369 --------------------------#
[32m[20221208 15:11:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |           0.0802 |           1.6841 |        -146.2197 |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |           0.0164 |           1.3457 |        -127.8915 |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |          -0.0166 |           1.2541 |        -126.7899 |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |          -0.0421 |           1.1960 |        -137.3367 |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |          -0.0516 |           1.1462 |        -137.5318 |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |          -0.0601 |           1.1213 |        -142.3070 |
[32m[20221208 15:11:08 @agent_ppo2.py:179][0m |          -0.0577 |           1.0757 |        -142.6488 |
[32m[20221208 15:11:09 @agent_ppo2.py:179][0m |          -0.0669 |           1.0635 |        -149.4748 |
[32m[20221208 15:11:09 @agent_ppo2.py:179][0m |          -0.0722 |           1.0381 |        -148.6791 |
[32m[20221208 15:11:09 @agent_ppo2.py:179][0m |          -0.0719 |           1.0163 |        -150.7004 |
[32m[20221208 15:11:09 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.65
[32m[20221208 15:11:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.42
[32m[20221208 15:11:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.65
[32m[20221208 15:11:09 @agent_ppo2.py:137][0m Total time:      34.58 min
[32m[20221208 15:11:09 @agent_ppo2.py:139][0m 2805760 total steps have happened
[32m[20221208 15:11:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1370 --------------------------#
[32m[20221208 15:11:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |           0.0410 |           1.8745 |        -105.1149 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0048 |           1.4380 |         -85.8894 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0276 |           1.3041 |         -97.2777 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0378 |           1.2460 |         -99.6733 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0419 |           1.2182 |         -98.6607 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0499 |           1.2032 |         -98.7572 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0546 |           1.1682 |        -105.2825 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0551 |           1.1560 |        -105.1175 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0543 |           1.1272 |        -104.6824 |
[32m[20221208 15:11:10 @agent_ppo2.py:179][0m |          -0.0564 |           1.1202 |        -105.5192 |
[32m[20221208 15:11:10 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.57
[32m[20221208 15:11:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.63
[32m[20221208 15:11:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.30
[32m[20221208 15:11:11 @agent_ppo2.py:137][0m Total time:      34.60 min
[32m[20221208 15:11:11 @agent_ppo2.py:139][0m 2807808 total steps have happened
[32m[20221208 15:11:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1371 --------------------------#
[32m[20221208 15:11:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |           0.0781 |           1.3164 |        -138.5700 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |           0.0748 |           1.0236 |         -81.6409 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |           0.0176 |           0.9462 |         -66.3591 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |          -0.0180 |           0.9108 |         -74.4718 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |          -0.0337 |           0.8704 |         -86.1438 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |          -0.0404 |           0.8496 |         -86.5324 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |          -0.0509 |           0.8311 |         -89.5329 |
[32m[20221208 15:11:11 @agent_ppo2.py:179][0m |          -0.0580 |           0.8245 |         -94.6587 |
[32m[20221208 15:11:12 @agent_ppo2.py:179][0m |          -0.0573 |           0.8122 |         -97.2863 |
[32m[20221208 15:11:12 @agent_ppo2.py:179][0m |          -0.0644 |           0.8016 |        -101.3757 |
[32m[20221208 15:11:12 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.57
[32m[20221208 15:11:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.77
[32m[20221208 15:11:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.44
[32m[20221208 15:11:12 @agent_ppo2.py:137][0m Total time:      34.63 min
[32m[20221208 15:11:12 @agent_ppo2.py:139][0m 2809856 total steps have happened
[32m[20221208 15:11:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1372 --------------------------#
[32m[20221208 15:11:12 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 15:11:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |           0.0585 |           0.6267 |        -139.4347 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |           0.0410 |           0.5931 |        -123.9544 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |           0.0087 |           0.5779 |        -122.9401 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0203 |           0.5714 |        -136.2167 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0194 |           0.5631 |        -129.2876 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0246 |           0.5644 |        -134.9310 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0223 |           0.5579 |        -135.1389 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0201 |           0.5527 |        -131.8252 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0312 |           0.5512 |        -133.8784 |
[32m[20221208 15:11:13 @agent_ppo2.py:179][0m |          -0.0353 |           0.5511 |        -136.9720 |
[32m[20221208 15:11:13 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.18
[32m[20221208 15:11:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.65
[32m[20221208 15:11:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.00
[32m[20221208 15:11:13 @agent_ppo2.py:137][0m Total time:      34.65 min
[32m[20221208 15:11:13 @agent_ppo2.py:139][0m 2811904 total steps have happened
[32m[20221208 15:11:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1373 --------------------------#
[32m[20221208 15:11:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.1240 |           0.8740 |         -84.7486 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.0758 |           0.7548 |         -88.9910 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.0747 |           0.7088 |         -56.6148 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.0712 |           0.6929 |         -55.6904 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.0492 |           0.6677 |         -67.8843 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.0363 |           0.6596 |         -70.0256 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |           0.0072 |           0.6506 |         -80.6428 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |          -0.0074 |           0.6499 |        -106.3674 |
[32m[20221208 15:11:14 @agent_ppo2.py:179][0m |          -0.0128 |           0.6438 |        -113.1415 |
[32m[20221208 15:11:15 @agent_ppo2.py:179][0m |          -0.0147 |           0.6439 |        -119.5833 |
[32m[20221208 15:11:15 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.82
[32m[20221208 15:11:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.82
[32m[20221208 15:11:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.60
[32m[20221208 15:11:15 @agent_ppo2.py:137][0m Total time:      34.68 min
[32m[20221208 15:11:15 @agent_ppo2.py:139][0m 2813952 total steps have happened
[32m[20221208 15:11:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1374 --------------------------#
[32m[20221208 15:11:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |           0.0688 |           1.4641 |        -106.5130 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |           0.0053 |           1.2017 |        -101.2446 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0199 |           1.0742 |        -105.6009 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0319 |           1.0205 |        -108.8728 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0432 |           0.9843 |        -112.0772 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0405 |           0.9367 |        -110.2904 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0397 |           0.9468 |        -110.8088 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0408 |           0.9013 |        -110.0214 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0490 |           0.8690 |        -110.0297 |
[32m[20221208 15:11:16 @agent_ppo2.py:179][0m |          -0.0492 |           0.8625 |        -111.3878 |
[32m[20221208 15:11:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.78
[32m[20221208 15:11:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.96
[32m[20221208 15:11:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.31
[32m[20221208 15:11:16 @agent_ppo2.py:137][0m Total time:      34.70 min
[32m[20221208 15:11:16 @agent_ppo2.py:139][0m 2816000 total steps have happened
[32m[20221208 15:11:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1375 --------------------------#
[32m[20221208 15:11:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |           0.0745 |           0.8531 |        -107.2220 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |           0.0579 |           0.6991 |         -81.8833 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |           0.0244 |           0.6674 |         -82.9783 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |           0.0030 |           0.6432 |         -86.1850 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |          -0.0189 |           0.6350 |         -87.2419 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |          -0.0277 |           0.6218 |         -95.4415 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |          -0.0277 |           0.6170 |         -91.4749 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |          -0.0382 |           0.6078 |         -96.2085 |
[32m[20221208 15:11:17 @agent_ppo2.py:179][0m |          -0.0446 |           0.6012 |         -99.6288 |
[32m[20221208 15:11:18 @agent_ppo2.py:179][0m |          -0.0505 |           0.6009 |        -103.4133 |
[32m[20221208 15:11:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:11:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.18
[32m[20221208 15:11:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.49
[32m[20221208 15:11:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.63
[32m[20221208 15:11:18 @agent_ppo2.py:137][0m Total time:      34.73 min
[32m[20221208 15:11:18 @agent_ppo2.py:139][0m 2818048 total steps have happened
[32m[20221208 15:11:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1376 --------------------------#
[32m[20221208 15:11:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:18 @agent_ppo2.py:179][0m |           0.0977 |           0.9150 |         -97.9596 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |           0.0731 |           0.7517 |         -70.3471 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |           0.0228 |           0.6929 |         -83.6189 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0088 |           0.6714 |         -89.6701 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0264 |           0.6599 |         -95.4229 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0351 |           0.6556 |         -97.0532 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0321 |           0.6430 |         -99.3508 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0277 |           0.6346 |         -97.2351 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0213 |           0.6308 |         -93.5477 |
[32m[20221208 15:11:19 @agent_ppo2.py:179][0m |          -0.0383 |           0.6183 |        -101.3186 |
[32m[20221208 15:11:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.38
[32m[20221208 15:11:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.38
[32m[20221208 15:11:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.36
[32m[20221208 15:11:19 @agent_ppo2.py:137][0m Total time:      34.75 min
[32m[20221208 15:11:19 @agent_ppo2.py:139][0m 2820096 total steps have happened
[32m[20221208 15:11:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1377 --------------------------#
[32m[20221208 15:11:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |           0.0619 |           1.7850 |        -106.0808 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |           0.0002 |           1.2479 |        -100.0801 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0409 |           1.1463 |        -106.6472 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0613 |           1.0789 |        -105.9472 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0709 |           1.0088 |        -107.4621 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0806 |           0.9745 |        -108.3276 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0844 |           0.9378 |        -108.7421 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0853 |           0.9003 |        -109.9066 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0930 |           0.8801 |        -110.0852 |
[32m[20221208 15:11:20 @agent_ppo2.py:179][0m |          -0.0962 |           0.8609 |        -109.3535 |
[32m[20221208 15:11:20 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.51
[32m[20221208 15:11:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.28
[32m[20221208 15:11:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.65
[32m[20221208 15:11:21 @agent_ppo2.py:137][0m Total time:      34.78 min
[32m[20221208 15:11:21 @agent_ppo2.py:139][0m 2822144 total steps have happened
[32m[20221208 15:11:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1378 --------------------------#
[32m[20221208 15:11:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:21 @agent_ppo2.py:179][0m |           0.0505 |           0.9481 |        -109.6290 |
[32m[20221208 15:11:21 @agent_ppo2.py:179][0m |          -0.0021 |           0.5766 |        -105.7670 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0310 |           0.4992 |        -108.4509 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0417 |           0.4666 |        -111.1151 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0471 |           0.4487 |        -109.7484 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0489 |           0.4385 |        -108.1166 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0516 |           0.4263 |        -109.9493 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0535 |           0.4180 |        -110.7936 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0556 |           0.4162 |        -109.8927 |
[32m[20221208 15:11:22 @agent_ppo2.py:179][0m |          -0.0579 |           0.4060 |        -111.9411 |
[32m[20221208 15:11:22 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.04
[32m[20221208 15:11:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.84
[32m[20221208 15:11:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.65
[32m[20221208 15:11:22 @agent_ppo2.py:137][0m Total time:      34.80 min
[32m[20221208 15:11:22 @agent_ppo2.py:139][0m 2824192 total steps have happened
[32m[20221208 15:11:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1379 --------------------------#
[32m[20221208 15:11:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |           0.0592 |           0.5603 |        -103.3014 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |           0.0363 |           0.4336 |         -69.0707 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |           0.0147 |           0.4148 |         -55.4210 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |           0.0005 |           0.4073 |         -55.7081 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |          -0.0069 |           0.4017 |         -66.0800 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |          -0.0138 |           0.4009 |         -71.3364 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |          -0.0166 |           0.3955 |         -71.6686 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |          -0.0181 |           0.3948 |         -78.0544 |
[32m[20221208 15:11:23 @agent_ppo2.py:179][0m |          -0.0166 |           0.3954 |         -76.1180 |
[32m[20221208 15:11:24 @agent_ppo2.py:179][0m |          -0.0216 |           0.3923 |         -76.9164 |
[32m[20221208 15:11:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 15:11:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 34.30
[32m[20221208 15:11:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.17
[32m[20221208 15:11:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.82
[32m[20221208 15:11:24 @agent_ppo2.py:137][0m Total time:      34.83 min
[32m[20221208 15:11:24 @agent_ppo2.py:139][0m 2826240 total steps have happened
[32m[20221208 15:11:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1380 --------------------------#
[32m[20221208 15:11:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:24 @agent_ppo2.py:179][0m |           0.0699 |           1.5925 |        -115.2425 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |           0.0151 |           1.2230 |        -113.3222 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |           0.0093 |           1.0933 |        -104.4656 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0258 |           1.0461 |        -114.7218 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0465 |           1.0088 |        -112.3642 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0494 |           0.9703 |        -115.3460 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0595 |           0.9332 |        -115.1999 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0617 |           0.9152 |        -115.9775 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0683 |           0.9069 |        -116.0578 |
[32m[20221208 15:11:25 @agent_ppo2.py:179][0m |          -0.0518 |           0.8888 |        -116.6400 |
[32m[20221208 15:11:25 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:11:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.15
[32m[20221208 15:11:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.48
[32m[20221208 15:11:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.49
[32m[20221208 15:11:26 @agent_ppo2.py:137][0m Total time:      34.85 min
[32m[20221208 15:11:26 @agent_ppo2.py:139][0m 2828288 total steps have happened
[32m[20221208 15:11:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1381 --------------------------#
[32m[20221208 15:11:26 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:11:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |           0.0381 |           0.9014 |        -115.4715 |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |           0.0305 |           0.5635 |         -92.7893 |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |          -0.0096 |           0.4976 |         -93.7721 |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |          -0.0271 |           0.4673 |        -102.6048 |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |          -0.0253 |           0.4540 |        -106.3859 |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |          -0.0355 |           0.4477 |        -104.1151 |
[32m[20221208 15:11:26 @agent_ppo2.py:179][0m |          -0.0357 |           0.4391 |        -110.3165 |
[32m[20221208 15:11:27 @agent_ppo2.py:179][0m |          -0.0373 |           0.4343 |        -107.7307 |
[32m[20221208 15:11:27 @agent_ppo2.py:179][0m |          -0.0404 |           0.4303 |        -109.8410 |
[32m[20221208 15:11:27 @agent_ppo2.py:179][0m |          -0.0444 |           0.4255 |        -110.7367 |
[32m[20221208 15:11:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:11:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.47
[32m[20221208 15:11:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 56.50
[32m[20221208 15:11:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.29
[32m[20221208 15:11:27 @agent_ppo2.py:137][0m Total time:      34.88 min
[32m[20221208 15:11:27 @agent_ppo2.py:139][0m 2830336 total steps have happened
[32m[20221208 15:11:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1382 --------------------------#
[32m[20221208 15:11:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:11:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |           0.0554 |           0.6928 |        -118.5580 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |           0.0145 |           0.4869 |        -121.8978 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0059 |           0.4545 |        -127.9124 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0184 |           0.4407 |        -130.1370 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0263 |           0.4287 |        -138.1071 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0111 |           0.4225 |        -128.1954 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0266 |           0.4217 |        -133.0941 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0039 |           0.4154 |        -117.5480 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0203 |           0.4151 |        -131.7387 |
[32m[20221208 15:11:28 @agent_ppo2.py:179][0m |          -0.0280 |           0.4097 |        -137.8859 |
[32m[20221208 15:11:28 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 15:11:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.02
[32m[20221208 15:11:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.44
[32m[20221208 15:11:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.95
[32m[20221208 15:11:29 @agent_ppo2.py:137][0m Total time:      34.91 min
[32m[20221208 15:11:29 @agent_ppo2.py:139][0m 2832384 total steps have happened
[32m[20221208 15:11:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1383 --------------------------#
[32m[20221208 15:11:29 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:11:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:29 @agent_ppo2.py:179][0m |           0.0682 |           0.9729 |         -98.1305 |
[32m[20221208 15:11:29 @agent_ppo2.py:179][0m |           0.0309 |           0.7667 |         -48.5400 |
[32m[20221208 15:11:29 @agent_ppo2.py:179][0m |           0.0021 |           0.7232 |         -64.7015 |
[32m[20221208 15:11:29 @agent_ppo2.py:179][0m |          -0.0263 |           0.6900 |         -74.9429 |
[32m[20221208 15:11:29 @agent_ppo2.py:179][0m |          -0.0413 |           0.6704 |         -91.3427 |
[32m[20221208 15:11:30 @agent_ppo2.py:179][0m |          -0.0552 |           0.6555 |        -100.9254 |
[32m[20221208 15:11:30 @agent_ppo2.py:179][0m |          -0.0588 |           0.6456 |        -100.8303 |
[32m[20221208 15:11:30 @agent_ppo2.py:179][0m |          -0.0638 |           0.6352 |        -103.2053 |
[32m[20221208 15:11:30 @agent_ppo2.py:179][0m |          -0.0689 |           0.6302 |        -107.4258 |
[32m[20221208 15:11:30 @agent_ppo2.py:179][0m |          -0.0683 |           0.6237 |        -108.7205 |
[32m[20221208 15:11:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:11:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.44
[32m[20221208 15:11:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.87
[32m[20221208 15:11:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.33
[32m[20221208 15:11:30 @agent_ppo2.py:137][0m Total time:      34.93 min
[32m[20221208 15:11:30 @agent_ppo2.py:139][0m 2834432 total steps have happened
[32m[20221208 15:11:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1384 --------------------------#
[32m[20221208 15:11:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |           0.0601 |           2.5630 |        -116.9555 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |           0.0371 |           1.9306 |        -102.3684 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0062 |           1.7463 |        -107.5456 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0373 |           1.6599 |        -113.8695 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0568 |           1.6058 |        -118.8236 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0700 |           1.5606 |        -121.3800 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0762 |           1.5383 |        -124.8475 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0803 |           1.5115 |        -127.3611 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0795 |           1.4722 |        -126.5293 |
[32m[20221208 15:11:31 @agent_ppo2.py:179][0m |          -0.0848 |           1.4316 |        -129.8630 |
[32m[20221208 15:11:31 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.60
[32m[20221208 15:11:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.24
[32m[20221208 15:11:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.78
[32m[20221208 15:11:32 @agent_ppo2.py:137][0m Total time:      34.96 min
[32m[20221208 15:11:32 @agent_ppo2.py:139][0m 2836480 total steps have happened
[32m[20221208 15:11:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1385 --------------------------#
[32m[20221208 15:11:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:32 @agent_ppo2.py:179][0m |           0.1256 |           0.4975 |         -91.4173 |
[32m[20221208 15:11:32 @agent_ppo2.py:179][0m |           0.1448 |           0.4637 |          -4.1133 |
[32m[20221208 15:11:32 @agent_ppo2.py:179][0m |           0.1172 |           0.4603 |         -12.2517 |
[32m[20221208 15:11:32 @agent_ppo2.py:179][0m |           0.0977 |           0.4585 |         -43.5246 |
[32m[20221208 15:11:32 @agent_ppo2.py:179][0m |           0.0956 |           0.4606 |         -48.7790 |
[32m[20221208 15:11:33 @agent_ppo2.py:179][0m |           0.0501 |           0.4602 |         -75.2433 |
[32m[20221208 15:11:33 @agent_ppo2.py:179][0m |           0.1535 |           0.4525 |         -25.3054 |
[32m[20221208 15:11:33 @agent_ppo2.py:179][0m |           0.1054 |           0.4525 |         -21.9491 |
[32m[20221208 15:11:33 @agent_ppo2.py:179][0m |           0.0495 |           0.4490 |         -67.7567 |
[32m[20221208 15:11:33 @agent_ppo2.py:179][0m |           0.0244 |           0.4486 |         -84.7985 |
[32m[20221208 15:11:33 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.43
[32m[20221208 15:11:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.80
[32m[20221208 15:11:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.35
[32m[20221208 15:11:33 @agent_ppo2.py:137][0m Total time:      34.98 min
[32m[20221208 15:11:33 @agent_ppo2.py:139][0m 2838528 total steps have happened
[32m[20221208 15:11:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1386 --------------------------#
[32m[20221208 15:11:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |           0.0629 |           0.8212 |        -110.3818 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |           0.0260 |           0.6921 |         -99.1625 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0128 |           0.6490 |        -106.0275 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0270 |           0.6281 |        -106.0865 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0342 |           0.6126 |        -106.5152 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0413 |           0.6047 |        -106.4104 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0508 |           0.5929 |        -110.0315 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0549 |           0.5860 |        -113.0824 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0496 |           0.5810 |        -112.4478 |
[32m[20221208 15:11:34 @agent_ppo2.py:179][0m |          -0.0554 |           0.5765 |        -114.1731 |
[32m[20221208 15:11:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:11:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.27
[32m[20221208 15:11:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.51
[32m[20221208 15:11:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.06
[32m[20221208 15:11:35 @agent_ppo2.py:137][0m Total time:      35.01 min
[32m[20221208 15:11:35 @agent_ppo2.py:139][0m 2840576 total steps have happened
[32m[20221208 15:11:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1387 --------------------------#
[32m[20221208 15:11:35 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:35 @agent_ppo2.py:179][0m |           0.0729 |           0.6808 |         -98.0721 |
[32m[20221208 15:11:35 @agent_ppo2.py:179][0m |           0.0329 |           0.5782 |         -95.3278 |
[32m[20221208 15:11:35 @agent_ppo2.py:179][0m |           0.0038 |           0.5628 |         -97.2883 |
[32m[20221208 15:11:35 @agent_ppo2.py:179][0m |          -0.0090 |           0.5453 |         -98.4629 |
[32m[20221208 15:11:35 @agent_ppo2.py:179][0m |          -0.0121 |           0.5392 |        -101.8488 |
[32m[20221208 15:11:36 @agent_ppo2.py:179][0m |          -0.0113 |           0.5350 |        -103.7829 |
[32m[20221208 15:11:36 @agent_ppo2.py:179][0m |           0.0093 |           0.5348 |         -99.1052 |
[32m[20221208 15:11:36 @agent_ppo2.py:179][0m |           0.0102 |           0.5286 |         -87.6091 |
[32m[20221208 15:11:36 @agent_ppo2.py:179][0m |          -0.0144 |           0.5198 |         -99.0330 |
[32m[20221208 15:11:36 @agent_ppo2.py:179][0m |          -0.0185 |           0.5190 |        -103.8586 |
[32m[20221208 15:11:36 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.42
[32m[20221208 15:11:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 57.24
[32m[20221208 15:11:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.94
[32m[20221208 15:11:36 @agent_ppo2.py:137][0m Total time:      35.03 min
[32m[20221208 15:11:36 @agent_ppo2.py:139][0m 2842624 total steps have happened
[32m[20221208 15:11:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1388 --------------------------#
[32m[20221208 15:11:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |           0.0754 |           1.0214 |         -97.4394 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |           0.0798 |           0.8187 |         -73.7644 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |           0.0204 |           0.7747 |         -80.0019 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0185 |           0.7562 |         -91.7946 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0320 |           0.7394 |         -97.5890 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0406 |           0.7359 |        -100.8141 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0468 |           0.7243 |        -102.0121 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0483 |           0.7210 |        -103.0407 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0502 |           0.7175 |        -108.2860 |
[32m[20221208 15:11:37 @agent_ppo2.py:179][0m |          -0.0558 |           0.7021 |        -109.1635 |
[32m[20221208 15:11:37 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.47
[32m[20221208 15:11:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.57
[32m[20221208 15:11:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.86
[32m[20221208 15:11:38 @agent_ppo2.py:137][0m Total time:      35.06 min
[32m[20221208 15:11:38 @agent_ppo2.py:139][0m 2844672 total steps have happened
[32m[20221208 15:11:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1389 --------------------------#
[32m[20221208 15:11:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:38 @agent_ppo2.py:179][0m |           0.0579 |           0.6641 |        -111.3259 |
[32m[20221208 15:11:38 @agent_ppo2.py:179][0m |           0.0241 |           0.6174 |         -94.6728 |
[32m[20221208 15:11:38 @agent_ppo2.py:179][0m |          -0.0129 |           0.5936 |         -99.3131 |
[32m[20221208 15:11:38 @agent_ppo2.py:179][0m |          -0.0216 |           0.5837 |        -101.4225 |
[32m[20221208 15:11:38 @agent_ppo2.py:179][0m |          -0.0302 |           0.5760 |        -102.0767 |
[32m[20221208 15:11:38 @agent_ppo2.py:179][0m |          -0.0378 |           0.5606 |        -102.1913 |
[32m[20221208 15:11:39 @agent_ppo2.py:179][0m |          -0.0418 |           0.5705 |        -103.8993 |
[32m[20221208 15:11:39 @agent_ppo2.py:179][0m |          -0.0412 |           0.5586 |        -103.3011 |
[32m[20221208 15:11:39 @agent_ppo2.py:179][0m |          -0.0437 |           0.5579 |        -107.0028 |
[32m[20221208 15:11:39 @agent_ppo2.py:179][0m |          -0.0469 |           0.5525 |        -107.7731 |
[32m[20221208 15:11:39 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.66
[32m[20221208 15:11:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.83
[32m[20221208 15:11:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.66
[32m[20221208 15:11:39 @agent_ppo2.py:137][0m Total time:      35.08 min
[32m[20221208 15:11:39 @agent_ppo2.py:139][0m 2846720 total steps have happened
[32m[20221208 15:11:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1390 --------------------------#
[32m[20221208 15:11:40 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |           0.0424 |           0.8646 |        -114.5368 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |           0.0113 |           0.7330 |        -101.3656 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0211 |           0.7014 |        -108.0386 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0378 |           0.6740 |        -107.6804 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0384 |           0.6774 |        -109.4203 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0517 |           0.6556 |        -109.5031 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0484 |           0.6471 |        -109.3994 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0529 |           0.6391 |        -110.8093 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0547 |           0.6376 |        -109.2731 |
[32m[20221208 15:11:40 @agent_ppo2.py:179][0m |          -0.0515 |           0.6341 |        -107.6434 |
[32m[20221208 15:11:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.30
[32m[20221208 15:11:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.55
[32m[20221208 15:11:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.67
[32m[20221208 15:11:41 @agent_ppo2.py:137][0m Total time:      35.11 min
[32m[20221208 15:11:41 @agent_ppo2.py:139][0m 2848768 total steps have happened
[32m[20221208 15:11:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1391 --------------------------#
[32m[20221208 15:11:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:41 @agent_ppo2.py:179][0m |           0.0265 |           0.7526 |         -82.1644 |
[32m[20221208 15:11:41 @agent_ppo2.py:179][0m |           0.0488 |           0.6282 |         -60.1145 |
[32m[20221208 15:11:41 @agent_ppo2.py:179][0m |           0.0319 |           0.6035 |         -58.4983 |
[32m[20221208 15:11:41 @agent_ppo2.py:179][0m |          -0.0054 |           0.5973 |         -71.6251 |
[32m[20221208 15:11:41 @agent_ppo2.py:179][0m |          -0.0301 |           0.5782 |         -77.6092 |
[32m[20221208 15:11:41 @agent_ppo2.py:179][0m |          -0.0158 |           0.5711 |         -74.5287 |
[32m[20221208 15:11:42 @agent_ppo2.py:179][0m |          -0.0215 |           0.5682 |         -74.4605 |
[32m[20221208 15:11:42 @agent_ppo2.py:179][0m |          -0.0234 |           0.5634 |         -77.6719 |
[32m[20221208 15:11:42 @agent_ppo2.py:179][0m |          -0.0314 |           0.5596 |         -81.1344 |
[32m[20221208 15:11:42 @agent_ppo2.py:179][0m |          -0.0328 |           0.5706 |         -76.3073 |
[32m[20221208 15:11:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.55
[32m[20221208 15:11:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.81
[32m[20221208 15:11:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.10
[32m[20221208 15:11:42 @agent_ppo2.py:137][0m Total time:      35.13 min
[32m[20221208 15:11:42 @agent_ppo2.py:139][0m 2850816 total steps have happened
[32m[20221208 15:11:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1392 --------------------------#
[32m[20221208 15:11:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |           0.1068 |           0.9462 |        -113.6733 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |           0.0480 |           0.7513 |         -81.6738 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |           0.0188 |           0.7032 |         -88.2150 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0044 |           0.6794 |         -93.6664 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0081 |           0.6721 |         -96.8588 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0136 |           0.6621 |         -95.2662 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0185 |           0.6473 |         -98.9911 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0230 |           0.6406 |         -98.3736 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0268 |           0.6322 |        -101.2124 |
[32m[20221208 15:11:43 @agent_ppo2.py:179][0m |          -0.0246 |           0.6432 |        -101.0948 |
[32m[20221208 15:11:43 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.58
[32m[20221208 15:11:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.11
[32m[20221208 15:11:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.46
[32m[20221208 15:11:44 @agent_ppo2.py:137][0m Total time:      35.16 min
[32m[20221208 15:11:44 @agent_ppo2.py:139][0m 2852864 total steps have happened
[32m[20221208 15:11:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1393 --------------------------#
[32m[20221208 15:11:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |           0.0674 |           0.8546 |         -99.5828 |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |           0.1071 |           0.7800 |         -64.4353 |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |           0.0245 |           0.7481 |         -53.8985 |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |          -0.0026 |           0.7288 |         -61.3780 |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |          -0.0192 |           0.7222 |         -71.3244 |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |          -0.0227 |           0.7050 |         -70.1781 |
[32m[20221208 15:11:44 @agent_ppo2.py:179][0m |          -0.0286 |           0.6947 |         -71.0012 |
[32m[20221208 15:11:45 @agent_ppo2.py:179][0m |          -0.0126 |           0.6935 |         -67.3116 |
[32m[20221208 15:11:45 @agent_ppo2.py:179][0m |          -0.0344 |           0.6899 |         -72.2246 |
[32m[20221208 15:11:45 @agent_ppo2.py:179][0m |          -0.0311 |           0.6754 |         -72.3753 |
[32m[20221208 15:11:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:11:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.64
[32m[20221208 15:11:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.60
[32m[20221208 15:11:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.11
[32m[20221208 15:11:45 @agent_ppo2.py:137][0m Total time:      35.18 min
[32m[20221208 15:11:45 @agent_ppo2.py:139][0m 2854912 total steps have happened
[32m[20221208 15:11:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1394 --------------------------#
[32m[20221208 15:11:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0522 |           0.7585 |        -101.4807 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0670 |           0.6807 |         -61.8629 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0307 |           0.6742 |         -70.9655 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0795 |           0.6693 |         -56.0107 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0938 |           0.6617 |         -50.1211 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0339 |           0.6486 |         -65.7386 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0213 |           0.6536 |         -78.3152 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0191 |           0.6488 |         -72.3193 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0162 |           0.6487 |         -80.6489 |
[32m[20221208 15:11:46 @agent_ppo2.py:179][0m |           0.0079 |           0.6431 |         -81.8766 |
[32m[20221208 15:11:46 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.08
[32m[20221208 15:11:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.06
[32m[20221208 15:11:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.31
[32m[20221208 15:11:47 @agent_ppo2.py:137][0m Total time:      35.20 min
[32m[20221208 15:11:47 @agent_ppo2.py:139][0m 2856960 total steps have happened
[32m[20221208 15:11:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1395 --------------------------#
[32m[20221208 15:11:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:11:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0865 |           0.6142 |         -62.7345 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0929 |           0.6079 |         -32.2404 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0493 |           0.6041 |         -40.1061 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0216 |           0.5989 |         -59.2834 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0478 |           0.6001 |         -64.4130 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0439 |           0.6034 |         -46.0682 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0207 |           0.5977 |         -66.5622 |
[32m[20221208 15:11:47 @agent_ppo2.py:179][0m |           0.0028 |           0.6050 |         -74.9632 |
[32m[20221208 15:11:48 @agent_ppo2.py:179][0m |          -0.0056 |           0.5960 |         -76.7661 |
[32m[20221208 15:11:48 @agent_ppo2.py:179][0m |          -0.0053 |           0.6010 |         -71.7184 |
[32m[20221208 15:11:48 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:11:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.54
[32m[20221208 15:11:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.86
[32m[20221208 15:11:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.13
[32m[20221208 15:11:48 @agent_ppo2.py:137][0m Total time:      35.23 min
[32m[20221208 15:11:48 @agent_ppo2.py:139][0m 2859008 total steps have happened
[32m[20221208 15:11:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1396 --------------------------#
[32m[20221208 15:11:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |           0.0583 |           1.0783 |         -67.0273 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |           0.0566 |           0.8740 |         -52.3922 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |           0.0125 |           0.8446 |         -39.1480 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0140 |           0.8138 |         -36.1105 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0239 |           0.7988 |         -44.1963 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0218 |           0.7962 |         -40.3308 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0298 |           0.7865 |         -40.9510 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0312 |           0.7820 |         -42.4087 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0342 |           0.7743 |         -44.0808 |
[32m[20221208 15:11:49 @agent_ppo2.py:179][0m |          -0.0356 |           0.7720 |         -45.6848 |
[32m[20221208 15:11:49 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.53
[32m[20221208 15:11:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.99
[32m[20221208 15:11:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.85
[32m[20221208 15:11:49 @agent_ppo2.py:137][0m Total time:      35.25 min
[32m[20221208 15:11:49 @agent_ppo2.py:139][0m 2861056 total steps have happened
[32m[20221208 15:11:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1397 --------------------------#
[32m[20221208 15:11:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |           0.0571 |           0.9919 |         -50.4141 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |           0.0203 |           0.8575 |         -33.8148 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0305 |           0.8218 |         -36.7575 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0560 |           0.7938 |         -37.2490 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0662 |           0.7767 |         -38.0101 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0713 |           0.7576 |         -39.3913 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0796 |           0.7584 |         -39.5056 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0846 |           0.7442 |         -39.7557 |
[32m[20221208 15:11:50 @agent_ppo2.py:179][0m |          -0.0812 |           0.7304 |         -39.7486 |
[32m[20221208 15:11:51 @agent_ppo2.py:179][0m |          -0.0861 |           0.7263 |         -40.3542 |
[32m[20221208 15:11:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:11:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.67
[32m[20221208 15:11:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.16
[32m[20221208 15:11:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.27
[32m[20221208 15:11:51 @agent_ppo2.py:137][0m Total time:      35.28 min
[32m[20221208 15:11:51 @agent_ppo2.py:139][0m 2863104 total steps have happened
[32m[20221208 15:11:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1398 --------------------------#
[32m[20221208 15:11:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:51 @agent_ppo2.py:179][0m |           0.0803 |           0.9442 |         -45.4622 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |           0.0637 |           0.8025 |         -35.6807 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |           0.0023 |           0.7591 |         -48.6079 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0143 |           0.7233 |         -50.1013 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0188 |           0.6951 |         -51.5804 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0252 |           0.6895 |         -51.9631 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0120 |           0.6779 |         -48.0737 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0219 |           0.6665 |         -52.6127 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0300 |           0.6575 |         -50.8419 |
[32m[20221208 15:11:52 @agent_ppo2.py:179][0m |          -0.0376 |           0.6467 |         -52.8581 |
[32m[20221208 15:11:52 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.43
[32m[20221208 15:11:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.91
[32m[20221208 15:11:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.52
[32m[20221208 15:11:52 @agent_ppo2.py:137][0m Total time:      35.30 min
[32m[20221208 15:11:52 @agent_ppo2.py:139][0m 2865152 total steps have happened
[32m[20221208 15:11:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1399 --------------------------#
[32m[20221208 15:11:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |           0.1026 |           0.8310 |         -52.3840 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |           0.0219 |           0.7182 |         -47.0116 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |           0.0406 |           0.6942 |         -44.4633 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |           0.0009 |           0.6674 |         -42.0983 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |          -0.0351 |           0.6671 |         -51.6270 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |          -0.0456 |           0.6440 |         -52.5416 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |          -0.0430 |           0.6410 |         -52.1611 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |          -0.0372 |           0.6332 |         -50.6094 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |          -0.0395 |           0.6275 |         -50.7513 |
[32m[20221208 15:11:53 @agent_ppo2.py:179][0m |          -0.0558 |           0.6196 |         -54.9623 |
[32m[20221208 15:11:53 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:11:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.57
[32m[20221208 15:11:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.23
[32m[20221208 15:11:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.12
[32m[20221208 15:11:54 @agent_ppo2.py:137][0m Total time:      35.33 min
[32m[20221208 15:11:54 @agent_ppo2.py:139][0m 2867200 total steps have happened
[32m[20221208 15:11:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1400 --------------------------#
[32m[20221208 15:11:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:54 @agent_ppo2.py:179][0m |           0.0881 |           0.6414 |         -46.0451 |
[32m[20221208 15:11:54 @agent_ppo2.py:179][0m |           0.1095 |           0.5878 |         -15.2563 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |           0.0365 |           0.5740 |         -35.4322 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |           0.0092 |           0.5666 |         -44.2952 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |          -0.0094 |           0.5587 |         -48.7303 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |           0.0409 |           0.5563 |         -48.5743 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |          -0.0099 |           0.5505 |         -42.5972 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |           0.0629 |           0.5500 |         -17.0509 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |           0.0381 |           0.5474 |         -11.0342 |
[32m[20221208 15:11:55 @agent_ppo2.py:179][0m |           0.0132 |           0.5486 |         -25.3700 |
[32m[20221208 15:11:55 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.73
[32m[20221208 15:11:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.54
[32m[20221208 15:11:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.39
[32m[20221208 15:11:55 @agent_ppo2.py:137][0m Total time:      35.35 min
[32m[20221208 15:11:55 @agent_ppo2.py:139][0m 2869248 total steps have happened
[32m[20221208 15:11:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1401 --------------------------#
[32m[20221208 15:11:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |           0.0489 |           0.9558 |         -44.2409 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |           0.1169 |           0.8200 |         -28.3490 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |           0.0481 |           0.7651 |         -34.0070 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |           0.0057 |           0.7361 |         -42.9663 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |           0.0010 |           0.7232 |         -44.6148 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |          -0.0155 |           0.7096 |         -46.6630 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |          -0.0258 |           0.6989 |         -50.3760 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |          -0.0064 |           0.6898 |         -46.2459 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |          -0.0226 |           0.6853 |         -47.9452 |
[32m[20221208 15:11:56 @agent_ppo2.py:179][0m |          -0.0293 |           0.6880 |         -51.1452 |
[32m[20221208 15:11:56 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:11:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.56
[32m[20221208 15:11:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.59
[32m[20221208 15:11:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.09
[32m[20221208 15:11:57 @agent_ppo2.py:137][0m Total time:      35.38 min
[32m[20221208 15:11:57 @agent_ppo2.py:139][0m 2871296 total steps have happened
[32m[20221208 15:11:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1402 --------------------------#
[32m[20221208 15:11:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:57 @agent_ppo2.py:179][0m |           0.0482 |           1.4831 |         -44.1306 |
[32m[20221208 15:11:57 @agent_ppo2.py:179][0m |           0.1110 |           1.1944 |         -31.7350 |
[32m[20221208 15:11:57 @agent_ppo2.py:179][0m |          -0.0071 |           1.1275 |         -36.2577 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0284 |           1.1040 |         -37.2613 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0346 |           1.0814 |         -38.2331 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0357 |           1.0592 |         -38.8794 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0402 |           1.0331 |         -37.0472 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0425 |           1.0163 |         -38.7466 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0402 |           1.0074 |         -38.0498 |
[32m[20221208 15:11:58 @agent_ppo2.py:179][0m |          -0.0415 |           1.0022 |         -39.2204 |
[32m[20221208 15:11:58 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:11:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.39
[32m[20221208 15:11:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.71
[32m[20221208 15:11:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.78
[32m[20221208 15:11:58 @agent_ppo2.py:137][0m Total time:      35.40 min
[32m[20221208 15:11:58 @agent_ppo2.py:139][0m 2873344 total steps have happened
[32m[20221208 15:11:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1403 --------------------------#
[32m[20221208 15:11:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:11:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |           0.0564 |           0.8699 |         -46.7397 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |           0.0161 |           0.7627 |         -39.8270 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0116 |           0.7173 |         -41.8254 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0181 |           0.6955 |         -42.6588 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0208 |           0.6689 |         -42.5878 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0225 |           0.6662 |         -42.0326 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0259 |           0.6602 |         -43.1864 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0107 |           0.6487 |         -42.2737 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0344 |           0.6390 |         -44.7601 |
[32m[20221208 15:11:59 @agent_ppo2.py:179][0m |          -0.0222 |           0.6345 |         -44.9059 |
[32m[20221208 15:11:59 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:12:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.65
[32m[20221208 15:12:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.85
[32m[20221208 15:12:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.45
[32m[20221208 15:12:00 @agent_ppo2.py:137][0m Total time:      35.42 min
[32m[20221208 15:12:00 @agent_ppo2.py:139][0m 2875392 total steps have happened
[32m[20221208 15:12:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1404 --------------------------#
[32m[20221208 15:12:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:00 @agent_ppo2.py:179][0m |           0.0618 |           0.9504 |         -44.4521 |
[32m[20221208 15:12:00 @agent_ppo2.py:179][0m |           0.0902 |           0.8667 |         -25.5615 |
[32m[20221208 15:12:00 @agent_ppo2.py:179][0m |           0.0243 |           0.8252 |         -35.7701 |
[32m[20221208 15:12:00 @agent_ppo2.py:179][0m |           0.0185 |           0.8053 |         -39.0404 |
[32m[20221208 15:12:01 @agent_ppo2.py:179][0m |          -0.0115 |           0.7929 |         -40.1287 |
[32m[20221208 15:12:01 @agent_ppo2.py:179][0m |          -0.0199 |           0.7720 |         -40.7591 |
[32m[20221208 15:12:01 @agent_ppo2.py:179][0m |          -0.0271 |           0.7596 |         -42.8232 |
[32m[20221208 15:12:01 @agent_ppo2.py:179][0m |          -0.0181 |           0.7495 |         -42.7311 |
[32m[20221208 15:12:01 @agent_ppo2.py:179][0m |          -0.0274 |           0.7418 |         -43.7442 |
[32m[20221208 15:12:01 @agent_ppo2.py:179][0m |          -0.0206 |           0.7325 |         -42.7342 |
[32m[20221208 15:12:01 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:12:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.82
[32m[20221208 15:12:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.38
[32m[20221208 15:12:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.00
[32m[20221208 15:12:01 @agent_ppo2.py:137][0m Total time:      35.45 min
[32m[20221208 15:12:01 @agent_ppo2.py:139][0m 2877440 total steps have happened
[32m[20221208 15:12:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1405 --------------------------#
[32m[20221208 15:12:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |           0.0673 |           1.1679 |         -35.7425 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |           0.0431 |           1.0133 |         -33.4602 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0031 |           0.9720 |         -33.2612 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0377 |           0.9270 |         -35.6122 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0454 |           0.9165 |         -35.6465 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0539 |           0.9116 |         -38.5498 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0607 |           0.8792 |         -39.3481 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0689 |           0.8637 |         -40.0265 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0715 |           0.8572 |         -40.4513 |
[32m[20221208 15:12:02 @agent_ppo2.py:179][0m |          -0.0678 |           0.8523 |         -38.6454 |
[32m[20221208 15:12:02 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.54
[32m[20221208 15:12:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.49
[32m[20221208 15:12:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.40
[32m[20221208 15:12:03 @agent_ppo2.py:137][0m Total time:      35.47 min
[32m[20221208 15:12:03 @agent_ppo2.py:139][0m 2879488 total steps have happened
[32m[20221208 15:12:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1406 --------------------------#
[32m[20221208 15:12:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:03 @agent_ppo2.py:179][0m |           0.0397 |           0.9840 |         -39.4182 |
[32m[20221208 15:12:03 @agent_ppo2.py:179][0m |           0.0294 |           0.8414 |         -33.5836 |
[32m[20221208 15:12:03 @agent_ppo2.py:179][0m |          -0.0234 |           0.7753 |         -36.8762 |
[32m[20221208 15:12:03 @agent_ppo2.py:179][0m |          -0.0267 |           0.7459 |         -39.4962 |
[32m[20221208 15:12:03 @agent_ppo2.py:179][0m |          -0.0311 |           0.7212 |         -38.6096 |
[32m[20221208 15:12:04 @agent_ppo2.py:179][0m |          -0.0339 |           0.7112 |         -40.4169 |
[32m[20221208 15:12:04 @agent_ppo2.py:179][0m |          -0.0378 |           0.6922 |         -42.4647 |
[32m[20221208 15:12:04 @agent_ppo2.py:179][0m |          -0.0309 |           0.6814 |         -40.4392 |
[32m[20221208 15:12:04 @agent_ppo2.py:179][0m |          -0.0377 |           0.6740 |         -40.4013 |
[32m[20221208 15:12:04 @agent_ppo2.py:179][0m |          -0.0362 |           0.6637 |         -42.8669 |
[32m[20221208 15:12:04 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:12:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.48
[32m[20221208 15:12:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.85
[32m[20221208 15:12:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.77
[32m[20221208 15:12:04 @agent_ppo2.py:137][0m Total time:      35.50 min
[32m[20221208 15:12:04 @agent_ppo2.py:139][0m 2881536 total steps have happened
[32m[20221208 15:12:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1407 --------------------------#
[32m[20221208 15:12:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |           0.0510 |           1.0480 |         -37.6169 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |           0.0132 |           0.7752 |         -25.7114 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0122 |           0.7411 |         -21.9343 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0228 |           0.7187 |          -7.8597 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0374 |           0.6925 |          -4.7687 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0470 |           0.6800 |          -7.0775 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0470 |           0.6728 |          -7.2027 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0528 |           0.6623 |          -7.1714 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0515 |           0.6569 |          -7.2331 |
[32m[20221208 15:12:05 @agent_ppo2.py:179][0m |          -0.0529 |           0.6565 |          -7.5434 |
[32m[20221208 15:12:05 @agent_ppo2.py:124][0m Policy update time: 0.60 s
[32m[20221208 15:12:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.47
[32m[20221208 15:12:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.78
[32m[20221208 15:12:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.03
[32m[20221208 15:12:06 @agent_ppo2.py:137][0m Total time:      35.52 min
[32m[20221208 15:12:06 @agent_ppo2.py:139][0m 2883584 total steps have happened
[32m[20221208 15:12:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1408 --------------------------#
[32m[20221208 15:12:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |           0.1034 |           0.8471 |         -35.5262 |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |           0.0633 |           0.6652 |         -41.5462 |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |          -0.0027 |           0.6415 |         -45.4298 |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |           0.0154 |           0.6261 |         -41.7409 |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |          -0.0122 |           0.6158 |         -49.6222 |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |          -0.0233 |           0.6111 |         -50.9599 |
[32m[20221208 15:12:06 @agent_ppo2.py:179][0m |          -0.0213 |           0.6018 |         -51.3045 |
[32m[20221208 15:12:07 @agent_ppo2.py:179][0m |           0.0011 |           0.6042 |         -50.7571 |
[32m[20221208 15:12:07 @agent_ppo2.py:179][0m |          -0.0164 |           0.5990 |         -51.6163 |
[32m[20221208 15:12:07 @agent_ppo2.py:179][0m |           0.0138 |           0.6004 |         -45.8728 |
[32m[20221208 15:12:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.61
[32m[20221208 15:12:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.34
[32m[20221208 15:12:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.18
[32m[20221208 15:12:07 @agent_ppo2.py:137][0m Total time:      35.55 min
[32m[20221208 15:12:07 @agent_ppo2.py:139][0m 2885632 total steps have happened
[32m[20221208 15:12:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1409 --------------------------#
[32m[20221208 15:12:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |           0.0492 |           1.1006 |         -48.2361 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |           0.0299 |           0.8195 |         -40.4497 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0136 |           0.7777 |         -40.6077 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0255 |           0.7400 |         -39.4701 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0468 |           0.7151 |         -43.1189 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0545 |           0.7064 |         -44.7932 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0476 |           0.6950 |         -43.8065 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0570 |           0.6852 |         -41.0305 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0611 |           0.6801 |         -40.2950 |
[32m[20221208 15:12:08 @agent_ppo2.py:179][0m |          -0.0694 |           0.6665 |         -40.5455 |
[32m[20221208 15:12:08 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.29
[32m[20221208 15:12:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.71
[32m[20221208 15:12:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.16
[32m[20221208 15:12:09 @agent_ppo2.py:137][0m Total time:      35.57 min
[32m[20221208 15:12:09 @agent_ppo2.py:139][0m 2887680 total steps have happened
[32m[20221208 15:12:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1410 --------------------------#
[32m[20221208 15:12:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |           0.0768 |           1.8817 |         -46.3024 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |           0.0104 |           1.6169 |         -41.6453 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |          -0.0236 |           1.5162 |         -42.1936 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |          -0.0501 |           1.4447 |         -44.6960 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |          -0.0583 |           1.4051 |         -45.0269 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |          -0.0648 |           1.3690 |         -46.0036 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |          -0.0667 |           1.3106 |         -47.7755 |
[32m[20221208 15:12:09 @agent_ppo2.py:179][0m |          -0.0735 |           1.2926 |         -47.6916 |
[32m[20221208 15:12:10 @agent_ppo2.py:179][0m |          -0.0769 |           1.2868 |         -48.3403 |
[32m[20221208 15:12:10 @agent_ppo2.py:179][0m |          -0.0763 |           1.2740 |         -49.0193 |
[32m[20221208 15:12:10 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.25
[32m[20221208 15:12:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.71
[32m[20221208 15:12:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.04
[32m[20221208 15:12:10 @agent_ppo2.py:137][0m Total time:      35.60 min
[32m[20221208 15:12:10 @agent_ppo2.py:139][0m 2889728 total steps have happened
[32m[20221208 15:12:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1411 --------------------------#
[32m[20221208 15:12:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |           0.0504 |           2.8542 |         -41.9664 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |           0.0359 |           2.4690 |         -31.5768 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0023 |           2.3617 |         -32.2131 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0230 |           2.3016 |         -31.1190 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0370 |           2.2457 |         -33.5883 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0405 |           2.2072 |         -34.9842 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0468 |           2.1838 |         -35.2818 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0501 |           2.1544 |         -36.3195 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0515 |           2.1314 |         -36.7961 |
[32m[20221208 15:12:11 @agent_ppo2.py:179][0m |          -0.0547 |           2.0879 |         -37.5125 |
[32m[20221208 15:12:11 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.12
[32m[20221208 15:12:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.16
[32m[20221208 15:12:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.37
[32m[20221208 15:12:12 @agent_ppo2.py:137][0m Total time:      35.62 min
[32m[20221208 15:12:12 @agent_ppo2.py:139][0m 2891776 total steps have happened
[32m[20221208 15:12:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1412 --------------------------#
[32m[20221208 15:12:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |           0.0530 |           1.9745 |         -49.2584 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |           0.0019 |           1.4018 |         -44.9180 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |          -0.0250 |           1.2201 |         -45.4264 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |          -0.0487 |           1.1174 |         -48.6613 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |          -0.0646 |           1.0481 |         -47.5709 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |          -0.0699 |           1.0162 |         -49.1842 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |          -0.0772 |           0.9713 |         -49.6307 |
[32m[20221208 15:12:12 @agent_ppo2.py:179][0m |          -0.0804 |           0.9490 |         -50.6397 |
[32m[20221208 15:12:13 @agent_ppo2.py:179][0m |          -0.0841 |           0.9074 |         -51.8725 |
[32m[20221208 15:12:13 @agent_ppo2.py:179][0m |          -0.0879 |           0.8864 |         -51.4220 |
[32m[20221208 15:12:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.31
[32m[20221208 15:12:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.03
[32m[20221208 15:12:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.61
[32m[20221208 15:12:13 @agent_ppo2.py:137][0m Total time:      35.65 min
[32m[20221208 15:12:13 @agent_ppo2.py:139][0m 2893824 total steps have happened
[32m[20221208 15:12:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1413 --------------------------#
[32m[20221208 15:12:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |           0.0863 |           1.2395 |         -39.1749 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |           0.0602 |           1.0494 |         -28.2363 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |           0.0194 |           0.9905 |         -30.8682 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0037 |           0.9512 |         -35.4001 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0136 |           0.9271 |         -36.7363 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0184 |           0.9041 |         -38.1548 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0279 |           0.8805 |         -40.1262 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0236 |           0.8728 |         -41.0103 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0186 |           0.8569 |         -37.8584 |
[32m[20221208 15:12:14 @agent_ppo2.py:179][0m |          -0.0348 |           0.8454 |         -42.5539 |
[32m[20221208 15:12:14 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.42
[32m[20221208 15:12:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.56
[32m[20221208 15:12:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.82
[32m[20221208 15:12:14 @agent_ppo2.py:137][0m Total time:      35.67 min
[32m[20221208 15:12:14 @agent_ppo2.py:139][0m 2895872 total steps have happened
[32m[20221208 15:12:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1414 --------------------------#
[32m[20221208 15:12:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.0812 |           0.5580 |         -45.1850 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.1192 |           0.5381 |         -24.6720 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.0420 |           0.5307 |         -28.2589 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.0153 |           0.5296 |         -32.0204 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.0237 |           0.5268 |         -29.7158 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.0163 |           0.5216 |         -32.6039 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |           0.0003 |           0.5229 |         -33.4702 |
[32m[20221208 15:12:15 @agent_ppo2.py:179][0m |          -0.0157 |           0.5186 |         -37.2314 |
[32m[20221208 15:12:16 @agent_ppo2.py:179][0m |           0.0007 |           0.5206 |         -37.0686 |
[32m[20221208 15:12:16 @agent_ppo2.py:179][0m |          -0.0179 |           0.5187 |         -38.0483 |
[32m[20221208 15:12:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 44.19
[32m[20221208 15:12:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 54.44
[32m[20221208 15:12:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.12
[32m[20221208 15:12:16 @agent_ppo2.py:137][0m Total time:      35.70 min
[32m[20221208 15:12:16 @agent_ppo2.py:139][0m 2897920 total steps have happened
[32m[20221208 15:12:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1415 --------------------------#
[32m[20221208 15:12:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |           0.0449 |           2.7960 |         -45.1177 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |           0.0203 |           2.4731 |         -40.2483 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0154 |           2.3919 |         -42.1605 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0422 |           2.3689 |         -43.8890 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0539 |           2.2903 |         -44.7237 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0639 |           2.2700 |         -46.0336 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0690 |           2.2476 |         -47.0914 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0715 |           2.2849 |         -47.0916 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0681 |           2.2387 |         -46.6148 |
[32m[20221208 15:12:17 @agent_ppo2.py:179][0m |          -0.0743 |           2.2119 |         -46.2489 |
[32m[20221208 15:12:17 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.24
[32m[20221208 15:12:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.17
[32m[20221208 15:12:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.66
[32m[20221208 15:12:17 @agent_ppo2.py:137][0m Total time:      35.72 min
[32m[20221208 15:12:17 @agent_ppo2.py:139][0m 2899968 total steps have happened
[32m[20221208 15:12:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1416 --------------------------#
[32m[20221208 15:12:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |           0.0712 |           1.1535 |         -37.4854 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |           0.0141 |           0.8819 |         -33.6524 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0291 |           0.8199 |         -36.0419 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0445 |           0.7856 |         -37.0206 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0536 |           0.7577 |         -39.0376 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0597 |           0.7344 |         -39.9779 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0640 |           0.7162 |         -41.5077 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0617 |           0.7078 |         -41.4236 |
[32m[20221208 15:12:18 @agent_ppo2.py:179][0m |          -0.0674 |           0.6928 |         -42.1665 |
[32m[20221208 15:12:19 @agent_ppo2.py:179][0m |          -0.0669 |           0.6859 |         -41.7755 |
[32m[20221208 15:12:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.22
[32m[20221208 15:12:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.94
[32m[20221208 15:12:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.67
[32m[20221208 15:12:19 @agent_ppo2.py:137][0m Total time:      35.74 min
[32m[20221208 15:12:19 @agent_ppo2.py:139][0m 2902016 total steps have happened
[32m[20221208 15:12:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1417 --------------------------#
[32m[20221208 15:12:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:19 @agent_ppo2.py:179][0m |           0.0875 |           1.1575 |         -38.4590 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |           0.0540 |           0.9366 |         -27.4513 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |           0.0353 |           0.8691 |         -26.7207 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |           0.0156 |           0.8348 |         -28.9519 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |          -0.0125 |           0.8215 |         -34.4607 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |          -0.0313 |           0.8084 |         -37.4097 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |          -0.0335 |           0.7954 |         -37.9246 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |          -0.0350 |           0.7892 |         -37.9066 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |          -0.0371 |           0.7813 |         -38.5210 |
[32m[20221208 15:12:20 @agent_ppo2.py:179][0m |          -0.0404 |           0.7780 |         -38.6919 |
[32m[20221208 15:12:20 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.31
[32m[20221208 15:12:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.89
[32m[20221208 15:12:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.31
[32m[20221208 15:12:20 @agent_ppo2.py:137][0m Total time:      35.77 min
[32m[20221208 15:12:20 @agent_ppo2.py:139][0m 2904064 total steps have happened
[32m[20221208 15:12:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1418 --------------------------#
[32m[20221208 15:12:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |           0.0603 |           2.8538 |         -40.6249 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |           0.0199 |           2.4720 |         -34.9531 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0117 |           2.2759 |         -38.2104 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0390 |           2.1836 |         -40.6170 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0483 |           2.0838 |         -40.7369 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0555 |           2.0363 |         -42.8635 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0592 |           1.9864 |         -43.4152 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0667 |           1.9680 |         -44.4192 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0680 |           1.8856 |         -44.9478 |
[32m[20221208 15:12:21 @agent_ppo2.py:179][0m |          -0.0692 |           1.8578 |         -45.2947 |
[32m[20221208 15:12:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.01
[32m[20221208 15:12:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.99
[32m[20221208 15:12:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.46
[32m[20221208 15:12:22 @agent_ppo2.py:137][0m Total time:      35.79 min
[32m[20221208 15:12:22 @agent_ppo2.py:139][0m 2906112 total steps have happened
[32m[20221208 15:12:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1419 --------------------------#
[32m[20221208 15:12:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:22 @agent_ppo2.py:179][0m |           0.1600 |           3.1508 |         -32.3523 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |           0.1764 |           2.7339 |         -23.4994 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |           0.0204 |           2.5383 |         -25.4718 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0147 |           2.4492 |         -29.8769 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0266 |           2.4151 |         -30.3181 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0377 |           2.3386 |         -33.4251 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0293 |           2.2911 |         -32.7461 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0388 |           2.2282 |         -33.3183 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0447 |           2.2076 |         -34.7060 |
[32m[20221208 15:12:23 @agent_ppo2.py:179][0m |          -0.0462 |           2.1908 |         -34.6640 |
[32m[20221208 15:12:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.11
[32m[20221208 15:12:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 93.95
[32m[20221208 15:12:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.30
[32m[20221208 15:12:23 @agent_ppo2.py:137][0m Total time:      35.82 min
[32m[20221208 15:12:23 @agent_ppo2.py:139][0m 2908160 total steps have happened
[32m[20221208 15:12:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1420 --------------------------#
[32m[20221208 15:12:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |           0.0517 |           1.6475 |         -39.3588 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |           0.0124 |           1.3501 |         -33.7319 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0329 |           1.2283 |         -37.5947 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0531 |           1.1579 |         -39.9015 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0627 |           1.1040 |         -41.4499 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0678 |           1.0689 |         -39.9914 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0735 |           1.0542 |         -42.2798 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0729 |           1.0120 |         -41.2306 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0728 |           1.0022 |         -41.0825 |
[32m[20221208 15:12:24 @agent_ppo2.py:179][0m |          -0.0809 |           0.9742 |         -42.4933 |
[32m[20221208 15:12:24 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.05
[32m[20221208 15:12:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.84
[32m[20221208 15:12:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.39
[32m[20221208 15:12:25 @agent_ppo2.py:137][0m Total time:      35.84 min
[32m[20221208 15:12:25 @agent_ppo2.py:139][0m 2910208 total steps have happened
[32m[20221208 15:12:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1421 --------------------------#
[32m[20221208 15:12:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:25 @agent_ppo2.py:179][0m |           0.1037 |           1.5129 |         -28.6427 |
[32m[20221208 15:12:25 @agent_ppo2.py:179][0m |           0.1099 |           1.2341 |          -7.9018 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |           0.0433 |           1.1663 |         -11.6165 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |           0.0276 |           1.0977 |         -11.8858 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |           0.0031 |           1.0631 |         -18.9098 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |          -0.0082 |           1.0231 |         -27.9737 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |          -0.0313 |           0.9983 |         -33.6919 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |          -0.0318 |           0.9801 |         -34.7111 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |          -0.0418 |           0.9708 |         -36.7262 |
[32m[20221208 15:12:26 @agent_ppo2.py:179][0m |          -0.0456 |           0.9498 |         -38.0595 |
[32m[20221208 15:12:26 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:12:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.91
[32m[20221208 15:12:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.62
[32m[20221208 15:12:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.92
[32m[20221208 15:12:26 @agent_ppo2.py:137][0m Total time:      35.87 min
[32m[20221208 15:12:26 @agent_ppo2.py:139][0m 2912256 total steps have happened
[32m[20221208 15:12:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1422 --------------------------#
[32m[20221208 15:12:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:12:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |           0.0405 |           0.9835 |         -32.8923 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |           0.0337 |           0.8899 |         -27.2088 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0038 |           0.8647 |         -30.1955 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0236 |           0.8454 |         -31.5958 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0294 |           0.8353 |         -32.4116 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0341 |           0.8291 |         -33.1989 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0395 |           0.8088 |         -33.9050 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0400 |           0.8076 |         -32.6332 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0437 |           0.8031 |         -34.3216 |
[32m[20221208 15:12:27 @agent_ppo2.py:179][0m |          -0.0443 |           0.7944 |         -34.9741 |
[32m[20221208 15:12:27 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.54
[32m[20221208 15:12:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.57
[32m[20221208 15:12:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.60
[32m[20221208 15:12:28 @agent_ppo2.py:137][0m Total time:      35.89 min
[32m[20221208 15:12:28 @agent_ppo2.py:139][0m 2914304 total steps have happened
[32m[20221208 15:12:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1423 --------------------------#
[32m[20221208 15:12:28 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:28 @agent_ppo2.py:179][0m |           0.0523 |           1.1334 |         -33.6504 |
[32m[20221208 15:12:28 @agent_ppo2.py:179][0m |           0.0213 |           0.9124 |         -26.1713 |
[32m[20221208 15:12:28 @agent_ppo2.py:179][0m |          -0.0028 |           0.8443 |         -30.9152 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0169 |           0.8166 |         -32.7054 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0173 |           0.7907 |         -32.2285 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0183 |           0.7722 |         -31.7804 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0168 |           0.7700 |         -29.6055 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0226 |           0.7488 |         -27.7478 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0202 |           0.7480 |         -28.5435 |
[32m[20221208 15:12:29 @agent_ppo2.py:179][0m |          -0.0220 |           0.7370 |         -28.5106 |
[32m[20221208 15:12:29 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:12:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.26
[32m[20221208 15:12:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.67
[32m[20221208 15:12:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.75
[32m[20221208 15:12:29 @agent_ppo2.py:137][0m Total time:      35.92 min
[32m[20221208 15:12:29 @agent_ppo2.py:139][0m 2916352 total steps have happened
[32m[20221208 15:12:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1424 --------------------------#
[32m[20221208 15:12:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |           0.0766 |           0.7367 |         -24.2607 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |           0.1131 |           0.6725 |         -15.3255 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |           0.0336 |           0.6507 |         -19.6902 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |           0.0295 |           0.6436 |         -22.4116 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |          -0.0007 |           0.6394 |         -22.9779 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |           0.0261 |           0.6326 |         -20.7615 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |           0.0161 |           0.6329 |         -20.1729 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |          -0.0234 |           0.6265 |         -23.6689 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |          -0.0126 |           0.6249 |         -24.0195 |
[32m[20221208 15:12:30 @agent_ppo2.py:179][0m |          -0.0136 |           0.6228 |         -24.8002 |
[32m[20221208 15:12:30 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:12:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.50
[32m[20221208 15:12:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 51.32
[32m[20221208 15:12:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.34
[32m[20221208 15:12:31 @agent_ppo2.py:137][0m Total time:      35.94 min
[32m[20221208 15:12:31 @agent_ppo2.py:139][0m 2918400 total steps have happened
[32m[20221208 15:12:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1425 --------------------------#
[32m[20221208 15:12:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:31 @agent_ppo2.py:179][0m |           0.0407 |           2.6814 |         -21.5897 |
[32m[20221208 15:12:31 @agent_ppo2.py:179][0m |          -0.0077 |           2.1921 |         -19.6684 |
[32m[20221208 15:12:31 @agent_ppo2.py:179][0m |          -0.0353 |           2.0823 |         -18.7108 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0518 |           2.0020 |         -20.9628 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0551 |           1.9338 |         -20.9361 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0645 |           1.9082 |         -21.3326 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0662 |           1.8630 |         -21.7378 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0681 |           1.8277 |         -21.9331 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0707 |           1.8281 |         -22.2058 |
[32m[20221208 15:12:32 @agent_ppo2.py:179][0m |          -0.0765 |           1.7691 |         -22.5310 |
[32m[20221208 15:12:32 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.84
[32m[20221208 15:12:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 106.39
[32m[20221208 15:12:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.49
[32m[20221208 15:12:32 @agent_ppo2.py:137][0m Total time:      35.97 min
[32m[20221208 15:12:32 @agent_ppo2.py:139][0m 2920448 total steps have happened
[32m[20221208 15:12:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1426 --------------------------#
[32m[20221208 15:12:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |           0.0588 |           0.6894 |         -21.6311 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |           0.0319 |           0.6594 |         -19.2627 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |           0.0050 |           0.6540 |         -23.6588 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0094 |           0.6449 |         -25.2667 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0181 |           0.6394 |         -25.3465 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0022 |           0.6386 |         -24.3446 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0092 |           0.6353 |         -23.7381 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0144 |           0.6348 |         -25.5094 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0095 |           0.6314 |         -25.7397 |
[32m[20221208 15:12:33 @agent_ppo2.py:179][0m |          -0.0156 |           0.6277 |         -25.6557 |
[32m[20221208 15:12:33 @agent_ppo2.py:124][0m Policy update time: 0.61 s
[32m[20221208 15:12:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.44
[32m[20221208 15:12:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.15
[32m[20221208 15:12:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.49
[32m[20221208 15:12:34 @agent_ppo2.py:137][0m Total time:      35.99 min
[32m[20221208 15:12:34 @agent_ppo2.py:139][0m 2922496 total steps have happened
[32m[20221208 15:12:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1427 --------------------------#
[32m[20221208 15:12:34 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:34 @agent_ppo2.py:179][0m |           0.0520 |           1.4357 |         -21.9391 |
[32m[20221208 15:12:34 @agent_ppo2.py:179][0m |           0.0050 |           1.1471 |         -21.3269 |
[32m[20221208 15:12:34 @agent_ppo2.py:179][0m |          -0.0211 |           1.0825 |         -21.3730 |
[32m[20221208 15:12:34 @agent_ppo2.py:179][0m |          -0.0366 |           1.0351 |         -21.6583 |
[32m[20221208 15:12:35 @agent_ppo2.py:179][0m |          -0.0470 |           1.0056 |         -21.2963 |
[32m[20221208 15:12:35 @agent_ppo2.py:179][0m |          -0.0492 |           0.9824 |         -20.6061 |
[32m[20221208 15:12:35 @agent_ppo2.py:179][0m |          -0.0449 |           0.9584 |         -21.9758 |
[32m[20221208 15:12:35 @agent_ppo2.py:179][0m |          -0.0428 |           0.9469 |         -20.7303 |
[32m[20221208 15:12:35 @agent_ppo2.py:179][0m |          -0.0525 |           0.9369 |         -21.0323 |
[32m[20221208 15:12:35 @agent_ppo2.py:179][0m |          -0.0527 |           0.9286 |         -20.2223 |
[32m[20221208 15:12:35 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.43
[32m[20221208 15:12:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 100.68
[32m[20221208 15:12:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.75
[32m[20221208 15:12:35 @agent_ppo2.py:137][0m Total time:      36.02 min
[32m[20221208 15:12:35 @agent_ppo2.py:139][0m 2924544 total steps have happened
[32m[20221208 15:12:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1428 --------------------------#
[32m[20221208 15:12:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |           0.0632 |           2.6542 |         -21.5006 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |           0.0161 |           2.1424 |         -17.7200 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0277 |           1.9697 |         -19.5630 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0598 |           1.8399 |         -20.5450 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0678 |           1.7607 |         -21.2646 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0767 |           1.7227 |         -20.9292 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0805 |           1.6838 |         -20.9969 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0846 |           1.6254 |         -20.9620 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0893 |           1.5817 |         -22.4338 |
[32m[20221208 15:12:36 @agent_ppo2.py:179][0m |          -0.0922 |           1.6021 |         -21.5042 |
[32m[20221208 15:12:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.22
[32m[20221208 15:12:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.90
[32m[20221208 15:12:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.53
[32m[20221208 15:12:37 @agent_ppo2.py:137][0m Total time:      36.04 min
[32m[20221208 15:12:37 @agent_ppo2.py:139][0m 2926592 total steps have happened
[32m[20221208 15:12:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1429 --------------------------#
[32m[20221208 15:12:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:37 @agent_ppo2.py:179][0m |           0.0539 |           0.9496 |         -23.5326 |
[32m[20221208 15:12:37 @agent_ppo2.py:179][0m |           0.0218 |           0.7634 |         -19.4093 |
[32m[20221208 15:12:37 @agent_ppo2.py:179][0m |          -0.0103 |           0.7301 |         -21.4386 |
[32m[20221208 15:12:37 @agent_ppo2.py:179][0m |          -0.0274 |           0.7120 |         -23.7173 |
[32m[20221208 15:12:37 @agent_ppo2.py:179][0m |          -0.0356 |           0.6980 |         -23.8463 |
[32m[20221208 15:12:38 @agent_ppo2.py:179][0m |          -0.0400 |           0.6948 |         -24.2122 |
[32m[20221208 15:12:38 @agent_ppo2.py:179][0m |          -0.0452 |           0.6833 |         -24.2489 |
[32m[20221208 15:12:38 @agent_ppo2.py:179][0m |          -0.0504 |           0.6778 |         -25.5093 |
[32m[20221208 15:12:38 @agent_ppo2.py:179][0m |          -0.0451 |           0.6720 |         -24.4197 |
[32m[20221208 15:12:38 @agent_ppo2.py:179][0m |          -0.0529 |           0.6662 |         -24.5745 |
[32m[20221208 15:12:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.58
[32m[20221208 15:12:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.86
[32m[20221208 15:12:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.04
[32m[20221208 15:12:38 @agent_ppo2.py:137][0m Total time:      36.07 min
[32m[20221208 15:12:38 @agent_ppo2.py:139][0m 2928640 total steps have happened
[32m[20221208 15:12:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1430 --------------------------#
[32m[20221208 15:12:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |           0.0480 |           4.0681 |         -23.5343 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |           0.0102 |           3.5819 |         -19.8931 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0301 |           3.3846 |         -19.8822 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0458 |           3.3375 |         -21.0170 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0566 |           3.2182 |         -22.0818 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0628 |           3.1957 |         -23.1196 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0665 |           3.2038 |         -23.3591 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0714 |           3.1800 |         -23.7809 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0748 |           3.0475 |         -24.0550 |
[32m[20221208 15:12:39 @agent_ppo2.py:179][0m |          -0.0760 |           3.0544 |         -24.7502 |
[32m[20221208 15:12:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:12:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.82
[32m[20221208 15:12:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 137.91
[32m[20221208 15:12:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.61
[32m[20221208 15:12:40 @agent_ppo2.py:137][0m Total time:      36.09 min
[32m[20221208 15:12:40 @agent_ppo2.py:139][0m 2930688 total steps have happened
[32m[20221208 15:12:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1431 --------------------------#
[32m[20221208 15:12:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:40 @agent_ppo2.py:179][0m |           0.0608 |           1.9885 |         -23.0768 |
[32m[20221208 15:12:40 @agent_ppo2.py:179][0m |           0.0388 |           1.6224 |         -18.0195 |
[32m[20221208 15:12:40 @agent_ppo2.py:179][0m |          -0.0072 |           1.4574 |         -19.0601 |
[32m[20221208 15:12:40 @agent_ppo2.py:179][0m |          -0.0351 |           1.3600 |         -25.5053 |
[32m[20221208 15:12:40 @agent_ppo2.py:179][0m |          -0.0531 |           1.2898 |         -27.4866 |
[32m[20221208 15:12:41 @agent_ppo2.py:179][0m |          -0.0542 |           1.2465 |         -26.0501 |
[32m[20221208 15:12:41 @agent_ppo2.py:179][0m |          -0.0590 |           1.2006 |         -27.7722 |
[32m[20221208 15:12:41 @agent_ppo2.py:179][0m |          -0.0638 |           1.1892 |         -27.4647 |
[32m[20221208 15:12:41 @agent_ppo2.py:179][0m |          -0.0633 |           1.1412 |         -28.1268 |
[32m[20221208 15:12:41 @agent_ppo2.py:179][0m |          -0.0631 |           1.1214 |         -29.1203 |
[32m[20221208 15:12:41 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.15
[32m[20221208 15:12:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.59
[32m[20221208 15:12:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.56
[32m[20221208 15:12:41 @agent_ppo2.py:137][0m Total time:      36.11 min
[32m[20221208 15:12:41 @agent_ppo2.py:139][0m 2932736 total steps have happened
[32m[20221208 15:12:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1432 --------------------------#
[32m[20221208 15:12:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |           0.0446 |           2.7374 |         -22.9836 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |           0.0148 |           2.3451 |         -23.9707 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0293 |           2.2452 |         -24.7070 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0505 |           2.1493 |         -24.7561 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0628 |           2.1022 |         -25.7916 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0713 |           2.0585 |         -26.2718 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0772 |           2.0237 |         -27.6706 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0793 |           1.9895 |         -28.1571 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0812 |           1.9631 |         -27.9247 |
[32m[20221208 15:12:42 @agent_ppo2.py:179][0m |          -0.0857 |           1.9508 |         -29.4574 |
[32m[20221208 15:12:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:12:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.34
[32m[20221208 15:12:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.83
[32m[20221208 15:12:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.22
[32m[20221208 15:12:43 @agent_ppo2.py:137][0m Total time:      36.14 min
[32m[20221208 15:12:43 @agent_ppo2.py:139][0m 2934784 total steps have happened
[32m[20221208 15:12:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1433 --------------------------#
[32m[20221208 15:12:43 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:12:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:43 @agent_ppo2.py:179][0m |           0.0433 |           1.8973 |         -28.1004 |
[32m[20221208 15:12:43 @agent_ppo2.py:179][0m |           0.0941 |           1.4093 |         -16.7934 |
[32m[20221208 15:12:43 @agent_ppo2.py:179][0m |           0.0049 |           1.2550 |         -15.7086 |
[32m[20221208 15:12:43 @agent_ppo2.py:179][0m |          -0.0316 |           1.1836 |         -20.4669 |
[32m[20221208 15:12:43 @agent_ppo2.py:179][0m |          -0.0414 |           1.1591 |         -21.6465 |
[32m[20221208 15:12:44 @agent_ppo2.py:179][0m |          -0.0290 |           1.0963 |         -19.0865 |
[32m[20221208 15:12:44 @agent_ppo2.py:179][0m |          -0.0509 |           1.0632 |         -18.0502 |
[32m[20221208 15:12:44 @agent_ppo2.py:179][0m |          -0.0582 |           1.0286 |         -20.6932 |
[32m[20221208 15:12:44 @agent_ppo2.py:179][0m |          -0.0641 |           1.0065 |         -21.8181 |
[32m[20221208 15:12:44 @agent_ppo2.py:179][0m |          -0.0639 |           0.9826 |         -23.0241 |
[32m[20221208 15:12:44 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.69
[32m[20221208 15:12:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.30
[32m[20221208 15:12:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.13
[32m[20221208 15:12:44 @agent_ppo2.py:137][0m Total time:      36.16 min
[32m[20221208 15:12:44 @agent_ppo2.py:139][0m 2936832 total steps have happened
[32m[20221208 15:12:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1434 --------------------------#
[32m[20221208 15:12:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:12:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |           0.0710 |           4.8864 |         -31.9842 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |           0.0362 |           4.4293 |         -26.4267 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |           0.0049 |           4.2724 |         -28.4724 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0295 |           4.2046 |         -31.5978 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0437 |           4.1393 |         -34.5273 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0541 |           4.1008 |         -35.8686 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0618 |           4.0466 |         -37.0580 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0664 |           4.0146 |         -37.9766 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0717 |           4.0026 |         -38.8462 |
[32m[20221208 15:12:45 @agent_ppo2.py:179][0m |          -0.0748 |           3.9676 |         -39.2843 |
[32m[20221208 15:12:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.72
[32m[20221208 15:12:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.39
[32m[20221208 15:12:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.03
[32m[20221208 15:12:46 @agent_ppo2.py:137][0m Total time:      36.19 min
[32m[20221208 15:12:46 @agent_ppo2.py:139][0m 2938880 total steps have happened
[32m[20221208 15:12:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1435 --------------------------#
[32m[20221208 15:12:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:46 @agent_ppo2.py:179][0m |           0.0613 |           4.6279 |         -33.8730 |
[32m[20221208 15:12:46 @agent_ppo2.py:179][0m |           0.0630 |           4.0461 |         -25.3554 |
[32m[20221208 15:12:46 @agent_ppo2.py:179][0m |           0.0003 |           3.8414 |         -27.7322 |
[32m[20221208 15:12:46 @agent_ppo2.py:179][0m |          -0.0322 |           3.7034 |         -31.5738 |
[32m[20221208 15:12:46 @agent_ppo2.py:179][0m |          -0.0497 |           3.5974 |         -33.7970 |
[32m[20221208 15:12:46 @agent_ppo2.py:179][0m |          -0.0565 |           3.5338 |         -34.7302 |
[32m[20221208 15:12:47 @agent_ppo2.py:179][0m |          -0.0654 |           3.4661 |         -35.7816 |
[32m[20221208 15:12:47 @agent_ppo2.py:179][0m |          -0.0684 |           3.3819 |         -36.7136 |
[32m[20221208 15:12:47 @agent_ppo2.py:179][0m |          -0.0734 |           3.3430 |         -37.4306 |
[32m[20221208 15:12:47 @agent_ppo2.py:179][0m |          -0.0755 |           3.2846 |         -38.1903 |
[32m[20221208 15:12:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.54
[32m[20221208 15:12:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.98
[32m[20221208 15:12:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.85
[32m[20221208 15:12:47 @agent_ppo2.py:137][0m Total time:      36.21 min
[32m[20221208 15:12:47 @agent_ppo2.py:139][0m 2940928 total steps have happened
[32m[20221208 15:12:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1436 --------------------------#
[32m[20221208 15:12:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |           0.0543 |           3.2816 |         -28.2763 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |           0.0081 |           2.5728 |         -22.9319 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0199 |           2.2467 |         -28.4185 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0221 |           2.1096 |         -27.2236 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0325 |           1.9485 |         -27.1771 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0440 |           1.8643 |         -29.5685 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0487 |           1.7778 |         -29.9828 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0519 |           1.7028 |         -32.3800 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0533 |           1.6438 |         -32.8796 |
[32m[20221208 15:12:48 @agent_ppo2.py:179][0m |          -0.0565 |           1.5977 |         -31.4814 |
[32m[20221208 15:12:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.63
[32m[20221208 15:12:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.12
[32m[20221208 15:12:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.88
[32m[20221208 15:12:49 @agent_ppo2.py:137][0m Total time:      36.24 min
[32m[20221208 15:12:49 @agent_ppo2.py:139][0m 2942976 total steps have happened
[32m[20221208 15:12:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1437 --------------------------#
[32m[20221208 15:12:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |           0.0670 |           1.9562 |         -32.5395 |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |          -0.0062 |           1.4439 |         -25.4043 |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |          -0.0383 |           1.3607 |         -27.6203 |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |          -0.0495 |           1.3276 |         -29.6025 |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |          -0.0606 |           1.2869 |         -31.0050 |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |          -0.0668 |           1.2907 |         -32.5382 |
[32m[20221208 15:12:49 @agent_ppo2.py:179][0m |          -0.0725 |           1.2533 |         -33.9701 |
[32m[20221208 15:12:50 @agent_ppo2.py:179][0m |          -0.0766 |           1.2378 |         -36.3336 |
[32m[20221208 15:12:50 @agent_ppo2.py:179][0m |          -0.0766 |           1.2264 |         -36.9295 |
[32m[20221208 15:12:50 @agent_ppo2.py:179][0m |          -0.0804 |           1.2267 |         -38.3390 |
[32m[20221208 15:12:50 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:12:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.22
[32m[20221208 15:12:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.08
[32m[20221208 15:12:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.18
[32m[20221208 15:12:50 @agent_ppo2.py:137][0m Total time:      36.26 min
[32m[20221208 15:12:50 @agent_ppo2.py:139][0m 2945024 total steps have happened
[32m[20221208 15:12:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1438 --------------------------#
[32m[20221208 15:12:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |           0.0782 |           1.6202 |         -46.4713 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |           0.0428 |           1.2755 |         -41.6580 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0120 |           1.1478 |         -45.3250 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0412 |           1.0764 |         -46.6066 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0484 |           1.0256 |         -47.8665 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0581 |           0.9875 |         -50.5396 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0608 |           0.9544 |         -49.2871 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0649 |           0.9364 |         -50.7205 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0686 |           0.9194 |         -50.5004 |
[32m[20221208 15:12:51 @agent_ppo2.py:179][0m |          -0.0716 |           0.8922 |         -51.8473 |
[32m[20221208 15:12:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.03
[32m[20221208 15:12:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.72
[32m[20221208 15:12:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 71.95
[32m[20221208 15:12:52 @agent_ppo2.py:137][0m Total time:      36.29 min
[32m[20221208 15:12:52 @agent_ppo2.py:139][0m 2947072 total steps have happened
[32m[20221208 15:12:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1439 --------------------------#
[32m[20221208 15:12:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |           0.0523 |           3.3783 |         -48.5257 |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |           0.0477 |           2.8872 |         -35.4997 |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |          -0.0084 |           2.6339 |         -39.3252 |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |          -0.0441 |           2.5102 |         -42.8417 |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |          -0.0588 |           2.4621 |         -44.6214 |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |          -0.0671 |           2.4235 |         -45.9174 |
[32m[20221208 15:12:52 @agent_ppo2.py:179][0m |          -0.0742 |           2.3389 |         -47.1027 |
[32m[20221208 15:12:53 @agent_ppo2.py:179][0m |          -0.0769 |           2.2975 |         -48.6123 |
[32m[20221208 15:12:53 @agent_ppo2.py:179][0m |          -0.0805 |           2.2833 |         -48.8046 |
[32m[20221208 15:12:53 @agent_ppo2.py:179][0m |          -0.0837 |           2.2117 |         -50.0553 |
[32m[20221208 15:12:53 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.85
[32m[20221208 15:12:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 118.55
[32m[20221208 15:12:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.65
[32m[20221208 15:12:53 @agent_ppo2.py:137][0m Total time:      36.31 min
[32m[20221208 15:12:53 @agent_ppo2.py:139][0m 2949120 total steps have happened
[32m[20221208 15:12:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1440 --------------------------#
[32m[20221208 15:12:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |           0.0795 |           2.6402 |         -42.5306 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |           0.0712 |           2.3008 |         -33.8017 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |           0.0037 |           2.2354 |         -37.6212 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0194 |           2.1331 |         -40.0683 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0307 |           2.0716 |         -41.0367 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0429 |           2.0709 |         -43.6461 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0560 |           1.9916 |         -44.7608 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0538 |           1.9617 |         -45.3369 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0633 |           1.9319 |         -46.9960 |
[32m[20221208 15:12:54 @agent_ppo2.py:179][0m |          -0.0698 |           1.8967 |         -47.7983 |
[32m[20221208 15:12:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.01
[32m[20221208 15:12:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 127.49
[32m[20221208 15:12:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.41
[32m[20221208 15:12:55 @agent_ppo2.py:137][0m Total time:      36.34 min
[32m[20221208 15:12:55 @agent_ppo2.py:139][0m 2951168 total steps have happened
[32m[20221208 15:12:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1441 --------------------------#
[32m[20221208 15:12:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |           0.0621 |           3.0068 |         -45.1109 |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |           0.0056 |           2.4839 |         -39.4410 |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |          -0.0228 |           2.3976 |         -40.9618 |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |          -0.0349 |           2.2373 |         -39.9843 |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |          -0.0528 |           2.1667 |         -41.0349 |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |          -0.0619 |           2.0964 |         -42.3186 |
[32m[20221208 15:12:55 @agent_ppo2.py:179][0m |          -0.0590 |           2.0554 |         -41.0057 |
[32m[20221208 15:12:56 @agent_ppo2.py:179][0m |          -0.0656 |           2.0090 |         -42.6877 |
[32m[20221208 15:12:56 @agent_ppo2.py:179][0m |          -0.0697 |           1.9787 |         -42.7725 |
[32m[20221208 15:12:56 @agent_ppo2.py:179][0m |          -0.0717 |           1.9392 |         -42.5746 |
[32m[20221208 15:12:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:12:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.06
[32m[20221208 15:12:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.44
[32m[20221208 15:12:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.88
[32m[20221208 15:12:56 @agent_ppo2.py:137][0m Total time:      36.36 min
[32m[20221208 15:12:56 @agent_ppo2.py:139][0m 2953216 total steps have happened
[32m[20221208 15:12:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1442 --------------------------#
[32m[20221208 15:12:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |           0.0898 |           0.7638 |         -44.1975 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |           0.0582 |           0.6868 |         -32.7574 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |           0.0094 |           0.6654 |         -40.4147 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0183 |           0.6541 |         -43.8941 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0300 |           0.6435 |         -44.6371 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0378 |           0.6382 |         -45.7788 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0405 |           0.6338 |         -45.3831 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0463 |           0.6265 |         -47.0980 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0461 |           0.6235 |         -48.1648 |
[32m[20221208 15:12:57 @agent_ppo2.py:179][0m |          -0.0489 |           0.6197 |         -48.5018 |
[32m[20221208 15:12:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.54
[32m[20221208 15:12:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.46
[32m[20221208 15:12:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.19
[32m[20221208 15:12:58 @agent_ppo2.py:137][0m Total time:      36.39 min
[32m[20221208 15:12:58 @agent_ppo2.py:139][0m 2955264 total steps have happened
[32m[20221208 15:12:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1443 --------------------------#
[32m[20221208 15:12:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |           0.0628 |           4.7607 |         -53.3952 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |           0.0136 |           4.2855 |         -49.8745 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |          -0.0272 |           4.1244 |         -51.7551 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |          -0.0494 |           3.9953 |         -53.6857 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |          -0.0570 |           3.9273 |         -55.0347 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |          -0.0630 |           3.8781 |         -55.0119 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |          -0.0695 |           3.8033 |         -55.0778 |
[32m[20221208 15:12:58 @agent_ppo2.py:179][0m |          -0.0732 |           3.7393 |         -55.8793 |
[32m[20221208 15:12:59 @agent_ppo2.py:179][0m |          -0.0733 |           3.7301 |         -56.5446 |
[32m[20221208 15:12:59 @agent_ppo2.py:179][0m |          -0.0755 |           3.6646 |         -57.2540 |
[32m[20221208 15:12:59 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:12:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.33
[32m[20221208 15:12:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.00
[32m[20221208 15:12:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.75
[32m[20221208 15:12:59 @agent_ppo2.py:137][0m Total time:      36.41 min
[32m[20221208 15:12:59 @agent_ppo2.py:139][0m 2957312 total steps have happened
[32m[20221208 15:12:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1444 --------------------------#
[32m[20221208 15:12:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:12:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |           0.0722 |           1.9272 |         -42.1811 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |           0.0646 |           1.6762 |         -29.0285 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0124 |           1.5795 |         -31.2355 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0366 |           1.5052 |         -33.9182 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0522 |           1.4705 |         -35.6308 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0607 |           1.4274 |         -36.6349 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0653 |           1.3975 |         -37.8967 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0682 |           1.3647 |         -38.3790 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0729 |           1.3355 |         -39.5487 |
[32m[20221208 15:13:00 @agent_ppo2.py:179][0m |          -0.0752 |           1.3223 |         -40.3961 |
[32m[20221208 15:13:00 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.75
[32m[20221208 15:13:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.36
[32m[20221208 15:13:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.35
[32m[20221208 15:13:00 @agent_ppo2.py:137][0m Total time:      36.44 min
[32m[20221208 15:13:00 @agent_ppo2.py:139][0m 2959360 total steps have happened
[32m[20221208 15:13:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1445 --------------------------#
[32m[20221208 15:13:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |           0.0768 |           3.0934 |         -53.9839 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0163 |           2.6383 |         -50.7096 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0496 |           2.4353 |         -54.2630 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0673 |           2.3655 |         -54.0916 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0745 |           2.2617 |         -55.0939 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0851 |           2.1750 |         -55.2999 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0898 |           2.1205 |         -57.0684 |
[32m[20221208 15:13:01 @agent_ppo2.py:179][0m |          -0.0941 |           2.0985 |         -57.4068 |
[32m[20221208 15:13:02 @agent_ppo2.py:179][0m |          -0.0970 |           2.0598 |         -58.6442 |
[32m[20221208 15:13:02 @agent_ppo2.py:179][0m |          -0.0994 |           2.0093 |         -59.2468 |
[32m[20221208 15:13:02 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.41
[32m[20221208 15:13:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.00
[32m[20221208 15:13:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.97
[32m[20221208 15:13:02 @agent_ppo2.py:137][0m Total time:      36.46 min
[32m[20221208 15:13:02 @agent_ppo2.py:139][0m 2961408 total steps have happened
[32m[20221208 15:13:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1446 --------------------------#
[32m[20221208 15:13:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |           0.0563 |           2.2922 |         -63.9518 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |           0.0381 |           1.7894 |         -53.7393 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0205 |           1.5223 |         -57.4185 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0428 |           1.3590 |         -61.7518 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0546 |           1.2507 |         -63.3420 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0629 |           1.1789 |         -64.3927 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0645 |           1.1148 |         -67.0411 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0681 |           1.0700 |         -68.2478 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0677 |           1.0376 |         -66.3005 |
[32m[20221208 15:13:03 @agent_ppo2.py:179][0m |          -0.0730 |           1.0029 |         -69.0212 |
[32m[20221208 15:13:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.10
[32m[20221208 15:13:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.45
[32m[20221208 15:13:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.99
[32m[20221208 15:13:03 @agent_ppo2.py:137][0m Total time:      36.49 min
[32m[20221208 15:13:03 @agent_ppo2.py:139][0m 2963456 total steps have happened
[32m[20221208 15:13:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1447 --------------------------#
[32m[20221208 15:13:04 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:13:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |           0.0613 |           3.8355 |         -65.8285 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |           0.1222 |           3.2290 |         -48.0990 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0092 |           3.0212 |         -54.7331 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0372 |           2.9025 |         -57.2727 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0478 |           2.8216 |         -58.0011 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0589 |           2.7105 |         -58.3779 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0657 |           2.6785 |         -59.1645 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0736 |           2.6033 |         -59.8404 |
[32m[20221208 15:13:04 @agent_ppo2.py:179][0m |          -0.0738 |           2.5849 |         -62.5795 |
[32m[20221208 15:13:05 @agent_ppo2.py:179][0m |          -0.0742 |           2.5545 |         -61.4342 |
[32m[20221208 15:13:05 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.69
[32m[20221208 15:13:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.51
[32m[20221208 15:13:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.45
[32m[20221208 15:13:05 @agent_ppo2.py:137][0m Total time:      36.51 min
[32m[20221208 15:13:05 @agent_ppo2.py:139][0m 2965504 total steps have happened
[32m[20221208 15:13:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1448 --------------------------#
[32m[20221208 15:13:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |           0.0664 |           2.6946 |         -62.8947 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |           0.0129 |           2.2185 |         -59.5846 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0250 |           2.0547 |         -61.5509 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0484 |           1.9752 |         -64.8960 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0571 |           1.8972 |         -66.5345 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0654 |           1.8433 |         -66.9362 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0701 |           1.7974 |         -68.7553 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0714 |           1.7433 |         -70.1426 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0727 |           1.6893 |         -71.2986 |
[32m[20221208 15:13:06 @agent_ppo2.py:179][0m |          -0.0777 |           1.6675 |         -70.5964 |
[32m[20221208 15:13:06 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.43
[32m[20221208 15:13:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.31
[32m[20221208 15:13:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.63
[32m[20221208 15:13:06 @agent_ppo2.py:137][0m Total time:      36.54 min
[32m[20221208 15:13:06 @agent_ppo2.py:139][0m 2967552 total steps have happened
[32m[20221208 15:13:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1449 --------------------------#
[32m[20221208 15:13:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |           0.0614 |           1.0693 |         -75.2899 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |           0.0711 |           0.9198 |         -56.7911 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |           0.0198 |           0.8703 |         -60.6389 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |          -0.0164 |           0.8489 |         -70.1994 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |          -0.0259 |           0.8398 |         -70.3168 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |          -0.0345 |           0.8345 |         -72.9847 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |          -0.0432 |           0.8219 |         -75.9574 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |          -0.0463 |           0.8158 |         -75.6136 |
[32m[20221208 15:13:07 @agent_ppo2.py:179][0m |          -0.0484 |           0.8130 |         -74.8133 |
[32m[20221208 15:13:08 @agent_ppo2.py:179][0m |          -0.0457 |           0.8100 |         -75.5383 |
[32m[20221208 15:13:08 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.00
[32m[20221208 15:13:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.48
[32m[20221208 15:13:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.17
[32m[20221208 15:13:08 @agent_ppo2.py:137][0m Total time:      36.56 min
[32m[20221208 15:13:08 @agent_ppo2.py:139][0m 2969600 total steps have happened
[32m[20221208 15:13:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1450 --------------------------#
[32m[20221208 15:13:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:08 @agent_ppo2.py:179][0m |           0.0679 |           1.6874 |         -58.8342 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |           0.0233 |           1.2856 |         -35.5625 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0198 |           1.1894 |         -41.7386 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0353 |           1.1454 |         -46.8773 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0454 |           1.0999 |         -47.9559 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0560 |           1.0700 |         -50.1069 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0609 |           1.0452 |         -54.6048 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0629 |           1.0259 |         -57.0882 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0643 |           1.0054 |         -57.0245 |
[32m[20221208 15:13:09 @agent_ppo2.py:179][0m |          -0.0701 |           0.9914 |         -57.1318 |
[32m[20221208 15:13:09 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.02
[32m[20221208 15:13:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.30
[32m[20221208 15:13:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.19
[32m[20221208 15:13:09 @agent_ppo2.py:137][0m Total time:      36.59 min
[32m[20221208 15:13:09 @agent_ppo2.py:139][0m 2971648 total steps have happened
[32m[20221208 15:13:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1451 --------------------------#
[32m[20221208 15:13:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |           0.0661 |           4.1106 |         -73.2417 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |           0.0072 |           3.6722 |         -72.3856 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0275 |           3.4906 |         -76.0497 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0504 |           3.4441 |         -76.4824 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0626 |           3.3354 |         -80.3273 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0680 |           3.3051 |         -80.6534 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0735 |           3.2220 |         -78.5574 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0769 |           3.2089 |         -81.4581 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0805 |           3.1691 |         -81.8924 |
[32m[20221208 15:13:10 @agent_ppo2.py:179][0m |          -0.0822 |           3.1391 |         -80.7903 |
[32m[20221208 15:13:10 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.15
[32m[20221208 15:13:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.23
[32m[20221208 15:13:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.76
[32m[20221208 15:13:11 @agent_ppo2.py:137][0m Total time:      36.61 min
[32m[20221208 15:13:11 @agent_ppo2.py:139][0m 2973696 total steps have happened
[32m[20221208 15:13:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1452 --------------------------#
[32m[20221208 15:13:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:11 @agent_ppo2.py:179][0m |           0.0545 |           2.9163 |         -67.3857 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |           0.0081 |           2.5604 |         -57.5974 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0243 |           2.4286 |         -59.5701 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0653 |           2.3261 |         -63.0396 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0722 |           2.2704 |         -65.8586 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0825 |           2.2336 |         -67.6751 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0898 |           2.1885 |         -71.5086 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0941 |           2.1326 |         -72.7818 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0951 |           2.1086 |         -72.4771 |
[32m[20221208 15:13:12 @agent_ppo2.py:179][0m |          -0.0960 |           2.0977 |         -73.5350 |
[32m[20221208 15:13:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:13:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.61
[32m[20221208 15:13:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.31
[32m[20221208 15:13:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.94
[32m[20221208 15:13:12 @agent_ppo2.py:137][0m Total time:      36.64 min
[32m[20221208 15:13:12 @agent_ppo2.py:139][0m 2975744 total steps have happened
[32m[20221208 15:13:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1453 --------------------------#
[32m[20221208 15:13:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |           0.0658 |           0.9873 |         -83.0109 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |           0.0566 |           0.8472 |         -80.5202 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |           0.0110 |           0.8090 |         -85.7829 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0061 |           0.7871 |         -88.1728 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0270 |           0.7713 |         -91.7193 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0141 |           0.7638 |         -90.6211 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0306 |           0.7561 |         -93.5279 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0317 |           0.7508 |         -93.5555 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0381 |           0.7469 |         -95.0843 |
[32m[20221208 15:13:13 @agent_ppo2.py:179][0m |          -0.0480 |           0.7427 |         -98.6535 |
[32m[20221208 15:13:13 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.95
[32m[20221208 15:13:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 43.40
[32m[20221208 15:13:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.55
[32m[20221208 15:13:14 @agent_ppo2.py:137][0m Total time:      36.66 min
[32m[20221208 15:13:14 @agent_ppo2.py:139][0m 2977792 total steps have happened
[32m[20221208 15:13:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1454 --------------------------#
[32m[20221208 15:13:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:14 @agent_ppo2.py:179][0m |           0.0907 |           1.0095 |         -76.9463 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |           0.1415 |           0.8707 |         -38.8968 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |           0.0526 |           0.8120 |         -50.9966 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |           0.0197 |           0.7739 |         -59.1813 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |           0.0017 |           0.7521 |         -63.9659 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |          -0.0012 |           0.7337 |         -67.8099 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |          -0.0163 |           0.7230 |         -72.1323 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |          -0.0286 |           0.7075 |         -77.1973 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |          -0.0105 |           0.7011 |         -74.9926 |
[32m[20221208 15:13:15 @agent_ppo2.py:179][0m |          -0.0357 |           0.6888 |         -80.2078 |
[32m[20221208 15:13:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.39
[32m[20221208 15:13:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.95
[32m[20221208 15:13:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.52
[32m[20221208 15:13:15 @agent_ppo2.py:137][0m Total time:      36.69 min
[32m[20221208 15:13:15 @agent_ppo2.py:139][0m 2979840 total steps have happened
[32m[20221208 15:13:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1455 --------------------------#
[32m[20221208 15:13:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:13:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |           0.0741 |           1.6544 |         -78.1812 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |           0.0233 |           1.3138 |         -76.3371 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0296 |           1.2108 |         -79.0697 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0577 |           1.1582 |         -80.9170 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0714 |           1.1124 |         -82.8919 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0803 |           1.0824 |         -83.5762 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0861 |           1.0546 |         -85.4174 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0899 |           1.0403 |         -85.5211 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0947 |           1.0237 |         -86.9097 |
[32m[20221208 15:13:16 @agent_ppo2.py:179][0m |          -0.0985 |           1.0011 |         -87.9243 |
[32m[20221208 15:13:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.78
[32m[20221208 15:13:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.72
[32m[20221208 15:13:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.36
[32m[20221208 15:13:17 @agent_ppo2.py:137][0m Total time:      36.71 min
[32m[20221208 15:13:17 @agent_ppo2.py:139][0m 2981888 total steps have happened
[32m[20221208 15:13:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1456 --------------------------#
[32m[20221208 15:13:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:17 @agent_ppo2.py:179][0m |           0.0595 |           2.0072 |         -79.9190 |
[32m[20221208 15:13:17 @agent_ppo2.py:179][0m |           0.0182 |           1.7702 |         -77.9374 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0105 |           1.5954 |         -83.2261 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0255 |           1.5381 |         -83.6514 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0374 |           1.4731 |         -86.9237 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0397 |           1.4213 |         -86.5770 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0460 |           1.3778 |         -88.6143 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0485 |           1.3454 |         -88.0468 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0456 |           1.3011 |         -86.7229 |
[32m[20221208 15:13:18 @agent_ppo2.py:179][0m |          -0.0541 |           1.2880 |         -89.8150 |
[32m[20221208 15:13:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.30
[32m[20221208 15:13:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 57.57
[32m[20221208 15:13:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.32
[32m[20221208 15:13:18 @agent_ppo2.py:137][0m Total time:      36.74 min
[32m[20221208 15:13:18 @agent_ppo2.py:139][0m 2983936 total steps have happened
[32m[20221208 15:13:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1457 --------------------------#
[32m[20221208 15:13:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |           0.0736 |           3.8584 |         -93.5624 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |           0.0138 |           3.2786 |         -84.1839 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0211 |           3.0887 |         -83.3251 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0489 |           2.9718 |         -86.6162 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0577 |           2.8987 |         -89.2628 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0776 |           2.8010 |         -91.8702 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0814 |           2.7378 |         -92.3763 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0888 |           2.6480 |         -91.8812 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0920 |           2.6305 |         -93.8924 |
[32m[20221208 15:13:19 @agent_ppo2.py:179][0m |          -0.0931 |           2.5789 |         -92.3782 |
[32m[20221208 15:13:19 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.64
[32m[20221208 15:13:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.45
[32m[20221208 15:13:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.39
[32m[20221208 15:13:20 @agent_ppo2.py:137][0m Total time:      36.76 min
[32m[20221208 15:13:20 @agent_ppo2.py:139][0m 2985984 total steps have happened
[32m[20221208 15:13:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1458 --------------------------#
[32m[20221208 15:13:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:20 @agent_ppo2.py:179][0m |           0.0636 |           2.5736 |         -84.4348 |
[32m[20221208 15:13:20 @agent_ppo2.py:179][0m |           0.0124 |           1.8288 |         -76.7043 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0256 |           1.5476 |         -81.4599 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0401 |           1.4229 |         -82.7853 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0521 |           1.3102 |         -82.9265 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0498 |           1.2446 |         -81.7949 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0612 |           1.2017 |         -82.8951 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0643 |           1.1688 |         -82.8914 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0673 |           1.1305 |         -83.9171 |
[32m[20221208 15:13:21 @agent_ppo2.py:179][0m |          -0.0684 |           1.1123 |         -84.6924 |
[32m[20221208 15:13:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.10
[32m[20221208 15:13:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.46
[32m[20221208 15:13:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.49
[32m[20221208 15:13:21 @agent_ppo2.py:137][0m Total time:      36.78 min
[32m[20221208 15:13:21 @agent_ppo2.py:139][0m 2988032 total steps have happened
[32m[20221208 15:13:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1459 --------------------------#
[32m[20221208 15:13:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |           0.0667 |           2.9697 |         -74.0232 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |           0.0318 |           2.6247 |         -59.0689 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0172 |           2.5005 |         -67.6113 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0363 |           2.4082 |         -71.5875 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0504 |           2.3450 |         -77.0706 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0572 |           2.2867 |         -78.0498 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0615 |           2.2485 |         -81.3655 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0653 |           2.2079 |         -81.7853 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0684 |           2.2042 |         -85.8734 |
[32m[20221208 15:13:22 @agent_ppo2.py:179][0m |          -0.0711 |           2.1575 |         -84.4493 |
[32m[20221208 15:13:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.22
[32m[20221208 15:13:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.32
[32m[20221208 15:13:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.93
[32m[20221208 15:13:23 @agent_ppo2.py:137][0m Total time:      36.81 min
[32m[20221208 15:13:23 @agent_ppo2.py:139][0m 2990080 total steps have happened
[32m[20221208 15:13:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1460 --------------------------#
[32m[20221208 15:13:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:23 @agent_ppo2.py:179][0m |           0.0775 |           1.6809 |         -66.4560 |
[32m[20221208 15:13:23 @agent_ppo2.py:179][0m |           0.0501 |           1.3520 |         -23.6377 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |           0.0055 |           1.1997 |         -37.4682 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0158 |           1.1149 |         -45.4933 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0332 |           1.0610 |         -52.4102 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0367 |           1.0290 |         -58.9242 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0386 |           0.9966 |         -59.1724 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0460 |           0.9653 |         -63.6687 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0481 |           0.9511 |         -65.9682 |
[32m[20221208 15:13:24 @agent_ppo2.py:179][0m |          -0.0532 |           0.9380 |         -66.0396 |
[32m[20221208 15:13:24 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.70
[32m[20221208 15:13:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.18
[32m[20221208 15:13:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.59
[32m[20221208 15:13:24 @agent_ppo2.py:137][0m Total time:      36.83 min
[32m[20221208 15:13:24 @agent_ppo2.py:139][0m 2992128 total steps have happened
[32m[20221208 15:13:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1461 --------------------------#
[32m[20221208 15:13:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |           0.0903 |           3.8258 |         -84.7108 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |           0.0543 |           3.0162 |         -71.9520 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0116 |           2.7714 |         -78.7293 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0338 |           2.6247 |         -83.3677 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0506 |           2.4989 |         -85.9666 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0593 |           2.4279 |         -91.1950 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0619 |           2.3281 |         -93.8078 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0692 |           2.2795 |         -95.6459 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0721 |           2.2189 |         -97.0143 |
[32m[20221208 15:13:25 @agent_ppo2.py:179][0m |          -0.0763 |           2.1703 |         -98.8313 |
[32m[20221208 15:13:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 15:13:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.57
[32m[20221208 15:13:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.79
[32m[20221208 15:13:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.76
[32m[20221208 15:13:26 @agent_ppo2.py:137][0m Total time:      36.86 min
[32m[20221208 15:13:26 @agent_ppo2.py:139][0m 2994176 total steps have happened
[32m[20221208 15:13:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1462 --------------------------#
[32m[20221208 15:13:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:26 @agent_ppo2.py:179][0m |           0.0795 |           6.7759 |         -76.5104 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |           0.0231 |           6.0513 |         -67.2863 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0180 |           5.8113 |         -79.1059 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0495 |           5.5699 |         -81.3810 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0668 |           5.4723 |         -84.6740 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0729 |           5.3896 |         -84.6063 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0821 |           5.2495 |         -87.7269 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0864 |           5.1786 |         -90.4351 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0927 |           5.1009 |         -92.8717 |
[32m[20221208 15:13:27 @agent_ppo2.py:179][0m |          -0.0941 |           5.0129 |         -92.7713 |
[32m[20221208 15:13:27 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 15:13:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.34
[32m[20221208 15:13:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.47
[32m[20221208 15:13:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.77
[32m[20221208 15:13:27 @agent_ppo2.py:137][0m Total time:      36.89 min
[32m[20221208 15:13:27 @agent_ppo2.py:139][0m 2996224 total steps have happened
[32m[20221208 15:13:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1463 --------------------------#
[32m[20221208 15:13:28 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 15:13:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |           0.0711 |           2.9937 |         -95.0096 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |           0.0041 |           2.3772 |         -86.9185 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |          -0.0469 |           2.1730 |         -90.2776 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |          -0.0679 |           2.0315 |         -94.9160 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |          -0.0806 |           1.9560 |         -96.2169 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |          -0.0867 |           1.8849 |         -98.2559 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |          -0.0907 |           1.8207 |         -99.2439 |
[32m[20221208 15:13:28 @agent_ppo2.py:179][0m |          -0.0961 |           1.7802 |         -99.4378 |
[32m[20221208 15:13:29 @agent_ppo2.py:179][0m |          -0.0970 |           1.7358 |         -98.1424 |
[32m[20221208 15:13:29 @agent_ppo2.py:179][0m |          -0.0999 |           1.6941 |        -102.5932 |
[32m[20221208 15:13:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:13:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.91
[32m[20221208 15:13:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.02
[32m[20221208 15:13:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.68
[32m[20221208 15:13:29 @agent_ppo2.py:137][0m Total time:      36.91 min
[32m[20221208 15:13:29 @agent_ppo2.py:139][0m 2998272 total steps have happened
[32m[20221208 15:13:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1464 --------------------------#
[32m[20221208 15:13:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |           0.0687 |           3.1712 |        -101.2728 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |           0.0330 |           2.7531 |         -86.3720 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0237 |           2.5013 |         -96.8835 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0411 |           2.3387 |        -102.9230 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0458 |           2.2295 |        -103.1330 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0617 |           2.1554 |        -105.0487 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0658 |           2.1101 |        -105.1406 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0681 |           2.0699 |        -106.0826 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0659 |           2.0016 |        -104.9469 |
[32m[20221208 15:13:30 @agent_ppo2.py:179][0m |          -0.0745 |           1.9631 |        -108.3541 |
[32m[20221208 15:13:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 15:13:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.83
[32m[20221208 15:13:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.09
[32m[20221208 15:13:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.64
[32m[20221208 15:13:31 @agent_ppo2.py:137][0m Total time:      36.94 min
[32m[20221208 15:13:31 @agent_ppo2.py:139][0m 3000320 total steps have happened
[32m[20221208 15:13:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1465 --------------------------#
[32m[20221208 15:13:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |           0.0917 |           2.6106 |         -99.2533 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |           0.0315 |           1.8970 |         -85.8880 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |          -0.0209 |           1.7030 |         -92.4349 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |          -0.0488 |           1.5844 |        -100.2961 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |          -0.0577 |           1.4965 |        -105.5886 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |          -0.0682 |           1.4024 |        -108.5674 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |          -0.0749 |           1.3430 |        -108.9261 |
[32m[20221208 15:13:31 @agent_ppo2.py:179][0m |          -0.0782 |           1.2972 |        -111.2265 |
[32m[20221208 15:13:32 @agent_ppo2.py:179][0m |          -0.0823 |           1.2520 |        -111.8285 |
[32m[20221208 15:13:32 @agent_ppo2.py:179][0m |          -0.0825 |           1.2132 |        -111.8099 |
[32m[20221208 15:13:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.20
[32m[20221208 15:13:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.50
[32m[20221208 15:13:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.13
[32m[20221208 15:13:32 @agent_ppo2.py:137][0m Total time:      36.96 min
[32m[20221208 15:13:32 @agent_ppo2.py:139][0m 3002368 total steps have happened
[32m[20221208 15:13:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1466 --------------------------#
[32m[20221208 15:13:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |           0.0974 |           4.2476 |         -88.5691 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |           0.0576 |           3.8960 |         -84.3924 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |           0.0172 |           3.7879 |        -102.8034 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0080 |           3.8253 |        -101.0537 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0209 |           3.7374 |        -105.4011 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0361 |           3.6647 |        -108.6592 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0337 |           3.6428 |        -105.0918 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0393 |           3.5432 |        -107.5794 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0243 |           3.5321 |        -105.9485 |
[32m[20221208 15:13:33 @agent_ppo2.py:179][0m |          -0.0263 |           3.4908 |        -102.7781 |
[32m[20221208 15:13:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.69
[32m[20221208 15:13:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 155.48
[32m[20221208 15:13:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.83
[32m[20221208 15:13:34 @agent_ppo2.py:137][0m Total time:      36.99 min
[32m[20221208 15:13:34 @agent_ppo2.py:139][0m 3004416 total steps have happened
[32m[20221208 15:13:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1467 --------------------------#
[32m[20221208 15:13:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |           0.0779 |           3.6386 |        -103.6507 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |           0.1887 |           3.0938 |         -86.9421 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |          -0.0018 |           2.8539 |         -81.2489 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |          -0.0219 |           2.6755 |         -81.5555 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |          -0.0337 |           2.5641 |         -74.4337 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |          -0.0436 |           2.4395 |         -76.5382 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |          -0.0486 |           2.3741 |         -76.5938 |
[32m[20221208 15:13:34 @agent_ppo2.py:179][0m |          -0.0428 |           2.3224 |         -75.6911 |
[32m[20221208 15:13:35 @agent_ppo2.py:179][0m |          -0.0478 |           2.2382 |         -75.8384 |
[32m[20221208 15:13:35 @agent_ppo2.py:179][0m |          -0.0529 |           2.2131 |         -77.6041 |
[32m[20221208 15:13:35 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.86
[32m[20221208 15:13:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.79
[32m[20221208 15:13:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.50
[32m[20221208 15:13:35 @agent_ppo2.py:137][0m Total time:      37.01 min
[32m[20221208 15:13:35 @agent_ppo2.py:139][0m 3006464 total steps have happened
[32m[20221208 15:13:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1468 --------------------------#
[32m[20221208 15:13:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |           0.1027 |           1.5502 |         -86.9735 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |           0.1500 |           1.2184 |         -64.2692 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0042 |           1.1136 |         -75.2726 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0315 |           1.0556 |         -81.4704 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0452 |           0.9979 |         -85.0043 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0489 |           0.9671 |         -85.7893 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0580 |           0.9380 |         -88.1409 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0615 |           0.9213 |         -91.6307 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0650 |           0.8952 |         -91.8129 |
[32m[20221208 15:13:36 @agent_ppo2.py:179][0m |          -0.0694 |           0.8785 |         -93.4960 |
[32m[20221208 15:13:36 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.52
[32m[20221208 15:13:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.66
[32m[20221208 15:13:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.52
[32m[20221208 15:13:36 @agent_ppo2.py:137][0m Total time:      37.04 min
[32m[20221208 15:13:36 @agent_ppo2.py:139][0m 3008512 total steps have happened
[32m[20221208 15:13:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1469 --------------------------#
[32m[20221208 15:13:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |           0.1207 |           1.6135 |         -77.1536 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |           0.1763 |           1.3368 |         -53.8201 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |           0.0830 |           1.2364 |         -52.4723 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |           0.0262 |           1.2225 |         -46.8834 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |          -0.0062 |           1.1533 |         -56.1390 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |          -0.0178 |           1.1539 |         -58.7317 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |          -0.0269 |           1.1202 |         -61.3712 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |          -0.0286 |           1.0978 |         -60.7776 |
[32m[20221208 15:13:37 @agent_ppo2.py:179][0m |          -0.0384 |           1.0798 |         -65.9174 |
[32m[20221208 15:13:38 @agent_ppo2.py:179][0m |          -0.0336 |           1.1134 |         -63.9654 |
[32m[20221208 15:13:38 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:13:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.78
[32m[20221208 15:13:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.00
[32m[20221208 15:13:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.92
[32m[20221208 15:13:38 @agent_ppo2.py:137][0m Total time:      37.06 min
[32m[20221208 15:13:38 @agent_ppo2.py:139][0m 3010560 total steps have happened
[32m[20221208 15:13:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1470 --------------------------#
[32m[20221208 15:13:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:38 @agent_ppo2.py:179][0m |           0.0467 |           2.4598 |        -108.9404 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |           0.0197 |           1.8359 |         -94.8880 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |           0.0164 |           1.6475 |         -86.8278 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0057 |           1.5534 |         -81.2128 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0351 |           1.4828 |         -88.5864 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0471 |           1.4608 |         -91.0884 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0482 |           1.4100 |         -88.6458 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0512 |           1.3595 |         -89.8080 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0573 |           1.3548 |         -91.1732 |
[32m[20221208 15:13:39 @agent_ppo2.py:179][0m |          -0.0616 |           1.3426 |         -93.6977 |
[32m[20221208 15:13:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:13:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.75
[32m[20221208 15:13:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.91
[32m[20221208 15:13:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.28
[32m[20221208 15:13:39 @agent_ppo2.py:137][0m Total time:      37.09 min
[32m[20221208 15:13:39 @agent_ppo2.py:139][0m 3012608 total steps have happened
[32m[20221208 15:13:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1471 --------------------------#
[32m[20221208 15:13:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |           0.0765 |           0.8187 |         -91.3117 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |           0.0646 |           0.7784 |         -77.6370 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |           0.0046 |           0.7573 |         -86.3138 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0249 |           0.7364 |         -94.7314 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0317 |           0.7277 |         -96.6975 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0439 |           0.7149 |        -100.7085 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0446 |           0.7099 |        -103.2922 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0511 |           0.7015 |        -101.9733 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0557 |           0.6958 |        -103.2126 |
[32m[20221208 15:13:40 @agent_ppo2.py:179][0m |          -0.0602 |           0.6910 |        -105.9095 |
[32m[20221208 15:13:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.27
[32m[20221208 15:13:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.85
[32m[20221208 15:13:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.47
[32m[20221208 15:13:41 @agent_ppo2.py:137][0m Total time:      37.11 min
[32m[20221208 15:13:41 @agent_ppo2.py:139][0m 3014656 total steps have happened
[32m[20221208 15:13:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1472 --------------------------#
[32m[20221208 15:13:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:41 @agent_ppo2.py:179][0m |           0.0502 |           3.3934 |        -101.9830 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |           0.0058 |           2.8297 |         -92.6404 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0152 |           2.6212 |         -89.4392 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0505 |           2.4998 |         -91.6343 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0512 |           2.4476 |         -93.2651 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0610 |           2.3421 |         -92.7280 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0645 |           2.2901 |         -91.0990 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0646 |           2.2162 |         -93.2119 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0637 |           2.1708 |         -89.5188 |
[32m[20221208 15:13:42 @agent_ppo2.py:179][0m |          -0.0686 |           2.1630 |         -90.4228 |
[32m[20221208 15:13:42 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.08
[32m[20221208 15:13:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 139.39
[32m[20221208 15:13:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 53.37
[32m[20221208 15:13:42 @agent_ppo2.py:137][0m Total time:      37.14 min
[32m[20221208 15:13:42 @agent_ppo2.py:139][0m 3016704 total steps have happened
[32m[20221208 15:13:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1473 --------------------------#
[32m[20221208 15:13:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |           0.0640 |           3.7232 |        -103.0853 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |           0.0401 |           3.1310 |         -87.4472 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0152 |           2.9603 |         -93.3242 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0502 |           2.8454 |         -99.0977 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0661 |           2.7547 |         -99.6072 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0740 |           2.6799 |        -100.1209 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0808 |           2.6372 |        -101.2064 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0857 |           2.6005 |        -102.2924 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0882 |           2.5596 |        -100.1869 |
[32m[20221208 15:13:43 @agent_ppo2.py:179][0m |          -0.0932 |           2.5126 |        -100.7582 |
[32m[20221208 15:13:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:13:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.81
[32m[20221208 15:13:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 124.83
[32m[20221208 15:13:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.04
[32m[20221208 15:13:44 @agent_ppo2.py:137][0m Total time:      37.16 min
[32m[20221208 15:13:44 @agent_ppo2.py:139][0m 3018752 total steps have happened
[32m[20221208 15:13:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1474 --------------------------#
[32m[20221208 15:13:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:44 @agent_ppo2.py:179][0m |           0.0454 |           2.9423 |         -94.0522 |
[32m[20221208 15:13:44 @agent_ppo2.py:179][0m |           0.0028 |           2.2827 |         -94.8232 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0302 |           1.9818 |        -100.1867 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0514 |           1.7971 |        -103.6322 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0583 |           1.6683 |        -105.2142 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0671 |           1.5868 |        -106.5706 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0703 |           1.5107 |        -107.1439 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0741 |           1.4547 |        -107.6247 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0772 |           1.4096 |        -109.1994 |
[32m[20221208 15:13:45 @agent_ppo2.py:179][0m |          -0.0770 |           1.3523 |        -109.6171 |
[32m[20221208 15:13:45 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.82
[32m[20221208 15:13:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.35
[32m[20221208 15:13:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.46
[32m[20221208 15:13:45 @agent_ppo2.py:137][0m Total time:      37.19 min
[32m[20221208 15:13:45 @agent_ppo2.py:139][0m 3020800 total steps have happened
[32m[20221208 15:13:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1475 --------------------------#
[32m[20221208 15:13:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |           0.0615 |           2.5269 |         -98.2721 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |           0.0223 |           1.9538 |         -78.6354 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0244 |           1.8130 |         -79.2888 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0463 |           1.7405 |         -82.6538 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0563 |           1.6620 |         -86.3889 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0670 |           1.6269 |         -88.1787 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0698 |           1.5784 |         -89.7028 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0710 |           1.5427 |         -89.3229 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0730 |           1.5245 |         -91.4570 |
[32m[20221208 15:13:46 @agent_ppo2.py:179][0m |          -0.0735 |           1.4922 |         -92.3599 |
[32m[20221208 15:13:46 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.26
[32m[20221208 15:13:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.94
[32m[20221208 15:13:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.26
[32m[20221208 15:13:47 @agent_ppo2.py:137][0m Total time:      37.21 min
[32m[20221208 15:13:47 @agent_ppo2.py:139][0m 3022848 total steps have happened
[32m[20221208 15:13:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1476 --------------------------#
[32m[20221208 15:13:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:47 @agent_ppo2.py:179][0m |           0.0684 |           1.8991 |        -103.9144 |
[32m[20221208 15:13:47 @agent_ppo2.py:179][0m |           0.0901 |           1.5082 |         -74.9802 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |           0.0165 |           1.3720 |         -85.1496 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0207 |           1.2808 |         -93.0797 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0248 |           1.2399 |         -98.7232 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0354 |           1.1828 |         -99.4326 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0412 |           1.1302 |        -102.8976 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0446 |           1.0999 |        -106.3329 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0423 |           1.0809 |        -103.7696 |
[32m[20221208 15:13:48 @agent_ppo2.py:179][0m |          -0.0475 |           1.0535 |        -107.0739 |
[32m[20221208 15:13:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.49
[32m[20221208 15:13:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.87
[32m[20221208 15:13:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.16
[32m[20221208 15:13:48 @agent_ppo2.py:137][0m Total time:      37.23 min
[32m[20221208 15:13:48 @agent_ppo2.py:139][0m 3024896 total steps have happened
[32m[20221208 15:13:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1477 --------------------------#
[32m[20221208 15:13:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |           0.0768 |           0.9136 |        -124.1624 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |           0.0588 |           0.8162 |        -101.9756 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0007 |           0.7899 |        -103.9185 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0118 |           0.7752 |        -102.6968 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0444 |           0.7615 |        -114.0126 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0563 |           0.7463 |        -119.9395 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0596 |           0.7394 |        -121.6952 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0652 |           0.7295 |        -125.2844 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0702 |           0.7208 |        -129.1514 |
[32m[20221208 15:13:49 @agent_ppo2.py:179][0m |          -0.0747 |           0.7154 |        -127.2379 |
[32m[20221208 15:13:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 40.70
[32m[20221208 15:13:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.83
[32m[20221208 15:13:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.81
[32m[20221208 15:13:50 @agent_ppo2.py:137][0m Total time:      37.26 min
[32m[20221208 15:13:50 @agent_ppo2.py:139][0m 3026944 total steps have happened
[32m[20221208 15:13:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1478 --------------------------#
[32m[20221208 15:13:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:50 @agent_ppo2.py:179][0m |           0.0666 |           3.1264 |        -114.3090 |
[32m[20221208 15:13:50 @agent_ppo2.py:179][0m |           0.0591 |           2.2836 |        -100.7574 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0187 |           2.1185 |        -101.5460 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0397 |           2.1052 |        -102.8615 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0581 |           2.0054 |        -105.2485 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0696 |           1.9376 |        -107.0961 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0746 |           1.9189 |        -107.9408 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0837 |           1.9477 |        -109.6469 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0860 |           1.8845 |        -109.1868 |
[32m[20221208 15:13:51 @agent_ppo2.py:179][0m |          -0.0919 |           1.8903 |        -112.0709 |
[32m[20221208 15:13:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.72
[32m[20221208 15:13:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.97
[32m[20221208 15:13:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.67
[32m[20221208 15:13:51 @agent_ppo2.py:137][0m Total time:      37.28 min
[32m[20221208 15:13:51 @agent_ppo2.py:139][0m 3028992 total steps have happened
[32m[20221208 15:13:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1479 --------------------------#
[32m[20221208 15:13:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |           0.0526 |           1.8501 |        -114.6788 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |           0.0136 |           1.5365 |        -119.3759 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |           0.0031 |           1.4799 |        -114.9355 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0256 |           1.4285 |        -115.9813 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0404 |           1.3925 |        -122.1083 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0446 |           1.3876 |        -122.4182 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0444 |           1.3603 |        -122.6365 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0456 |           1.3317 |        -121.6723 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0542 |           1.3106 |        -122.5975 |
[32m[20221208 15:13:52 @agent_ppo2.py:179][0m |          -0.0564 |           1.2961 |        -122.8749 |
[32m[20221208 15:13:52 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.01
[32m[20221208 15:13:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.36
[32m[20221208 15:13:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.77
[32m[20221208 15:13:53 @agent_ppo2.py:137][0m Total time:      37.31 min
[32m[20221208 15:13:53 @agent_ppo2.py:139][0m 3031040 total steps have happened
[32m[20221208 15:13:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1480 --------------------------#
[32m[20221208 15:13:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:53 @agent_ppo2.py:179][0m |           0.0771 |           1.8302 |        -121.0257 |
[32m[20221208 15:13:53 @agent_ppo2.py:179][0m |           0.0776 |           1.5391 |         -96.8960 |
[32m[20221208 15:13:53 @agent_ppo2.py:179][0m |           0.0077 |           1.4298 |        -106.2285 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0234 |           1.3680 |        -115.7579 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0345 |           1.3511 |        -118.7736 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0433 |           1.2608 |        -121.2102 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0457 |           1.2280 |        -123.2777 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0420 |           1.2084 |        -123.2855 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0483 |           1.1733 |        -123.2061 |
[32m[20221208 15:13:54 @agent_ppo2.py:179][0m |          -0.0509 |           1.1678 |        -125.6227 |
[32m[20221208 15:13:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.34
[32m[20221208 15:13:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.37
[32m[20221208 15:13:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.45
[32m[20221208 15:13:54 @agent_ppo2.py:137][0m Total time:      37.33 min
[32m[20221208 15:13:54 @agent_ppo2.py:139][0m 3033088 total steps have happened
[32m[20221208 15:13:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1481 --------------------------#
[32m[20221208 15:13:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.0581 |           2.6923 |        -114.1298 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.1512 |           2.1582 |         -81.1164 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.0952 |           1.9962 |         -48.5808 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.0649 |           1.9463 |         -51.7990 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.0444 |           1.9177 |         -60.5425 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.0111 |           1.8520 |         -66.5294 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |          -0.0011 |           1.8257 |         -72.1614 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |          -0.0040 |           1.8002 |         -75.0612 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |          -0.0032 |           1.7900 |         -72.5564 |
[32m[20221208 15:13:55 @agent_ppo2.py:179][0m |           0.0020 |           1.7901 |         -77.4318 |
[32m[20221208 15:13:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:13:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.17
[32m[20221208 15:13:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.22
[32m[20221208 15:13:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.93
[32m[20221208 15:13:56 @agent_ppo2.py:137][0m Total time:      37.36 min
[32m[20221208 15:13:56 @agent_ppo2.py:139][0m 3035136 total steps have happened
[32m[20221208 15:13:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1482 --------------------------#
[32m[20221208 15:13:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:56 @agent_ppo2.py:179][0m |           0.0951 |           1.9219 |        -111.2706 |
[32m[20221208 15:13:56 @agent_ppo2.py:179][0m |           0.0243 |           1.4746 |         -87.1316 |
[32m[20221208 15:13:56 @agent_ppo2.py:179][0m |          -0.0274 |           1.2968 |         -95.2627 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0471 |           1.1956 |        -100.5671 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0545 |           1.1375 |        -105.3183 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0654 |           1.0839 |        -109.2518 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0705 |           1.0398 |        -109.5647 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0673 |           1.0157 |        -113.0307 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0704 |           0.9917 |        -115.0537 |
[32m[20221208 15:13:57 @agent_ppo2.py:179][0m |          -0.0768 |           0.9550 |        -113.0540 |
[32m[20221208 15:13:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:13:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.72
[32m[20221208 15:13:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.46
[32m[20221208 15:13:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.37
[32m[20221208 15:13:57 @agent_ppo2.py:137][0m Total time:      37.38 min
[32m[20221208 15:13:57 @agent_ppo2.py:139][0m 3037184 total steps have happened
[32m[20221208 15:13:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1483 --------------------------#
[32m[20221208 15:13:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:13:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |           0.0481 |           1.3066 |        -109.7191 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |           0.0274 |           1.0965 |         -97.2916 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |           0.0062 |           1.0469 |         -92.2171 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0164 |           0.9833 |        -102.2757 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0365 |           0.9441 |        -106.5833 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0510 |           0.9179 |        -115.4611 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0564 |           0.9012 |        -118.6941 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0614 |           0.8804 |        -118.1557 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0616 |           0.8728 |        -119.6399 |
[32m[20221208 15:13:58 @agent_ppo2.py:179][0m |          -0.0660 |           0.8505 |        -121.3214 |
[32m[20221208 15:13:58 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:13:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.17
[32m[20221208 15:13:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.75
[32m[20221208 15:13:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.95
[32m[20221208 15:13:59 @agent_ppo2.py:137][0m Total time:      37.41 min
[32m[20221208 15:13:59 @agent_ppo2.py:139][0m 3039232 total steps have happened
[32m[20221208 15:13:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1484 --------------------------#
[32m[20221208 15:13:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:13:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:13:59 @agent_ppo2.py:179][0m |           0.0625 |           1.1357 |        -105.4328 |
[32m[20221208 15:13:59 @agent_ppo2.py:179][0m |           0.0501 |           0.9565 |         -41.9336 |
[32m[20221208 15:13:59 @agent_ppo2.py:179][0m |          -0.0038 |           0.8981 |         -57.5655 |
[32m[20221208 15:13:59 @agent_ppo2.py:179][0m |          -0.0282 |           0.8708 |         -70.5710 |
[32m[20221208 15:14:00 @agent_ppo2.py:179][0m |          -0.0431 |           0.8453 |         -75.0549 |
[32m[20221208 15:14:00 @agent_ppo2.py:179][0m |          -0.0501 |           0.8215 |         -80.6782 |
[32m[20221208 15:14:00 @agent_ppo2.py:179][0m |          -0.0569 |           0.8032 |         -84.1587 |
[32m[20221208 15:14:00 @agent_ppo2.py:179][0m |          -0.0585 |           0.7859 |         -83.0879 |
[32m[20221208 15:14:00 @agent_ppo2.py:179][0m |          -0.0618 |           0.7740 |         -88.4477 |
[32m[20221208 15:14:00 @agent_ppo2.py:179][0m |          -0.0655 |           0.7632 |         -91.0788 |
[32m[20221208 15:14:00 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:14:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.64
[32m[20221208 15:14:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.95
[32m[20221208 15:14:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.00
[32m[20221208 15:14:00 @agent_ppo2.py:137][0m Total time:      37.43 min
[32m[20221208 15:14:00 @agent_ppo2.py:139][0m 3041280 total steps have happened
[32m[20221208 15:14:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1485 --------------------------#
[32m[20221208 15:14:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |           0.1485 |           4.2844 |         -96.3536 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |           0.0634 |           3.6654 |         -83.5236 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |           0.0051 |           3.4102 |         -91.9580 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0292 |           3.3075 |         -98.3629 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0471 |           3.1720 |        -104.5453 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0600 |           3.0702 |        -106.6477 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0700 |           3.0031 |        -110.0185 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0720 |           2.9312 |        -112.5192 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0805 |           2.8513 |        -115.3067 |
[32m[20221208 15:14:01 @agent_ppo2.py:179][0m |          -0.0845 |           2.8127 |        -117.9305 |
[32m[20221208 15:14:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.50
[32m[20221208 15:14:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.56
[32m[20221208 15:14:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 64.97
[32m[20221208 15:14:02 @agent_ppo2.py:137][0m Total time:      37.46 min
[32m[20221208 15:14:02 @agent_ppo2.py:139][0m 3043328 total steps have happened
[32m[20221208 15:14:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1486 --------------------------#
[32m[20221208 15:14:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:02 @agent_ppo2.py:179][0m |           0.0817 |           1.8648 |        -115.4056 |
[32m[20221208 15:14:02 @agent_ppo2.py:179][0m |           0.0187 |           1.4870 |        -105.0364 |
[32m[20221208 15:14:02 @agent_ppo2.py:179][0m |          -0.0247 |           1.3698 |        -101.3408 |
[32m[20221208 15:14:02 @agent_ppo2.py:179][0m |          -0.0468 |           1.2888 |        -111.2035 |
[32m[20221208 15:14:02 @agent_ppo2.py:179][0m |          -0.0600 |           1.2304 |        -118.5868 |
[32m[20221208 15:14:03 @agent_ppo2.py:179][0m |          -0.0673 |           1.1946 |        -122.0555 |
[32m[20221208 15:14:03 @agent_ppo2.py:179][0m |          -0.0710 |           1.1611 |        -126.5082 |
[32m[20221208 15:14:03 @agent_ppo2.py:179][0m |          -0.0753 |           1.1347 |        -127.4706 |
[32m[20221208 15:14:03 @agent_ppo2.py:179][0m |          -0.0781 |           1.1093 |        -133.0039 |
[32m[20221208 15:14:03 @agent_ppo2.py:179][0m |          -0.0781 |           1.0949 |        -134.0074 |
[32m[20221208 15:14:03 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:14:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.26
[32m[20221208 15:14:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.86
[32m[20221208 15:14:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.92
[32m[20221208 15:14:03 @agent_ppo2.py:137][0m Total time:      37.48 min
[32m[20221208 15:14:03 @agent_ppo2.py:139][0m 3045376 total steps have happened
[32m[20221208 15:14:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1487 --------------------------#
[32m[20221208 15:14:04 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:14:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.1256 |           3.4151 |        -102.5910 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.1030 |           3.0623 |         -54.2304 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0506 |           2.9102 |         -63.6906 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0252 |           2.8632 |         -69.8771 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0197 |           2.7999 |         -71.5272 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0087 |           2.7281 |         -72.6560 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0108 |           2.6826 |         -78.7782 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0168 |           2.6382 |         -70.2005 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |          -0.0041 |           2.6163 |         -75.7149 |
[32m[20221208 15:14:04 @agent_ppo2.py:179][0m |           0.0004 |           2.5736 |         -80.8362 |
[32m[20221208 15:14:04 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 15:14:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.38
[32m[20221208 15:14:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.87
[32m[20221208 15:14:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.50
[32m[20221208 15:14:05 @agent_ppo2.py:137][0m Total time:      37.51 min
[32m[20221208 15:14:05 @agent_ppo2.py:139][0m 3047424 total steps have happened
[32m[20221208 15:14:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1488 --------------------------#
[32m[20221208 15:14:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:05 @agent_ppo2.py:179][0m |           0.0477 |           2.3452 |        -124.8951 |
[32m[20221208 15:14:05 @agent_ppo2.py:179][0m |           0.0139 |           1.8798 |        -126.1704 |
[32m[20221208 15:14:05 @agent_ppo2.py:179][0m |          -0.0321 |           1.8040 |        -129.7487 |
[32m[20221208 15:14:05 @agent_ppo2.py:179][0m |          -0.0482 |           1.7051 |        -138.3124 |
[32m[20221208 15:14:05 @agent_ppo2.py:179][0m |          -0.0527 |           1.6740 |        -141.7752 |
[32m[20221208 15:14:05 @agent_ppo2.py:179][0m |          -0.0620 |           1.6590 |        -144.1301 |
[32m[20221208 15:14:06 @agent_ppo2.py:179][0m |          -0.0720 |           1.6000 |        -147.7380 |
[32m[20221208 15:14:06 @agent_ppo2.py:179][0m |          -0.0756 |           1.5861 |        -149.4757 |
[32m[20221208 15:14:06 @agent_ppo2.py:179][0m |          -0.0765 |           1.5778 |        -150.4707 |
[32m[20221208 15:14:06 @agent_ppo2.py:179][0m |          -0.0809 |           1.5496 |        -151.3663 |
[32m[20221208 15:14:06 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.86
[32m[20221208 15:14:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.66
[32m[20221208 15:14:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.98
[32m[20221208 15:14:06 @agent_ppo2.py:137][0m Total time:      37.53 min
[32m[20221208 15:14:06 @agent_ppo2.py:139][0m 3049472 total steps have happened
[32m[20221208 15:14:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1489 --------------------------#
[32m[20221208 15:14:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |           0.0753 |           3.5595 |        -131.4758 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |           0.0305 |           2.9797 |        -123.6815 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0122 |           2.7295 |        -134.4137 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0425 |           2.5987 |        -141.1408 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0489 |           2.5108 |        -145.5446 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0592 |           2.4401 |        -151.1632 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0648 |           2.3287 |        -153.8652 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0733 |           2.2739 |        -154.5572 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0757 |           2.2206 |        -158.4943 |
[32m[20221208 15:14:07 @agent_ppo2.py:179][0m |          -0.0780 |           2.1716 |        -160.7777 |
[32m[20221208 15:14:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.39
[32m[20221208 15:14:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.25
[32m[20221208 15:14:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.68
[32m[20221208 15:14:08 @agent_ppo2.py:137][0m Total time:      37.56 min
[32m[20221208 15:14:08 @agent_ppo2.py:139][0m 3051520 total steps have happened
[32m[20221208 15:14:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1490 --------------------------#
[32m[20221208 15:14:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:08 @agent_ppo2.py:179][0m |           0.0694 |           3.0746 |        -143.8899 |
[32m[20221208 15:14:08 @agent_ppo2.py:179][0m |           0.0412 |           2.7719 |        -129.0535 |
[32m[20221208 15:14:08 @agent_ppo2.py:179][0m |           0.0064 |           2.6487 |        -135.1665 |
[32m[20221208 15:14:08 @agent_ppo2.py:179][0m |          -0.0349 |           2.5319 |        -149.2579 |
[32m[20221208 15:14:08 @agent_ppo2.py:179][0m |          -0.0478 |           2.4553 |        -151.9390 |
[32m[20221208 15:14:08 @agent_ppo2.py:179][0m |          -0.0535 |           2.4087 |        -153.3235 |
[32m[20221208 15:14:09 @agent_ppo2.py:179][0m |          -0.0610 |           2.3495 |        -157.1923 |
[32m[20221208 15:14:09 @agent_ppo2.py:179][0m |          -0.0662 |           2.3256 |        -162.6673 |
[32m[20221208 15:14:09 @agent_ppo2.py:179][0m |          -0.0693 |           2.3109 |        -161.2923 |
[32m[20221208 15:14:09 @agent_ppo2.py:179][0m |          -0.0680 |           2.2501 |        -165.1864 |
[32m[20221208 15:14:09 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.30
[32m[20221208 15:14:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.72
[32m[20221208 15:14:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.93
[32m[20221208 15:14:09 @agent_ppo2.py:137][0m Total time:      37.58 min
[32m[20221208 15:14:09 @agent_ppo2.py:139][0m 3053568 total steps have happened
[32m[20221208 15:14:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1491 --------------------------#
[32m[20221208 15:14:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.1123 |           2.5720 |        -126.8106 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.1059 |           2.2366 |         -90.6112 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.1495 |           2.0497 |         -63.0169 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.0903 |           1.9647 |         -62.7432 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.0575 |           1.8758 |         -82.8512 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.0454 |           1.8187 |        -101.4580 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.0488 |           1.8054 |        -100.4949 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.0252 |           1.7634 |         -93.0683 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |          -0.0087 |           1.7230 |        -115.5241 |
[32m[20221208 15:14:10 @agent_ppo2.py:179][0m |           0.0102 |           1.7167 |        -100.3688 |
[32m[20221208 15:14:10 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:14:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.07
[32m[20221208 15:14:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.10
[32m[20221208 15:14:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.44
[32m[20221208 15:14:11 @agent_ppo2.py:137][0m Total time:      37.61 min
[32m[20221208 15:14:11 @agent_ppo2.py:139][0m 3055616 total steps have happened
[32m[20221208 15:14:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1492 --------------------------#
[32m[20221208 15:14:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:11 @agent_ppo2.py:179][0m |           0.0487 |           2.0911 |        -129.9610 |
[32m[20221208 15:14:11 @agent_ppo2.py:179][0m |           0.1430 |           1.7777 |        -105.6530 |
[32m[20221208 15:14:11 @agent_ppo2.py:179][0m |           0.0176 |           1.6741 |        -106.3889 |
[32m[20221208 15:14:11 @agent_ppo2.py:179][0m |          -0.0287 |           1.5862 |        -118.4136 |
[32m[20221208 15:14:11 @agent_ppo2.py:179][0m |          -0.0452 |           1.5322 |        -126.4219 |
[32m[20221208 15:14:11 @agent_ppo2.py:179][0m |          -0.0544 |           1.4812 |        -131.2436 |
[32m[20221208 15:14:12 @agent_ppo2.py:179][0m |          -0.0606 |           1.4633 |        -133.9303 |
[32m[20221208 15:14:12 @agent_ppo2.py:179][0m |          -0.0640 |           1.3963 |        -134.4497 |
[32m[20221208 15:14:12 @agent_ppo2.py:179][0m |          -0.0685 |           1.3727 |        -139.9063 |
[32m[20221208 15:14:12 @agent_ppo2.py:179][0m |          -0.0729 |           1.3262 |        -141.5492 |
[32m[20221208 15:14:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 15:14:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.17
[32m[20221208 15:14:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.18
[32m[20221208 15:14:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.11
[32m[20221208 15:14:12 @agent_ppo2.py:137][0m Total time:      37.63 min
[32m[20221208 15:14:12 @agent_ppo2.py:139][0m 3057664 total steps have happened
[32m[20221208 15:14:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1493 --------------------------#
[32m[20221208 15:14:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:14:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0711 |           1.2122 |        -129.9072 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0725 |           1.1644 |        -105.2177 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.1212 |           1.1466 |        -106.1069 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0601 |           1.1499 |         -96.8675 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0177 |           1.1244 |        -110.7716 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |          -0.0047 |           1.1268 |        -123.0233 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0410 |           1.1217 |        -118.5270 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0564 |           1.1169 |         -98.1567 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0267 |           1.1139 |        -108.3793 |
[32m[20221208 15:14:13 @agent_ppo2.py:179][0m |           0.0037 |           1.1340 |        -116.3656 |
[32m[20221208 15:14:13 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:14:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.83
[32m[20221208 15:14:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.35
[32m[20221208 15:14:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.40
[32m[20221208 15:14:14 @agent_ppo2.py:137][0m Total time:      37.66 min
[32m[20221208 15:14:14 @agent_ppo2.py:139][0m 3059712 total steps have happened
[32m[20221208 15:14:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1494 --------------------------#
[32m[20221208 15:14:14 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:14:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |           0.0728 |           2.6739 |        -102.0092 |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |           0.0327 |           2.2743 |         -93.8490 |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |           0.0061 |           2.1760 |         -99.3639 |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |          -0.0118 |           2.0686 |        -102.6811 |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |          -0.0200 |           2.0161 |        -105.1301 |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |          -0.0206 |           1.9785 |        -108.1751 |
[32m[20221208 15:14:14 @agent_ppo2.py:179][0m |          -0.0282 |           1.9568 |        -106.7632 |
[32m[20221208 15:14:15 @agent_ppo2.py:179][0m |          -0.0275 |           1.9410 |        -111.9115 |
[32m[20221208 15:14:15 @agent_ppo2.py:179][0m |          -0.0381 |           1.9849 |        -113.6542 |
[32m[20221208 15:14:15 @agent_ppo2.py:179][0m |          -0.0287 |           1.9121 |        -121.9472 |
[32m[20221208 15:14:15 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:14:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.98
[32m[20221208 15:14:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 125.49
[32m[20221208 15:14:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.23
[32m[20221208 15:14:15 @agent_ppo2.py:137][0m Total time:      37.68 min
[32m[20221208 15:14:15 @agent_ppo2.py:139][0m 3061760 total steps have happened
[32m[20221208 15:14:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1495 --------------------------#
[32m[20221208 15:14:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |           0.0586 |           3.1445 |        -118.0864 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |           0.0334 |           2.7326 |         -98.9838 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0019 |           2.6007 |         -95.5287 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0275 |           2.4756 |        -103.1068 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0517 |           2.3621 |        -108.7910 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0591 |           2.2802 |        -111.3388 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0648 |           2.2100 |        -115.7549 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0698 |           2.1575 |        -117.1379 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0741 |           2.1177 |        -119.6736 |
[32m[20221208 15:14:16 @agent_ppo2.py:179][0m |          -0.0769 |           2.0680 |        -121.2162 |
[32m[20221208 15:14:16 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:14:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.31
[32m[20221208 15:14:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.42
[32m[20221208 15:14:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.94
[32m[20221208 15:14:17 @agent_ppo2.py:137][0m Total time:      37.70 min
[32m[20221208 15:14:17 @agent_ppo2.py:139][0m 3063808 total steps have happened
[32m[20221208 15:14:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1496 --------------------------#
[32m[20221208 15:14:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 15:14:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |           0.1120 |           2.1257 |        -121.9528 |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |           0.0456 |           1.5412 |         -98.5369 |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |          -0.0043 |           1.4185 |        -107.7503 |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |          -0.0271 |           1.3486 |        -117.1401 |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |          -0.0414 |           1.2964 |        -123.6581 |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |          -0.0458 |           1.2597 |        -125.5890 |
[32m[20221208 15:14:17 @agent_ppo2.py:179][0m |          -0.0497 |           1.2291 |        -129.6060 |
[32m[20221208 15:14:18 @agent_ppo2.py:179][0m |          -0.0530 |           1.1961 |        -130.8761 |
[32m[20221208 15:14:18 @agent_ppo2.py:179][0m |          -0.0600 |           1.1819 |        -135.9877 |
[32m[20221208 15:14:18 @agent_ppo2.py:179][0m |          -0.0599 |           1.1565 |        -137.2878 |
[32m[20221208 15:14:18 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 15:14:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.13
[32m[20221208 15:14:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.75
[32m[20221208 15:14:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.55
[32m[20221208 15:14:18 @agent_ppo2.py:137][0m Total time:      37.73 min
[32m[20221208 15:14:18 @agent_ppo2.py:139][0m 3065856 total steps have happened
[32m[20221208 15:14:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1497 --------------------------#
[32m[20221208 15:14:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |           0.1076 |           1.7005 |        -124.8408 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |           0.0729 |           1.4767 |        -109.3261 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |           0.0101 |           1.4071 |        -111.3225 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0305 |           1.3535 |        -119.4841 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0527 |           1.3172 |        -124.5199 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0654 |           1.2772 |        -126.5201 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0753 |           1.2624 |        -131.7039 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0842 |           1.2478 |        -136.1296 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0877 |           1.2130 |        -138.7665 |
[32m[20221208 15:14:19 @agent_ppo2.py:179][0m |          -0.0918 |           1.1978 |        -140.4202 |
[32m[20221208 15:14:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.95
[32m[20221208 15:14:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.64
[32m[20221208 15:14:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.26
[32m[20221208 15:14:20 @agent_ppo2.py:137][0m Total time:      37.75 min
[32m[20221208 15:14:20 @agent_ppo2.py:139][0m 3067904 total steps have happened
[32m[20221208 15:14:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1498 --------------------------#
[32m[20221208 15:14:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |           0.0767 |           0.9527 |        -124.1515 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |           0.0318 |           0.8052 |        -110.8868 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |          -0.0057 |           0.7481 |        -117.3016 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |          -0.0322 |           0.7253 |        -124.2761 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |          -0.0463 |           0.7048 |        -130.1080 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |          -0.0542 |           0.6935 |        -135.5250 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |          -0.0619 |           0.6781 |        -139.4061 |
[32m[20221208 15:14:20 @agent_ppo2.py:179][0m |          -0.0641 |           0.6720 |        -143.4345 |
[32m[20221208 15:14:21 @agent_ppo2.py:179][0m |          -0.0674 |           0.6641 |        -145.3619 |
[32m[20221208 15:14:21 @agent_ppo2.py:179][0m |          -0.0706 |           0.6582 |        -148.0971 |
[32m[20221208 15:14:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.32
[32m[20221208 15:14:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 52.77
[32m[20221208 15:14:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.49
[32m[20221208 15:14:21 @agent_ppo2.py:137][0m Total time:      37.78 min
[32m[20221208 15:14:21 @agent_ppo2.py:139][0m 3069952 total steps have happened
[32m[20221208 15:14:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1499 --------------------------#
[32m[20221208 15:14:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 15:14:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |           0.0853 |           4.9083 |        -132.8580 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |           0.0439 |           4.2081 |        -117.0307 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |           0.0005 |           3.9037 |        -118.8014 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0304 |           3.7860 |        -128.4812 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0143 |           3.6966 |        -118.1600 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0441 |           3.5739 |        -127.3726 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0589 |           3.5162 |        -136.6744 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0577 |           3.4822 |        -139.9455 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0662 |           3.4236 |        -143.2617 |
[32m[20221208 15:14:22 @agent_ppo2.py:179][0m |          -0.0604 |           3.3755 |        -147.5858 |
[32m[20221208 15:14:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 15:14:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 113.04
[32m[20221208 15:14:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.32
[32m[20221208 15:14:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.25
[32m[20221208 15:14:23 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 209.75
[32m[20221208 15:14:23 @agent_ppo2.py:137][0m Total time:      37.80 min
[32m[20221208 15:14:23 @agent_ppo2.py:139][0m 3072000 total steps have happened
[32m[20221208 15:14:23 @train.py:58][0m [4m[34mCRITICAL[0m Training completed!
